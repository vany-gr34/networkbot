An Introduction to Computer Networks Release 2.0.11 Peter L Dordal Jul 20, 2023
CONTENTS Preface 3 Second Edition. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 3 Licensing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 4 German Edition. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 5 Classroom Use. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 5 Acknowledgments. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 7 Progress Notes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 8 Technical considerations. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 9 A Note On the Cover. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 10 Recent Changes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 11 1 An Overview of Networks 13 1.1 Layers. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 13 1.2 Data Rate, Throughput and Bandwidth. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 14 1.3 Packets. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 14 1.4 Datagram Forwarding. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 16 1.5 Topology. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 18 1.6 Routing Loops. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 20 1.7 Congestion. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 20 1.8 Packets Again. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 21 1.9 LANs and Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 22 1.10 IP - Internet Protocol. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 24 1.11 DNS. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 30 1.12 Transport. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 31 1.13 Firewalls. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 35 1.14 Some Useful Utilities. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 35 1.15 IETF and OSI. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 38 1.16 Berkeley Unix. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 41 1.17 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 41 1.18 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 41 2 Ethernet Basics 47 2.1 10-Mbps Classic Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 48 2.2 100 Mbps (Fast) Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 59 2.3 Gigabit Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 60 i
2.4 Ethernet Switches. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 61 2.5 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 65 2.6 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 65 3 Advanced Ethernet 69 3.1 Spanning Tree Algorithm and Redundancy. .. .. .. .. .. .. .. .. .. .. .. .. .. 69 3.2 Virtual LAN (VLAN). .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 74 3.3 TRILL and SPB. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 78 3.4 Software-DeÔ¨Åned Networking. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 80 3.5 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 87 3.6 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 87 4 Wireless LANs 93 4.1 Adventures in Radioland. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 93 4.2 Wi-Fi. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 98 4.3 WiMAX and LTE. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 125 4.4 Fixed Wireless. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 128 4.5 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 131 4.6 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 131 5 Other LANs 135 5.1 Virtual Private Networks. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 135 5.2 Carrier Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 136 5.3 Token Ring. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 137 5.4 Virtual Circuits. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 138 5.5 Asynchronous Transfer Mode: ATM. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 142 5.6 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 144 5.7 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 144 6 Links 149 6.1 Encoding and Framing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 149 6.2 Time-Division Multiplexing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 154 6.3 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 159 6.4 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 159 7 Packets 161 7.1 Packet Delay. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 161 7.2 Packet Delay Variability. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 164 7.3 Packet Size. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 165 7.4 Error Detection. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 167 7.5 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 172 7.6 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 173 8 Abstract Sliding Windows 179 8.1 Building Reliable Transport: Stop-and-Wait. .. .. .. .. .. .. .. .. .. .. .. .. . 179 8.2 Sliding Windows. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 184 8.3 Linear Bottlenecks. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 187 8.4 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 195 8.5 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 195 ii
9 IP version 4 201 9.1 The IPv4 Header. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 202 9.2 Interfaces. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 204 9.3 Special Addresses. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 206 9.4 Fragmentation. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 208 9.5 The Classless IP Delivery Algorithm. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 210 9.6 IPv4 Subnets. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 213 9.7 Network Address Translation. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 219 9.8 Unnumbered Interfaces. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 223 9.9 Mobile IP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 224 9.10 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 226 9.11 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 226 10 IPv4 Companion Protocols 229 10.1 DNS. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 229 10.2 Address Resolution Protocol: ARP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 242 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP). .. .. .. .. .. .. .. .. .. .. .. . 246 10.4 Internet Control Message Protocol. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 247 10.5 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 251 10.6 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 251 11 IPv6 253 11.1 The IPv6 Header. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 254 11.2 IPv6 Addresses. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 255 11.3 Network PreÔ¨Åxes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 258 11.4 IPv6 Multicast. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 259 11.5 IPv6 Extension Headers. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 259 11.6 Neighbor Discovery. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 262 11.7 IPv6 Host Address Assignment. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 266 11.8 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 271 11.9 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 271 12 IPv6 Additional Features 273 12.1 Globally Exposed Addresses. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 273 12.2 ICMPv6. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 273 12.3 IPv6 Subnets. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 275 12.4 Using IPv6 and IPv4 Together. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 276 12.5 IPv6 Examples Without a Router. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 280 12.6 IPv6 Connectivity via Tunneling. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 283 12.7 IPv6-to-IPv4 Connectivity. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 286 12.8 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 289 12.9 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 289 13 Routing-Update Algorithms 291 13.1 Distance-Vector Routing-Update Algorithm. .. .. .. .. .. .. .. .. .. .. .. .. . 292 13.2 Distance-Vector Slow-Convergence Problem. .. .. .. .. .. .. .. .. .. .. .. .. 296 13.3 Observations on Minimizing Route Cost. .. .. .. .. .. .. .. .. .. .. .. .. .. . 298 13.4 Loop-Free Distance Vector Algorithms. .. .. .. .. .. .. .. .. .. .. .. .. .. . 300 iii
13.5 Link-State Routing-Update Algorithm. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 306 13.6 Routing on Other Attributes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 310 13.7 ECMP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 312 13.8 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 313 13.9 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 313 14 Large-Scale IP Routing 319 14.1 Classless Internet Domain Routing: CIDR. .. .. .. .. .. .. .. .. .. .. .. .. .. 319 14.2 Hierarchical Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 321 14.3 Legacy Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 322 14.4 Provider-Based Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 323 14.5 Geographical Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 328 14.6 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 329 14.7 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 329 15 Border Gateway Protocol (BGP) 333 15.1 AS-paths. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 334 15.2 AS-Paths and Route Aggregation. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 336 15.3 Transit TrafÔ¨Åc. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 337 15.4 BGP Filtering and Routing Policies. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 337 15.5 BGP Table Size. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 339 15.6 BGP Path attributes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 340 15.7 BGP and TrafÔ¨Åc Engineering. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 344 15.8 BGP and Anycast. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 347 15.9 BGP for Interior Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 347 15.10 BGP Relationships. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 348 15.11 Examples of BGP Instability. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 352 15.12 BGP Security and Route Registries. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 354 15.13 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 357 15.14 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 358 16 UDP Transport 361 16.1 User Datagram Protocol ‚Äì UDP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 361 16.2 Trivial File Transport Protocol, TFTP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 373 16.3 Fundamental Transport Issues. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 375 16.4 Other TFTP notes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 380 16.5 Remote Procedure Call (RPC). .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 383 16.6 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 388 16.7 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 388 17 TCP Transport Basics 393 17.1 The End-to-End Principle. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 394 17.2 TCP Header. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 394 17.3 TCP Connection Establishment. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 396 17.4 TCP and WireShark. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 400 17.5 TCP OfÔ¨Çoading. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 402 17.6 TCP simplex-talk. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 402 17.7 TCP and bind(). .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 407 iv
17.8 TCP state diagram. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 408 17.9 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 414 17.10 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 414 18 TCP Issues and Alternatives 419 18.1 TCP Old Duplicates. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 419 18.2 TIMEWAIT. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 420 18.3 The Three-Way Handshake Revisited. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 421 18.4 Anomalous TCP scenarios. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 423 18.5 TCP Faster Opening. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 424 18.6 Path MTU Discovery. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 425 18.7 TCP Sliding Windows. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 426 18.8 TCP Delayed ACKs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 426 18.9 Nagle Algorithm. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 427 18.10 TCP Flow Control. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 428 18.11 Silly Window Syndrome. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 428 18.12 TCP Timeout and Retransmission. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 429 18.13 KeepAlive. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 430 18.14 TCP timers. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 431 18.15 Variants and Alternatives. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 431 18.16 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 441 18.17 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 441 19 TCP Reno and Congestion Management 443 19.1 Basics of TCP Congestion Management. .. .. .. .. .. .. .. .. .. .. .. .. .. . 444 19.2 Slow Start. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 448 19.3 TCP Tahoe and Fast Retransmit. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 453 19.4 TCP Reno and Fast Recovery. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 454 19.5 TCP NewReno. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 458 19.6 Selective Acknowledgments (SACK). .. .. .. .. .. .. .. .. .. .. .. .. .. .. 460 19.7 TCP and Bottleneck Link Utilization. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 461 19.8 Single Packet Losses. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 464 19.9 TCP Assumptions and Scalability. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 465 19.10 TCP Parameters. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 466 19.11 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 466 19.12 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 467 20 Dynamics of TCP 471 20.1 A First Look At Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 471 20.2 Bottleneck Links with Competition. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 472 20.3 TCP Reno Fairness with Synchronized Losses. .. .. .. .. .. .. .. .. .. .. .. . 480 20.4 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 487 20.5 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 487 21 Further Dynamics of TCP 493 21.1 Notions of Fairness. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 493 21.2 TCP Reno loss rate versus cwnd. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 495 21.3 TCP Friendliness. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 497 v
21.4 AIMD Revisited. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 499 21.5 Active Queue Management. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 501 21.6 The High-Bandwidth TCP Problem. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 506 21.7 The Lossy-Link TCP Problem. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 508 21.8 The Satellite-Link TCP Problem. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 508 21.9 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 509 21.10 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 509 22 Newer TCP Implementations 515 22.1 Choosing a TCP on Linux. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 515 22.2 High-Bandwidth Desiderata. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 518 22.3 RTTs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 519 22.4 A Roadmap. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 519 22.5 Highspeed TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 519 22.6 TCP Vegas. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 522 22.7 FAST TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 525 22.8 TCP Westwood. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 527 22.9 TCP Illinois. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 529 22.10 Compound TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 530 22.11 TCP Veno. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 532 22.12 TCP Hybla. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 533 22.13 DCTCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 533 22.14 H-TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 536 22.15 TCP CUBIC. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 537 22.16 TCP BBR. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 542 22.17 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 546 22.18 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 547 23 Queuing and Scheduling 553 23.1 Queuing and Real-Time TrafÔ¨Åc. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 554 23.2 TrafÔ¨Åc Management. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 554 23.3 Priority Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 555 23.4 Queuing Disciplines. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 555 23.5 Fair Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 556 23.6 Applications of Fair Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 569 23.7 Hierarchical Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 571 23.8 Hierarchical Weighted Fair Queuing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 574 23.9 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 580 23.10 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 580 24 Token Bucket Rate Limiting 585 24.1 Token Bucket DeÔ¨Ånition. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 586 24.2 Token-Bucket Examples. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 588 24.3 Multiple Token Buckets. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 589 24.4 GCRA. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 590 24.5 Guaranteeing V oIP Bandwidth. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 591 24.6 Limiting Delay. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 592 24.7 Token Bucket Through One Router. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 593 vi
24.8 Token Bucket Through Multiple Routers. .. .. .. .. .. .. .. .. .. .. .. .. .. 594 24.9 Delay Constraints. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 594 24.10 CBQ. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 597 24.11 Linux HTB. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 597 24.12 Parekh-Gallager Theorem. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 598 25 Quality of Service 603 25.1 Net Neutrality. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 604 25.2 Where the Wild Queues Are. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 604 25.3 Real-time TrafÔ¨Åc. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 605 25.4 Integrated Services / RSVP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 607 25.5 Global IP Multicast. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 608 25.6 RSVP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 613 25.7 Differentiated Services. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 617 25.8 RED with In and Out. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 621 25.9 NSIS. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 622 25.10 Comcast Congestion-Management System. .. .. .. .. .. .. .. .. .. .. .. .. . 622 25.11 Real-time Transport Protocol (RTP). .. .. .. .. .. .. .. .. .. .. .. .. .. .. 623 25.12 Multi-Protocol Label Switching (MPLS). .. .. .. .. .. .. .. .. .. .. .. .. .. 628 25.13 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 630 25.14 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 630 26 Network Management and SNMP 633 26.1 Network Architecture. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 635 26.2 SNMP Basics. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 635 26.3 SNMP Naming and OIDs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 637 26.4 MIBs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 639 26.5 SNMPv1 Data Types. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 640 26.6 ASN.1 Syntax and SNMP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 640 26.7 SNMP Tables. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 642 26.8 SNMP Operations. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 646 26.9 MIB Browsing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 651 26.10 MIB-2. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 652 26.11 SNMPv1 communities and security. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 661 26.12 SNMP and ASN.1 Encoding. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 662 26.13 Network Management Systems. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 665 26.14 SNMP Alternatives. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 665 26.15 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 666 27 SNMP versions 2 and 3 669 27.1 SNMPv2. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 669 27.2 Table Row Creation. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 680 27.3 SNMPv3. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 689 27.4 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 699 28 Security 701 28.1 Code-Execution Intrusion. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 702 28.2 Stack Buffer OverÔ¨Çow. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 703 vii
28.3 Heap Buffer OverÔ¨Çow. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 712 28.4 Network Intrusion Detection. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 717 28.5 Cryptographic Goals. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 718 28.6 Secure Hashes. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 719 28.7 Shared-Key Encryption. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 724 28.8 DifÔ¨Åe-Hellman-Merkle Exchange. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 733 28.9 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 737 29 Public-Key Encryption 739 29.1 RSA. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 739 29.2 Forward Secrecy. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 742 29.3 Trust and the Man in the Middle. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 743 29.4 End-to-End Encryption. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 744 29.5 SSH and TLS. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 744 29.6 IPsec. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 763 29.7 DNSSEC. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 766 29.8 RSA Key Examples. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 775 29.9 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 778 30 Mininet 781 30.1 Installing Mininet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 782 30.2 A Simple Mininet Example. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 784 30.3 Multiple Switches in a Line. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 786 30.4 IP Routers in a Line. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 789 30.5 IP Routers With Simple Distance-Vector Implementation. .. .. .. .. .. .. .. .. .. 791 30.6 Quagga Routing and BGP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 794 30.7 TCP Competition. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 799 30.8 Linux TrafÔ¨Åc Control (tc). .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 809 30.9 OpenFlow and the POX Controller. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 812 30.10 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 824 31 Network Simulations: ns-2 827 31.1 The ns-2 simulator. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 827 31.2 A Single TCP Sender. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 829 31.3 Two TCP Senders Competing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 841 31.4 TCP Loss Events and Synchronized Losses. .. .. .. .. .. .. .. .. .. .. .. .. . 857 31.5 TCP Reno versus TCP Vegas. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 866 31.6 Wireless Simulation. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 868 31.7 Epilog. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 874 31.8 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 874 32 The ns-3 Network Simulator 877 32.1 Installing and Running ns-3. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 877 32.2 A Single TCP Sender. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 878 32.3 Wireless. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 887 32.4 Exercises. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 893 33 Bibliography 895 viii
Exercise-Numbering Conversion Tables 897 34 Selected Solutions 909 34.1 Solutions for An Overview of Networks. .. .. .. .. .. .. .. .. .. .. .. .. .. . 909 34.2 Solutions for Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 910 34.3 Solutions for Advanced Ethernet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 910 34.4 Solutions for Wireless LANs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 911 34.5 Solutions for Other LANs. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 912 34.6 Solutions for Links. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 912 34.7 Solutions for Packets. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 913 34.8 Solutions for Sliding Windows. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 915 34.9 Solutions for IPv4. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 916 34.10 Solutions for Routing-Update Algorithms. .. .. .. .. .. .. .. .. .. .. .. .. . 917 34.11 Solutions for Large-Scale IP Routing. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 917 34.12 Solutions for Border Gateway Protocol. .. .. .. .. .. .. .. .. .. .. .. .. .. . 918 34.13 Solutions for UDP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 918 34.14 Solutions for TCP Reno and Congestion Management. .. .. .. .. .. .. .. .. .. . 919 34.15 Solutions for Dynamics of TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 919 34.16 Solutions for Dynamics of TCP. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . 920 34.17 Solutions for Queuing and Scheduling. .. .. .. .. .. .. .. .. .. .. .. .. .. . 921 34.18 Solutions for Mininet. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 922 Indices and tables 923 Bibliography 925 Index 935 ix
x
An Introduction to Computer Networks, Release 2.0.11 Peter L Dordal Department of Computer Science Loyola University Chicago Contents: CONTENTS 1
An Introduction to Computer Networks, Release 2.0.11 2 CONTENTS
PREFACE ‚ÄúNo man but a blockhead ever wrote, except for money.‚Äù - Samuel Johnson The textbook world is changing. On the one hand, open source software and creative-commons licensing have been great successes; on the other hand, unauthorized PDFs of popular textbooks are widely available, and it is time to consider Ô¨Çowing with rather than Ô¨Åghting the tide. Hence this open-access textbook, released for free under the Creative Commons license described below. Mene, mene, tekel pharsin. Perhaps the last straw, for me, was patent 8195571 for a roundabout method to force students to purchase textbooks. (A simpler strategy might be to include the price of the book in the course.) At some point, faculty have to be advocates for their students rather than, well, Hirudinea. This is not to say that I have anything against for-proÔ¨Åt publishing. It is just that this particular book does not ‚Äì and will not ‚Äì belong to that category; the online edition will always be free. In this it is in good company: there is Wikipedia, there is a steadily increasing number of other open online textbooks out there, and there is the entire open-source world. Although the open-source-software and open-textbook models are not completely parallel, they are similar enough. The market inefÔ¨Åciencies of traditional publishing are sobering: the return to authors of advanced textbooks is usually modest, while the lost-opportunity costs to users due to lack of access can be quite substantial. (None of this is meant to imply there will never be a print edition; when I started this project it seemed inconceivable that a print publisher would ever agree to having the online edition remain free, but times are changing.) The book is updated multiple times per year ( Recent Changes ). This is another feature that paper publishing can‚Äôt touch, though the rate has slowed down in recent years. The ofÔ¨Åcial book website is intronetworks.cs.luc.edu. The book is available there as online html, as a zipped archive of html Ô¨Åles, in .pdf format, and in other formats as may prove useful. Note that there are three html variants: the original, a version using a more universally available set of unicode characters, and a version better suited to smaller screen. In an ideal world, my Javascript would Ô¨Ågure out which version to serve to your browser automatically; in our real world, it took me three years to get the html quick-search facility to work again after I broke it with the collapsible sidebar. See Technical considerations below for more information. Second Edition The second edition is Ô¨Ånally here! Mainly this involved breaking up the longer chapters, partly to make scrolling less fussy (at least on some devices), partly to restore the sense of accomplishment at Ô¨Ånishing a chapter, and partly so the collabsible contents sidebar Ô¨Åts in the viewport of most devices, making sidebar scrolling unnecessary. Division of the overlong chapters in many cases followed a logical divide: thus the IPv4 chapter became the basic IPv4 chapter and the IPv4 companion protocols, the Ethernet chapter became a chapter on basic Ethernet and a chapter on more advanced features, and the security chapter divided naturally into public-key encryption for the second half, and everything else in the Ô¨Årst half. 3
An Introduction to Computer Networks, Release 2.0.11 In other cases, the division was less natural; the IPv6 chapters comes to mind here as the companion protocols Neighbor Discovery, DHCPv6 and SLAAC ended up in the same chapter as the IPv6 core protocol. Throughout the eight-year lifetime of the Ô¨Årst edition, I never changed any exercise numbers; new exercises that needed to be grouped with older exercises were always inserted with Ô¨Çoating-point numbering, eg4.7. For a book that might get updated in the middle of the semester, that is essential. However, the second edition involved splitting chapters and dividing the exercises appropriately, and I realized that the time for renumbering was at hand. To make life easier for instructors (including me) who have assignments based on the old numbering, there is a handy conversion table here: Exercise-Numbering Conversion Tables. After much demand, I have Ô¨Ånally accepted that I need to create a solutions manual for instructors, covering at least a majority of the exercises. It is in progress; instructors should contact me if interested. The second edition html version became available July 31, 2020. Licensing This text is released under the Creative Commons license Attribution-NonCommercial-NoDerivs. This text is like a conventional book, in other words, except that it is free. You may copy the work and distribute it to others for any noncommercial use (and for some commercial uses; see below), but all reuse requires attribution. The Creative Commons license does not precisely spell out what constitutes ‚Äúnoncommercial‚Äù use. The author considers any sale of printed copies of this book, even by a non-proÔ¨Åt organization and even if the price just covers expenses, to be commercial use. Personal printing, and free distribution of printed selections, do qualify as noncommercial. Starting with Edition 1.9.16, commercial use is also explicitly allowed, provided that printed copies are not distributed. In other words, the text is also released under the terms of the Creative Commons license Attribution-NoDerivs, amended to include a prohibition on the distribution of printed copies. The Creative Commons license summary linked to here states that ‚Äúyou are free to share ‚Äì copy and redistribute the material in any medium or format for any purpose, even commercially‚Äù; this license is amended by this paragraph to limit ‚Äúany medium‚Äù to non-printed media. Permissible commercial uses may include, but are not limited to, use in internal training programs, use in for-proÔ¨Åt training and educational programs, sale of the work in the electronic formats available here, and installation throughout the Amazon EC2. The Attribution clause of the Creative Commons licenses [Section 4, (b) for by-nd or (c) for by-nc-nd] requires that redistributors of the work provide ‚Äú.. . (iii) to the extent reasonably practicable, the URI, if any, that Licensor speciÔ¨Åes to be associated with the Work‚Äù. That URI is intronetworks.cs.luc.edu. Under the Creative Commons licenses, creation of derivative works requires permission. It is not entirely clear, however, what would be considered a derivative work, beyond the traditional examples of abridgment and translation. Any supplemental materials like exams, labs, slides or coverage of additional topics would be new, independent works, and would require no permission. Even the inclusion in such supplements of modest amounts of material from this book would have a strong claim to Fair Use. In the open-source software world, the right to make derivative works is exercised whenever the software is modiÔ¨Åed, but it is hard to see how this applies to textbook supplements. The bottom line is that if you have a situation you‚Äôre concerned about in this regard, let me know and I‚Äôll probably be happy to grant permission. 4 Preface
An Introduction to Computer Networks, Release 2.0.11 Some of the chapters contain source code; this is licensed under the Apache 2.0 license. Some code Ô¨Åles (those which are derivative works) contain an ofÔ¨Åcial Apache license statement; others do not. German Edition I am delighted to announce that Rheinwerk Verlag has undertaken the translation of the book into German, with technical review by Dr Matthias W√ºbbeling of the University of Bonn, and has now published it as a commercial print edition. The book is available at www.rheinwerk-verlag.de/rechnernetze-das-umfassendelehrbuch. Classroom Use This book is meant as a serious and more-or-less thorough text for an introductory college or graduate course in computer networks, carefully researched, with consistent notation and style, and complete with diagrams and exercises. I have also tried to rethink the explanations of many protocols and algorithms, with the goal of making them easier to understand. My intent is to create a text that covers to a reasonable extent whythe Internet is the way it is, to avoid the endless dreary focus on TLA‚Äôs (Three-Letter Acronyms), and to remain nottoomathematical. For the last, I have avoided calculus, linear algebra, and, for that matter, quadratic terms (though some inequalities do sneak in at times). That said, the book includes a large number of backof-the-envelope calculations ‚Äì in settings as concrete as I could make them ‚Äì illustrating various networking concepts. Overall, I tried to Ô¨Ånd a happy medium between practical matters and underlying principles. My goal has been to create a book that is useful to a broad audience, including those interested in network management, in high-performance networking, in software development, or just in how the Internet is put together. One of the best ways to gain insight into why a certain design choice was made is to look at a few alternative implementations. To that end, this book includes coverage of some topics one may never encounter in practice, but which may be useful as points of comparison. These topics arguably include ATM ( 5.5 Asynchronous Transfer Mode: ATM ), SCTP ( 18.15.2 SCTP ) and even 10 Mbps Ethernet ( 2.1 10-Mbps Classic Ethernet ). The book can also be used as a networks supplement or companion to other resources for a variety of other courses that overlap to some greater or lesser degree with networking. At Loyola, this book has been used ‚Äì sometimes coupled with a second textbook ‚Äì in courses in computer security, network management, telecommunications, and even introduction-to-computing courses for non-majors. Another possibility is an alternative or nontraditional presentation of networking itself. It is when used in concert with other works, in particular, that this book‚Äôs being free is of marked advantage. Finally, I hope the book may also be useful as a reference work. To this end, I have attempted to ensure that the indexing and cross-referencing is sufÔ¨Åcient to support the drop-in reader. Similarly, obscure or specialized notation is kept to a minimum. Much is sometimes made, in the world of networking textbooks, about top-down versus bottom-up sequencing. This book is not really either, although the chapters are mostly numbered in bottom-up fashion. Instead, the Ô¨Årst chapter provides a relatively complete overview of the LAN, IP and transport network layers (along with a few other things), allowing subsequent chapters to refer to all network layers without forward German Edition 5
An Introduction to Computer Networks, Release 2.0.11 reference, and, more importantly, allowing the chapters to be covered in a variety of different orders. As a practical matter, when I use this text to teach Loyola‚Äôs Introduction to Computer Networks course, I cover the IP/routing and TCP material more or less in parallel. A distinctive feature of the book is the extensive coverage of TCP: TCP dynamics, newer versions of TCP such as TCP Cubic and BBR TCP, and chapters on using the ns-2 and ns-3 simulators and the Mininet emulator. This has its roots in a longstanding goal to Ô¨Ånd better ways to present competition and congestion in the classroom. Another feature is the detailed chapter on queuing disciplines. One thing this book makes little attempt to cover in detail is the application layer; the token example included is SNMP. While SNMP actually makes a pretty good example of a self-contained application, my recommendation to instructors who wish to cover more familiar examples is to combine this text with the appropriate application documentation. Although the book is continuously updated, I try very hard to ensure that all editions are classroomcompatible. To this end, section renumbering is avoided to the extent practical, and, except between different editions, existing exercises are never renumbered. New exercises are regularly inserted, but with fractional (Ô¨Çoating point) numbers. Existing integral exercise numbers have been given a trailing .0, to reduce confusion between exercise 12.0, say, and 12.5. For those interested in using the book for a ‚Äútraditional‚Äù networks course, I with some trepidation offer the following set of core material. In solidarity with those who prefer alternatives to a bottom-up ordering, I emphasize that this represents a setand not a sequence. - 1 An Overview of Networks 
- Selected sections from 2 Ethernet Basics, particularly switched Ethernet 
- Selected sections from 4.2 Wi-Fi 
- Selected sections from 7 Packets 
- 8 Abstract Sliding Windows 
- 9 IP version 4 and/or 11 IPv6 
- Selected sections from 13 Routing-Update Algorithms, probably including the distance-vector algorithm 
- Selected sections from 14 Large-Scale IP Routing 
- 16 UDP Transport 
- 17 TCP Transport Basics 
- 19 TCP Reno and Congestion Management With some care in the topic-selection details, the above can be covered in one semester along with a survey of selected important network applications, or the basics of network programming, or the introductory conÔ¨Åguration of switches and routers, or coverage of additional material from this book, or some other set of additional topics. Of course, non-traditional networks courses may focus on a quite different sets of topics. Instructors who adopt this book in a course, as either a primary or a secondary text, are strongly encouraged to let me know, as this helps support continued work on the book. Below is a list of the institutions I‚Äôm aware of so far where the book has been adopted. Commercial publishers get this information from sales records, but that won‚Äôt work here; if you want to see your institution listed, contact me! 6 Preface
An Introduction to Computer Networks, Release 2.0.11 Augustana College California State University, Fresno Eastern Washington University Edith Cowan University, Australia Kennesaw State University The King‚Äôs University, Edmonton, Canada Loyola University Maryland Middle Tennessee State University Murray State University Muskingum University Ohio University Saint Martin‚Äôs University Seattle PaciÔ¨Åc University SUNY Delhi University of Arkansas Community College at Batesville University of California, Santa Cruz University of Luxembourg University of Maryland, University College University of New Hampshire at Manchester University of North Alabama University of Texas at El Paso University of the People Villanova University Wellington Institute of Technology Acknowledgments I would like to thank the many Loyola students who have provided invaluable feedback on the text and the exercises. The result, I hope, is greater clarity for both. I would also like to thank the following people from outside Loyola who have contributed technical or editorial comments. I‚Äôve included institutional afÔ¨Åliation if I could Ô¨Ågure it out. If I‚Äôve missed anyone, or their institution, please let me know. Acknowledgments 7
An Introduction to Computer Networks, Release 2.0.11 Anonymous Jose Alvarado University of the People Jim Davis Ben Erickson Eric Freudenthal University of Texas at El Paso David GarÔ¨Åeld Jeff Harrang Emmanuel Lochin Institut sup√©rieur de l‚Äôa√©ronautique et de l‚Äôespace Hamir Mahal Robert Michael Alisa Neeman Muskingum University Fred Nerk Natale Patriciello Centre Tecnol√≤gic Telecomunicacions Catalunya Donald Privitera Kennesaw State University Yves Rene Shema British Columbia Institute of Technology Charles Stimler Herman Torjussen J Wiedemann-Heinzelmann Alexander Wijesinha Towson University Mattias W√ºbbeling University of Bonn Justin Yang Comments ‚Äì from anyone ‚Äì on clarity, completeness, consistency and correctness are much appreciated. Even single comments or corrections are very welcome, though I continue to seek reviewers willing to review an entire section or chapter. I can be contacted at pld AT cs.luc.edu, or via the book comment form. Peter Dordal Shabbona, Illinois Progress Notes This work was started in the summer of 2012. Edition 1.0 was declared complete as of March 2014, and Edition 2.0 (mostly reorganization) in July 2020. The current edition is 2.0.11. The intronetworks.cs.luc.edu website carries the current 2.x edition (recommended), and also editions 1.0 and the Ô¨Ånal 1.9.21 edition. 8 Preface
An Introduction to Computer Networks, Release 2.0.11 Technical considerations The book was prepared in reStructuredText using the Linux Sphinx package, which can produce multiple formats from the same source. That said, the primary format is html. The table-of-contents sidebar and the text sidebars work best there. Most of the diagrams were drawn using LibreOfÔ¨Åce Draw. The html version also provides a ‚ÄúQuick search‚Äù box, which, with the aid of Javascript‚Äôs stopPropagation() method, Ô¨Ånally coexists with the collapsible sidebar. Quick search, however, only works for all-alphabetic strings; strings with hyphens such as ‚Äúwi-Ô¨Å‚Äù and ‚ÄúDifÔ¨Åe-Hellman‚Äù fail. The index is an effective alternative. The book uses a modest set of unicode special characters. Unfortunately, some of these characters are not universally available in all browsers (and I have not yet Ô¨Ågured out how to encapsulate fonts in the html). The comma-separated characters in the Ô¨Årst line, below, appear to have the most limited support. The math-italic Greek letters are not present in the so-called Unicode Basic Multilingual Plane, but the symbols are, so that is not the entire explanation. x,y,,ùõº,ùõΩ,ùõæ,ùúÜ,ùúë,ùúè,ùúö,,",√ù√ë,√ê√ù,√ê√ë (,),,ùõº,ùõΩ,ùõæ,ùúÜ,ùúë,ùúè,ùúå,,^,√ë,√ê,√ê√ë ¬µ,?,8,¬§,¬•,,,,‚Äì,,√ë,√ê,,,,,,,,,,, The characters above should look roughly as they do in the following image (the Ô¨Årst line is the one most likely to fail): If they do not, there are two options for browser-based viewing. If the second and third rows above display successfully, there is a unicode-safer version of the book (both online and zipped) available at intronetworks.cs.luc.edu that has the characters in the Ô¨Årst row above replaced by those in the second row. The other alternative is to add an appropriate font. Generally Firefox and Internet Explorer display the necessary characters out of the box, but Chrome may not. The Chrome situation can usually be Ô¨Åxed, at least on ‚Äúreal‚Äù computers, by adding a font and then tweaking the Chrome font settings. I have had good luck with Symbola (at shapecatcher.com/unicodefonts.html and other places). To install the font, extract the .ttf Ô¨Åle and double-click on it. Then to adjust Chrome, go to Settings √ëShow advanced settings √ë Customize fonts (button), and change at a minimum the default Sans-serif font to Symbola. Then restart Chrome. Unfortunately, adding fonts to (non-rooted) Android devices continues to be very difÔ¨Åcult. Worse, Android often fails to display even a box symbol ‚Äú l‚Äù in the place of missing characters. If no available browser properly displays the symbols above, I recommend the pdf format. The unicode-safer version, however, should work on most systems. At some point I hope to Ô¨Ågure out how to handle this font situation a little better using Javascript. This turns out, however, not to be straightforward, and progress has been slow. Technical considerations 9
An Introduction to Computer Networks, Release 2.0.11 I could have gone with TeX and MathJax, but then I likely would have gotten rather too carried away with mathematical formulas. The character set above keeps things (relatively) simple. As of 2020 MathJax renders better than it used to, but you still cannot copy and paste. The diagrams in the body of the text have now all been migrated to the vector-graphics .svg format, although a few diagrams rendered with line-drawing characters appear in the exercises. Most browsers now (2018) appear to support zooming in on .svg images, which is a signiÔ¨Åcant step forward. A Note On the Cover The photo is of mahogany leaves, presumably Swietenia mahagoni. The original image was taken by Homer Edward Price and placed at https://commons.wikimedia.org/wiki/File:Mahogany-leaves_(5606894105).gif under a Creative Commons license; the image as used here has been cropped. I began with the idea that the cover should depict some networking reference from the natural world. The connection between mahogany and networking comes from Bertolt Brecht‚Äôs work The City of Mahagonny, ‚Äúthe city of nets‚Äù. Ok, ‚Äúnets‚Äù in the sense of traps rather than communication, but close enough. 10 Preface
An Introduction to Computer Networks, Release 2.0.11 Alas, this turned out to be based on a misapprehension. As musicologist John Simon puts it [JS05], Where did Brecht get the name for that lawless city that was his symbol for a capitalist society in distress? In coining the name Mahagonny, the opera‚Äôs Leokadja Begbick explains it as ‚Äúthe City of Nets‚Äù, i.e. traps. But the word ‚Äúmahogany‚Äù, from which the name must stem, has nothing to do with nets. It remains unclear just what ‚ÄúMahagonny‚Äù did mean to Brecht. After learning this, a picture of mahogany seemed to be out. But, as the book progressed, with more and more reading of papers and RFCs, I began to see the non-connection here as a symbol of diligent factchecking. So there it is. And besides, it‚Äôs green. Recent Changes July 20, 2023 (ver 2.0.11): changes to 19.4 TCP Reno and Fast Recovery ,11.6.3 Neighbor Solicitation, 3.1.2 Spanning Tree Example 2: Switches and Segments ,13.1.1 Distance-Vector Update Rules, and several typos. There is also new material in 22.15 TCP CUBIC December 30, 2022 (ver 2.0.10): miscellaneous changes; German Edition ,12.7.1 IPv6-to-IPv6 Connectivity. August 20, 2022 (ver 2.0.9): Greatly expanded discussion of the use of Mininet for TCP competitions (30.7 TCP Competition ); other miscellaneous changes. June 5, 2022 (ver 2.0.8): Homa ( 16.5.6 Homa ), gNMI ( 26.14 SNMP Alternatives ), beginning of transition to numbered Ô¨Ågures (and captions that are then part of the text rather than embedded in the image). Currently this is done through ; there are still issues with the Ô¨Ågure numbering. February 4, 2022 (ver 2.0.7): Updates related to BGP, DNS and bind() November 1, 2021 (ver 2.0.6): Minor Ô¨Åxes including to 10.1 DNS September 3, 2021 (ver 2.0.5): Added Mininet BGP example; other minor additions January 3, 2021 (ver 2.0.4): Minor Ô¨Åxes to the text and to exercises. October 31, 2020 (ver 2.0.3): DNS Ô¨Åxes and new material including 10.1.2.1 Query Name Minimization and30.2.2 Mininet WireShark Demos; clariÔ¨Åcations in 4.2.1.1 Link-Layer ACKs; miscellaneous other changes. September 1, 2020 (ver 2.0.2): more Ô¨Åxed exercise numbers (mostly exercise references to other exercises), new discussion of checksums at 16.1.3.2 UDP and IP addresses and17.5 TCP OfÔ¨Çoading. August 19, 2020 (ver 2.0.1): some Ô¨Åxed exercise numbers. July 31, 2020 (ver 2.0.0.0.0): start of second edition Recent Changes 11
An Introduction to Computer Networks, Release 2.0.11 12 Preface
1 AN OVERVIEW OF NETWORKS Somewhere there might be a Ô¨Åeld of interest in which the order of presentation of topics is well agreed upon. Computer networking is not it. There are many interconnections in the Ô¨Åeld of networking, as in most technical Ô¨Åelds, and it is difÔ¨Åcult to Ô¨Ånd an order of presentation that does not involve endless ‚Äúforward references‚Äù to future chapters; this is true even if ‚Äì as is done here ‚Äì a largely bottom-up ordering is followed. I have therefore taken here a different approach: this Ô¨Årst chapter is a summary of the essentials ‚Äì LANs, IP and TCP ‚Äì across the board, and later chapters expand on the material here. Local Area Networks, or LANs, are the ‚Äúphysical‚Äù networks that provide the connection between machines within, say, a home, school or corporation. LANs are, as the name says, ‚Äúlocal‚Äù; it is the IP, or Internet Protocol, layer that provides an abstraction for connecting multiple LANs into, well, the Internet. Finally, TCP deals with transport and connections and actually sending user data. This chapter also contains some important other material. The section on datagram forwarding, central to packet-based switching and routing, is essential. This chapter also discusses packets generally, congestion, and sliding windows, but those topics are revisited in later chapters. Firewalls and network address translation are also covered here and not elsewhere. 1.1 Layers These three topics ‚Äì LANs, IP and TCP ‚Äì are often called layers; they constitute the Link layer, the Internetwork layer, and the Transport layer respectively. Together with the Application layer (the software you use), these form the ‚Äú four-layer model ‚Äù for networks. A layer, in this context, corresponds strongly to the idea of a programming interface or library, with the understanding that a given layer communicates directly only with the two layers immediately above and below it. An application hands off a chunk of data to the TCP library, which in turn makes calls to the IP library, which in turn calls the LAN layer for actual delivery. An application does notinteract directly with the IP and LAN layers at all. The LAN layer is in charge of actual delivery of packets, using LAN-layer-supplied addresses. It is often conceptually subdivided into the ‚Äúphysical layer‚Äù dealing with, eg, the analog electrical, optical or radio signaling mechanisms involved, and above that an abstracted ‚Äúlogical‚Äù LAN layer that describes all the digital ‚Äì that is, non-analog ‚Äì operations on packets; see 2.1.4 The LAN Layer. The physical layer is generally of direct concern only to those designing LAN hardware; the kernel software interface to the LAN corresponds to the logical LAN layer. This LAN physical/logical division gives us the Internet Ô¨Åve-layer model. This is less a formal hierarchy than an ad hoc classiÔ¨Åcation method. We will return to this below in 1.15 IETF and OSI, where we will also introduce two more rather obscure layers that complete the seven -layer model. 13
An Introduction to Computer Networks, Release 2.0.11 Application Transport IP Logical LAN Physical LAN Fig. 1:: Five-layer network model 1.2 Data Rate, Throughput and Bandwidth Any one network connection ‚Äì egat the LAN layer ‚Äì has a data rate: the rate at which bits are transmitted. In some LANs ( egWi-Fi) the data rate can vary with time. Throughput refers to the overall effective transmission rate, taking into account things like transmission overhead, protocol inefÔ¨Åciencies and perhaps even competing trafÔ¨Åc. It is generally measured at a higher network layer than the data rate. The term bandwidth can be used to refer to either of these, though we here use it mostly as a synonym for data rate. The term comes from radio transmission, where the width of the frequency band available is proportional, all else being equal, to the data rate that can be achieved. In discussions about TCP, the term goodput is sometimes used to refer to what might also be called ‚Äúapplication-layer throughput‚Äù: the amount of usable data delivered to the receiving application. Specifically, retransmitted data is counted only once when calculating goodput but might be counted twice under some interpretations of ‚Äúthroughput‚Äù. Data rates are generally measured in kilobits per second (kbps) or megabits per second (Mbps); the use of the lower-case ‚Äúb‚Äù here denotes bits. In the context of data rates, a kilobit is 103bits (not 210) and a megabit is 106bits. Somewhat inconsistently, we follow the tradition of using kB and MB to denote data volumes of 210and 220bytes respectively, with the upper-case B denoting bytes. The newer abbreviations KiB and MiB would be more precise, but the consequences of confusion are modest. 1.3 Packets Packets are modest-sized sequences of bytes, transmitted as a unit through some shared set of links. Of necessity, packets need to be preÔ¨Åxed with a header containing delivery information. In the common case known as datagram forwarding, the header contains a destination address; headers in networks using socalled virtual-circuit forwarding contain instead an identiÔ¨Åer for the connection. Almost all networking today (and for the past 50 years) is packet-based, although we will later look brieÔ¨Çy at some ‚Äúcircuitswitched‚Äù options for voice telephony. At the LAN layer, packets can be viewed as the imposition of a buffer (and addressing) structure on top of low-level serial lines; additional layers then impose additional structure. Informally, packets are often referred to as frames at the LAN layer, and as segments at the Transport layer. The maximum packet size supported by a given LAN ( egEthernet, Token Ring or ATM) is an intrinsic attribute of that LAN. Ethernet allows a maximum of 1500 bytes of data. By comparison, TCP/IP packets originally often held only 512 bytes of data, while early Token Ring packets could contain up to 4 kB of 14 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 data dataheader header1header2 Fig. 2:: Packets with headers data. While there are proponents of very large packet sizes, larger even than 64 kB, at the other extreme the ATM (Asynchronous Transfer Mode) protocol uses 48 bytes of data per packet, and there are good reasons for believing in modest packet sizes. One potential issue is how to forward packets from a large-packet LAN to (or through) a small-packet LAN; in later chapters we will look at how the IP (or Internet Protocol) layer addresses this. Generally each layer adds its own header. Ethernet headers are typically 14 bytes, IP headers 20 bytes, and TCP headers 20 bytes. If a TCP connection sends 512 bytes of data per packet, then the headers amount to 10% of the total, a not-unreasonable overhead. For one common V oice-over-IP option, packets contain 160 bytes of data and 54 bytes of headers, making the header about 25% of the total. Compressing the 160 bytes of audio, however, may bring the data portion down to 20 bytes, meaning that the headers are now 73% of the total; see 25.11.4 RTP and VoIP. In datagram-forwarding networks the appropriate header will contain the address of the destination and perhaps other delivery information. Internal nodes of the network called routers orswitches will then try to ensure that the packet is delivered to the requested destination. The concept of packets and packet switching was Ô¨Årst introduced by Paul Baran in 1962 ([PB62]). Baran‚Äôs primary concern was with network survivability in the event of node failure; existing centrally switched protocols were vulnerable to central failure. In 1964, Donald Davies independently developed many of the same concepts; it was Davies who coined the term ‚Äúpacket‚Äù. It is perhaps worth noting that packets are buffers built of 8-bit bytes, and all hardware today agrees what a byte is (hardware agrees by convention on the order in which the bits of a byte are to be transmitted). 8-bit bytes are universal now, but it was not always so. Perhaps the last great non-byte-oriented hardware platform, which did indeed overlap with the Internet era broadly construed, was the DEC-10, which had a 36-bit word size; a word could hold Ô¨Åve 7-bit ASCII characters. The early Internet speciÔ¨Åcations introduced the term octet (an 8-bit byte) and required that packets be sequences of octets; non-octet-oriented hosts had to be able to convert. Thus was chaos averted. Note that there are still byte-oriented data issues; as one example, binary integers can be represented as a sequence of bytes in either big-endian orlittle-endian byte order ( 16.1.5 Binary Data ).RFC 1700 speciÔ¨Åes that Internet protocols use big-endian byte order, therefore sometimes called network byte order. 1.3 Packets 15
An Introduction to Computer Networks, Release 2.0.11 1.4 Datagram Forwarding In the datagram-forwarding model of packet delivery, packet headers contain a destination address. It is up to the intervening switches or routers to look at this address and get the packet to the correct destination. In datagram forwarding this is achieved by providing each switch with a forwarding table of xdestination,next_hop ypairs. When a packet arrives, the switch looks up the destination address (presumed globally unique) in its forwarding table and Ô¨Ånds the next_hop information: the immediate-neighbor address to which ‚Äì or interface by which ‚Äì the packet should be forwarded in order to bring it one step closer to its Ô¨Ånal destination. The next_hop value in a forwarding table is a single entry; each switch is responsible for only one step in the packet‚Äôs path. However, if all is well, the network of switches will be able to deliver the packet, one hop at a time, to its ultimate destination. The ‚Äúdestination‚Äù entries in the forwarding table do not have to correspond exactly with the packet destination addresses, though in the examples here they do, and they do for Ethernet datagram forwarding. However, for IP routing, the table ‚Äúdestination‚Äù entries will correspond to preÔ¨Åxes of IP addresses; this leads to a huge savings in space. The fundamental requirement is that the switch can perform a lookup operation, using its forwarding table and the destination address in the arriving packet, to determine the next hop. Just how the forwarding table is built is a question for later; we will return to this for Ethernet switches in 2.4.1 Ethernet Learning Algorithm and for IP routers in 13 Routing-Update Algorithms. For now, the forwarding tables may be thought of as created through initial conÔ¨Åguration. In the Ô¨Ågure below, switch S1 has interfaces 0, 1 and 2, and S2 has interfaces 0,1,2,3. If A is to send a packet to B, S1 must have a forwarding-table entry indicating that destination B is reached via its interface 2, and S2 must have an entry forwarding the packet out on interface 3. S1 S201 23 01 2 A BC D E Fig. 3:: Two switches, S1 and S2 Small numeric labels are interface numbers A complete forwarding table for S1, using interface numbers in the next_hop column, would be: 16 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 S1 destination next_hop A 0 C 1 B 2 D 2 E 2 The table for S2 might be as follows, where we have consolidated destinations A and C for visual simplicity. S2 destination next_hop A,C 0 D 1 E 2 B 3 In the network diagrammed above, all links are point-to-point, and so each interface corresponds to the unique immediate neighbor reached by that interface. We can thus replace the interface entries in the next_hop column with the name of the corresponding neighbor. For human readers, using neighbors in thenext_hop column is usually much more readable. S1‚Äôs table can now be written as follows (with consolidation of the entries for B, D and E): S1 destination next_hop A A C C B,D,E S2 A central feature of datagram forwarding is that each packet is forwarded ‚Äúin isolation‚Äù; the switches involved do not have any awareness of any higher-layer logical connections established between endpoints. This is also called stateless forwarding, in that the forwarding tables have no per-connection state. RFC 1122 put it this way (in the context of IP-layer datagram forwarding): To improve robustness of the communication system, gateways are designed to be stateless, forwarding each IP datagram independently of other datagrams. As a result, redundant paths can be exploited to provide robust service in spite of failures of intervening gateways and networks. The fundamental alternative to datagram forwarding is virtual circuits ,5.4 Virtual Circuits. In virtualcircuit networks, each router maintains state about each connection passing through it; different connections can be routed differently. If packet forwarding depends, for example, on per-connection information ‚Äì eg both TCP port numbers ‚Äì it is not datagram forwarding. (That said, it arguably still isdatagram forwarding if web trafÔ¨Åc ‚Äì to TCP port 80 ‚Äì is forwarded differently than all other trafÔ¨Åc, because that rule does not depend on the speciÔ¨Åc connection.) Datagram forwarding is sometimes allowed to use other information beyond the destination address. In theory, IP routing can be done based on the destination address and some quality-of-service information, 1.4 Datagram Forwarding 17
An Introduction to Computer Networks, Release 2.0.11 allowing, for example, different routing to the same destination for high-bandwidth bulk trafÔ¨Åc and for lowlatency real-time trafÔ¨Åc. In practice, most Internet Service Providers (ISPs) ignore user-provided qualityof-service information in the IP header, except by prearranged agreement, and route only based on the destination. By convention, switching devices acting at the LAN layer and forwarding packets based on the LAN address are called switches (or, originally, bridges; some still prefer that term), while such devices acting at the IP layer and forwarding on the IP address are called routers. Datagram forwarding is used both by Ethernet switches and by IP routers, though the destinations in Ethernet forwarding tables are individual nodes while the destinations in IP routers are entire networks (that is, sets of nodes). In IP routers within end-user sites it is common for a forwarding table to include a catchall default entry, matching any IP address that is nonlocal and so needs to be routed out into the Internet at large. Unlike the consolidated entries for B, D and E in the table above for S1, which likely would have to be implemented as actual separate entries, a default entry is a single record representing where to forward the packet if no other destination match is found. Here is a forwarding table for S1, above, with a default entry replacing the last three entries: S1 destination next_hop A 0 C 1 default 2 Default entries make sense only when we can tell by looking at an address that it does not represent a nearby node. This is common in IP networks because an IP address encodes the destination network, and routers generally know all the local networks. It is however rare in Ethernets, because there is generally no correlation between Ethernet addresses and locality. If S1 above were an Ethernet switch, and it had some means of knowing that interfaces 0 and 1 connected directly to individual hosts, not switches ‚Äì and S1 knew the addresses of these hosts ‚Äì then making interface 2 a default route would make sense. In practice, however, Ethernet switches do not know what kind of device connects to a given interface. 1.5 Topology In the network diagrammed in the previous section, there are no loops; graph theorists might describe this by saying the network graph is acyclic, or is a tree. In a loop-free network there is a unique path between any pair of nodes. The forwarding-table algorithm has only to make sure that every destination appears in the forwarding tables; the issue of choosing between alternative paths does not arise. However, if there are no loops then there is no redundancy: any broken link will result in partitioning the network into two pieces that cannot communicate. All else being equal (which it is not, but never mind for now), redundancy is a good thing. However, once we start including redundancy, we have to make decisions among the multiple paths to a destination. Consider, for a moment, the following network: Should S1 list S2 or S3 as the next_hop to B? Both paths A S1S2S4B and A S1S3S4B get there. There is no right answer. Even if one path is ‚Äúfaster‚Äù than the other, taking the slower path is not exactly wrong (especially if the slower path is, say, less expensive). Some sort of protocol must exist to provide a mechanism by which S1 can make the choice (though this mechanism might be as simple as choosing to 18 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S3 S4 BA Fig. 4:: A network with more than one path from A to B route via the Ô¨Årst path discovered to the given destination). We also want protocols to make sure that, if S1 reaches B via S2 and the S2 S4 link fails, then S1 will switch over to the still-working S1 S3S4B route. As we shall see, many LANs (in particular Ethernet) prefer ‚Äútree‚Äù networks with no redundancy, while IP has complex protocols in support of redundancy ( 13 Routing-Update Algorithms ). 1.5.1 TrafÔ¨Åc Engineering In some cases the decision above between routes A S1S2S4B and A S1S3S4B might be of material signiÔ¨Åcance ‚Äì perhaps the S2‚ÄìS4 link is slower than the others, or is more congested. We will use the term trafÔ¨Åc engineering to refer to any intentional selection of one route over another, or any elevation of the priority of one class of trafÔ¨Åc. The route selection can either be directly intentional, through conÔ¨Åguration, or can be implicit in the selection or tuning of algorithms that then make these route-selection choices automatically. As an example of the latter, the algorithms of 13.1 Distance-Vector Routing-Update Algorithm build forwarding tables on their own, but those tables are greatly inÔ¨Çuenced by the administrative assignment of link costs. With pure datagram forwarding, used at either the LAN or the IP layer, the path taken by a packet is determined solely by its destination, and trafÔ¨Åc engineering is limited to the choices made between alternative paths. We have already, however, suggested that datagram forwarding can be extended to take quality-ofservice information into account; this may be used to have voice trafÔ¨Åc ‚Äì with its relatively low bandwidth but intolerance for delay ‚Äì take an entirely different path than bulk Ô¨Åle transfers. Alternatively, the network manager may simply assign voice trafÔ¨Åc a higher priority, so it does not have to wait in queues behind Ô¨Åle-transfer trafÔ¨Åc. The quality-of-service information may be set by the end-user, in which case an ISP may wish to recognize it only for designated users, which in turn means that the ISP will implicitly use the trafÔ¨Åc source when making routing decisions. Alternatively, the quality-of-service information may be set by the ISP itself, based on its best guess as to the application; this means that the ISP may be using packet size, port number ( 1.12 Transport ) and other contents as part of the routing decision. For some explicit mechanisms supporting this kind of routing, see 13.6 Routing on Other Attributes. At the LAN layer, trafÔ¨Åc-engineering mechanisms are historically limited, though see 3.4 Software-DeÔ¨Åned Networking. At the IP layer, more strategies are available; see 25 Quality of Service. 1.5 Topology 19
An Introduction to Computer Networks, Release 2.0.11 1.6 Routing Loops A potential drawback to datagram forwarding is the possibility of a routing loop: a set of entries in the forwarding tables that cause some packets to circulate endlessly. For example, in the previous picture we would have a routing loop if, for (nonexistent) destination C, S1 forwarded to S2, S2 forwarded to S4, S4 forwarded to S3, and S3 forwarded to S1. A packet sent to C would not only not be delivered, but in circling endlessly it might easily consume a large majority of the bandwidth. Routing loops typically arise because the creation of the forwarding tables is often ‚Äúdistributed‚Äù, and there is no global authority to detect inconsistencies. Even when there is such an authority, temporary routing loops can be created due to notiÔ¨Åcation delays. Routing loops can also occur in networks where the underlying link topology is loop-free; for example, in the previous diagram we could, again for destination C, have S1 forward to S2 and S2 forward back to S1. We will refer to such a case as a linear routing loop. All datagram-forwarding protocols need some way of detecting and avoiding routing loops. Ethernet, for example, avoids nonlinear routing loops by disallowing loops in the underlying network topology, and avoids linear routing loops by not having switches forward a packet back out the interface by which it arrived. IP provides for a one-byte ‚ÄúTime to Live‚Äù (TTL) Ô¨Åeld in the IP header; it is set by the sender and decremented by 1 at each router; a packet is discarded if its TTL reaches 0. This limits the number of times a wayward packet can be forwarded to the initial TTL value, typically 64. In datagram routing, a switch is responsible only for the next hop to the ultimate destination; if a switch has a complete path in mind, there is no guarantee that the next_hop switch or any other downstream switch will continue to forward along that path. Misunderstandings can potentially lead to routing loops. Consider this network: B A DCB E Fig. 5:: Network consisting of Ô¨Åve nodes in a ring D might feel that the best path to B is D‚ÄìE‚ÄìC‚ÄìB (perhaps because it believes the A‚ÄìD link is to be avoided). If E similarly decides the best path to B is E‚ÄìD‚ÄìA‚ÄìB, and if D and E both choose their next_hop for B based on these best paths, then a linear routing loop is formed: D routes to B via E and E routes to B via D. Although each of D and E have identiÔ¨Åed a usable path, that path is not in fact followed. Moral: successful datagram routing requires cooperation and a consistent view of the network. 1.7 Congestion Switches introduce the possibility of congestion: packets arriving faster than they can be sent out. This can happen with just two interfaces, if the inbound interface has a higher bandwidth than the outbound interface; 20 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 another common source of congestion is trafÔ¨Åc arriving on multiple inputs and all destined for the same output. Whatever the reason, if packets are arriving for a given outbound interface faster than they can be sent, a queue will form for that interface. Once that queue is full, packets will be dropped. The most common strategy (though not the only one) is to drop any packets that arrive when the queue is full. The term ‚Äúcongestion‚Äù may refer either to the point where the queue is just beginning to build up, or to the point where the queue is full and packets are lost. In their paper [CJ89], Chiu and Jain refer to the Ô¨Årst point as the knee; this is where the slope of the load vs throughput graph Ô¨Çattens. They refer to the second point as the cliff; this is where packet losses may lead to a precipitous decline in throughput. Other authors use the term contention for knee-congestion. In the Internet, most packet losses are due to congestion. This is not because congestion is especially bad (though it can be, at times), but rather that other types of losses ( egdue to packet corruption) are insigniÔ¨Åcant by comparison. When to Upgrade? Deciding when a network really does have insufÔ¨Åcient bandwidth is not a technical issue but an economic one. The number of customers may increase, the cost of bandwidth may decrease or customers may simply be willing to pay more to have data transfers complete in less time; ‚Äúcustomers‚Äù here can be external or in-house. Monitoring of links and routers for congestion can, however, help determine exactly what parts of the network would most beneÔ¨Åt from upgrade. We emphasize that the presence of congestion does notmean that a network has a shortage of bandwidth. Bulk-trafÔ¨Åc senders (though not real-time senders) attempt to send as fast as possible, and congestion is simply the network‚Äôs feedback that the maximum transmission rate has been reached. For further discussion, including alternative deÔ¨Ånitions of longer-term congestion, see [BCL09]. Congestion isa sign of a problem in real-time networks, which we will consider in 25 Quality of Service. In these networks losses due to congestion must generally be kept to an absolute minimum; one way to achieve this is to limit the acceptance of new connections unless sufÔ¨Åcient resources are available. 1.8 Packets Again Perhaps the core justiÔ¨Åcation for packets, Baran‚Äôs concerns about node failure notwithstanding, is that the same link can carry, at different times, different packets representing trafÔ¨Åc to different destinations and from different senders. Thus, packets are the key to supporting shared transmission lines; that is, they support themultiplexing of multiple communications channels over a single cable. The alternative of a separate physical line between every pair of machines grows prohibitively complex very quickly (though virtual circuits between every pair of machines in a datacenter are not uncommon; see 5.4 Virtual Circuits ). From this shared-medium perspective, an important packet feature is the maximum packet size, as this represents the maximum time a sender can send before other senders get a chance. The alternative of unbounded packet sizes would lead to prolonged network unavailability for everyone else if someone downloaded a large Ô¨Åle in a single 1 Gigabit packet. Another drawback to large packets is that, if the packet is corrupted, the entire packet must be retransmitted; see 7.3.1 Error Rates and Packet Size. 1.8 Packets Again 21
An Introduction to Computer Networks, Release 2.0.11 When a router or switch receives a packet, it (generally) reads in the entire packet before looking at the header to decide to what next node to forward it. This is known as store-and-forward, and introduces aforwarding delay equal to the time needed to read in the entire packet. For individual packets this forwarding delay is hard to avoid (though some higher-end switches do implement cut-through switching to begin forwarding a packet before it has fully arrived), but if one is sending a long train of packets then by keeping multiple packets en route at the same time one can essentially eliminate the signiÔ¨Åcance of the forwarding delay; see 7.3 Packet Size. Total packet delay from sender to receiver is the sum of the following: 
- Bandwidth delay ,iesending 1000 Bytes at 20 Bytes/millisecond will take 50 ms. This is a per-link delay. 
- Propagation delay due to the speed of light. For example, if you start sending a packet right now on a 5000-km cable across the US with a propagation speed of 200 m/¬µsec (= 200 km/ms, about 2/3 the speed of light in vacuum), the Ô¨Årst bit will not arrive at the destination until 25 ms later. The bandwidth delay then determines how much after that the entire packet will take to arrive. 
- Store-and-forward delay, equal to the sum of the bandwidth delays out of each router along the path 
- Queuing delay, or waiting in line at busy routers. At bad moments this can exceed 1 sec, though that is rare. Generally it is less than 10 ms and often is less than 1 ms. Queuing delay is the only delay component amenable to reduction through careful engineering. See7.1 Packet Delay for more details. 1.9 LANs and Ethernet Alocal-area network, or LAN, is a system consisting of 
- physical links that are, ultimately, serial lines 
- common interfacing hardware connecting the hosts to the links 
- protocols to make everything work together We will explicitly assume that every LAN node is able to communicate with every other LAN node. Sometimes this will require the cooperation of intermediate nodes acting as switches. Far and away the most common type of (wired) LAN is Ethernet, originally described in a 1976 paper by Metcalfe and Boggs [MB76]. Ethernet‚Äôs popularity is due to low cost more than anything else, though the primary reason Ethernet cost is low is that high demand has led to manufacturing economies of scale. The original Ethernet had a bandwidth of 10 Mbps (megabits per second; we will use lower-case ‚Äúb‚Äù for bits and upper-case ‚ÄúB‚Äù for bytes), though nowadays most Ethernet operates at 100 Mbps and gigabit (1000 Mbps) Ethernet (and faster) is widely used in server rooms. (By comparison, as of this writing (2015) the data transfer rate to a typical faster hard disk is about 1000 Mbps.) Wireless (‚ÄúWi-Fi‚Äù) LANs are gaining popularity, and in many settings have supplanted wired Ethernet to end-users. Many early Ethernet installations were unswitched; each host simply tapped in to one long primary cable that wound through the building (or Ô¨Çoor). In principle, two stations could then transmit at the same time, rendering the data unintelligible; this was called a collision. Ethernet has several design features intended to minimize the bandwidth wasted on collisions: stations, before transmitting, check to be sure the line is 22 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 idle, they monitor the line while transmitting to detect collisions during the transmission, and, if a collision is detected, they execute a random backoff strategy to avoid an immediate recollision. See 2.1.5 The Slot Time and Collisions. While Ethernet collisions deÔ¨Ånitely reduce throughput, in the larger view they should perhaps be thought of as a part of a remarkably inexpensive shared-access mediation protocol. In unswitched Ethernets every packet is received by every host and it is up to the network card in each host to determine if the arriving packet is addressed to that host. It is almost always possible to conÔ¨Ågure the card to forward allarriving packets to the attached host; this poses a security threat and ‚Äúpassword sniffers‚Äù that surreptitiously collected passwords via such eavesdropping used to be common. Password SnifÔ¨Ång In the fall of 1994 at Loyola University I remotely changed the root password on several CS-department unix machines at the other end of campus, using telnet. I told no one. Within two hours, someone else logged into one of these machines, using the new password, from a host in Europe. Password snifÔ¨Ång was the likely culprit. Two months later was the so-called ‚ÄúChristmas Day Attack‚Äù ( 18.3.1 ISNs and spooÔ¨Ång ). One of the hosts used to launch this attack was Loyola‚Äôs hacked apollo.it.luc.edu. It is unclear the degree to which password snifÔ¨Ång played a role in that exploit. Due to both privacy and efÔ¨Åciency concerns, almost all Ethernets today are fully switched; this ensures that each packet is delivered only to the host to which it is addressed. One advantage of switching is that it effectively eliminates most Ethernet collisions; while in principle it replaces them with a queuing issue, in practice Ethernet switch queues so seldom Ô¨Åll up that they are almost invisible even to network managers (unlike IP router queues). Switching also prevents host-based eavesdropping, though arguably a better solution to this problem is encryption. Perhaps the more signiÔ¨Åcant tradeoff with switches, historically, was that Once Upon A Time they were expensive and unreliable; tapping directly into a common cable was dirt cheap. Ethernet addresses are six bytes long. Each Ethernet card (or network interface ) is assigned a (supposedly) unique address at the time of manufacture; this address is burned into the card‚Äôs ROM and is called the card‚Äôs physical address or hardware address or MAC (Media Access Control) address. The Ô¨Årst three bytes of the physical address have been assigned to the manufacturer; the subsequent three bytes are a serial number assigned by that manufacturer. By comparison, IP addresses are assigned administratively by the local site. The basic advantage of having addresses in hardware is that hosts automatically know their own addresses on startup; no manual conÔ¨Åguration or server query is necessary. It is not unusual for a site to have a large number of identically conÔ¨Ågured workstations, for which all network differences derive ultimately from each workstation‚Äôs unique Ethernet address. The network interface continually monitors all arriving packets; if it sees any packet containing a destination address that matches its own physical address, it grabs the packet and forwards it to the attached CPU (via a CPU interrupt). Ethernet also has a designated broadcast address. A host sending to the broadcast address has its packet received by every other host on the network; if a switch receives a broadcast packet on one port, it forwards the packet out every other port. This broadcast mechanism allows host A to contact host B when A does not yet know B‚Äôs physical address; typical broadcast queries have forms such as ‚ÄúWill the designated server 1.9 LANs and Ethernet 23
An Introduction to Computer Networks, Release 2.0.11 please answer‚Äù or (from the ARP protocol) ‚Äúwill the host with the given IP address please tell me your physical address‚Äù. TrafÔ¨Åc addressed to a particular host ‚Äì that is, not broadcast ‚Äì is said to be unicast. Because Ethernet addresses are assigned by the hardware, knowing an address does not provide any direct indication of where that address is located on the network. In switched Ethernet, the switches must thus have a forwarding-table record for each individual Ethernet address on the network; for extremely large networks this ultimately becomes unwieldy. Consider the analogous situation with postal addresses: Ethernet is somewhat like attempting to deliver mail using social-security numbers as addresses, where each postal worker is provided with a large catalog listing each person‚Äôs SSN together with their physical location. Real postal mail is, of course, addressed ‚Äúhierarchically‚Äù using ever-more-precise speciÔ¨Åers: state, city, zipcode, street address, and name / room#. Ethernet, in other words, does not scale well to ‚Äúlarge‚Äù sizes. Switched Ethernet works quite well, however, for networks with up to 10,000-100,000 nodes. Forwarding tables with size in that range are straightforward to manage. To forward packets correctly, switches must know where all active destination addresses in the LAN are located; traditional Ethernet switches do this by a passive learning algorithm. (IP routers, by comparison, use ‚Äúactive‚Äù protocols, and some newer Ethernet switches take the approach of 3.4 Software-DeÔ¨Åned Networking .) Typically a host physical address is entered into a switch‚Äôs forwarding table when a packet from that host is Ô¨Årst received; the switch notes the packet‚Äôs arrival interface and source address and assumes that the same interface is to be used to deliver packets back to that sender. If a given destination address has not yet been seen, and thus is not in the forwarding table, Ethernet switches still have the backup delivery option of Ô¨Çooding: forwarding the packet to everyone by treating the destination address like the broadcast address, and allowing the host Ethernet cards to sort it out. Since this broadcast-like process is not generally used for more than one packet (after that, the switches will have learned the correct forwarding-table entries), the risks of excessive trafÔ¨Åc and of eavesdropping are minimal. Thexhost,interfaceyforwarding table is often easier to think of as xhost,next_hopy, where the next_hop node is whatever switch or host is at the immediate other end of the link connecting to the given interface. In a fully switched network where each link connects only two interfaces, the two perspectives are equivalent. 1.10 IP - Internet Protocol To solve the scaling problem with Ethernet, and to allow support for other types of LANs and point-to-point links as well, the Internet Protocol was developed. Perhaps the central issue in the design of IP was to support universal connectivity (everyone can connect to everyone else) in such a way as to allow scaling to enormous size (in 2013 there appear to be around ~109nodes, although IP should work to 1010nodes or more), without resulting in unmanageably large forwarding tables (currently the largest tables have about 300,000 entries.) In the early days, IP networks were considered to be ‚Äúinternetworks‚Äù of basic networks (LANs); nowadays users generally ignore LANs and think of the Internet as one large (virtual) network. To support universal connectivity, IP provides a global mechanism for addressing and routing, so that packets can actually be delivered from any host to any other host. IP addresses (for the most-common version 4, which we denote IPv4 ) are 4 bytes (32 bits), and are part of the IP header that generally follows the Ethernet header. The Ethernet header only stays with a packet for one hop; the IP header stays with the packet for its entire journey across the Internet. 24 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 An essential feature of IPv4 (and IPv6) addresses is that they can be divided into a network part (a preÔ¨Åx) and a host part (the remainder). The ‚Äúlegacy‚Äù mechanism for designating the IPv4 network and host address portions was to make the division according to the Ô¨Årst few bits: Ô¨Årst few bits Ô¨Årst byte network bits host bits name application 0 0-127 8 24 class A a few very large networks 10 128-191 16 16 class B institution-sized networks 110 192-223 24 8 class C sized for smaller entities For example, the original IP address allocation for Loyola University Chicago was 147.126.0.0, a class B. In binary, 147 is 10010011. IP addresses, unlike Ethernet addresses, are administratively assigned. Once upon a time, you would get your Class B network preÔ¨Åx from the Internet Assigned Numbers Authority, or IANA (they now delegate this task), and then you would in turn assign the host portion in a way that was appropriate for your local site. As a result of this administrative assignment, an IP address usually serves not just as an endpoint identiÔ¨Åer but also as a locator, containing embedded location information (at least in the sense of location within the IP-address-assignment hierarchy, which may not be geographical). Ethernet addresses, by comparison, are endpoint identiÔ¨Åers but notlocators. The Class A/B/C deÔ¨Ånition above was spelled out in 1981 in RFC 791, which introduced IP. Class D was added in 1986 by RFC 988; class D addresses must begin with the bits 1110. These addresses are for multicast, that is, sending an IP packet to every member of a set of recipients (ideally without actually transmitting it more than once on any one link). Nowadays the division into the network and host bits is dynamic, and can be made at different positions in the address at different levels of the network. For example, a small organization might receive a /27 address block (1/8 the size of a class-C /24) from its ISP, eg200.1.130.96/ 27. The ISP routes to the organization based on this /27 preÔ¨Åx. At some higher level, however, routing might be based on the preÔ¨Åx 200.1.128/ 18; this might, for example, represent an address block assigned to the ISP (note that the Ô¨Årst 18 bits of 200.1.130.x match 200.1.128; the Ô¨Årst two bits of 128 and 130, taken as 8-bit quantities, are ‚Äú10‚Äù). The network/host division point is notcarried within the IP header; routers negotiate this division point when they negotiate the next_hop forwarding information. We will return to this in 9.5 The Classless IP Delivery Algorithm. The network portion of an IP address is sometimes called the network number ornetwork address or network preÔ¨Åx. As we shall see below, most forwarding decisions are made using only the network preÔ¨Åx. The network preÔ¨Åx is commonly denoted by setting the host bits to zero and ending the resultant address with a slash followed by the number of network bits in the address: eg12.0.0.0/8 or 147.126.0.0/16. Note that 12.0.0.0/8 and 12.0.0.0/9 represent different things; in the latter, the second byte of any host address extending the network address is constrained to begin with a 0-bit. An anonymous block of IP addresses might be referred to only by the slash and following digit, eg‚Äúwe need a /22 block to accommodate all our customers‚Äù. All hosts with the same network address (same network bits) are said to be on the same IP network and must be located together on the same LAN; as we shall see below, if two hosts share the same network address then they will assume they can reach each other directly via the underlying LAN, and if they cannot then connectivity fails. A consequence of this rule is that outside of the site only the network bits need to be looked at to route a packet to the site. 1.10 IP - Internet Protocol 25
An Introduction to Computer Networks, Release 2.0.11 Usually, all hosts (or more precisely all network interfaces) on the same physical LAN share the same network preÔ¨Åx and thus are part of the same IP network. Occasionally, however, one LAN is divided into multiple IP networks. Each individual LAN technology has a maximum packet size it supports; for example, Ethernet has a maximum packet size of about 1500 bytes but the once-competing Token Ring had a maximum of 4 kB. Today the world has largely standardized on Ethernet and almost entirely standardized on Ethernet packetsize limits, but this was not the case when IP was introduced and there was real concern that two hosts on separate large-packet networks might try to exchange packets too large for some small-packet intermediate network to carry. Therefore, in addition to routing and addressing, the decision was made that IP must also support fragmentation: the division of large packets into multiple smaller ones (in other contexts this may also be called segmentation ). The IP approach is not very efÔ¨Åcient, and IP hosts go to considerable lengths to avoid fragmentation. IP does require that packets of up to 576 bytes be supported, and so a common legacy strategy was for a host to limit a packet to at most 512 user-data bytes whenever the packet was to be sent via a router; packets addressed to another host on the same LAN could of course use a larger packet size. Despite its limited use, however, fragmentation is essential conceptually, in order for IP to be able to support large packets without knowing anything about the intervening networks. IP is a best effort system; there are no IP-layer acknowledgments or retransmissions. We ship the packet off, and hope it gets there. Most of the time, it does. Architecturally, this best-effort model represents what is known as connectionless networking: the IP layer does not maintain information about endpoint-to-endpoint connections, and simply forwards packets like a giant LAN. Responsibility for creating and maintaining connections is left for the next layer up, the TCP layer. Connectionless networking is notthe only way to do things: the alternative could have been some form connection-oriented internetworking, in which routers domaintain state information about individual connections. Later, in 5.4 Virtual Circuits, we will examine how virtual-circuit networking can be used to implement a connection-oriented approach; virtual-circuit switching is the primary alternative to datagram switching. Connectionless (IP-style) and connection-oriented networking each have advantages. Connectionless networking is conceptually more reliable: if routers do not hold connection state, then they cannot lose connection state. The path taken by the packets in some higher-level connection can easily be dynamically rerouted. Finally, connectionless networking makes it hard for providers to bill by the connection; once upon a time (in the era of dollar-a-minute phone calls) this was a source of mild astonishment to many new users. (This was not always a given; the paper [CK74] considers, among other things, the possibility of per-packet accounting.) The primary advantage of connection-oriented networking, on the other hand, is that the routers are then much better positioned to accept reservations and to make quality-of-service guarantees. This remains something of a sore point in the current Internet: if you want to use V oice-over-IP, or VoIP, telephones, or if you want to engage in video conferencing, your packets will be treated by the Internet core just the same as if they were low-priority Ô¨Åle transfers. There is no ‚Äúpriority service‚Äù option. The most common form of IP packet loss is router queue overÔ¨Çows, representing network congestion. Packet losses due to packet corruption are rare ( egless than one in 104; perhaps much less). But in a connectionless world a large number of hosts can simultaneously attempt to send trafÔ¨Åc through one router, in which case queue overÔ¨Çows are hard to avoid. Although we will often assume, for simplicity, that routers have a Ô¨Åxed input queue size, the reality is often 26 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 a little more complicated. See 21.5 Active Queue Management and23 Queuing and Scheduling. 1.10.1 IP Forwarding IP routers use datagram forwarding, described in 1.4 Datagram Forwarding above, to deliver packets, but the ‚Äúdestination‚Äù values listed in the forwarding tables are network preÔ¨Åxes ‚Äì representing entire LANs ‚Äì instead of individual hosts. The goal of IP forwarding, then, becomes delivery to the correct LAN; a separate process is used to deliver to the Ô¨Ånal host once the Ô¨Ånal LAN has been reached. The entire point, in fact, of having a network/host division within IP addresses is so that routers need to list only the network preÔ¨Åxes of the destination addresses in their IP forwarding tables. This strategy is the key to IP scalability: it saves large amounts of forwarding-table space, it saves time as smaller tables allow faster lookup, and it saves the bandwidth and overhead that would be needed for routers to keep track of individual addresses. To get an idea of the forwarding-table space savings, there are currently (2013) around a billion hosts on the Internet, but only 300,000 or so networks listed in top-level forwarding tables. With IP‚Äôs use of network preÔ¨Åxes as forwarding-table destinations, matching an actual packet address to a forwarding-table entry is no longer a matter of simple equality comparison; routers must compare appropriate preÔ¨Åxes. IP forwarding tables are sometimes also referred to as ‚Äúrouting tables‚Äù; in this book, however, we make at least a token effort to use ‚Äúforwarding‚Äù to refer to the packet forwarding process, and ‚Äúrouting‚Äù to refer to mechanisms by which the forwarding tables are maintained and updated. (If we were to be completely consistent here, we would use the term ‚Äúforwarding loop‚Äù rather than ‚Äúrouting loop‚Äù.) Now let us look at an example of how IP forwarding (or routing) works. We will assume that all network nodes are either hosts ‚Äì user machines, with a single network connection ‚Äì or routers, which do packetforwarding only. Routers are not directly visible to users, and always have at least two different network interfaces representing different networks that the router is connecting. (Machines can be both hosts and routers, but this introduces complications.) Suppose A is the sending host, sending a packet to a destination host D. The IP header of the packet will contain D‚Äôs IP address in the ‚Äúdestination address‚Äù Ô¨Åeld (it will also contain A‚Äôs own address as the ‚Äúsource address‚Äù). The Ô¨Årst step is for A to determine whether D is on the same LAN as itself or not; that is, whether D is local. This is done by looking at the network part of the destination address, which we will denote by Dnet. If this net address is the same as A‚Äôs (that is, if it is equal numerically to A net), then A Ô¨Ågures D is on the same LAN as itself, and can use direct LAN delivery. It looks up the appropriate physical address for D (probably with the ARP protocol, 10.2 Address Resolution Protocol: ARP ), attaches a LAN header to the packet in front of the IP header, and sends the packet straight to D via the LAN. If, however, A netand D netdonotmatch ‚Äì D is non-local ‚Äì then A looks up a router to use. Most ordinary hosts use only one router for all non-local packet deliveries, making this choice very simple. A then forwards the packet to the router, again using direct delivery over the LAN. The IP destination address in the packet remains D in this case, although the LAN destination address will be that of the router. When the router receives the packet, it strips off the LAN header but leaves the IP header with the IP destination address. It extracts the destination D, and then looks at D net. The router Ô¨Årst checks to see if any of itsnetwork interfaces are on the same LAN as D; recall that the router connects to at least one additional network besides the one for A. If the answer is yes, then the router uses direct LAN delivery to the destination, as above. If, on the other hand, D netis not a LAN to which the router is connected directly, then 1.10 IP - Internet Protocol 27
An Introduction to Computer Networks, Release 2.0.11 the router consults its internal forwarding table. This consists of a list of networks each with an associated next_hop address. These xnet,next_hopytables compare with switched-Ethernet‚Äôs xhost,next_hopytables; the former type will be smaller because there are many fewer nets than hosts. The next_hop addresses in the table are chosen so that the router can always reach them via direct LAN delivery via one of its interfaces; generally they are other routers. The router looks up D netin the table, Ô¨Ånds the next_hop address, and uses direct LAN delivery to get the packet to that next_hop machine. The packet‚Äôs IP header remains essentially unchanged, although the router most likely attaches an entirely new LAN header. The packet continues being forwarded like this, from router to router, until it Ô¨Ånally arrives at a router that is connected to D net; it is then delivered by that Ô¨Ånal router directly to D, using the LAN. To make this concrete, consider the following diagram: R1 R2 R3A B C F E D: 200.0.1.37 200.0.0/24 200.0.1/24 Fig. 6:: Two LANs (200.0.0 and 200.0.1) joined by three routers R1,R2,R3 With Ethernet-style forwarding, R2 would have to maintain entries for each of A,B,C,D,E,F. With IP forwarding, R2 has just two entries to maintain in its forwarding table: 200.0.0/24 and 200.0.1/24. If A sends to D, at 200.0.1.37, it puts this address into the IP header, notes that 200.0.0 200.0.1, and thus concludes D is not a local delivery. A therefore sends the packet to its router R1, using LAN delivery. R1 looks up the destination network 200.0.1 in its forwarding table and forwards the packet to R2, which in turn forwards it to R3. R3 now sees that it isconnected directly to the destination network 200.0.1, and delivers the packet via the LAN to D, by looking up D‚Äôs physical address. In this diagram, IP addresses for the ends of the R1‚ÄìR2 and R2‚ÄìR3 links are not shown. They could be assigned global IP addresses, but they could also use ‚Äúprivate‚Äù IP addresses. Assuming these links are point-to-point links, they might not actually need IP addresses at all; we return to this in 9.8 Unnumbered Interfaces. One can think of the network-preÔ¨Åx bits as analogous to the ‚Äúzip code‚Äù on postal mail, and the host bits as analogous to the street address. The internal parts of the post ofÔ¨Åce get a letter to the right zip code, and then an individual letter carrier (the LAN) gets it to the right address. Alternatively, one can think of the network bits as like the area code of a phone number, and the host bits as like the rest of the digits. Newer protocols that support different net/host division points at different places in the network ‚Äì sometimes called hierarchical routing ‚Äì allow support for addressing schemes that correspond to, say, zip/street/user, or areacode/exchange/subscriber. The Invertebrate Internet The backbone is not as essential as it once was. Once Upon A Time, all trafÔ¨Åc between different providers passed through the backbone. The legacy backbone still exists, but today it is also common for trafÔ¨Åc from large providers such as Google to take a backbone-free path; such providers connect (or ‚Äúpeer‚Äù) directly with large residential ISPs such as Comcast. Google refers to this as their ‚ÄúEdge Network‚Äù; see peering.google.com and also 15.7.1 MED values and trafÔ¨Åc engineering. 28 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 We will refer to the Internet backbone as those IP routers that specialize in large-scale routing on the commercial Internet, and which generally have forwarding-table entries covering all public IP addresses; note that this is essentially a business deÔ¨Ånition rather than a technical one. We can revise the table-size claim of the previous paragraph to state that, while there are many private IP networks, there are about 800,000 separate network preÔ¨Åxes (as of 2019) visible to the backbone. (In 2012, the year this book was started, there were about 400,000 preÔ¨Åxes.) A forwarding table of 800,000 entries is quite feasible; a table a hundred times larger is not, let alone a thousand times larger. For a graph of the growth in network preÔ¨Åxes / forwarding-table entries, see 15.5 BGP Table Size. IP routers at non-backbone sites generally know all locally assigned network preÔ¨Åxes, eg200.0.0/24 and 200.0.1/24 above. If a destination does not match any locally assigned network preÔ¨Åx, the packet needs to be routed out into the Internet at large; for typical non-backbone sites this almost always this means the packet is sent to the ISP that provides Internet connectivity. Generally the local routers will contain a catchall default entry covering all nonlocal networks; this means that the router needs an explicit entry only for locally assigned networks. This greatly reduces the forwarding-table size. The Internet backbone can be approximately described, in fact, as those routers that do nothave a default entry. For most purposes, the Internet can be seen as a combination of end-user LANs together with point-to-point links joining these LANs to the backbone, point-to-point links also tie the backbone together. Both LANs and point-to-point links appear in the diagram above. Just how routers build their xdestnet,next_hop yforwarding tables is a major topic itself, which we cover in 13 Routing-Update Algorithms. Unlike Ethernet, IP routers do nothave a ‚ÄúÔ¨Çooding‚Äù delivery mechanism as a fallback, so the tables must be constructed in advance. (There is a limited form of IP broadcast, but it is basically intended for reaching the local LAN only, and does not help at all with delivery in the event that the destination network is unknown.) Most forwarding-table-construction algorithms used on a set of routers under common management fall into either the distance-vector or the link-state category; these are described in 13 Routing-Update Algorithms. Routers notunder common management ‚Äì that is, neighboring routers belonging to different organizations ‚Äì exchange information through the Border Gateway Protocol, BGP ( 14 Large-Scale IP Routing ). BGP allows routing decisions to be based on a fusion of ‚Äútechnical‚Äù information (which sites are reachable at all, and through where) together with ‚Äúpolicy‚Äù information representing legal or commercial agreements: which outside routers are ‚Äúpreferred‚Äù, whose trafÔ¨Åc an ISP will carry even if it isn‚Äôt to one of the ISP‚Äôs customers, etc. Most common residential ‚Äúrouters‚Äù involve network address translation in addition to packet forwarding. See9.7 Network Address Translation. 1.10.2 The Future of IPv4 As mentioned earlier, allocation of blocks of IP addresses is the responsibility of the Internet Assigned Numbers Authority. IANA long ago delegated the job of allocating network preÔ¨Åxes to individual sites; they limited themselves to handing out /8 blocks (class A blocks) to the Ô¨Åve regional registries, which are 
- ARIN ‚Äì North America 
- RIPE ‚Äì Europe, the Middle East and parts of Asia 
- APNIC ‚Äì East Asia and the PaciÔ¨Åc 1.10 IP - Internet Protocol 29
An Introduction to Computer Networks, Release 2.0.11 
- AfriNIC ‚Äì most of Africa 
- LACNIC ‚Äì Central and South America As of the end of January 2011, the IANA Ô¨Ånally ran out of /8 blocks. There is a table at http://www.iana. org/assignments/ipv4-address-space/ipv4-address-space.xml of all IANA assignments of /8 blocks; examination of the table shows all have now been allocated. In September 2015, ARIN ran out of its pool of IPv4 addresses. Most of ARIN‚Äôs customers are ISPs, which can now obtain new IPv4 addresses only by buying unused address blocks from other organizations. A few months after the IANA pool ran out in 2011, Microsoft purchased 666,624 IP addresses (2604 ClassC blocks) in a Nortel bankruptcy auction for $7.5 million. Three years later, IP-address prices fell to half that, but, by 2019, had climbed to the $20-and-up range. In that year Amazon bought a 4-million-address block (44.0.0.0/10) for $108 million, or $27/address. Prices remained in the $20 range through 2020, but by the end of 2021 had climbed to $50-$55, with several /19 blocks sold in January 2022 in that range. The market for IPv4 address blocks continues to develop, but the only real solution is widespread adoption of IPv6 with its plentiful 128-bit addresses. An IPv4 address price in the range of $20-30 is unlikely to have much impact in residential Internet access, where annual connection fees are often $600. Large organizations use NAT ( 9.7 Network Address Translation ) extensively, leading to the need for only a small number of globally visible addresses. The IPv4 address shortage does not even seem to have affected wireless networking. It does, however, lead to inefÔ¨Åcient routing tables, as sites that might once have had a single /17 address block ‚Äì and thus a single backbone forwarding-table entry ‚Äì might now be spread over more than a hundred /24 blocks and concomitant forwarding entries. 1.11 DNS IP addresses are hard to remember (nearly impossible in IPv6). The domain name system, or DNS (10.1 DNS ), comes to the rescue by creating a way to convert hierarchical text names to IP addresses. Thus, for example, one can type www.luc.edu instead of 147.126.1.230. Virtually all Internet software uses the same basic library calls to convert DNS names to actual addresses. One thing DNS makes possible is changing a website‚Äôs IP address while leaving the name alone. This allows moving a site to a new provider, for example, without requiring users to learn anything new. It is also possible to have several different DNS names resolve to the same IP address, and ‚Äì through some modest trickery ‚Äì have the http (web) server at that IP address handle the different DNS names as completely different websites. DNS is hierarchical and distributed. In looking up cs.luc.edu four different DNS servers may be queried: for the so-called ‚ÄúDNS root zone‚Äù, for edu, forluc.edu and forcs.luc.edu. Searching a hierarchy can be cumbersome, so DNS search results are normally cached locally. If a name is not found in the cache, the lookup may take a couple seconds. The DNS hierarchy need have nothing to do with the IP-address hierarchy. 30 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 1.12 Transport The IP layer gets packets from one node to another, but it is not well-suited to transport. First, IP routing is a ‚Äúbest-effort‚Äù mechanism, which means packets can and do get lost sometimes. Additionally, data that does arrive can arrive out of order. Finally, IP only supports sending to a speciÔ¨Åc host; normally, one wants to send to a given application running on that host. Email and web trafÔ¨Åc, or two different web sessions, should not be commingled! The Transport layer is the layer above the IP layer that handles these sorts of issues, often by creating some sort of connection abstraction. Far and away the most popular mechanism in the Transport layer is the Transmission Control Protocol, or TCP. TCP extends IP with the following features: 
- reliability: TCP numbers each packet, and keeps track of which are lost and retransmits them after a timeout. It holds early-arriving out-of-order packets for delivery at the correct time. Every arriving data packet is acknowledged by the receiver; timeout and retransmission occurs when an acknowledgment packet isn‚Äôt received by the sender within a given time. 
- connection-orientation: Once a TCP connection is made, an application sends data simply by writing to that connection. No further application-level addressing is needed. TCP connections are managed by the operating-system kernel, not by the application. 
- stream-orientation: An application using TCP can write 1 byte at a time, or 100 kB at a time; TCP will buffer and/or divide up the data into appropriate sized packets. 
- port numbers: these provide a way to specify the receiving application for the data, and also to identify the sending application. 
- throughput management: TCP attempts to maximize throughput, while at the same time not contributing unnecessarily to network congestion. TCP endpoints are of the form xhost,porty; these pairs are known as socket addresses, or sometimes as just sockets though the latter refers more properly to the operating-system objects that receive the data sent to the socket addresses. Servers (or, more precisely, server applications) listen for connections to sockets they have opened; the client is then any endpoint that initiates a connection to a server. When you enter a host name in a web browser, it opens a TCP connection to the server‚Äôs port 80 (the standard web-trafÔ¨Åc port), that is, to the server socket with socket-address xserver,80y. If you have several browser tabs open, each might connect to the same server socket, but the connections are distinguishable by virtue of using separate ports (and thus having separate socket addresses) on the client end (that is, your end). A busy server may have thousands of connections to its port 80 (the web port) and hundreds of connections to port 25 (the email port). Web and email trafÔ¨Åc are kept separate by virtue of the different ports used. All those clients to the same port, though, are kept separate because each comes from a unique xhost,portypair. A TCP connection is determined by the xhost,portysocket address at each end; trafÔ¨Åc on different connections does not intermingle. That is, there may be multiple independent connections to xwww.luc.edu,80 y. This is somewhat analogous to certain business telephone numbers of the ‚Äú operators are standing by ‚Äù type, which support multiple callers at the same time to the same toll-free number. Each call to that number is answered by a different operator (corresponding to a different cpu process), and different calls do not ‚Äúoverhear‚Äù each other. TCP uses the sliding-windows algorithm ,8 Abstract Sliding Windows, to keep multiple packets en route at any one time. The window size represents the number of packets simultaneously in transit (TCP actually 1.12 Transport 31
An Introduction to Computer Networks, Release 2.0.11 keeps track of the window size in bytes, but packets are easier to visualize). If the window size is 10 packets, for example, then at any one time 10 packets are in transit (perhaps 5 data packets and 5 returning acknowledgments). Assuming no packets are lost, then as each acknowledgment arrives the window ‚Äúslides forward‚Äù by one packet. The data packet 10 packets ahead is then sent, to maintain a total of 10 packets on the wire. For example, consider the moment when the ten packets 20-29 are in transit. When ACK[20] is received, the number of packets outstanding drops to 9 (packets 21-29). To keep 10 packets in Ô¨Çight, Data[30] is sent. When ACK[21] is received, Data[31] is sent, and so on. Sliding windows minimizes the effect of store-and-forward delays, and propagation delays, as these then only count once for the entire windowful and not once per packet. Sliding windows also provides an automatic, if partial, brake on congestion: the queue at any switch or router along the way cannot exceed the window size. In this it compares favorably with constant-rate transmission, which, if the available bandwidth falls below the transmission rate, always leads to overÔ¨Çowing queues and to a signiÔ¨Åcant percentage of dropped packets. Of course, if the window size is too large, a sliding-windows sender may also experience dropped packets. The ideal window size, at least from a throughput perspective, is such that it takes one round-trip time to send an entire window, so that the next ACK will always be arriving just as the sender has Ô¨Ånished transmitting the window. Determining this ideal size, however, is difÔ¨Åcult; for one thing, the ideal size varies with network load. As a result, TCP approximates the ideal size. The most common TCP strategy ‚Äì that of so-called TCP Reno ‚Äì is that the window size is slowly raised until packet loss occurs, which TCP takes as a sign that it has reached the limit of available network resources. At that point the window size is reduced to half its previous value, and the slow climb resumes. The effect is a ‚Äúsawtooth‚Äù graph of window size with time, which oscillates (more or less) around the ‚Äúoptimal‚Äù window size. For an idealized sawtooth graph, see 19.1.1 The Somewhat-Steady State; for some ‚Äúreal‚Äù (simulation-created) sawtooth graphs see 31.4.1 Some TCP Reno cwnd graphs. While this window-size-optimization strategy has its roots in attempting to maximize the available bandwidth, it also has the effect of greatly limiting the number of packet-loss events. As a result, TCP has come to be the Internet protocol charged with reducing (or at least managing) congestion on the Internet, and ‚Äì relatedly ‚Äì with ensuring fairness of bandwidth allocations to competing connections. Core Internet routers ‚Äì at least in the classical case ‚Äì essentially have no role in enforcing congestion or fairness restrictions at all. The Internet, in other words, places responsibility for congestion avoidance cooperatively into the hands of end users. While ‚Äúcheating‚Äù is possible, this cooperative approach has worked remarkably well. While TCP is ubiquitous, the real-time performance of TCP is not always consistent: if a packet is lost, the receiving TCP host will not turn over anything further to the receiving application until the lost packet has been retransmitted successfully; this is often called head-of-line blocking. This is a serious problem for sound and video applications, which can discreetly handle modest losses but which have much more difÔ¨Åculty with sudden large delays. A few lost packets ideally should mean just a few brief voice dropouts (pretty common on cell phones) or Ô¨Çicker/snow on the video screen (or just reuse of the previous frame); both of these are better than pausing completely. The basic alternative to TCP is known as UDP, for User Datagram Protocol. UDP, like TCP, provides port numbers to support delivery to multiple endpoints within the receiving host, in effect to a speciÔ¨Åc process on the host. As with TCP, a UDP socket consists of a xhost,portypair. UDP also includes, like TCP, a checksum over the data. However, UDP omits the other TCP features: there is no connection setup, no lost-packet detection, no automatic timeout/retransmission, and the application must manage its own packetization. This simplicity should not be seen as all negative: the absence of connection setup means data transmission can get started faster, and the absence of lost-packet detection means there is no head-of-line blocking. See 32 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 16 UDP Transport. The Real-time Transport Protocol, or RTP, sits above UDP and adds some additional support for voice and video applications. 1.12.1 Transport Communications Patterns The two ‚Äúclassic‚Äù trafÔ¨Åc patterns for Internet communication are these: 
- Interactive or bursty communications such as via ssh or telnet, with long idle times between short bursts 
- Bulk Ô¨Åle transfers, such as downloading a web page TCP handles both of these well, although its congestion-management features apply only when a large amount of data is in transit at once. Web browsing is something of a hybrid; over time, there is usually considerable burstiness, but individual pages now often exceed 1 MB. To the above we might add request/reply operations, egto query a database or to make DNS requests. TCP is widely used here as well, though most DNS trafÔ¨Åc still uses UDP. There are periodic calls for a new protocol speciÔ¨Åcally addressing this pattern, though at this point the use of TCP is well established. If a sequence of request/reply operations is envisioned, a single TCP connection makes excellent sense, as the connection-setup overhead is minimal by comparison. See also 16.5 Remote Procedure Call (RPC) and 18.15.2 SCTP. This century has seen an explosion in streaming video (25.3.2 Streaming Video ), in lengths from a few minutes to a few hours. Streaming radio stations might be left playing indeÔ¨Ånitely. TCP generally works well here, assuming the receiver can get, say, a minute ahead, buffering the video that has been received but not yet viewed. That way, if there is a dip in throughput due to congestion, the receiver has time to recover. Buffering works a little less well for streaming radio, as the listener doesn‚Äôt want to get too far behind, though ten seconds is reasonable. Fortunately, audio bandwidth is smaller. Another issue with streaming video is the bandwidth demand. Most streaming-video services attempt to estimate the available throughput, and then adapt to that throughput by changing the video resolution (25.3 Real-time TrafÔ¨Åc ). Typically, video streaming operates on a start/stop basis: the sender pauses when the receiver‚Äôs playback buffer is ‚Äúfull‚Äù, and resumes when the playback buffer drops below a certain threshold. If the video (or, for that matter, voice audio) is interactive, there is much less opportunity for stream buffering. If someone asks a simple question on an Internet telephone call, they generally want an answer more or less immediately; they do not expect to wait for the answer to make it through the other party‚Äôs stream buffer. 200 ms of buffering is noticeable. Here we enter the realm of genuine real-time trafÔ¨Åc ( 25.3 Realtime TrafÔ¨Åc ). UDP is often used to avoid head-of-line blocking. Lower bandwidth helps; voice-grade communications traditionally need only 8 kB/sec, less if compression is used. On the other hand, there may be constraints on the variation in delivery time (known as jitter; see 25.11.3 RTP Control Protocol for a speciÔ¨Åc numeric interpretation). Interactive video, with its much higher bandwidth requirements, is more difÔ¨Åcult; fortunately, users seem to tolerate the common pauses and freezes. Within the Transport layer, essentially all network connections involve a client and a server. Often this pattern is repeated at the Application layer as well: the client contacts the server and initiates a login session, 1.12 Transport 33
An Introduction to Computer Networks, Release 2.0.11 or browses some web pages, or watches a movie. Sometimes, however, Application-layer exchanges Ô¨Åt the peer-to-peer model better, in which the two endpoints are more-or-less co-equals. Some examples include 
- Internet telephony: there is no beneÔ¨Åt in designating the party who place the call as the ‚Äúclient‚Äù 
- Message passing in a CPU cluster, often using 16.5 Remote Procedure Call (RPC) 
- The routing-communication protocols of 13 Routing-Update Algorithms. When router A reports to router B we might call A the client, but over time, as A and B report to one another repeatedly, the peer-to-peer model makes more sense. 
- So-called peer-to-peer Ô¨Åle-sharing, where individuals exchange Ô¨Åles with other individuals (and as opposed to ‚Äúcloud-based‚Äù Ô¨Åle-sharing in which the ‚Äúcloud‚Äù is the server). RFC 5694 contains additional discussion of peer-to-peer patterns. 1.12.2 Content-Distribution Networks Sites with an extremely large volume of content to distribute often turn to a specialized communication pattern called a content-distribution network or CDN. To reduce the amount of long-distance trafÔ¨Åc, or to reduce the round-trip time, a site replicates its content at multiple datacenters (also called Points of Presence (PoPs), nodes ,access points oredge servers ). When a user makes a request ( egfor a web page or a video), the request is routed to the nearest (or approximately nearest) datacenter, and the content is delivered from there. CDN Mapping For a geographical map of the servers in the NetFlix CDN as of 2016, see [BCTCU16]. The map was created solely through end-user measurements. Most of the servers are in North and South America and Europe. Large web pages typically contain both static content and also individualized dynamic content. On a typical Facebook page, for example, the videos and javascript might be considered static, while the individual wall posts might be considered dynamic. The CDN may cache all or most of the static content at each of its edge servers, leaving the dynamic content to come from a centralized server. Alternatively, the dynamic content may be replicated at each CDN edge node as well, though this introduces some real-time coordination issues. If dynamic content is notreplicated, the CDN may include private high-speed links between its nodes, allowing for rapid low-congestion delivery to any node. Alternatively, CDN nodes may simply communicate using the public Internet. Finally, the CDN may (or may not) be conÔ¨Ågured to support fast interactive trafÔ¨Åc between nodes, egteleconferencing trafÔ¨Åc, as is outlined in 25.6.1 A CDN Alternative to IntServ. Organizations can create their own CDNs, but often turn to specialized CDN providers, who often combine their CDN services with website-hosting services. In principle, all that is needed to create a CDN is a multiplicity of datacenters, each with its own connection to the Internet; private links between datacenters are also common. In practice, many CDN providers also try to build direct connections with the ISPs that serve their customers; the Google Edge Network above does this. This can improve performance and reduce trafÔ¨Åc costs; we will return to this in 15.7.1 MED values and trafÔ¨Åc engineering. 34 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 Finding the edge server that is closest to a given user is a tricky issue. There are three techniques in common use. In the Ô¨Årst, the edge servers are all given different IP addresses, and DNS is conÔ¨Ågured to have users receive the IP address of the closest edge server, 10.1 DNS. In the second, each edge server has the same IP address, and anycast routing is used to route trafÔ¨Åc from the user to the closest edge server, 15.8 BGP and Anycast. Finally, for HTTP applications a centralized server can look up the approximate location of the user, and then redirect the web page to the closest edge server. 1.13 Firewalls One problem with having a program on your machine listening on an open TCP port is that someone may connect and then, using some Ô¨Çaw in the software on your end, do something malicious to your machine. Damage can range from the unintended downloading of personal data to compromise and takeover of your entire machine, making it a distributor of viruses and worms or a steppingstone in later break-ins of other machines. A strategy known as buffer overÔ¨Çow (28.2 Stack Buffer OverÔ¨Çow ) has been the basis for a great many total-compromise attacks. The idea is to identify a point in a server program where it Ô¨Ålls a memory buffer with network-supplied data without careful length checking; almost any call to the C library function gets(buf) will sufÔ¨Åce. The attacker then crafts an oversized input string which, when read by the server and stored in memory, overÔ¨Çows the buffer and overwrites subsequent portions of memory, typically containing the stack-frame pointers. The usual goal is to arrange things so that when the server reaches the end of the currently executing function, control is returned not to the calling function but instead to the attacker‚Äôs own payload code located within the string. AÔ¨Årewall is a mechanism to block connections deemed potentially risky, egthose originating from outside the site. Generally ordinary workstations do not ever need to accept connections from the Internet; client machines instead initiate connections to (better-protected) servers. So blocking incoming connections works reasonably well; when necessary ( egfor games) certain ports can be selectively unblocked. The original Ô¨Årewalls were built into routers. Incoming trafÔ¨Åc to servers was often blocked unless it was sent to one of a modest number of ‚Äúopen‚Äù ports; for non-servers, typically all inbound connections were blocked. This allowed internal machines to operate reasonably safely, though being unable to accept incoming connections is sometimes inconvenient. Nowadays per-host Ô¨Årewalls ‚Äì in addition to router-based Ô¨Årewalls ‚Äì are common: you can conÔ¨Ågure your workstation not to accept inbound connections to most (or all) ports regardless of whether software on the workstation requests such a connection. Outbound connections can, in many cases, also be prevented. The typical home router implements something called network-address translation ( 9.7 Network Address Translation ), which, in addition to conserving IPv4 addresses, also provides Ô¨Årewall protection. 1.14 Some Useful Utilities There exists a great variety of useful programs for probing and diagnosing networks. Here we list a few of the simpler, more common and available ones; some of these are addressed in more detail in subsequent chapters. Some of these, like ping, are generally present by default; others will have to be installed from somewhere. 1.13 Firewalls 35
An Introduction to Computer Networks, Release 2.0.11 ping Ping is useful to determine if another machine is accessible, eg ping www.cs.luc.edu ping 147.126.1.230 See10.4 Internet Control Message Protocol for how it works. Sometimes ping fails because the necessary packets are blocked by a Ô¨Årewall. ifconÔ¨Åg ,ipconÔ¨Åg ,ip To Ô¨Ånd your own IP address you can use ipconfig on Windows, ifconfig on Linux and Macintosh systems, or the newer ip addr list on Linux. The output generally lists all active interfaces but can be restricted to selected interfaces if desired. The ipcommand in particular can do many other things as well. The Windows command netsh interface ip show config also provides IP addresses. nslookup ,digandhost This trio of programs, all developed by the Internet Systems Consortium, are all used for DNS lookups. They differ in convenience and options. The oldest is nslookup, the one with the most options (by a rather wide margin) is dig, and the newest and arguably most convenient for normal usage is host. nslookup intronetworks.cs.luc.edu Non-authoritative answer: Name: intronetworks.cs.luc.edu Address: 162.216.18.28 dig intronetworks.cs.luc.edu ... ;; ANSWER SECTION: intronetworks.cs.luc.edu. 86400 IN A 162.216.18.28 ... host intronetworks.cs.luc.edu intronetworks.cs.luc.edu has address 162.216.18.28 intronetworks.cs.luc.edu has IPv6 address 2600:3c03::f03c:91ff:fe69:f438 See10.1.2 nslookup and dig. traceroute This lists the route from you to a remote host: traceroute intronetworks.cs.luc.edu 1 147.126.65.1 (147.126.65.1) 0.751 ms 0.753 ms 0.783 ms 2 147.126.95.54 (147.126.95.54) 1.319 ms 1.286 ms 1.253 ms 3 12.31.132.169 (12.31.132.169) 1.225 ms 1.231 ms 1.193 ms 4 cr83.cgcil.ip.att.net (12.123.7.46) 4.983 ms cr84.cgcil.ip.att.net (12. √£√ë123.7.170) 4.825 ms 4.812 ms(continues on next page) 36 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) 5 cr83.cgcil.ip.att.net (12.123.7.46) 4.926 ms 4.904 ms 4.888 ms 6 cr1.cgcil.ip.att.net (12.122.99.33) 5.043 ms cr2.cgcil.ip.att.net (12. √£√ë122.132.109) 5.343 ms 5.317 ms 7 gar13.cgcil.ip.att.net (12.122.132.121) 3.879 ms 18.347 ms ggr4.cgcil. √£√ëip.att.net (12.122.133.33) 2.987 ms 8 chi-b21-link.telia.net (213.248.87.253) 2.344 ms 2.305 ms 2.409 ms 9 nyk-bb2-link.telia.net (80.91.248.197) 24.065 ms nyk-bb1-link.telia.net √£√ë(213.155.136.70) 24.986 ms nyk-bb2-link.telia.net (62.115.137.58) 23.158 √£√ëms 10 nyk-b3-link.telia.net (62.115.112.255) 23.557 ms 23.548 ms nyk-b3-link. √£√ëtelia.net (80.91.248.178) 24.510 ms 11 netaccess-tic-133837-nyk-b3.c.telia.net (213.248.99.90) 23.957 ms 24. √£√ë382 ms 24.164 ms 12 0.e1-4.tbr1.mmu.nac.net (209.123.10.101) 24.922 ms 24.737 ms 24.754 ms 13 207.99.53.42 (207.99.53.42) 24.024 ms 24.249 ms 23.924 ms The last router (and intronetworks.cs.luc.edu itself) don‚Äôt respond to the traceroute packets, so the list is not quite complete. The Windows tracert utility is functionally equivalent. See 10.4.1 Traceroute and Time Exceeded for further information. Traceroute sends, by default, three probes for each router. Sometimes the responses do not all come back from the same router, as happened above at routers 4, 6, 7, 9 and 10. Router 9 sent back three distinct responses. On Linux systems the mtr command may be available as an alternative to traceroute; it repeats the traceroute at one-second intervals and generates cumulative statistics. route andnetstat The commands route ,route print (Windows), ip route show (Linux), and netstat -r (all systems) display the host‚Äôs local IP forwarding table. For workstations not acting as routers, this includes the route to the default router and, usually, not much else. The default route is sometimes listed as destination 0.0.0.0 with netmask 0.0.0.0 (equivalent to 0.0.0.0/0). The command netstat -a shows the existing TCP connections and open UDP sockets. netcat Thenetcat program, often called nc, allows the user to create TCP or UDP connections and send lines of text back and forth. It is seldom included by default. See 16.1.4 netcat and17.7.1 netcat again. WireShark This is a convenient combination of packet capture and packet analysis, from wireshark.org. See 17.4 TCP and WireShark and12.4 Using IPv6 and IPv4 Together for examples. WireShark was originally named Etherreal. An earlier command-line-only packet-capture program is tcpdump, though WireShark has greatly expanded support for packet-format decoding. Both WireShark and tcpdump support both live packet capture and reading from .pcap (packet capture) and .pcapng (next generation) Ô¨Åles. WireShark is the only non-command-line program listed here. It is sometimes desired to monitor packets on a remote system. If X-windows is involved ( egon Linux), this can be done by logging in from one‚Äôs local system using ssh -X, which enables X-windows forwarding, and then starting wireshark (or 1.14 Some Useful Utilities 37
An Introduction to Computer Networks, Release 2.0.11 perhapssudo wireshark ) from the command line. Other alternatives include tcpdump and tshark; the latter is part of the WireShark distribution and supports the same packet-decoding facilities as WireShark. Finally, there is termshark, a frontend for tshark that offers a terminal-based interface reasonably similar to WireShark‚Äôs graphical interface. 1.15 IETF and OSI The Internet protocols discussed above are deÔ¨Åned by the Internet Engineering Task Force, or IETF (under the aegis of the Internet Architecture Board, or IAB, in turn under the aegis of the Internet Society, ISOC). The IETF publishes ‚ÄúRequest For Comment‚Äù or RFC documents that contain all the formal Internet standards; these are available at http://www.ietf.org/rfc.html (note that, by the time a document appears here, the actual comment-requesting period is generally long since closed). The Ô¨Åve-layer model is closely associated with the IETF, though is not an ofÔ¨Åcial standard. RFC standards sometimes allow modest Ô¨Çexibility. With this in mind, RFC 2119 declares ofÔ¨Åcial understandings for the words MUST and SHOULD. A feature labeled with MUST is ‚Äúan absolute requirement for the speciÔ¨Åcation‚Äù, while the term SHOULD is used when there may exist valid reasons in particular circumstances to ignore a particular item, but the full implications must be understood and carefully weighed before choosing a different course. The original ARPANET network was developed by the US government‚Äôs Defense Advanced Research Projects Agency, or DARPA; it went online in 1969. The National Science Foundation began NSFNet in 1986; this largely replaced ARPANET. In 1991, operation of the NSFNet backbone was turned over to ANSNet, a private corporation. The ISOC was founded in 1992 as the NSF continued to retreat from the networking business. Hallmarks of the IETF design approach were David Clark‚Äôs declaration We reject: kings, presidents and voting. We believe in: rough consensus and running code. and RFC Editor Jon Postel‚Äôs Robustness Principle, here stated in its RFC 761 /RFC 793 form: TCP implementations should follow a general principle of robustness: be conservative in what you do, be liberal in what you accept from others. Postel‚Äôs aphorism is often shortened to ‚Äúbe liberal in what you accept, and conservative in what you send‚Äù. As such, it has come in for occasional criticism in recent years with regard to cryptographic protocols, for which lax enforcement can lead to security vulnerabilities. To be fair, however, Postel wrote this in an era when protocol speciÔ¨Åcations sometimes failed to fully spell out the rules for every possible situation, and too-strict implementations sometimes failed to interoperate. Just what should happen if a TCP packet arrives with the SYN bit set, for creating a new connection, and also the FIN bit, for terminating that connection? However, TCP speciÔ¨Åcations today are generally much more complete, and cryptographic protocols even moreso. One way to read Postel‚Äôs rule is that protocol implementations should be as strict as necessary, but no stricter. There is a persistent ‚Äì though false ‚Äì notion that the distributed-routing architecture of IP was due to a US Department of Defense mandate that the original ARPAnet be built to survive a nuclear attack. In fact, the developers of IP seemed unconcerned with this. However, Paul Baran did write, in his 1962 paper outlining the concept of packet switching, that 38 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 If [the number of stations] is made sufÔ¨Åciently large, it can be shown that highly survivable system structures can be built ‚Äì even in the thermonuclear era. In 1977 the International Organization for Standardization, or ISO, founded the Open Systems Interconnection project, or OSI, a process for creation of new network standards. OSI represented an attempt at the creation of networking standards independent of any individual government. The OSI project is today perhaps best known for its seven-layer networking model: between Transport and Application were inserted the Session andPresentation layers. The Session layer was to handle ‚Äúsessions‚Äù between applications (including the graceful closing of Transport-layer connections, something included in TCP, and the re-establishment of ‚Äúbroken‚Äù Transport-layer connections, which TCP could sorely use), and the Presentation layer was to handle things like deÔ¨Åning universal data formats ( egfor binary numeric data, or for non-ASCII character sets), and eventually came to include compression and encryption as well. Data presentation and session management are important concepts, but in many cases it has not proved necessary, or even particularly useful, to make them into true layers. The layer approach has been very helpful in organizing networking software into separate sections, but is hard to deÔ¨Åne precisely. One approach is to declare that a layer should communicate directly only with the layers immediately above and below it. The application passes its data to the Transport layer to receive the Transport header, which passes the data to the IP layer to receive the IP header, and on to the LAN layer. Each layer can be seen as an encapsulated software object, or module, by the layer above, and each layer in turn encapsulates the packet from its parent layer by adding a new header. This is mostly true for the Transport, IP and LAN layers, but there are irregularities: the transport-layer checksum, for example, needs information from the IP layer, and may in fact end up being calculated at the LAN layer ( 16.1.3.2 UDP and IP addresses and17.5 TCP OfÔ¨Çoading ). Even allowing for these kinds of irregularities, however, it is hard to justify full-Ô¨Çedged layer status for the Session and Presentation actions. What often happens is that the Application layer manages its own Transport connections, and is responsible for reading and writing data directly from and to the Transport layer. The application then uses conventional libraries for Presentation actions such as encryption, compression and format translation, and for Session actions such as handling broken Transport connections and multiplexing streams of data over a single Transport connection. Version 2 of the HTTP protocol, for example, contains a subprotocol for managing multiple streams; this is generally regarded as part of the Application layer, if for no other reason than that it is not available to any other application. An opposing view is that it is possible to view the SSL/TLS transport-encryption service, 29.5.2 TLS, as an example of a true Presentation layer. Applications generally read and write data directly to the SSL/TLS endpoint, which in turn mostly encapsulates (as a software module) the underlying TCP connection. The encapsulation is incomplete, though, in that SSL/TLS applications generally are responsible for creating their own Transport-layer (TCP) connections; see 29.5.3 A TLS Programming Example and the note at the end of 29.5.3.2 TLSserver. In the end, while the seven-layer model is reasonably popular, from a softwarearchitecture standpoint those two extra layers aren‚Äôt really in the same league as those of the Ô¨Åve-layer model. OSI has its own version of IP and TCP. The IP equivalent is CLNP, the ConnectionLess Network Protocol, although OSI also deÔ¨Ånes a connectionoriented protocol CMNS. The TCP equivalent is TP4; OSI also deÔ¨Ånes TP0 through TP3 but those are for connection-oriented networks. It seems clear that the primary reasons the OSI protocols failed in the marketplace were their ponderous bureaucracy for protocol management, their principle that protocols be completed before implementation began, and their insistence on rigid adherence to the speciÔ¨Åcations to the point of non-interoperability; indeed, Postel‚Äôs aphorism above may have been intended as a response to this last point. In contrast, the 1.15 IETF and OSI 39
An Introduction to Computer Networks, Release 2.0.11 IETF had (and still has) a ‚Äútwo working implementations‚Äù rule for a protocol to become a ‚ÄúDraft Standard‚Äù. From RFC 2026: A speciÔ¨Åcation from which at least two independent and interoperable implementations from different code bases have been developed, and for which sufÔ¨Åcient successful operational experience has been obtained, may be elevated to the ‚ÄúDraft Standard‚Äù level. [emphasis added] This rule has often facilitated the discovery of protocol design weaknesses early enough that the problems could be Ô¨Åxed. The OSI approach is a striking failure for the ‚Äúwaterfall‚Äù design model, when competing with the IETF‚Äôs cyclic ‚Äúprototyping‚Äù model. However, it is worth noting that the IETF has similarly been unable to keep up with rapid changes in html, particularly at the browser end; the OSI mistakes were mostly evident only in retrospect. Trying to Ô¨Åt protocols into speciÔ¨Åc layers is often both futile and irrelevant. By one perspective, the RealTime Protocol RTP lives at the Transport layer, but just above the UDP layer; others have put RTP into the Application layer. Parts of the RTP protocol resemble the Session and Presentation layers. A key component of the IP protocol is the set of various router-update protocols; some of these freely use higher-level layers. Similarly, tunneling might be considered to be a Link-layer protocol, but tunnels are often created and maintained at the Application layer. A sometimes-more-successful approach to understanding ‚Äúlayers‚Äù is to view them instead as parts of a protocol graph. Thus, in the following diagram we have two protocol sublayers within the transport layer (UDP and RTP), and one protocol (ARP) not easily assigned to a layer. IP ATM LANEthernet LANARP ???TCP transportUDP transportRTP transport Fig. 7:: A protocol graph TCP and UDP/RTP are above IP, and ATM and Ethernet are below Note the thin ‚Äúwasp‚Äù waist at the IP layer: there are many protocols above and below, but only one protocol at the IP layer. This universality of the IP layer has been one of the things contributing to IP‚Äôs success. 40 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 1.16 Berkeley Unix Though not ofÔ¨Åcially tied to the IETF, the Berkeley Unix releases became de facto reference implementations for most of the TCP/IP protocols. 4.1BSD (BSD for Berkeley Software Distribution) was released in 1981, 4.2BSD in 1983, 4.3BSD in 1986, 4.3BSD-Tahoe in 1988, 4.3BSD-Reno in 1990, and 4.4BSD in 1994. Descendants today include FreeBSD, OpenBSD and NetBSD. The TCP implementations TCP Tahoe and TCP Reno ( 19 TCP Reno and Congestion Management ) took their names from the corresponding 4.3BSD releases. 1.17 Epilog This completes our tour of the basics. In the remaining chapters we will expand on the material here. 1.18 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.1 Solutions for An Overview of Networks. 1.0. Give forwarding tables for each of the switches S1-S4 in the following network with destinations A, B, C, D. For the next_hop column, give the neighbor on the appropriate link rather than the interface number. A B C S1 S2 S3 S4D 2.0. Give forwarding tables for each of the switches S1-S4 in the following network with destinations A, B, C, D. Again, use the neighbor form of next_hop rather than the interface form. Try to keep the route to each destination as short as possible. What decision has to be made in this exercise that did not arise in the preceding exercise? AS1 S2B DS4 S3C 3.0. In the network of the previous exercise, suppose that destinations directly connected to an immediate neighbor are always reached via that neighbor; egS1‚Äôs forwarding table will always include xB,S2yand xD,S4y. This leaves only routes to the diagonally opposite nodes undetermined ( egS1 to C). Show that, no matter which next_hop entries are chosen for the diagonally opposite destinations, no routing loops can ever be formed. (Hint: the number of links to any diagonally opposite switch is always 2.) 4.0.‚ô¢Give forwarding tables for each of the switches A-E in the following network. Destinations are A-E themselves. Keep all route lengths the minimum possible (one hop for an immediate neighbor, two hops for 1.16 Berkeley Unix 41
An Introduction to Computer Networks, Release 2.0.11 everything else). If a destination is an immediate neighbor, you may list its next_hop as direct orlocal for simplicity. Indicate destinations for which there is more than one choice for next_hop. A DC EB 1 11 11 1 5.0. Consider the following arrangement of switches and destinations. Give forwarding tables (in neighbor form) for S1-S4 that include default forwarding entries; the default entries should point toward S5. The default entries will thus automatically forward to the ‚Äúpossible other destinations‚Äù shown below right. Eliminate all table entries that are implied by the default entry (that is, if the default entry is to S3, eliminate all other entries for which the next hop is S3). AS1 D CS3 S4 S5... possible other destinations E BS2 6.0. Four switches are arranged as below. The destinations are S1 through S4 themselves. S1 S2 S4 S3 (a). Give the forwarding tables for S1 through S4 assuming packets to adjacent nodes are sent along the connecting link, and packets to diagonally opposite nodes are sent clockwise. (b). Give the forwarding tables for S1 through S4 assuming the S1‚ÄìS4 link is not used at all, not even for S1√ê√ëS4 trafÔ¨Åc. 7.0. Suppose we have switches S1 through S4; the forwarding-table destinations are the switches themselves. The tables for S2 and S3 are as below, where the next_hop value is speciÔ¨Åed in neighbor form: S2:xS1,S1yxS3,S3yxS4,S3y S3:xS1,S2yxS2,S2yxS4,S4y From the above we can conclude that S2 must be directly connected to both S1 and S3 as its table lists them as next_hops; similarly, S3 must be directly connected to S2 and S4. 42 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 (a). The given tables are consistent with the network diagrammed in exercise 6.0. Are the tables also consistent with a network in which S1 and S4 are notdirectly connected? If so, give such a network; if not, explain why S1 and S4 must be connected. (b). Now suppose S3‚Äôs table is changed to the following. Find a network layout consistent with these tables in which S1 and S4 are not directly connected. Do not add additional switches. S3:xS1,S4yxS2,S2yxS4,S4y While the table for S4 is not given, you may assume that forwarding does work correctly. However, you should notassume that paths are the shortest possible. Hint: It follows from the S3 table above that the path from S3 to S1 starts S3 √ù√ë S4; how will this path continue? The next switch along the path cannot be S1, because of the hypothesis that S1 and S4 are not directly connected. 8.0. (a) Suppose a network is as follows, with the only path from A to C passing through B: ...ABC... Explain why a single routing loop cannot include both A and C. Hint: if the loop involves destination D, how does B forward to D? (b). Suppose a routing loop follows the path A S1S2.. . SnA, where none of the S iare equal to A. Show that all the S imust be distinct. (A corollary of this is that any routing loop created by datagramforwarding either involves forwarding back and forth between a pair of adjacent switches, or else involves an actual graph cycle in the network topology; linear loops of length greater than 1 are impossible.) 9.0. Consider the following arrangement of switches: S1S4S10AE S2S5S11B S3S6S12CDF Suppose S1-S6 have the forwarding tables below. For each of the following destinations, suppose a packet is sent to the destination from S1. (a). A (b). B (c). C (d).‚ô¢D (e). E (f). F Give the switches the packet will pass through, including the initial switch S1, up until the Ô¨Ånal switch S10-S12. 1.18 Exercises 43
An Introduction to Computer Networks, Release 2.0.11 S1: (A,S4), (B,S2), (C,S4), (D,S2), (E,S2), (F,S4) S2: (A,S5), (B,S5), (D,S5), (E,S3), (F,S3) S3: (B,S6), (C,S2), (E,S6), (F,S6) S4: (A,S10), (C,S5), (E,S10), (F,S5) S5: (A,S6), (B,S11), (C,S6), (D,S6), (E,S4), (F,S2) S6: (A,S3), (B,S12), (C,S12), (D,S12), (E,S5), (F,S12) 10.0. Suppose a set of nodes A-F and switches S1-S6 are connected as shown. AS15S4D 1 1 BS22S5E 8 1 CS34S6F The links between switches are labeled with weights, which are used by some routing applications. The weights represent the cost of using that link. You are to Ô¨Ånd the path through S1-S6 with lowest total cost (that is, with smallest sum of weights), for each of the following transmissions. For example, the lowest-cost path from A to E is A‚ÄìS1‚ÄìS2‚ÄìS5‚ÄìE, for a total cost of 1+2=3; the alternative path A‚ÄìS1‚ÄìS4‚ÄìS5‚ÄìE has total cost 5+1=6. (a).‚ô¢A√ëF (b). A√ëD (c). A√ëC (d). Give the complete forwarding table for S 2, where all routes are selected for lowest total cost. 11.0. In exercise 7.0, the routes taken by packets A-D are reasonably direct, but the routes for E and F are rather circuitous. (a). Assign weights to the seven links S1‚ÄìS2, S2‚ÄìS3, S1‚ÄìS4, S2‚ÄìS5, S3‚ÄìS6, S4‚ÄìS5 and S5‚ÄìS6, as in exercise 10.0, so that destination E‚Äôs route in exercise 9.0 becomes the optimum (lowest total link weight) path. (b). Assign weights to the seven links that make destination F‚Äôs route in exercise 9.0 optimal. (This will be a different set of weights from part (a).) Hint: you can do this by assigning a weight of 1 to all links except to one or two ‚Äúbad‚Äù links; the ‚Äúbad‚Äù links get a weight of 10. In each of (a) and (b) above, the route taken will be the route that avoids all the ‚Äúbad‚Äù 44 1 An Overview of Networks
An Introduction to Computer Networks, Release 2.0.11 links. You must treat (a) entirely differently from (b); there is no assignment of weights that can account for both routes. 12.0. Suppose we have the following three Class C IP networks, joined by routers R1‚ÄìR4. There is no connection to the outside Internet. Give the forwarding table for each router. For networks directly connected to a router ( eg200.0.1/24 and R1), include the network in the table but list the next hop as direct orlocal. R1200.0.1/24 R4 R2200.0.2/24 R3200.0.3/24 1.18 Exercises 45
An Introduction to Computer Networks, Release 2.0.11 46 1 An Overview of Networks
2 ETHERNET BASICS We now turn to a deeper analysis of the ubiquitous Ethernet LAN protocol. In this chapter we cover the more universal Ethernet concepts, such as would be encountered in any residential or small-ofÔ¨Åce Ethernet setting and including switching and learning. The following chapter covers more advanced features, such as the spanning-tree algorithm, virtual LANs, Ethernet hardware, TRILL/SPB and software-deÔ¨Åned networking. Current user-level Ethernet today (2020) is usually 100 Mbps or Gigabit, with Gigabit and 10 Gigabit Ethernet standard in server rooms and backbones. However, because the potential for packet collisions makes Ethernet speeds scale in odd ways, we will start with the 10 Mbps formulation. While the 10 Mbps speed is obsolete, and while even the Ethernet collision mechanism is largely obsolete, collision management itself continues to play a signiÔ¨Åcant role in wireless networks. The original Ethernet speciÔ¨Åcation was the 1976 paper of Metcalfe and Boggs, [MB76]. The data rate was 10 megabits per second, and all connections were made with coaxial cable instead of today‚Äôs twisted pair. The authors described their passive architecture as follows: We cannot afford the redundant connections and dynamic routing of store-and-forward packet switching to assure reliable communication, so we choose to achieve reliability through simplicity. We choose to make the shared communication facility passive so that the failure of an active element will tend to affect the communications of only a single station. Classic Ethernet was indeed simple, and ‚Äì mostly ‚Äì passive. In its most basic form, the Ethernet medium was one long piece of coaxial cable, onto which stations could be connected via taps. If two stations happened to transmit at the same time ‚Äì most likely because they were both waiting for a third station to Ô¨Ånish ‚Äì their signals were lost to the resultant collision. The only active components besides the stations were repeaters, originally intended simply to make end-to-end joins between cable segments. Repeaters soon evolved into multiport devices, allowing the creation of arbitrary tree (that is, loop-free) topologies. At this point the standard wiring model shifted from one long cable, snaking from host to host, to a ‚Äústar‚Äù network, where each host connected directly to a central multipoint repeater. This shift allowed for the replacement of expensive coaxial cable by the much-cheaper twisted pair; links could not be as long, but they did not need to be. Repeaters, which forwarded collisions, soon gave way to switches, which did not ( 2.4 Ethernet Switches ). Switches thus partitioned an Ethernet into disjoint collision domains, or physical Ethernets, through which collisions could propagate; an aggregation of physical Ethernets connected by switches was then sometimes known as a virtual Ethernet. Collision domains became smaller and smaller, eventually down to individual links and then vanishing entirely. Throughout all these early changes, Ethernet never implemented true redundant connections, in that at any one instant the topology was always required to be loop-free. However, by 1985 Ethernet did adopt a mechanism by which idle backup links can quickly be placed into service after a primary link fails; 3.1 Spanning Tree Algorithm and Redundancy. Finally, in the early part of this century, support for redundant connections (and looping topologies) arrived in the form of TRILL and SPB ( 3.3 TRILL and SPB). 47
An Introduction to Computer Networks, Release 2.0.11 2.1 10-Mbps Classic Ethernet Originally, Ethernet consisted of a long piece of cable (possibly spliced by repeaters ). When a station transmitted, the data went everywhere along that cable. Such an arrangement is known as a broadcast bus; all packets were, at least at the physical layer, broadcast onto the shared medium and could be seen, theoretically, by all other nodes. Logically, however, most packets would appear to be transmitted point-topoint, not broadcast. This was because between each station CPU and the cable there was a peripheral device (that is, a card) known as a network interface, which would take care of the details of transmitting and receiving. The network interface would (and still does) decide when a received packet should be forwarded to the host, via a CPU interrupt. A B C D NI NI NI NI Fig. 8:: Hosts and cable, with network interfaces in between Whenever two stations transmitted at the same time, the signals would collide, and interfere with one another; both transmissions would fail as a result. Proper handling of collisions was an essential part of the access-mediation strategy for the shared medium. In order to minimize collision loss, each station implemented the following: 1. Before transmission, wait for the line to become quiet 2. While transmitting, continually monitor the line for signs that a collision has occurred; if a collision is detected, cease transmitting 3. If a collision occurs, use a backoff-and-retransmit strategy These properties can be summarized with the CSMA/CD acronym: Carrier Sense, Multiple Access, Collision Detect. (The term ‚Äúcarrier sense‚Äù was used by Metcalfe and Boggs as a synonym for ‚Äúsignal sense‚Äù; there is no literal carrier frequency to be sensed.) It should be emphasized that collisions are a normal event in Ethernet, well-handled by the mechanisms above. IEEE 802 Network Standards The IEEE network standards all begin with 802: 802.3 is Ethernet, 802.11 is Wi-Fi, 802.16 is WiMAX, and there are many others. One sometimes encounters the claim that 802 represents the date of an early meeting: February 1980. However, the IEEE has a continuous stream of standards (with occasional gaps): 799: Handling and Disposal of Transformer PCBs, 800: D-C Aircraft Rotating Machines, 803: Recommended Practice for Unique IdentiÔ¨Åcation in Power Plants, etc. Classic Ethernet came in version 1 [1980, DEC-Intel-Xerox], version 2 [1982, DIX], and IEEE 802.3. There are some minor electrical differences between these, and one rather substantial packet-format difference, below. In addition to these, the Berkeley Unix trailing-headers packet format was used for a while. 48 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 There were three physical formats for 10 Mbps Ethernet cable: thick coax (10BASE-5), thin coax (10BASE2), and, last to arrive, twisted pair (10BASE-T). Thick coax was the original; economics drove the successive development of the later two. The cheaper twisted-pair cabling eventually almost entirely displaced coax, at least for host connections. The original speciÔ¨Åcation included support for repeaters, which were in effect signal ampliÔ¨Åers although they might attempt to clean up a noisy signal. Repeaters processed each bit individually and did no buffering. In the telecom world, a repeater might be called a digital regenerator. A repeater with more than two ports was commonly called a hub; hubs allowed branching and thus much more complex topologies. It was the rise of hubs that enabled star topologies in which each host connects directly to the hub rather than to one long run of coax. This in turn enabled twisted-pair cable: while this supported maximum runs of about 100 meters, versus the 500 meters of thick coax, each run simply had to go from the host to the central hub in the wiring closet. This was much more convenient than having to snake coax all around the building. A hub failure would bring the network down, but hubs proved largely reliable. Bridges ‚Äì later known as switches ‚Äì came along a short time later. While repeaters act at the bit layer, a switch reads in and forwards an entire packet as a unit, and the destination address is consulted to determine to where the packet is forwarded. Except for possible collision-related performance issues, hubs and switches are interchangeable. Eventually, most wiring-closet hubs were replaced with switches. Hubs propagate collisions; switches do not. If the signal representing a collision were to arrive at one port of a hub, it would, like any other signal, be retransmitted out all other ports. If a switch were to detect a collision on one port, no other ports would be involved; only packets received successfully are ever retransmitted out other ports. Originally, switches were seen as providing interconnection (‚Äúbridging‚Äù) between separate physical Ethernets; a switch for such a purpose needed just two ports. Later, a switched Ethernet was seen as one large ‚Äúvirtual‚Äù Ethernet, composed of smaller collision domains. Although the term ‚Äúswitch‚Äù is now much more common than ‚Äúbridge‚Äù, the latter is still in use, particularly by the IEEE. For some, a switch is a bridge with more than two ports, though that distinction is relatively meaningless as it has been years since two-port bridges were last manufactured. We return to switching below in 2.4 Ethernet Switches. In the original thick-coax cabling, connections were made via taps, often literally drilled into the coax central conductor. Thin coax allowed the use of T-connectors to attach hosts. Twisted-pair does not allow mid-cable attachment; it is only used for point-to-point links between hosts, switches and hubs. Midcable attachment, however, was always simply a way of avoiding the need for active devices like hubs and switches. There is still a role for hubs today when one wants to monitor the Ethernet signal from A to B ( egfor intrusion detection analysis), although some switches now also support a form of monitoring. All three cable formats could interconnect, although only through repeaters and hubs, and all used the same 10 Mbps transmission speed. While twisted-pair cable is still used by 100 Mbps Ethernet, it generally needs to be a higher-performance version known as Category 5, versus the 10 Mbps Category 3. Data in 10 Mbps Ethernets was transmitted using Manchester encoding; see 6.1.3 Manchester. This meant that the electronics had to operate, in effect, at 20 Mbps. Faster Ethernets use different encodings. 2.1 10-Mbps Classic Ethernet 49
An Introduction to Computer Networks, Release 2.0.11 2.1.1 Ethernet Packet Format Here is the format of a typical Ethernet packet (DIX speciÔ¨Åcation); it is still used for newer, faster Ethernets: dest addr src addr type data CRC Fig. 9:: Ethernet packet with header Ô¨Åelds The destination and source addresses are 48-bit quantities; the type is 16 bits, the data length is variable up to a maximum of 1500 bytes, and the Ô¨Ånal CRC checksum is 32 bits. The checksum is added by the Ethernet hardware, never by the host software. There is also a preamble, not shown: a block of 1 bits followed by a 0, in the front of the packet, for synchronization. The type Ô¨Åeld identiÔ¨Åes the next higher protocol layer; a few common type values are 0x0800 = IP, 0x8137 = IPX, 0x0806 = ARP. The IEEE 802.3 speciÔ¨Åcation replaced the type Ô¨Åeld by the length Ô¨Åeld, though this change never caught on. The two formats can be distinguished as long as the type values used are larger than the maximum Ethernet length of 1500 (or 0x05dc); the type values given in the previous paragraph all meet this condition. The Ethernet maximum packet length of 1500 bytes worked well in the past, but can seem inconveniently small at 10 Gbit speeds. But 1500 bytes has become the de facto maximum packet size throughout the Internet, not just on Ethernet LANs; increasing it would be difÔ¨Åcult. TCP TSO ( 17.5 TCP OfÔ¨Çoading ) is one alternative. Each Ethernet card has a (hopefully unique) physical address in ROM; by default any packet sent to this address will be received by the board and passed up to the host system. Packets addressed to other physical addresses will be seen by the card, but ignored (by default). All Ethernet devices also agree on a broadcast address of all 1‚Äôs: a packet sent to the broadcast address will be delivered to all attached hosts. It is sometimes possible to change the physical address of a given card in software. It is almost universally possible to put a given card into promiscuous mode, meaning that all packets on the network, no matter what the destination address, are delivered to the attached host. This mode was originally intended for diagnostic purposes but became best known for the security breach it opens: it was once not unusual to Ô¨Ånd a host with network board in promiscuous mode and with a process collecting the Ô¨Årst 100 bytes (presumably including userid and password) of every telnet connection. 2.1.2 Ethernet Multicast Another category of Ethernet addresses is multicast, used to transmit to a setof stations; streaming video to multiple simultaneous viewers might use Ethernet multicast. The lowest-order bit in the Ô¨Årst byte of an address indicates whether the address is physical or multicast. To receive packets addressed to a given multicast address, the host must inform its network interface that it wishes to do so; once this is done, any arriving packets addressed to that multicast address are forwarded to the host. The set of subscribers to a given multicast address may be called a multicast group. While higher-level protocols might prefer that the subscribing host also notiÔ¨Åes some other host, egthe sender, this is not required, although that might be the easiest way to learn the multicast address involved. If several hosts subscribe to the same multicast address, then each will receive a copy of each multicast packet transmitted. 50 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 We are now able to list all cases in which a network interface forwards a received packet up to its attached host: 
- if the destination address of the received packet matches the physical address of the interface 
- if the destination address of the received packet is the broadcast address 
- if the interface is in promiscuous mode 
- if the destination address of the received packet is a multicast address and the host has told the network interface to accept packets sent to that multicast address If switches (below) are involved, they must normally forward multicast packets on all outbound links, exactly as they do for broadcast packets; switches have no obvious way of telling where multicast subscribers might be. To avoid this, some switches do try to engage in some form of multicast Ô¨Åltering, sometimes by snooping on higher-layer multicast protocols. Multicast Ethernet is seldom used by IPv4, but plays a larger role in IPv6 conÔ¨Åguration. 2.1.3 Ethernet Address Internal Structure The second-to-lowest-order bit of a physical Ethernet address indicates whether that address is believed to be globally unique or if it is only locally unique; this is known as the Universal/Local bit. For real Ethernet physical addresses, the multicast and universal/local bits of the Ô¨Årst byte should both be 0. At the physical layer, Ethernet transmits the bits of each byte from low-order to high-order (as do many other serial protocols such as HDLC ( 6.1.5.1 HDLC ) and the ubiquitous last-century serial protocol RS-232); this means that these two Ô¨Çag bits are in fact the Ô¨Årst address bits to be transmitted. It is not clear whether this was ever important. When (global) Ethernet IDs are assigned to physical Ethernet cards by the manufacturer, the Ô¨Årst three bytes serve to indicate the manufacturer. They are allocated by the IEEE, and are ofÔ¨Åcially known asorganizationally unique identiÔ¨Åers. These can be looked up at any of several sites on the Internet to identify the manufacturer associated with any given Ethernet address; the ofÔ¨Åcial IEEE site is standards.ieee.org/develop/regauth/oui/public.html (OUIs must be entered here without colons). As long as the manufacturer involved is diligent in assigning the second three bytes, every manufacturerprovided Ethernet address should be globally unique. Lapses, however, are not unheard of. Ethernet addresses for virtual machines must be distinct from the Ethernet address of the host system, and may be ( egwith so-called ‚Äúbridged‚Äù conÔ¨Ågurations) as visible on the LAN as that host system‚Äôs address. The Ô¨Årst three bytes of virtual Ethernet addresses are often taken from the OUI assigned to the manufacturer whose card is being emulated; the last three bytes are then either set randomly or via conÔ¨Åguration. In principle, the universal/local bit should be 1, as the address is only locally unique, but this is often ignored. It is entirely possible for virtual Ethernet addresses to be assigned so as to have some local meaning, though this appears not to be common. 2.1.4 The LAN Layer The LAN layer, at its upper end, supplies to the network layer a mechanism for addressing a packet and sending it from one station to another. At its lower end, it handles interactions with the physical layer. The 2.1 10-Mbps Classic Ethernet 51
An Introduction to Computer Networks, Release 2.0.11 LAN layer covers packet addressing, delivery and receipt, forwarding, error detection, collision detection and collision-related retransmission attempts. In IEEE protocols, the LAN layer is divided into the media access control, or MAC, sublayer and a higher logical link control, or LLC, sublayer for higher-level Ô¨Çow-control functions. The LLC layer is deÔ¨Åned in IEEE standard 802.2 (versus 802.3 for the MAC layer), and is applicable to non-Ethernet LANs as well. For Ethernet, many of the LLC functions have been almost entirely supplanted by Ô¨Çow-control at the Transport layer ( egTCP). The LLC layer deÔ¨Ånes (but does not mandate) a form of the HDLC protocol ( 6.1.5.1 HDLC ), which in turn supports sliding windows ( 8.2 Sliding Windows ) as an option. The much-earlier X.25 LAN protocol also offered partial support for HDLC. Similarly, ATM, 5.5 Asynchronous Transfer Mode: ATM, supports some higher-level transport-like functions, though not sliding windows. Because use of the LLC layer is so often insigniÔ¨Åcant, and because the most well-known LAN-layer functions are in fact part of the MAC sublayer, it is common to identify the LAN layer with its MAC sublayer, especially for IEEE protocols where the MAC layer has ofÔ¨Åcial standing. In particular, LAN-layer addresses are perhaps most often called MAC addresses. Generally speaking, much of the operation of the LAN/MAC layer takes place in the network card. Host systems (including drivers) are, for example, generally oblivious to collisions (although they may query the card for collision statistics). In some cases, egwith Wi-Fi rate scaling ( 4.2.2 Dynamic Rate Scaling ), the host-system driver may get involved. 2.1.5 The Slot Time and Collisions Thediameter of an Ethernet is the maximum distance between any pair of stations. The actual total length of cable can be much greater than this, if, for example, the topology is a ‚Äústar‚Äù conÔ¨Åguration. The maximum allowed diameter, measured in bits, is limited to 232 (a sample ‚Äúbudget‚Äù for this is below). This makes the round-trip-time 464 bits. As each station involved in a collision discovers it, it transmits a special jam signal of up to 48 bits. These 48 jam bits bring the total above to 512 bits, or 64 bytes. The time to send these 512 bits is the slot time of an Ethernet; time intervals on Ethernet are often described in bit times but in conventional time units the slot time is 51.2 ¬µsec. The value of the slot time determines several subsequent aspects of Ethernet. If a station has transmitted for one slot time, then no collision can occur (unless there is a hardware error) for the remainder of that packet. This is because one slot time is enough time for any other station to have realized that the Ô¨Årst station has started transmitting, so after that time they will wait for the Ô¨Årst station to Ô¨Ånish. Thus, after one slot time a station is said to have acquired the network. The slot time is also used as the basic interval for retransmission scheduling, below. Conversely, a collision canbe received, in principle, at any point up until the end of the slot time. As a result, Ethernet has a minimum packet size, equal to the slot time, ie64 bytes (or 46 bytes in the data portion). A station transmitting a packet this size is assured that ifa collision were to occur, the sender would detect it (and be able to apply the retransmission algorithm, below). Smaller packets might collide and yet the sender not know it, ultimately leading to greatly reduced throughput. If we need to send less than 46 bytes of data (for example, a 40-byte TCP ACK packet), the Ethernet packet must be padded out to the minimum length. As a result, all protocols running on top of Ethernet need to provide some way to specify the actual data length, as it cannot be inferred from the received packet size. As a speciÔ¨Åc example of a collision occurring as late as possible, consider the diagram below. A and B are 52 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 5 units apart, and the bandwidth is 1 byte/unit. A begins sending ‚Äúhelloworld‚Äù at T=0; B starts sending just as A‚Äôs message arrives, at T=5. B has listened before transmitting, but A‚Äôs signal was not yet evident. A doesn‚Äôt discover the collision until 10 units have elapsed, which is twice the distance. T=0 A B T=1 A B T=2 A B T=3 A B T=4 A B T=4.99 A B T=5 A B T=6 A Bh eh eh l e hl l ll o ol le ehhA just starting to send just before collision, B sees line is idle B transmits; COLLISION! T=7 A B T=8 A B T=9 A B T=10 A Bow l l wo o or lcollision propagates back to A A detects the collisiond Fig. 10:: Ethernet collision, showing one full RTT before collision detectability Here are typical maximum values for the delay in 10 Mbps Ethernet due to various components. These are taken from the Digital-Intel-Xerox (DIX) standard of 1982, except that ‚Äúpoint-to-point link cable‚Äù is replaced by standard cable. The DIX speciÔ¨Åcation allows 1500m of coax with two repeaters and 1000m of point-to-point cable; the table below shows 2500m of coax and four repeaters, following the later IEEE 802.3 Ethernet speciÔ¨Åcation. Some of the more obscure delays have been eliminated. Entries are one-way delay times, in bits. The maximum path may have four repeaters, and ten transceivers (simple electronic devices between the coax cable and the NI cards), each with its drop cable (two transceivers per repeater, plus one at each endpoint). Ethernet delay budget item length delay, in bits explanation (c = speed of light) coax 2500 m 110 bits 23 meters/bit (.77c) transceiver cables 500 m 25 bits 19.5 meters/bit (.65c) transceivers 40 bits, max 10 units 4 bits each repeaters 25 bits, max 4 units 6+ bits each (DIX 7.6.4.1) encoders 20 bits, max 10 units 2 bits each (for signal generation) The total here is 220 bits; in a full accounting it would be 232. Some of the numbers shown are a little high, but there are also signal rise time delays, sense delays, and timer delays that have been omitted. It works out fairly closely. Implicit in the delay budget table above is the ‚Äúlength‚Äù of a bit. The speed of propagation in copper is about 0.77c, where c=3108m/sec = 300 m/¬µsec is the speed of light in vacuum. So, in 0.1 microseconds (the time to send one bit at 10 Mbps), the signal propagates approximately 0.77 c10-7= 23 meters. 2.1 10-Mbps Classic Ethernet 53
An Introduction to Computer Networks, Release 2.0.11 Ethernet packets also have a maximum packet size, of 1500 bytes. This limit is primarily for the sake of fairness, so one station cannot unduly monopolize the cable (and also so stations can reserve buffers guaranteed to hold an entire packet). At one time hardware vendors often marketed their own incompatible ‚Äúextensions‚Äù to Ethernet which enlarged the maximum packet size to as much as 4 kB. There is no technical reason, actually, not to do this, except compatibility. The signal loss in any single segment of cable is limited to 8.5 db, or about 14% of original strength. Repeaters will restore the signal to its original strength. The reason for the per-segment length restriction is that Ethernet collision detection requires a strict limit on how much the remote signal can be allowed to lose strength. It is possible for a station to detect and reliably read very weak remote signals, but not at the same time that it is transmitting locally. This is exactly what must be done, though, for collision detection to work: remote signals must arrive with sufÔ¨Åcient strength to be heard even while the receiving station is itself transmitting. The per-segment limit, then, has nothing to do with the overall length limit; the latter is set only to ensure that a sender is guaranteed of detecting a collision, even if it sends the minimum-sized packet. 2.1.6 Exponential Backoff Algorithm Whenever there is a collision the exponential backoff algorithm ‚Äì operating at the MAC layer ‚Äì is used to determine when each station will retry its transmission. Backoff here is called exponential because the range from which the backoff value is chosen is doubled after every successive collision involving the same packet. Here is the full Ethernet transmission algorithm, including backoff and retransmissions: 1. Listen before transmitting (‚Äúcarrier detect‚Äù) 2. If line is busy, wait for sender to stop and then wait an additional 9.6 microseconds (96 bits). One consequence of this is that there is always a 96-bit gap between packets, so packets do not run together. 3. Transmit while simultaneously monitoring for collisions 4. If a collision does occur, send the jam signal, and choose a backoff time as follows: For transmission N, 1¬§N¬§10 (N=0 represents the original attempt), choose k randomly with 0 ¬§k < 2N. Wait k slot times (k51.2 ¬µsec). Then check if the line is idle, waiting if necessary for someone else to Ô¨Ånish, and then retry step 3. For 11 ¬§N¬§15, choose k randomly with 0 ¬§k < 1024 (= 210) 5. If we reach N=16 (16 transmission attempts), give up. If an Ethernet sender does not reach step 5, there is a very high probability that the packet was delivered successfully. Exponential backoff means that if two hosts have waited for a third to Ô¨Ånish and transmit simultaneously, and collide, then when N=1 they have a 50% chance of recollision; when N=2 there is a 25% chance, etc. When N¬•10 the maximum wait is 52 milliseconds; without this cutoff the maximum wait at N=15 would be 1.5 seconds. As indicated above in the minimum-packet-size discussion, this retransmission strategy assumes that the sender is able to detect the collision while it is still sending, so it knows that the packet must be resent. In the following diagram is an example of several stations attempting to transmit all at once, and using the above transmission/backoff algorithm to sort out who actually gets to acquire the channel. We assume we have Ô¨Åve prospective senders A1, A2, A3, A4 and A5, all waiting for a sixth station to Ô¨Ånish. We will assume that collision detection always takes one slot time (it will take much less for nodes closer together) 54 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 and that the slot start-times for each station are synchronized; this allows us to measure time in slots. A solid arrow at the start of a slot means that sender began transmission in that slot; a red X signiÔ¨Åes a collision. If a collision occurs, the backoff value k is shown underneath. A dashed line shows the station waiting k slots for its next attempt. A1 A2 A3 A4 A5Slot 1 Slot 2 Slot 3 Slot 4 Slot 5 Slot 6 k=1 k=1 k=0 k=0 k=1k=3 k=0T=1 T=0 T=2 T=3 T=4 T=5 k=6 k=3k=1k=2: Attempt to transmit: collision Fig. 11:: Ethernet collisions with exponential backoff At T=0 we assume the transmitting station Ô¨Ånishes, and all the Ai transmit and collide. At T=1, then, each of the Ai has discovered the collision; each chooses a random k<2. Let us assume that A1 chooses k=1, A2 chooses k=1, A3 chooses k=0, A4 chooses k=0, and A5 chooses k=1. Those stations choosing k=0 will retransmit immediately, at T=1. This means A3 and A4 collide again, and at T=2 they now choose random k<4. We will Assume A3 chooses k=3 and A4 chooses k=0; A3 will try again at T=2+3=5 while A4 will try again at T=2, that is, now. At T=2, we now have the original A1, A2, and A5 transmitting for the second time, while A4 trying again for the third time. They collide. Let us suppose A1 chooses k=2, A2 chooses k=1, A5 chooses k=3, and A4 chooses k=6 (A4 is choosing k<8 at random). Their scheduled transmission attempt times are now A1 at T=3+2=5, A2 at T=4, A5 at T=6, and A4 at T=9. At T=3, nobody attempts to transmit. But at T=4, A2 is the only station to transmit, and so successfully seizes the channel. By the time T=5 rolls around, A1 and A3 will check the channel, that is, listen Ô¨Årst, and wait for A2 to Ô¨Ånish. At T=9, A4 will check the channel again, and also begin waiting for A2 to Ô¨Ånish. A maximum of 1024 hosts is allowed on an Ethernet. This number apparently comes from the maximum range for the backoff time as 0 ¬§k < 1024. If there are 1024 hosts simultaneously trying to send, then, once the backoff range has reached k<1024 (N=10), we have a good chance that one station will succeed in seizing the channel, that is; the minimum value of all the random k‚Äôs chosen will be unique. This backoff algorithm is not ‚Äúfair‚Äù, in the sense that the longer a station has been waiting to send, the lower its priority sinks. Newly transmitting stations with N=0 need not delay at all. The Ethernet capture effect, below, illustrates this unfairness. 2.1 10-Mbps Classic Ethernet 55
An Introduction to Computer Networks, Release 2.0.11 2.1.7 Capture effect The capture effect is a scenario illustrating the potential lack of fairness in the exponential backoff algorithm. The unswitched Ethernet must be fully busy, in that each of two senders always has a packet ready to transmit. Let A and B be two such busy nodes, simultaneously starting to transmit their Ô¨Årst packets. They collide. Suppose A wins, and sends. When A is Ô¨Ånished, B tries to transmit again. But A has a second packet, and so A tries too. A chooses a backoff k<2 (that is, between 0 and 1 inclusive), but since B is on its second attempt it must choose k<4. This means A is favored to win. Suppose it does. After that transmission is Ô¨Ånished, A and B try yet again: A on its Ô¨Årst attempt for its third packet, and B on its third attempt for its Ô¨Årst packet. Now A again chooses k<2 but B must choose k<8; this time A is much more likely to win. Each time B fails to win a given backoff, its probability of winning the next one is reduced by about 1/2. It is quite possible, and does occur in practice, for B to lose allthe backoffs until it reaches the maximum of N=16 attempts; once it has lost the Ô¨Årst three or four this is in fact quite likely. At this point B simply discards the packet and goes on to the next one with N reset to 1 and k chosen from {0,1}. The capture effect can be Ô¨Åxed with appropriate modiÔ¨Åcation of the backoff algorithm; the Binary Logarithmic Arbitration Method (BLAM) was proposed in [MM94]. The BLAM algorithm was considered for the then-nascent 100 Mbps Fast Ethernet standard. But in the end a hardware strategy won out: Fast Ethernet supports ‚Äúfull-duplex‚Äù mode which is collision-free (see 2.2 100 Mbps (Fast) Ethernet, below). While Fast Ethernet continues to support the original ‚Äúhalf-duplex‚Äù mode, it was assumed that any sites concerned enough about performance to be worried about the capture effect would opt for full-duplex. 2.1.8 Hubs and topology Ethernet hubs (multiport repeaters) change the topology, but not the fundamental constraints. Hubs enabled the model in which each station now had its own link to the wiring closet. Loops are still forbidden. The maximum diameter of an Ethernet consisting of multiple segments joined by hubs is still constrained by the round-trip-time, and the need to detect collisions before the sender has completed sending, as before. However, the network ‚Äúdiameter‚Äù, or maximum distance between two hosts, is no longer synonymous with ‚Äútotal length‚Äù. Because twisted-pair links are much shorter, about 100 meters, the diameter constraint is often immaterial. 2.1.9 Errors Packets can have bits Ô¨Çipped or garbled by electrical noise on the cable; estimates of the frequency with which this occurs range from 1 in 104to 1 in 106. Bit errors are not uniformly likely; when they occur, they are likely to occur in bursts. Packets can also be lost in hubs, although this appears less likely. Packets can be lost due to collisions only if the sending host makes 16 unsuccessful transmission attempts and gives up. Ethernet packets contain a 32-bit CRC error-detecting code (see 7.4.1 Cyclical Redundancy Check: CRC ) to detect bit errors. Packets can also be misaddressed by the sending host, or, most likely of all, they can arrive at the receiving host at a point when the receiver has no free buffers and thus be dropped by a higher-layer protocol. 56 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 2.1.10 CSMA persistence A carrier-sense/multiple-access transmission strategy is said to be nonpersistent if, when the line is busy, the sender waits a randomly selected time. A strategy is p-persistent if, after waiting for the line to clear, the sender sends with probability p ¬§1. Ethernet uses 1-persistence. A consequence of 1-persistence is that, if more than one station is waiting for line to clear, then when the line does clear a collision is certain. However, Ethernet then gracefully handles the resulting collision via the usual exponential backoff. If N stations are waiting to transmit, the time required for one station to win the backoff is linear in N. When we consider the Wi-Fi collision-handling mechanisms in 4.2 Wi-Fi, we will see that collisions cannot be handled quite as cheaply: for one thing, there is no way to detect a collision in progress, so the entire packet-transmission time is wasted. In the Wi-Fi case, p-persistence is used with p<1. An Ethernet broadcast storm was said to occur when there were too many transmission attempts, and most of the available bandwidth was tied up in collisions. A properly functioning classic Ethernet had an effective bandwidth of as much as 50-80% of the nominal 10Mbps capacity, but attempts to transmit more than this typically resulted in successfully transmitting a good deal less. 2.1.11 Analysis of Classic Ethernet How much time does Ethernet ‚Äúwaste‚Äù on collisions? A paradoxical attribute of Ethernet is that raising the transmission-attempt rate on a busy segment can reduce the actual throughput. More transmission attempts can lead to longer contention intervals between packets, as senders use the transmission backoff algorithm to attempt to acquire the channel. What effective throughput can be achieved? It is convenient to refer to the time between packet transmissions as the contention interval even if there is no actual contention, that is, even if the network is idle; we cannot tell if stations are not transmitting because they have nothing to send, or if they are simply waiting for their backoff timer to expire. Thus, a timeline for Ethernet always consists of alternating packet transmissions and contention intervals: ... contention contention contention contention packet packet packet packet Fig. 12:: Ethernet packet transmissions alternating with contention intervals As a Ô¨Årst look at contention intervals, assume that there are N stations waiting to transmit at the start of the interval. It turns out that, if all follow the exponential backoff algorithm, we can expect O(N) slot times before one station successfully acquires the channel; thus, Ethernets are happiest when N is small and there are only a few stations simultaneously transmitting. However, multiple stations are not necessarily a severe problem. Often the number of slot times needed turns out to be about N/2, and slot times are short. If N=20, then N/2 is 10 slot times, or 640 bytes. However, one packet time might be 1500 bytes. If packet intervals are 1500 bytes and contention intervals are 640 byes, this gives an overall throughput of 1500/(640+1500) = 70% of capacity. In practice, this seems to be a reasonable upper limit for the throughput of classic shared-media Ethernet. 2.1 10-Mbps Classic Ethernet 57
An Introduction to Computer Networks, Release 2.0.11 2.1.11.1 The ALOHA models Another approach to analyzing the Ethernet contention interval is by using the ALOHA model that was a precursor to Ethernet. In the ALOHA model, stations transmit packets without listening Ô¨Årst for a quiet line or monitoring the transmission for collisions (this models the situation of several ground stations transmitting to a satellite; the ground stations are presumed unable to see one another). Similarly, during the Ethernet contention interval, stations transmit one-slot packets under what are effectively the same conditions (we return to this below). The ALOHA model yields roughly similar throughput values to the O(N) model of the previous section. We make, however, a rather artiÔ¨Åcial assumption: that there are a very large number of active senders, each transmitting at a very low rate. The model may thus have limited direct applicability to typical Ethernets. To model the success rate of ALOHA, assume all the packets are the same size and let T be the time to send one (Ô¨Åxed-size) packet; T represents the Aloha slot time. We will Ô¨Ånd the transmission rate that optimizes throughput. The core assumption of this model is that that a large number N of hosts are transmitting, each at a relatively low rate of s packets/slot. Denote by G the average number of transmission attempts per slot; we then have G = Ns. We will derive an expression for S, the average rate of successful transmissions per slot, in terms of G. If two packets overlap during transmissions, both are lost. Thus, a successful transmission requires everyone else quiet for an interval of 2T: if a sender succeeds in the interval from t to t+T, then no other node can have tried to begin transmission in the interval t‚ÄìT to t+T. The probability of one station transmitting during an interval of time T is G = Ns; the probability of the remaining N‚Äì1 stations all quiet for an interval of 2T is (1‚Äìs)2(N‚Äì1). The probability of a successful transmission is thus S = Ns*(1‚Äìs)2(N‚Äì1) = G(1‚ÄìG/N)2N Math Warning Finding the limit of G(1‚ÄìG/N)2Nand Ô¨Ånding the maximum of Ge-2Grealistically requires a little background in calculus. However, these are not central to applying the model. As N gets large, the second line approaches Ge-2G. The function S = G e-2Ghas a maximum at G=1/2, S=1/2e. The rate G=1/2 means that, on average, a transmission is attempted every other slot; this yields the maximum successful-transmission throughput of 1/2e. In other words, at this maximum attempt rate G=1/2, we expect about 2e‚Äì1 slot times worth of contention between successful transmissions. What happens to the remaining G‚ÄìS unsuccessful attempts is not addressed by this model; presumably some higher-level mechanism ( egbackoff) leads to retransmissions. A given throughput S<1/2e may be achieved at either of two values for G; that is, a given success rate may be due to a comparable attempt rate or else due to a very high attempt rate with a similarly high failure rate. 58 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 2.1.11.2 ALOHA and Ethernet The relevance of the Aloha model to Ethernet is that during one Ethernet slot time there is no way to detect collisions (they haven‚Äôt reached the sender yet!) and so the Ethernet contention phase resembles ALOHA with an Aloha slot time T of 51.2 microseconds. Once an Ethernet sender succeeds, however, it continues with a full packet transmission, which is presumably many times longer than T. The average length of the contention interval, at the maximum throughput calculated above, is 2e‚Äì1 slot times (from ALOHA); recall that our model here supposed many senders sending at very low individual rates. This is the minimum contention interval; with lower loads the contention interval is longer due to greater idle times and with higher loads the contention interval is longer due to more collisions. Finally, let P be the time to send an entire packet in units of T; iethe average packet size in units of T. P is thus the length of the ‚Äúpacket‚Äù phase in the diagram above. The contention phase has length 2e‚Äì1, so the total time to send one packet (contention+packet time) is 2e‚Äì1+P. The useful fraction of this is, of course, P, so the effective maximum throughput is P/(2e‚Äì1+P). At 10Mbps, T=51.2 microseconds is 512 bits, or 64 bytes. For P=128 bytes = 2*64, the effective bandwidth becomes 2/(2e-1+2), or 31%. For P=512 bytes=8*64, the effective bandwidth is 8/(2e+7), or 64%. For P=1500 bytes, the model here calculates an effective bandwidth of 80%. These numbers are quite similar to our earlier values based on a small number of stations sending constantly. 2.2 100 Mbps (Fast) Ethernet Classic Ethernet, at 10 Mbps, is quite slow by modern standards, and so by 1995 the IEEE had created standards for Ethernet that operated at 100 Mbps. Ethernet at this speed is commonly known as Fast Ethernet; this name is used even today as ‚ÄúFast‚Äù Ethernet is being supplanted by Gigabit Ethernet (below). By far the most popular form of 100 Mbps Ethernet is ofÔ¨Åcially known as 100BASE-TX; it operates over twisted-pair cable. In the previous analysis of 10 Mbps Ethernet, the bandwidth, minimum packet size and maximum network diameter were all interrelated, in order to ensure that collisions could always be detected by the sender. Increasing the speed means that at least one of the other constraints must be scaled as well. For example, if the network physical diameter were to remain the same when moving to 100 Mbps, then the Fast-Ethernet round-trip time would be the same in microseconds but would be 10-fold larger measured in bits; this might mean a minimum packet size of 640 bytes instead of 64 bytes. (Actually, the minimum packet size might be somewhat smaller, partly because the ‚Äújam signal‚Äù doesn‚Äôt have to become longer, and partly because some of the numbers in the 10 Mbps delay budget above were larger than necessary, but it would still be large enough that a substantial amount of bandwidth would be consumed by padding.) The designers of Fast Ethernet felt that such a large minimum-packet size was impractical. However, Fast Ethernet was developed at a time (~1995) when reliable switches (below) were widely available; the quote above at 2 Ethernet Basics from [MB76] had become obsolete. Large ‚Äúvirtual‚Äù Ethernet networks could be formed by connecting small physical Ethernets with switches, effectively eliminating the need to support large-diameter physical Ethernets. So instead of increasing the minimum packet size, the decision was made to ensure collision detectability by reducing the network diameter instead. The network diameter chosen was a little over 400 meters, with reductions to account for the presence of hubs. At 2.3 2.2 100 Mbps (Fast) Ethernet 59
An Introduction to Computer Networks, Release 2.0.11 meters/bit, 400 meters is 174 bits, for a round-trip of 350 bits. The slot time (and minimum packet size) remains 512 bits ‚Äì now 5.12 ¬µsec ‚Äì which is safely large enough to ensure collision detection. This 400-meter diameter, however, may be misleading: the speciÔ¨Åc 100BASE-TX standard, which uses so-called Category 5 twisted-pair cabling (or better), limits the length of any individual cable segment to 100 meters. The maximum 100BASE-TX network diameter ‚Äì allowing for hubs ‚Äì is just over 200 meters. The 400-meter distance does apply to optical-Ô¨Åber-based 100BASE-FX in half-duplex mode, but this is not common. The 100BASE-TX network-diameter limit of 200 meters might seem small; it amounts in many cases to a single hub with multiple 100-meter cable segments radiating from it. In practice, however, such ‚Äústar‚Äù conÔ¨Ågurations could easily be joined with switches. As we will see below in 2.4 Ethernet Switches, switches partition an Ethernet into separate ‚Äúcollision domains‚Äù; the network-diameter rules apply to each domain separately but not to the aggregated whole. In a fully switched (that is, no hubs) 100BASE-TX LAN, each collision domain is simply a single twisted-pair link, subject to the 100-meter maximum length. Fast Ethernet also introduced the concept of full-duplex Ethernet: two twisted pairs could be used, one for each direction. Full-duplex Ethernet is limited to paths not involving hubs, that is, to single station-tostation links, where a station is either a host or a switch. Because such a link has only two potential senders, and each sender has its own transmit line, full-duplex Ethernet is entirely collision-free. Fast Ethernet (at least the 100BASE-TX form) uses 4B/5B encoding, covered in 6.1.4 4B/5B. This means that the electronics have to handle 125 Mbps, versus the 200 Mbps if Manchester encoding were still used. Fast Ethernet 100BASE-TX does not particularly support links between buildings, due to the maximumcable-length limitation. However, Ô¨Åber-optic point-to-point links are an effective alternative here, provided full-duplex is used to avoid collisions. We mentioned above that the coax-based 100BASE-FX standard allowed a maximum half-duplex run of 400 meters, but 100BASE-FX is much more likely to use full duplex, where the maximum cable length rises to 2,000 meters. 2.3 Gigabit Ethernet The problem of scaling Ethernet to handle collision detection gets harder as the transmission rate increases. If we were continue to maintain the same 51.2 ¬µsec slot time but raise the transmission rate to 1000 Mbps, the maximum network diameter would now be 20-40 meters. Instead of that, Gigabit Ethernet moved to a 4096-bit (512-byte, or 4.096 ¬µsec) slot time, at least for the twisted-pair versions. Short frames need to be padded, but this padding is done by the hardware. Gigabit Ethernet 1000Base-T uses so-called PAM5 encoding, below, which supports a special pad pattern (or symbol) that cannot appear in the data. The hardware pads the frame with these special patterns, and the receiver can thus infer the unpadded length as set by the host operating system. Gigabit vs Disks Once a network has reached Gigabit speed, the network is generally as fast as reading from or writing to a disk. Keeping data on another node no longer slows things down. This greatly expands the range of possibilities for constructing things like clustered databases. 60 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 However, the Gigabit Ethernet slot time is largely irrelevant, as full-duplex (bidirectional) operation is almost always supported. Combined with the restriction that each length of cable is a station-to-station link (that is, hubs are no longer allowed), this means that collisions simply do not occur and the network diameter is no longer a concern. (10 Gigabit Ethernet has ofÔ¨Åcially abandoned any pretense of supporting collisions; everything must be full-duplex.) There are actually multiple Gigabit Ethernet standards (as there are for Fast Ethernet). The different standards apply to different cabling situations. There are full-duplex optical-Ô¨Åber formulations good for many miles ( eg1000Base-LX10), and even a version with a 25-meter maximum cable length (1000Base-CX), which would in theory make the original 512-bit slot practical. The most common gigabit Ethernet over copper wire is 1000BASE-T (sometimes incorrectly referred to as 1000BASE-TX. While there exists a TX, it requires Category 6 cable and is thus seldom used; many devices labeled TX are in fact 1000BASE-T). For 1000BASE-T, all four twisted pairs in the cable are used. Each pair transmits at 250 Mbps, and each pair is bidirectional, thus supporting full-duplex communication. Bidirectional communication on a single wire pair takes some careful echo cancellation at each end, using a circuit known as a ‚Äúhybrid‚Äù that in effect allows detection of the incoming signal by Ô¨Åltering out the outbound signal. On any one cable pair, there are Ô¨Åve signaling levels. These are used to transmit two-bit symbols at a rate of 125 symbols/¬µsec, for a data rate of 250 bits/¬µsec. Two-bit symbols in theory only require four signaling levels; the Ô¨Åfth symbol allows for some redundancy which is used for error detection and correction, for avoiding long runs of identical symbols, and for supporting a special pad symbol, as mentioned above. The encoding is known as 5-level pulse-amplitude modulation, or PAM-5. The target bit error rate (BER) for 1000BASE-T is 10-10, meaning that the packet error rate is less than 1 in 106. In developing faster Ethernet speeds, economics plays at least as important a role as technology. As new speeds reach the market, the earliest adopters often must take pains to buy cards, switches and cable known to ‚Äúwork together‚Äù; this in effect amounts to installing a proprietary LAN. The real beneÔ¨Åt of Ethernet, however, is arguably that it isstandardized, at least eventually, and thus a site can mix and match its cards and devices. Having a given Ethernet standard support existing cable is even more important economically; the costs of replacing inter-ofÔ¨Åce cable often dwarf the costs of the electronics. As Ethernet speeds continue to climb, it has become harder and harder for host systems to keep up. As a result, it is common for quite a bit of higher-layer processing to be ofÔ¨Çoaded onto the Ethernet hardware, for example, TCP checksum calculation. See 17.5 TCP OfÔ¨Çoading. 2.4 Ethernet Switches Switches join separate physical Ethernets (or sometimes Ethernets and other kinds of networks). A switch has two or more Ethernet interfaces; when a packet is received on one interface it is retransmitted on one or more other interfaces. Only valid packets are forwarded; collisions are notpropagated. The term collision domain is sometimes used to describe the region of an Ethernet in between switches; a given collision propagates only within its collision domain. The diagram below shows some physical Ethernets joined by three switches S1, S2 and S3. Hosts A, B, C and D are on two separate traditional multiple-access Ethernets; hosts E and F have direct point-to-point links to S2 and S3 respectively. The S2‚ÄìS3 link is also a point-to-point link. If A and B both transmit at the same time, or C and D, the packets collide as usual. But if A and C both transmit, then S1 receives the two 2.4 Ethernet Switches 61
An Introduction to Computer Networks, Release 2.0.11 packets on its two separate interfaces in parallel; they do not collide. S1 can then forward C‚Äôs packet on its lefthand interface at the same time it forwards A‚Äôs packet on its righthand interface. S1 S2 AB C DE S3F Fig. 13:: Ethernet with switches S1, S2 and S3 Similarly, if E and F both transmit at the same time, their packets are received by S2 and S3 respectively. In the absence of forwarding information, S2 will Ô¨Çood E‚Äôs packet out both its lefthand and downward interfaces in parallel, though in the next section we will see how real switches do learn forwarding information. S3 will forward F‚Äôs packet out its upwards interface, and it is possible that E and F‚Äôs packets will collide on the vertical S2‚ÄìS3 link. But it is also possible the S2‚ÄìS3 link is full-duplex, able to handle simultaneous transmission in opposite directions; if not then the usual collision-backoff mechanism would allow E‚Äôs packet to reach S3 and F‚Äôs to reach S2. Switches have revolutionized Ethernet layout: all the collision-detection rules, including the rules for maximum network diameter, apply only to collision domains, and not to the larger ‚Äúvirtual Ethernets‚Äù created by stringing collision domains together with switches. As we shall see below, a switched Ethernet also offers much more resistance to eavesdropping than a non-switched ( eghub-based) Ethernet. Switch Costs In the 1980‚Äôs the author once installed a two-port 10-Mbps Ethernet switch (then called a ‚Äúbridge‚Äù) that cost $3000; cfthe [MB76] quote at 2 Ethernet Basics. Today a wide variety of multiport 100-Mbps Ethernet switches are available for around $10, and almost all installed Ethernets are fully switched. Like simpler unswitched Ethernets, the topology for a switched Ethernet is in principle required to be loopfree. In practice, however, most switches support the spanning-tree loop-detection protocol and algorithm, 3.1 Spanning Tree Algorithm and Redundancy, which automatically ‚Äúprunes‚Äù the network topology to make it loop-free while allowing the pruned links to be placed back in service if a primary link fails. While a switch does not propagate collisions, it must maintain a queue for each outbound interface in case it needs to forward a packet at a moment when the interface is busy; on (rare) occasion packets are lost when this queue overÔ¨Çows. Ethernet does in principle support PAUSE management frames; a switch with a queue that has grown too large can in principle transmit these to trafÔ¨Åc sources to pause trafÔ¨Åc for a period of time speciÔ¨Åed in the PAUSE frame. PAUSE frames appear to be rarely used, however. 62 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 2.4.1 Ethernet Learning Algorithm Traditional Ethernet switches use datagram forwarding as described in 1.4 Datagram Forwarding; the trick is to build their forwarding tables without any cooperation from ordinary, non-switch hosts. Indeed, to the extent that a switch is to act as a drop-in replacement for a hub, it cannot count on cooperation from other switches. The solution is for the switch to start out with an empty forwarding table, and then incrementally build the table through a learning process. If a switch does not have an entry for a particular destination, it will fall back to Ô¨Çooding: it will forward the packet out every interface other than the one on which the packet arrived. This is sometimes also called ‚Äúunknown unicast Ô¨Çooding‚Äù; it is equivalent to treating the destination as a broadcast address. The availability of fallback-to-Ô¨Çooding for unknown destinations is what makes it possible for Ethernet switches to learn their forwarding tables without any switch-to-switch or switch-tohost communication or coordination. This learning process is now part of the IEEE 802.1D standard, and is occasionally referred to as transparent bridging. A switch learns address locations as follows: for each interface, the switch maintains a table of physical (MAC) addresses that have appeared as source addresses in packets arriving via that interface. The switch thus knows that to reach these addresses, if one of them later shows up as a destination address, the packet needs to be sent only via that interface. SpeciÔ¨Åcally, when a packet arrives on interface I with source address S and destination unicast address D, the switch enters xS,Iyinto its forwarding table. To actually deliver the packet, the switch also looks up the destination D in the forwarding table. If there is an entryxD,Jywith JI ‚Äì that is, D is known to be reached via interface J ‚Äì then the switch forwards the packet out interface J. If J=I, that is, the packet has arrived on the same interfaces by which the destination is reached, then the packet does not get forwarded at all; it presumably arrived at interface I only because that interface was connected to a shared Ethernet segment that also either contained D or contained another switch that would bring the packet closer to D. If there is no entry for D, the switch must Ô¨Çood the packet out all interfaces J with J I; this represents the unknown-destination fallback to Ô¨Çooding. After a short while, the fallback-to-Ô¨Çooding alternative is needed less and less often, as switches learn where the active hosts are located. (However, in some switch implementations, forwarding tables also include timestamps, and entries are removed if they have not been used for, say, Ô¨Åve minutes.) If the destination address D is the broadcast address, or, for many switches, a multicast address, broadcast (Ô¨Çooding) is required. Some switches try to keep track of multicast groups, so as to forward multicast trafÔ¨Åc only out interfaces with known subscribers; see 2.1.2 Ethernet Multicast. S1 S2 S3 S4 S5A B CA A A,C A,CAB B B CC Fig. 14:: Five learning switches after three packet transmissions In the diagram above, each switch‚Äôs tables are indicated by listing near each interface the destinations (iden2.4 Ethernet Switches 63
An Introduction to Computer Networks, Release 2.0.11 tiÔ¨Åed by MAC addresses) known to be reachable by that interface. The entries shown are the result of the following packets: 
- A sends to B; all switches learn where A is 
- B sends to A; this packet goes directly to A; only S3, S2 and S1 learn where B is 
- C sends to B; S4 does not know where B is so this packet goes to S5; S2 does know where B is so the packet does notgo to S1. It is worth observing that, at the application layer, hosts do not commonly identify one another by their MAC addresses. In an IPv4-based network, the use of ARP ( 10.2 Address Resolution Protocol: ARP ) to translate from IPv4 to MAC addresses would introduce additional broadcasts, which would cause the above scenario to play out differently. See exercise 10.0. Switches do notautomatically discover directly connected neighbors; S1 does not learn about A until A transmits a packet. Once all the switches have learned where all (or most of) the hosts are, each packet is forwarded rather than Ô¨Çooded. At this point packets are never sent on links unnecessarily; a packet from A to B only travels those links that lie along the (unique) path from A to B. (Paths must be unique because switched Ethernet networks cannot have loops, at least not active ones. If a loop existed, then a packet sent to an unknown destination would be forwarded around the loop endlessly, though in practice this is almost always prevented by the spanning-tree algorithm, 3.1 Spanning Tree Algorithm and Redundancy .) Switches have an additional privacy advantage in that trafÔ¨Åc that does not Ô¨Çow where it does not need to Ô¨Çow is much harder to eavesdrop on. On an unswitched Ethernet, one host conÔ¨Ågured to receive all packets can eavesdrop on all trafÔ¨Åc. Early Ethernets were notorious for allowing one unscrupulous station in promiscuous mode to capture, for instance, all passwords in use on the network. On a fully switched Ethernet, a host physically sees only the trafÔ¨Åc actually addressed to it; other trafÔ¨Åc remains inaccessible. This switch-based privacy protection is, however, potentially vulnerable to an attack known as MAC Ô¨Çooding, in which a malicious host transmits a very large number of packets each with a unique, fake source address. These fake addresses Ô¨Åll up the switch forwarding tables, forcing the switches to discard older, legitimate entries. This in turn means that the switches will have to use fallback-to-Ô¨Çooding mode for legitimate addresses, allowing eavesdropping. A common Ethernet-layer defense against MAC Ô¨Çooding is port security, in which, typically, each port on a switch is limited in the number of forwarding-table entries it can be associated with. With this in effect, the MAC-Ô¨Çooding entries from any one port will be ignored once that port‚Äôs limit is reached; hopefully this will occur before legitimate entries on other ports need to be discarded. Port security is often accompanied by a mechanism to specify selected MAC addresses as secure, meaning that they do not get removed from the forwarding table even if MAC Ô¨Çooding is occuring. Application-layer encryption is another defense against MAC Ô¨Çooding. Typical large switches have room for a forwarding table with 104105entries, though fully switched networks at the upper end of this size range are not common. The main size limitations speciÔ¨Åc to switching are the requirement that the topology must be loop-free (thus disallowing duplicate paths which might otherwise provide redundancy), and that all broadcast trafÔ¨Åc must always be forwarded everywhere. As a switched Ethernet grows, broadcast trafÔ¨Åc comprises a larger and larger percentage of the total trafÔ¨Åc, and the organization must at some point move to a routing architecture ( egas in 9.6 IPv4 Subnets ). A common recommendation is to have no more than 1000 hosts per LAN (or VLAN, 3.2 Virtual LAN (VLAN) ). 64 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 2.5 Epilog Ethernet dominates the (wired) LAN layer. That said, there are lots of different forms of Ethernet, covering a variety of cable types and a speed range spanning four orders of magnitude. In many ways, Ethernet is alogical interface, encompassing multiple implementations that can be interconnected only with switches. Higher-speed Ethernet has embraced full switching and exclusively point-to-point links. Once Ethernet Ô¨Ånally abandons physical links that are bi-directional (half-duplex links), it will be collision-free and thus will no longer need a minimum packet size. Other wired networks have largely disappeared (or have been renamed ‚ÄúEthernet‚Äù). Wireless networks, however, are here to stay, and for the time being at least have inherited the original Ethernet‚Äôs collisionmanagement concerns that Ethernet itself has grown out of. 2.6 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.2 Solutions for Ethernet. 1.0. Simulate the contention period of Ô¨Åve Ethernet stations that all attempt to transmit at T=0 (presumably when some sixth station has Ô¨Ånished transmitting), in the style of the diagram in 2.1.6 Exponential Backoff Algorithm. Assume that time is measured in slot times, and that exactly one slot time is needed to detect a collision (so that if two stations transmit at T=1 and collide, and one of them chooses a backoff time k=0, then that station will transmit again at T=2). Use coin Ô¨Çips or some other source of randomness. 2.0. Suppose we have Ethernet switches S1 through S3 arranged as below; each switch uses the learning algorithm of 2.4 Ethernet Switches. All forwarding tables are initially empty. S1 S2 S3D A B C (a). If A sends to B, which switches see this packet? (b). If B then replies to A, which switches see this packet? (c). If C then sends to B, which switches see this packet? (d). If C then sends to D, which switches see this packet? 3.0.‚ô¢Suppose we have the Ethernet switches S1 through S4 arranged as below. All forwarding tables are empty; each switch uses the learning algorithm of 2.4 Ethernet Switches. B S4 AS1 S2 S3C D 2.5 Epilog 65
An Introduction to Computer Networks, Release 2.0.11 Now suppose the following packet transmissions take place: 
- A sends to D 
- D sends to A 
- A sends to B 
- B sends to D For each switch S1-S4, list what source addresses ( egA,B,C,D) it has seen (and thus what nodes it has learned the location of). 4.0. Repeat the previous exercise (3.0), with the same network layout, except that instead the following packet transmissions take place: 
- A sends to B 
- B sends to A 
- C sends to B 
- D sends to A For each switch, list what source addresses ( egA,B,C,D) it has seen (and thus what nodes it has learned the location of). 5.0. In the switched-Ethernet network below, Ô¨Ånd two packet transmissions so that, when a third transmission A√ù√ëD occurs, the packet isseen by B (that is, it is Ô¨Çooded out all ports by S2), but is notsimilarly seen by C (because it is forwarded to D, not Ô¨Çooded, by S3). All forwarding tables are initially empty, and each switch uses the learning algorithm of 2.4 Ethernet Switches. B C AS1 S2 S3D Hint: Destination D must be in S3‚Äôs forwarding table, but must not be in S2‚Äôs. So there must have been a packet sent by D that was seen by S3 but not by S2. 6.0. Given the Ethernet network with learning switches below, with (disjoint) unspeciÔ¨Åed parts represented by ?, explain why it is impossible for a packet sent from A to B to be forwarded by S1 directly to S2, but to be Ô¨Çooded by S2 out all of S2‚Äôs other ports. ? ? | | AS1 S2B 7.0. In the diagram below, from 2.4.1 Ethernet Learning Algorithm, suppose node D is connected to S5. Now, with the tables as shown by the labels in the diagram (that is, S5 knows about A and C, etc), D sends to B. 66 2 Ethernet Basics
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S3 S4 S5A B CA A A,C A,CAB B B CC D Which switches will see this D √ëB packet, and thus learn about D? Of these switches, which do notalready know where B is and will use fallback-to-Ô¨Çooding? 8.0. Suppose two Ethernet switches are connected in a loop as follows; S1 and S2 have their interfaces 1 and 2 labeled. These switches do notuse the spanning-tree algorithm, 3.1 Spanning Tree Algorithm and Redundancy. S1 S21 201 2A Suppose A attempts to send a packet to destination B, which is unknown. S1 will therefore Ô¨Çood the packet out interfaces 1 and 2. What happens then? How long will A‚Äôs packet circulate? 9.0. Suppose you want to develop a new protocol so that Ethernet switches participating in a VLAN all keep track of the VLAN ‚Äúcolor‚Äù associated with every destination in their forwarding tables. Assume that each switch knows which of its ports (interfaces) connect to other switches and which may connect to hosts, and in the latter case knows the color assigned to that port. (a). Suggest a way by which switches might propagate this destination-color information to other switches. (b). What must be done if a port formerly reserved for connection to another switch is now used for a host? 10.0. Consider the scenario from 2.4.1 Ethernet Learning Algorithm: 2.6 Exercises 67
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S3 S4 S5A B CA A A,C A,CAB B B CC The following packet transmissions occur: 
- A sends to B 
- B sends to A 
- C sends to B Now suppose that, before each packet transmission above, the sender Ô¨Årst sends a broadcast packet, and the destination then sends a unicast reply packet (this is roughly the ARP protocol, used to translate from IPv4 addresses to Ethernet physical addresses, 10.2 Address Resolution Protocol: ARP ). After the three transmissions listed above, what destinations do the switches S1-S5 have in their forwarding tables? 68 2 Ethernet Basics
3 ADVANCED ETHERNET The previous chapter covered the basic mechanics of Ethernet, including switching and learning. In this chapter we examine more advanced features, typically encountered primarily in large-scale installations: 
- The spanning-tree algorithm 
- Virtual LANs 
- Ethernet switch hardware 
- TRILL and SPB 
- Software deÔ¨Åned networking and OpenFlow The Ô¨Årst of these is a way to tolerate loops in the underlying link topology. Loops have a way of sneaking into the cabling unintentionally, but also have the potential to provide much-needed redundancy. The spanningtree algorithm achieves its goal by Ô¨Ånding loops in the physical links and then deÔ¨Åning a ‚Äúlogical‚Äù tree (loop-free) topology by pruning away the loops. Virtual LANs, or VLANs, are a mechanism by which disjoint subsets of the hosts on a large Ethernet can be sequestered from one another, with packet exchanges only under controllable conditions. For example, Sales and Engineering workstations can be physically commingled, but kept logically separate. TRILL and SPB bring to Ethernet the active embrace of loops, with trafÔ¨Åc intentionally routed along one of multiple paths to a destination (typically the shortest path). Multiple paths allow for redundancy, an important consideration in the event of link failure. Software-deÔ¨Åned networking, or SDN, represents the most radical step here. Ethernet forwarding is placed under the control of a program, rather than under control of the learning algorithm of 2.4.1 Ethernet Learning Algorithm. At a minimum, this allows much more strategic handling of loops in the topology. SDN does not stop there, however, and also enables many other strategies to design and control large and complex networks. 3.1 Spanning Tree Algorithm and Redundancy In theory, if you form a loop with Ethernet switches, any packet with destination not already present in the forwarding tables will circulate endlessly, consuming most available throughput. Some early switches would actually do this; it was generally regarded as catastrophic failure. In practice, however, loops allow redundancy ‚Äì if one link breaks there is still 100% connectivity ‚Äì and so can be desirable. As a result, Ethernet switches have incorporated a switch-to-switch protocol to construct a subset of the switch-connections graph that has no loops and yet allows reachability of every host, known in graph theory as a spanning tree. Once the spanning tree is built, links that are not part of the tree are disabled, even if they would represent the most efÔ¨Åcient path between two nodes. If a link that is part of the spanning tree fails, partitioning the network, a new tree is constructed, and some formerly disabled links may now return to service. 69
An Introduction to Computer Networks, Release 2.0.11 One might ask, if switches can work together to negotiate a a spanning tree, whether they can also work together to negotiate loop-free forwarding tables for the original non-tree topology, thus keeping all links active. The difÔ¨Åculty here is not the switches‚Äô ability to coordinate, but the underlying Ethernet broadcast feature. As long as the topology has loops and broadcast is enabled, broadcast packets might circulate forever. And disabling broadcast is not a straightforward option; switches rely on the broadcast-based fallback-to-Ô¨Çooding strategy of 2.4.1 Ethernet Learning Algorithm to deliver to unknown destinations. However, we will return to this point in 3.4 Software-DeÔ¨Åned Networking. See also exercise 4.0. The presence of hubs and other unswitched Ethernet segments slightly complicates the switch-connections graph. In the absence of these, the graph‚Äôs nodes and edges are simply the hosts (including switches) and links of the Ethernet itself. If unswitched multi-host Ethernet segments are present, then each of these becomes a single node in the graph, with a graph edge to each switch to which it directly connects. (Any Ethernet switches not participating in the spanning-tree algorithm would be treated as hubs.) Every switch has an ID, egits smallest Ethernet address, and every edge that attaches to a switch does so via a particular, numbered interface. The goal is to disable redundant (cyclical) paths while retaining the ability to deliver to any segment. The algorithm is due to Radia Perlman, [RP85]. The switches Ô¨Årst elect a root node, egthe one with the smallest ID. Then, if a given segment connects to two switches that both connect to the root node, the switch with the shorter path to the root is used, if possible; in the event of ties, the switch with the smaller ID is used. The simplest measure of path cost is the number of hops, though current implementations generally use a cost factor inversely proportional to the bandwidth (so larger bandwidth has lower cost). Some switches permit other conÔ¨Åguration here. The process is dynamic, so if an outage occurs then the spanning tree is recomputed. If the outage should partition the network into two pieces, both pieces will build spanning trees. All switches send out regular messages on all interfaces called bridge protocol data units, or BPDUs (or ‚ÄúHello‚Äù messages). These are sent to the Ethernet multicast address 01:80:c2:00:00:00, from the Ethernet physical address of the interface. (Note that Ethernet switches do not otherwise need a unique physical address for each interface.) The BPDUs contain 
- The switch ID 
- the ID of the node the switch believes is the root 
- the path cost ( egnumber of hops) to that root These messages are recognized by switches and are not forwarded naively. Switches process each message, looking for 
- a switch with a lower ID than any the receiving switch has seen before (thus becoming the new root) 
- a shorter path to the existing root 
- an equal-length path to the existing root, but via a neighbor switch with a lower ID (the tie-breaker rule). If there are two ports that connect to that switch, the port number is used as an additional tie-breaker. In a heterogeneous Ethernet we would also introduce a preference for faster paths, but we will assume here that all links have the same bandwidth. When a switch sees a new root candidate, it sends BPDUs on all interfaces, indicating the distance. The switch includes the interface leading towards the root. Once this process has stabilized, each switch knows 70 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 
- its own path to the root 
- which of its ports any further-out switches will be using to reach the root 
- for each port, its directly connected neighboring switches Now the switch can ‚Äúprune‚Äù some (or all!) of its interfaces. It disables all interfaces that are not enabled by the following rules: 1. It enables the port via which it reaches the root 2. It enables any of its ports that further-out switches use to reach the root 3. If a remaining port connects to a segment to which other ‚Äúsegment-neighbor‚Äù switches connect as well, the port is enabled if the switch has the minimum cost to the root among those segment-neighbors, or, if a tie, the smallest ID among those neighbors, or, if two ports are tied, the port with the smaller ID. 4. If a port has no directly connected switch-neighbors, it presumably connects to a host or segment, and the port is enabled. Rules 1 and 2 construct the spanning tree; if S3 reaches the root via S2, then Rule 1 makes sure S3‚Äôs port towards S2 is open, and Rule 2 makes sure S2‚Äôs corresponding port towards S3 is open. Rule 3 ensures that each network segment that connects to multiple switches gets a unique path to the root: if S2 and S3 are segment-neighbors each connected to segment N, then S2 enables its port to N and S3 does not (because 2<3). The primary concern here is to create a path for any host nodes on segment N; S2 and S3 will create their own paths via Rules 1 and 2. Rule 4 ensures that any ‚Äústub‚Äù segments retain connectivity; these would include all hosts directly connected to switch ports. 3.1.1 Spanning Tree Example 1: Switches Only We can simplify the situation somewhat if we assume that the network is fully switched: each switch port connects to another switch or to a (single-interface) host; that is, no repeater hubs (or coax segments!) are in use. In this case we can dispense with Rule 3 entirely. Any switch ports directly connected to a host can be identiÔ¨Åed because they are ‚Äúsilent‚Äù; the switch never receives any BPDU messages on these interfaces because hosts do not send these. All these host port ends up enabled via Rule 4. Here is our sample network, where the switch numbers ( eg5 for S5) represent their IDs; no hosts are shown and interface numbers are omitted. S1 S2 S3 S4 S5 S6 S1 has the lowest ID, and so becomes the root. S2 and S4 are directly connected, so they will enable the interfaces by which they reach S1 (Rule 1) while S1 will enable its interfaces by which S2 and S4 reach it (Rule 2). S3 has a unique lowest-cost route to S1, and so again by Rule 1 it will enable its interface to S2, while by Rule 2 S2 will enable its interface to S3. 3.1 Spanning Tree Algorithm and Redundancy 71
An Introduction to Computer Networks, Release 2.0.11 S5 has two choices; it hears of equal-cost paths to the root from both S2 and S4. It picks the lower-numbered neighbor S2; the interface to S4 will never be enabled. Similarly, S4 will never enable its interface to S5. Similarly, S6 has two choices; it selects S3. After these links are enabled (strictly speaking it is interfaces that are enabled, not links, but in all cases here either both interfaces of a link will be enabled or neither), the network in effect becomes: S1 S2 S3 S4 S5 S6 3.1.2 Spanning Tree Example 2: Switches and Segments As an example involving switches that may join via unswitched Ethernet segments, consider the following network; S1, S2 and S3, for example, are all segment-neighbors via their common segment B. As before, the switch numbers represent their IDs. The letters in the clouds represent network segments; these clouds may include multiple hosts. Note that switches have no way to detect these hosts; only (as above) other switches. S1S2 A FC S4DS5ES3 BS6GH 1 235 4 13 121 2312 1 23 J2S712K Fig. 15:: Switched Ethernet with potentially multiple hosts connected to switch ports Eventually, all switches discover S1 is the root (because 1 is the smallest of {1,2,3,4,5,6}). S2, S3 and S4 are one (unique) hop away; S5, S6 and S7 are two hops away. 72 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 Algorhyme I think that I shall never see a graph more lovely than a tree. A tree whose crucial property is loop-free connectivity. A tree that must be sure to span so packet can reach every LAN. First, the root must be selected. By ID, it is elected. Least-cost paths from root are traced. In the tree, these paths are placed. A mesh is made by folks like me, then bridges Ô¨Ånd a spanning tree. Radia Perlman For the switches one hop from the root, Rule 1 enables S2‚Äôs port 1, S3‚Äôs port 1, and S4‚Äôs port 1. Rule 2 enables the corresponding ports on S1: ports 1, 5 and 4 respectively. Without the spanning-tree algorithm S2 could reach S1 via port 2 as well as port 1, but port 1 has a smaller number. S5 has two equal-cost paths to the root: S5 √ù√ëS4√ù√ëS1 and S5√ù√ëS3√ù√ëS1. S3 is the switch with the lower ID; its port 2 is enabled and S5 port 2 is enabled. S6 and S7 reach the root through S2 and S3 respectively; we enable S6 port 1, S2 port 3, S7 port 2 and S3 port 3. The ports still disabled at this point are S1 ports 2 and 3, S2 port 2, S4 ports 2 and 3, S5 port 1, S6 port 2 and S7 port 1. Now we get to Rule 3, dealing with how segments (and thus their hosts) connect to the root. Applying Rule 3, 
- We do not enable S2 port 2, because the network (B) has a direct connection to the root, S1 
- We do enable S4 port 3, because S4 and S5 connect that way and S4 is closer to the root. This enables connectivity of network D. We do not enable S5 port 1. 
- S6 and S7 are tied for the path-length to the root. But S6 has smaller ID, so it enables port 2. S7‚Äôs port 1 is not enabled. Finally, Rule 4 enables S4 port 2, and thus connectivity for host J. It also enables S1 port 2; network F has two connections to S1 and port 2 is the lower-numbered connection. At this point the disabled ports are S1 port 3, S2 port 2, S5 port 1 and S7 port 1. All this port-enabling is done using only the data collected during the root-discovery phase; there is no additional negotiation. The BPDU exchanges continue, however, so as to detect any changes in the topology. If a link is disabled, it is not used even in cases where it would be more efÔ¨Åcient to use it. That is, trafÔ¨Åc 3.1 Spanning Tree Algorithm and Redundancy 73
An Introduction to Computer Networks, Release 2.0.11 from D to E is sent via S4, S1 and S3, with respective arrival ports 3, 4 and 1; it does not pass through S5. IP routing, on the other hand, uses the ‚Äúshortest path‚Äù. To put it another way, all spanning-tree Ethernet trafÔ¨Åc goes through the root node, or along a path to or from the root node. The traditional (IEEE 802.1D) spanning-tree protocol is relatively slow; the need to go through the treebuilding phase means that after switches are Ô¨Årst turned on no normal trafÔ¨Åc can be forwarded for ~30 seconds. Faster, revised protocols have been proposed to reduce this problem. Another issue with the spanning-tree algorithm is that a rogue switch can announce an ID of 0 (or some similar artiÔ¨Åcially small value), thus likely becoming the new root; this leaves that switch well-positioned to eavesdrop on a considerable fraction of the trafÔ¨Åc. One of the goals of the Cisco ‚ÄúRoot Guard‚Äù feature is to prevent this. Another goal of this and related features is to put the spanning-tree topology under some degree of administrative control. One likely wants the root switch, for example, to be geographically at least somewhat centered, and for the high-speed backbone links to be preferred to slow links. 3.2 Virtual LAN (VLAN) What do you do when you have different people in different places who are ‚Äúlogically‚Äù tied together? For example, for a while the Loyola University CS department was split, due to construction, between two buildings. One approach is to continue to keep LANs local, and use IP routing between different subnets. However, it is often convenient (printers are one reason) to conÔ¨Ågure workgroups onto a single ‚Äúvirtual‚Äù LAN, or VLAN. A VLAN looks like a single LAN, usually a single Ethernet LAN, in that all VLAN members will see broadcast packets sent by other members and the VLAN will ultimately be considered to be a single IP subnet (9.6 IPv4 Subnets ). Different VLANs are ultimately connected together, but likely only by passing through a single, central IP router. Broadcast trafÔ¨Åc on one VLAN will generally notpropagate to any other VLAN; this isolation of broadcast trafÔ¨Åc is another important justiÔ¨Åcation for VLAN use. While there are exceptions, a common rule of thumb is that any one VLAN should have no more than 256 hosts, the size of a /24 IPv4 subnet. VLANs can be visualized and designed by using the concept of coloring. We logically assign all nodes on the same VLAN the same color, and switches forward packets accordingly. That is, if S1 connects to red machines R1 and R2 and blue machines B1 and B2, and R1 sends a broadcast packet, then it goes to R2 but not to B1 or B2. Switches must, of course, be told the color of each of their ports. In the diagram above, S1 and S3 each have both red and blue ports. The switch network S1-S4 will deliver trafÔ¨Åc only when the source and destination ports are the same color. Red packets can be forwarded to the blue VLAN only by passing through the router R, entering R‚Äôs red port and leaving its blue port. R may apply Ô¨Årewall rules to restrict red‚Äìblue trafÔ¨Åc. When the source and destination ports are on the same switch, nothing needs to be added to the packet; the switch can keep track of the color of each of its ports. However, switch-to-switch trafÔ¨Åc must be additionally tagged to indicate the source. Consider, for example, switch S1 above sending packets to S3 which has nodes R3 (red) and B3 (blue). TrafÔ¨Åc between S1 and S3 must be tagged with the color, so that S3 will know to what ports it may be delivered. The IEEE 802.1Q protocol is typically used for this packet-tagging; a 32-bit Ô¨Åeld, including a 12-bit ‚Äúcolor‚Äù tag, is inserted into the Ethernet header after the source address and before the type Ô¨Åeld. The Ô¨Årst 16 bits of this Ô¨Åeld is 0x8100, which becomes the new Ethernet type Ô¨Åeld and which 74 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S3 S4 Router RR1 R2B1 B2R3 B3 Fig. 16:: One network of switches S1-S4 divided into two VLANs, red and blue identiÔ¨Åes the frame as tagged. A separate 802.3 amendment allows Ethernet packets to be slightly larger, to accommodate the tags. Double-tagging is possible; this would allow an ISP to have one level of tagging and its customers to have another level. The 802.1Q header also supports a three-bit priority Ô¨Åeld. Packets are forwarded by cooperating switches using priority queuing ( 23.3 Priority Queuing ) based on this Ô¨Åeld, though even in many relatively large installations all packets are assigned the same ‚Äì default ‚Äì priority. One example of the use of this Ô¨Åeld would be to give voice packets higher priority than data packets. Another example would be to map the IP header priority Ô¨Åeld ( egthe IPv4 DS Ô¨Åeld, 9.1 The IPv4 Header ) to the 802.1Q priority. This would normally be useful only within an ISP‚Äôs Ethernet-based network, or within a data center. For a more dynamic example, see16.5.6 Homa. Linux supports basic conÔ¨Åguration of VLAN-connected interfaces through the vconfig command. This allows a given Ethernet interface to support one or more VLAN tags through the creation of separate virtual interfaces for each tag. It also allows for the assignment of a 802.1Q priority value to be used for a given VLAN. Often, though, VLANs are meant to be transparent to host devices. Finally, most commercial-grade switches do provide some way of selectively allowing trafÔ¨Åc between different VLANs; with such switches, for example, rules could be created to allow R1 to connect to B3 without the use of the router R. One difÔ¨Åculty with this approach is the lack of standardization between switch manufacturers. This makes it difÔ¨Åcult to create, for example, authorization applications that allow opening inter-VLAN connections on the Ô¨Çy. Another issue is that some switches allow inter-VLAN rules based only on MAC addresses, and not, for example, on TCP port numbers. The OpenFlow protocol ( 3.4.1 OpenFlow Switches ) has the potential to create the necessary standardization here. Even without OpenFlow, however, some specialty access-and-authentication systems have been developed that do enable host access by dynamic creation of the appropriate switch rules. 3.2.1 Switch Hardware One of the differences between an inexpensive Ethernet switch and a pricier one is the degree of internal parallelism it can support. If three packets arrive simultaneously on ports 1, 2 and 3, and are destined for respective ports 4, 5 and 6, can the switch actually transmit the packets simultaneously? A ‚Äúyes‚Äù answer here is the gold performance standard for an Ethernet switch: to keep up with packets as fast as they arrive. 3.2 Virtual LAN (VLAN) 75
An Introduction to Computer Networks, Release 2.0.11 The worst-case load, for a switch with 2N ports, is for packets to arrive continuously on N ports, and depart on a different N ports. This means that, in the time required to transmit one packet, the switch must internally forward N packets in parallel. This is sometimes much faster than necessary. If all the load is departing (or arriving) via just one of the ports ‚Äì for example, the port connected to the server, or to the Internet ‚Äì then the above standard is N times faster than necessary; the switch need only handle one packet at a time. Such a switch may be forced to queue outbound packets on that one port, but that does not represent a lack of performance on the part of the switch. Still, greater parallelism is generally viewed as a good thing in switches. The simplest switch architecture ‚Äì used whenever a switch is built around a ‚Äústandard‚Äù computer ‚Äì is the shared-memory model. Such a system consists of a single CPU, single memory and peripheral busses, and multiple Ethernet cards. When a packet arrives, the CPU must copy the packet from the arrival interface into RAM, determine the forwarding, and then copy the packet to the output interface. To keep up with one-at-a-time 100 Mbps transmission, the internal transfer rate must therefore be at least 200 Mbps. The maximum speed of such a device depends largely on the speed of the peripheral-to-RAM bus (the socalled front-side bus). The USB 3.0 bus operates at 5 Gbps. At an Ethernet speed of 100 Mbps, such a bus can theoretically transfer 5 Gbps/200 Mbps = 25 packets in and out in the time it takes one packet to arrive, supporting up to 50 ports total. However, with gigabit Ethernet, only two packets can be handled. For commodity Ô¨Åve-port switches, this is enough, and such switches can generally handle this degree of parallelism. Bus speeds go up at least ten-fold, but 10 Gbps and even 40 Gbps Ethernet is now common in datacenters, and 24 ports is a bare minimum. As a result, the shared-memory architecture is generally not regarded as adequate for high-performance switches. When a high degree of parallelism is required, there are various architectures ‚Äì known as switch fabrics ‚Äì that can be implemented. One common solution to this internal-bottleneck problem is a so-called crossbar switch fabric, consisting of a grid of NN normally open switch nodes that can be closed under CPU control. Packets travel, via a connected path through the crossbar, directly from one Ethernet interface to another. The crossbar allows parallel connections between any of N inputs and any of N outputs. 1 2 3 4 5Inputs 12345 Outputs Fig. 17:: 55 crossbar with 5 parallel connections 1 √ë1, 2√ë3, 3√ë5, 4√ë2, 5√ë4 76 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 The diagram above illustrates a 5 5 crossbar, with 5 inputs and 5 separate outputs. (In a real Ethernet switch, any port can be an input or an output, but this is a relatively inessential difference). There are 5 parallel connections shown, from inputs 1-5 to outputs 1,3,5, 2 and 4 respectively; the large dots represent solid-state switching elements in the closed state. Packets are transmitted serially through each switch path. We saw in 1.7 Congestion that, for any switch (or router), if packets arrive over time for a given output port faster than they can be transmitted by that port, then the excess packets will need to be queued at that output port. However, in this crossbar example, if multiple packets destined for the same output arrive simultaneously, all but one will also have to be queued brieÔ¨Çy at the input side: packets cannot be delivered in parallel to the same output-port queue. The same is true for most other switch fabrics as well, including the simple shared-memory switch discussed above. So, while the ‚Äúprimary‚Äù queues of switches (and routers) are at the output side, as packets wait their turn for transmission, there is also a need for smaller input-side queues, as packets wait their turn to enter the switch fabric. Crossbars, and variations, are one common approach in the design of high-speed switches that support multiple parallel transfers. The other hardware innovation often used by high-performance switches is Content-Addressable Memory, or CAM; this allows for the search of the forwarding table in a single memory load. In a shared-memory switch, each destination address must be looked up in a hash table or other data structure; including the calculation of the hash value, this process may take as long as several tens of memory loads. On some brands of switches, the forwarding table is often referred to as the CAM table. CAM memory consists of a large number N of memory registers all attached to a common data-input bus; for Ethernet switching, the data width of the bus and registers needs to be at least as large as the 48-bit address size. When the input bus is activated, each memory register simultaneously compares the value on the bus with its own data value; if there is a match, the register triggers its output line. A binary-encoder circuit then converts the number k<N of that register to a binary value representing the address k of the register. It is straightforward to have the encoder resolve ties by choosing, for example, the number of the Ô¨Årst register to match. 0 1 2 3 4 56 7 8 9 a b c d e f 1 0 0 0 1 1 Data Input Address 0110 = 60 1 1 0 Fig. 18:: Content-Addressable Memory 3.2 Virtual LAN (VLAN) 77
An Introduction to Computer Networks, Release 2.0.11 In the diagram above, the data width is 6, and there are 16 vertically oriented registers numbered (in hex) 0-f. Register 6 contains the entry 100011 matching the data input, and enables its signal into the encoder. The output of the encoder is the corresponding address, 0110. A common variant of CAM is Ternary CAM, or TCAM, in which each memory register is paired with a corresponding ‚Äúmask‚Äù register. For any given bit, a match is declared only if the bus bit matches the register bit,orthe corresponding mask bit is 0. A mask bit of 0 thus represents a wildcard value, matching any input. TCAM is most useful when the addresses being looked up are IP addresses rather than Ethernet addresses, in which case the goal is to match only the address bits corresponding to the network preÔ¨Åx ( 1.10 IP - Internet Protocol ). In this setting the mask contents represents the IP address mask ( 9.6 IPv4 Subnets ). In order to implement the longest-match rule ( 14.1 Classless Internet Domain Routing: CIDR ) it is essential that addresses with shorter preÔ¨Åxes ‚Äì longer runs of terminal wildcard bits ‚Äì appear before addresses with longer preÔ¨Åxes (and also that, in the event of multiple matches, the encoder prefers the righthand ‚Äì that is, longer ‚Äì match). This implies that new TCAM entries must be inserted in relatively speciÔ¨Åc positions, which may involve a signiÔ¨Åcant amount of shifting the positions of existing entries. See 9.5.1 EfÔ¨Åcient Forwarding-Table Lookup. 3.3 TRILL and SPB As Ethernets get larger, the spanning-tree algorithm becomes more and more a problem, primarily because useful links are disabled and redundancy is lost. In a high-performance network, such as a campus backbone or within a datacenter, disabled links are a wasted resource. A secondary issue is that, in the event of link failure, the spanning-tree approach can take many seconds to create a new tree and restore connectivity. To address these problems, there are now protocols which allow Ethernet to have active loops in the topology, making Ô¨Årst-class use of alllinks. The idea is to generate forwarding tables within the Ethernet switches ‚Äì or at least within those that support the appropriate protocol ‚Äì that route every packet along the shortest path, or at least an approximation to the shortest path. All physical links can remain in active service. This has long been a staple in the IP world ( 13 Routing-Update Algorithms ), but is deÔ¨Ånitely a break with tradition at the LAN layer. There are two competing protocols here: TRILL (TRansparent Interconnection of Lots of Links) and SPB (Shortest-Path Bridging). TRILL is documented in [RP04] and RFC 6325 and companions, while SPB is standardized by IEEE 802.1aq. We will focus here on TRILL (though we do note that link-layer protocols are normally the province of the IEEE, not the IETF). Both TRILL and SPB envision that, initially, only a few switches will be smart enough to do shortest-path routing, just as, once upon a time, only a few switches implemented the spanning-tree algorithm. But, with time, it is likely that eventually most if not all Ethernet switches will be shortest-path aware. In highperformance datacenters it is particularly likely that forwarding will be based on TRILL or SPB. In TRILL, the Ethernet switches that are TRILL-aware are known as Router-Bridges, or RBridges (the terms RSwitches and TRILL Switches might also be appropriate). In between the RBridges are Legacy Ethernets (called ‚Äúlinks‚Äù in [RP04] and RFC 6325, though this term is misleading); Legacy Ethernets consist of maximal subnetworks of Ethernet hosts and non-TRILL-aware switches. The intent is for the RBridges to partition the entire Ethernet into relatively small Legacy Ethernets. In the ultimate case where allswitches are RBridges, the Legacy Ethernets are simply individual hosts. In the diagram below, four RBridges isolate Legacy Ethernets 1, 2, 3 and 4, though Legacy Ethernet 5 represents a degree of partitioning inefÔ¨Åciency. 78 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 RB1 RB4 RB3 RB2LE1 LE2LE4 LE3LE5 Fig. 19:: TRILL network with RBridges RB1-RB4 and Legacy Ethernets LE1-LE5 Each Legacy Ethernet elects a single connected RBridge to represent it. There is a unique choice for LE1 through LE4 above, but LE5 must make a decision. This elected RBridge is known as the Designated RBridge, or DRB. Each Legacy Ethernet then builds its own spanning tree, perhaps (though not necessarily) rooted at its Designated RBridge. TrafÔ¨Åc from a Legacy Ethernet to the outside will generally be forwarded through its Designated RBridge; connections to other RBridges will not be used. The idea is for packets from one Legacy Ethernet to another to be delivered Ô¨Årst to the source node‚Äôs DRB, and then to the destination node‚Äôs DRB via true shortest-path forwarding between the RBridges, and from there to the destination node. Of course, in the ultimate case where every switch is an RBridge, trafÔ¨Åc will take the shortest path from start to Ô¨Ånish. The one exception to this rule about forwarding through the Designated RBridge is that the DRB can delegate this forwarding task to other RBridges for different VLANs within the Legacy Ethernet. If this is done, each VLAN will always use the same RBridge for all its outside trafÔ¨Åc. The second part of the process is for the RBridges each to Ô¨Ågure out the overall topology; that is, each builds a complete map of all the RBridges and their interconnections. This is done using a link-state routing-update protocol, described in 13.5 Link-State Routing-Update Algorithm. Of the two primary link-state protocols, IS-IS and OSPF, TRILL has selected the former, as it is more easily adapted to a setting in which, as here, nodes do not necessarily have IP addresses. The RBridges each send out appropriate ‚Äúlink-state packets‚Äù, using multicast and using per-RBridge databases to ensure that these packets are not re-forwarded endlessly. These link-state packets can be compared to spanning-tree Hello messages. As is fundamental to link-state forwarding, once each RBridge has a complete map of all the RBridges, each RBridge can calculate an optimal route to any other RBridge. As Designated RBridges see packets from their Legacy Ethernets, they learn the MAC addresses of the active hosts within, via the usual Ethernet learning protocol. They then share these addresses with other RBridges, using the IS-IS link-state protocol, so other RBridges eventually learn how to reach most if not all Ethernet addresses present in the overall network. TRILL still must make use of fallback-to-Ô¨Çooding, however, when delivering to previously unknown destinations. To this end, the RBridges negotiate among themselves a spanning tree covering all the RBridges. Any packet with unknown destination is Ô¨Çooded along this RBridge spanning tree, and then, as the packet reaches each Designated RBridge for a Legacy Ethernet, is Ô¨Çooded along the spanning tree of that Ethernet. This process is also used for delivery of broadcast and multicast packets. As RBridges talk to one another, they negotiate compact two-byte addresses ‚Äì known as ‚Äúnicknames‚Äù ‚Äì for one another, versus the standard Ethernet six-byte addresses. This saves space in the RBridge-to-RBridge 3.3 TRILL and SPB 79
An Introduction to Computer Networks, Release 2.0.11 communications. As packets travel between RBridges, a special TRILL header is added. This header includes a hopcount Ô¨Åeld, otherwise not present in Ethernet, which means any packets caught in transient routing loops will eventually be discarded. IS-IS may occasionally generate such routing loops, though they are rare. The TRILL header also includes the nicknames of the source and destination RBridges. This means that actual packet forwarding between RBridges does not involve the MAC address of the destination host; that is used only after the packet has reached the Designated RBridge for the destination Legacy Ethernet, at which point the TRILL header is removed. If a link between two RBridges fails, then the link‚Äôs endpoints send out IS-IS update messages to notify all the other RBridges of the failure. The other RBridges can then recalculate their forwarding tables so as not to use the broken link. Recovery time is typically under 0.1 seconds, a roughly hundredfold improvement over spanning-tree recovery times. TRILL supports the use of multiple equal-cost paths to improve throughput between two RBridges; cf 13.7 ECMP. In a high-performance datacenter, this feature is very important. Like TRILL, SPB uses IS-IS between the SPB-aware bridges to Ô¨Ånd shortest paths, and encapsulates packets with a special header as they travel between RBridges. SPB does not include a hopcount in the encapsulation header; instead, it more carefully controls forwarding. SPB also uses the original destination MAC address for inter-RBridge forwarding. 3.4 Software-DeÔ¨Åned Networking While TRILL and SPB offer one way to handle to the scaling problems of spanning trees, Software-DeÔ¨Åned Networking, or SDN, offers another, much more general, approach. The core idea of SDN is to place the forwarding mechanism of each participating switch under the aegis of a controller, a user-programmable device that is capable of giving each switch instructions on how to forward packets. Like TRILL and SPB, this approach also allows forwarding and redundant links to coexist. The controller can be a single node on the network, or can be a distributed set of nodes. The controller manages the forwarding tables of each of the switches. To handle legitimate broadcast trafÔ¨Åc, the controller can, at startup, probe the switches to determine their layout, and, from this, construct a suitable spanning tree. The switches can then be instructed to Ô¨Çood broadcast trafÔ¨Åc only along the links of this spanning tree. Links that are not part of the spanning tree can still be used for forwarding to known destinations, however, unlike conventional switches using the spanning tree algorithm. Typically, if a switch sees a packet addressed to an unknown destination, it reports it to the controller, which then must Ô¨Ågure out what to do next. One option is to have trafÔ¨Åc to unknown destinations Ô¨Çooded along the same spanning tree used for broadcast trafÔ¨Åc. This allows fallback-to-Ô¨Çooding to coexist safely with the full use of loop topologies. Switches are often conÔ¨Ågured to report new source addresses to the controller, so that the controller can tell all the other switches the best route to that new source. SDN controllers can be conÔ¨Ågured as simple Ô¨Årewalls, disallowing forwarding between selected pairs of nodes for security reasons. For example, if a datacenter has customers A and B, each with multiple nodes, 80 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 then it is possible to conÔ¨Ågure the network so that no node belonging to customer A can send packets to a node belonging to customer B. See also the following section. At many sites, the SDN implementation is based on standardized modules. However, controller software can also be developed locally, allowing very precise control of network functionality. This control, rather than the ability to combine loop topologies with Ethernet, is arguably SDN‚Äôs most important feature. See [FRZ13]. 3.4.1 OpenFlow Switches At the heart of SDN is the ability of controllers to tell switches how to forward packets. We next look at the packet-forwarding architecture for OpenFlow switches; OpenFlow is a speciÔ¨Åc SDN standard created by the Open Networking Foundation. See [MABPPRST08] and the OpenFlow switch speciÔ¨Åcation (2015 version). OpenFlow forwarding is built around one or more Ô¨Çow tables. The primary components of a Ô¨Çow-table entry are a set of match Ô¨Åelds and a set of packet-response instructions, or actions, if the match succeeds. Some common actions include 
- dropping the packet 
- forwarding the packet out a speciÔ¨Åed single interface 
- Ô¨Çooding the packet out a setof interfaces 
- forwarding the packet to the controller 
- modifying some Ô¨Åeld of the packet 
- processing the packet at another (higher-numbered) Ô¨Çow table The match Ô¨Åelds can, of course, be a single entry for the destination Ethernet address. But it can also include any other packet bit-Ô¨Åeld, and can include the ingress interface number. For example, the forwarding can be done entirely (or partially) on IP addresses rather than Ethernet addresses, thus allowing the OpenFlow switch to act as a so-called Layer 3 switch ( 9.6.3 Subnets versus Switching ), that is, resembling an IP router. Matching can be by the destination IP address and the destination TCP port, allowing separate forwarding for different TCP-based applications. In 13.6 Routing on Other Attributes we deÔ¨Åne policy-based routing; arbitrary such routing decisions can be implemented using OpenFlow switches. In SDN settings the policybased-routing abilities are sometimes used to segregate real-time trafÔ¨Åc and large-volume ‚Äúelephant‚Äù Ô¨Çows. In thel2_pairs.py example of the following section, matching is done on both Ethernet source and destination addresses. Flow tables bear a rough similarity to forwarding tables, with the match Ô¨Åelds corresponding to destinations and the actions corresponding to the next_hop. In simple cases, the match Ô¨Åeld contains a single destination address and the action is to forward out the corresponding switch port. Normally, OpenFlow switches handle broadcast packets by Ô¨Çooding them; that is, by forwarding them out all interfaces other than the arrival interface. It is possible, however, to set the NO_FLOOD attribute on speciÔ¨Åc interfaces, which means that packets designated for Ô¨Çooding (broadcast or otherwise) will not be sent out on those interfaces. This is typically how spanning trees for broadcast trafÔ¨Åc are implemented (see 30.9.6 l2_multi.py for a Mininet example). An interface marked NO_FLOOD, however, may still be used for unicast trafÔ¨Åc. Generally, broadcast Ô¨Çooding does not require a Ô¨Çow-table entry. 3.4 Software-DeÔ¨Åned Networking 81
An Introduction to Computer Networks, Release 2.0.11 Match Ô¨Åelds are also assigned a priority value. In the event that a packet matches two or more Ô¨Çow-table entries, the entry with the highest priority wins. The table-miss entry is the entry with no match Ô¨Åelds (thereby matching every packet) and with priority 0. Often the table-miss entry‚Äôs action is to forward the packet to the controller, although a packet that matches no entry is simply dropped. Flow-table instructions can also involve modifying (‚Äúmangling‚Äù) packets. One Ethernet-layer application might be VLAN coloring ( 3.2 Virtual LAN (VLAN) ); at the IPv4 layer, this could be used to decrement the TTL and update the checksum ( 9.1 The IPv4 Header ). In addition to match Ô¨Åelds and instructions, Ô¨Çow tables also include counters, Ô¨Çags, and a last_used time. The latter allows Ô¨Çows to be removed if no matching packets have been seen for a while. The counters allow the OpenFlow switch to implement Quality-of-Service constraints ‚Äì egbandwidth limiting ‚Äì on the trafÔ¨Åc. 3.4.2 Learning Switches in OpenFlow Suppose we want to implement a standard Ethernet learning switch ( 2.4.1 Ethernet Learning Algorithm ). The obvious approach is to use Ô¨Çows matching only on the destination address. But we encounter a problem because, by default, packets are reported to the controller only when there is no Ô¨Çow-entry match. Suppose switch S sees a packet from host B to host A and reports it to the controller, which installs a Ô¨Çow entry in S matching destination B (much as a real learning switch would do). If a packet now arrives at S from a third host C to B, it would simply be forwarded, as it would match the B Ô¨Çow entry, and therefore would not be reported to the controller. This means the controller would never learn about address C, and would never install a Ô¨Çow entry for C. One straightforward alternative approach that avoids this problem is to match on Ethernet xdestaddr,srcaddr y pairs. If a packet from A to B arrives at switch S and does not match any existing Ô¨Çow entry at S, it is reported to the controller, which now learns that A is reached via the port by which the packet arrived at S. A S Bport 1 port 2 In the network above, suppose A sends a packet to B (or broadcasts a packet meant for B), and the Ô¨Çow table of S is empty. S will report the packet to the controller (not shown in the diagram), which will send it back to S to be Ô¨Çooded. However, the controller will also record that A can be reached from S via port 1. Next, suppose B responds. When this packet from B arrives at S, there are still no Ô¨Çow-table entries, and so S again reports the packet to the controller. But this time the controller knows, because it learned from the Ô¨Årst packet, that S can reach A via port 1. The controller also now knows, from the just-arrived packet, that B can be reached via port 2. Knowing both of these forwarding rules, the controller now installs two Ô¨Çow-table entries in S: dst=B,src=A: forward out port 2 dst=A,src=B: forward out port 1 If a packet from a third host Cnow arrives at S, addressed to B, it will not be forwarded, even though its destination address matches the Ô¨Årst rule above, as its source address does not match A. It will instead be sent to the controller (and ultimately be Ô¨Çooded). When B responds to C, the controller will install rules for dst=C,src=B and dst=B,src=C. If the packet from C were not reported to the controller ‚Äì perhaps because 82 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 S had a Ô¨Çow rule for dst=B only ‚Äì then the controller would never learn about C, and would never be in a position to install a Ô¨Çow rule for reaching C. The pairs approach to OpenFlow learning is pretty much optimal, if a single Ô¨Çow-entry table is available. The problem with this approach is that it does not scale well; with 10,000 addresses on the network, we will need 100,000,000 Ô¨Çowtable-entry pairs to describe all the possible forwarding. This is prohibitive. We examine a real implementation (in Python) of the pairs approach in 30.9.2 l2_pairs.py, using the Mininet network emulator and the Pox controller ( 30 Mininet ). A more compact approach is to use multiple Ô¨Çow tables: one for matching the destination, and one for matching the source. In this version, the controller never has to remember partial forwarding information, as the controller in the version above had to do after receiving the Ô¨Årst packet from A. When a packet arrives, it is matched against the Ô¨Årst table, and any actions speciÔ¨Åed by the match are carried out. One of the actions may be a request to repeat the match against the second table, which may lead to a second set of actions. We repeat the A √ëB, B√ëA example above, letting T 0denote the Ô¨Årst table and T 1denote the second. Initially, before any packets are seen, the controller installs the following low-priority match rules in S: T0: match nothing: Ô¨Çood, send to T 1 T1: match nothing: send to controller These are in effect default rules: because there are no packet Ô¨Åelds to match, they match all packets. The low priority ensures that better-matching rules are always used when available. When the packet from A to B arrives, the T 0rule above means the packet is Ô¨Çooded to B, while the T 1rule means the packet is sent to the controller. The controller then installs the following rules in S: T0: match dst=A: forward via port 1, send to T 1 T1: match src=A: do nothing Now B sends its reply to A. The Ô¨Årst rule above matches, so the packet is forwarded by S to A, and is resubmitted to T 1. The T 1rule immediately above, however, does notmatch. The only match is to the original default rule, and the packet is sent to the controller. The controller then installs another two rules in S: T0: match dst=B: forward via port 2, send to T 1 T1: match src=B: do nothing At this point, as A and B continue to communicate, the T 0rules ensure proper forwarding, while the T 1 rules ensure that no more packets from this Ô¨Çow are sent to the controller. Note that the controller always installs the same address in the T 0table and the T 1table, so the list of addresses present in these two tables will always be identical. The T 0table always matches destinations, though, while the T 1table matches source addresses. The Mininet/Pox version of this appears in 30.9.3 l2_nx.py. Another application for multiple Ô¨Çow tables involves switches that make quality-of-service prioritization decisions. A packet‚Äôs destination would be found using the Ô¨Årst Ô¨Çow table, while its priority would be found by matching to the second table. The packet would then be forwarded out the port determined by the Ô¨Årst table, using the priority determined by the second table. Like building a learning switch, this can be done with a single table by listing all combinations of xdestaddr,priority y, but sometimes that‚Äôs too many entries. 3.4 Software-DeÔ¨Åned Networking 83
An Introduction to Computer Networks, Release 2.0.11 We mentioned above that SDN controllers can be used as Ô¨Årewalls. At the Ethernet-address level this is tedious to conÔ¨Ågure, but by taking advantage of OpenFlow‚Äôs understanding of IP addresses, it is straightforward, for example, to block trafÔ¨Åc between different IP subnets, much like a router might do. OpenFlow also allows blocking all such trafÔ¨Åc except that between speciÔ¨Åc pairs of hosts using speciÔ¨Åc protocols. For example, we might want customer A‚Äôs web servers to be able to communicate with A‚Äôs database servers using the standard TCP port, while still blocking all other web-to-database trafÔ¨Åc. 3.4.3 Other OpenFlow examples After emulating a learning switch, perhaps the next most straightforward OpenFlow application, conceptually, is the support of Ethernet topologies that contain loops. This can be done quite generically; the controller does not need any special awareness of the network topology. On startup, switches are instructed by the controller to report their neighboring switches. With this information the controller is then able to form a complete map of the switch topology. (One way to implement this is for the controller to order each switch to send a special marked packet out each of its ports, and then for the receiving switches to report these packets back to the controller.) Once the controller knows the switch topology, it can calculate a spanning tree, and then instruct each switch that Ô¨Çooded packets should be sent out only via ports that correspond to links on the spanning tree. Once the location of a destination host has been learned by the controller (that is, the controller learns which switch the host is directly connected to), the controller calculates the shortest (or lowest-cost, if the application supports differential link costs) path from each switch to that host. The controller then instructs each switch how to forward to that host. Forwarding will likely use links that are not part of the spanning tree, unlike traditional Ethernet switches. We outline an implementation of this strategy in 30.9.6 l2_multi.py. 3.4.3.1 Interconnection Fabric The previous Ethernet-loop example is quite general; it works for any switch topology. Many other OpenFlow applications require that the controller contains some prior knowledge of the switch topology. As an example of this, we present what we will refer to as an interconnection fabric. This is the S1-S5 network illustrated below, in which every upper (S1-S2) switch connects directly to every lower (S3-S5) switch. The bottom row in the diagram represents server racks, as interconnection fabrics in some form or other are very common in datacenters. (For a real-world datacenter example, see this Facebook example, although the Facebook example is connected using routing rather than switching with SDN (see 15.9 BGP for Interior Routing ). Facebook refers to their upper-level fabric as a spine plane .) The red and blue numbers in the diagram below identify the switch ports. The Ô¨Årst two rows here contain many loops, eg S1‚ÄìS3‚ÄìS2‚ÄìS4‚ÄìS1 (omitting the S3-S5 row, and having S1 and S2 connect directly to the server racks, does not eliminate loops). In the previous example we described how we could handle loops in a switched network by computing a spanning tree and then allowing packet Ô¨Çooding only along this spanning tree. This is certainly possible here, but if we allow the spanning-tree algorithm to prune the loops, we will lose most of the parallelism between the S1-S2 and S3-S5 layers; see exercise 8.5. This generic spanning-tree approach is not what we want. To start, let us assume that S1 and S2 represent entry points, and our goal is to route trafÔ¨Åc entering these switches via port 0 to the appropriate server rack. That is, trafÔ¨Åc Ô¨Çow is between the top and the bottom row. 84 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S5 S4 S30 12 30 1 23 0 0 01 1 1 234 234 234 If we use IP routing at S1 through S5, as in 13 Routing-Update Algorithms, we then need the three clusters of server racks below S3, S4 and S5 to be on three separate IP subnets ( 9.6 IPv4 Subnets ). While this is always technically possible, it does mean that subnets must be allocated based on physical rack location, rather than on any particular logical organization. One OpenFlow approach is to assume that the three clusters of server racks below S3-S4-S5 form a single IP subnet. We can then conÔ¨Ågure S1-S5 with OpenFlow so that trafÔ¨Åc from the subnet is always forwarded upwards while trafÔ¨Åc tothe subnet is always forwarded downwards. But an even simpler solution ‚Äì one not requiring any knowledge of the server subnet layout ‚Äì is to use OpenFlow to conÔ¨Ågure the switches S1-S5 so that unknown-destination trafÔ¨Åc entering on a red (upper) port is Ô¨Çooded out only on the blue (lower) ports, and vice-versa. This eliminates loops by ensuring that all trafÔ¨Åc goes through the interconnection fabric either upwards-only or downwards-only. After the destination server below S3-S5 has replied, of course, S1 or S2 will learn to which of S3-S5 it should forward future packets to that server. In real datacenters, trafÔ¨Åc volume between server racks is often much larger than trafÔ¨Åc volume entering or exiting the site. So we might alternatively assume that the goal is to provide multiple parallel paths between pairs of S3, S4, and S5, each path traveling through either S1 or S2. With this perspective, our color rule fails at the upper layer: trafÔ¨Åc entering S1 or S2 on a blue interface will also exit via a blue interface. However, the color trick still works, provided we restrict it only to S3, S4 and S5: trafÔ¨Åc that enters on blue will exit on red, and vice-versa. (Dividing trafÔ¨Åc among the multiple paths can also be done with SDN; it helps that all parallel paths have the same length and so we avoid the need for best-path selection. An alternative approach is Equal-Cost Multipath Routing, 13.7 ECMP .) This example works the way it does because of particular properties of the topology. With the Ô¨Årst, top-tobottom, assumption, once we eliminate paths that both enter and leave S1 or S2 via blue nodes, or that enter and leave S3, S4 and S5 via red nodes, there is a unique path between any input port (red upper port) and any output port (towards the server racks). Loops are avoided trivially. For the second, bottom-to-bottom, assumption, we eliminated red-to-red paths from the bottom row only. There are now multiple paths between any pair of S3, S4 and S5 (which is the goal), but, again, no loops. Given more-general topologies, on the other hand, in which loop-free paths cannot be guaranteed by such port-color-based rules, the OpenFlow controller has a more complicated job in choosing the path. This generally entails path discovery, shortest-path selection and loop avoidance, as in the previous section. 3.4 Software-DeÔ¨Åned Networking 85
An Introduction to Computer Networks, Release 2.0.11 3.4.3.2 Load Balancer The previous example was quite local, in that all the OpenFlow actions are contained within the interconnection fabric. As a larger-scale (and possiby more typical) special-purpose OpenFlow example, we next describe how to achieve server load-balancing via SDN; that is, users are connected transparently to one of several identical servers. Each server handles only its assigned fraction of the total load. For this example, the controller must not only have knowledge of the topology, but also of the implementation goal. To build the load-balancer, imagine that the SDN controller is in charge of, in the diagram of the previous section, all switches in the interconnection fabric and also all switches among the server racks below. At this point, we conÔ¨Ågure all the frontline servers within the server racks identically, including giving them all identical IPv4 addresses. When an incoming TCP connection request arrives, the controller picks a server (perhaps using round robin, perhaps selecting the server with the lowest load) and sets up OpenFlow forwarding rules so all trafÔ¨Åc on that TCP connection arriving from the outside world is sent to the designated server, and vice-versa. Different servers with the same IPv4 address are not allowed to talk directly with one another at all, thereby averting chaos. The lifetime of the OpenFlow forwarding rule can be adjusted as desired, egto match the typical lifetime of a user session. When the Ô¨Årst TCP packet of a connection arrives at the selected server, the server may or may not need to use ARP to Ô¨Ågure out the appropriate internal LAN address to which to send its reply. Sometimes ARP needs to be massaged a bit to work acceptably in an environment in which some hosts have the same IPv4 address. At no point is the fact that multiple servers have been assigned the same IPv4 address directly exposed: not to other servers, not to internal routers, and not to end users. (Servers never initiate outbound connections to users, so there is no risk of two servers contacting the same user.) For an example of this sort of load balancing implemented in Mininet and Pox, see 30.9.5 loadbalance31.py. The identical frontline servers might need to access a common internal database cluster. This can be implemented by assigning each server a second IPv4 address for this purpose, not shared with other servers, or by using the common public-facing IPv4 address and a little more OpenFlow cleverness in setting up appropriate forwarding rules. If the latter approach is taken, it is now in principle possible that two servers would connect to the database using the same TCP port, by coincidence. This would expose the identical IPv4 addresses, and the SDN controllers would have to take care to ensure that this did not happen. One approach, if supported, would be to have the OpenFlow switches ‚Äúmangle‚Äù the server IPv4 addresses or ports, as is done with NAT ( 9.7 Network Address Translation ). There are also several ‚Äútraditional‚Äù strategies for implementing load balancing. For example, one can give each server its own IPv4 address but then use round-robin DNS ( 10.1 DNS ) to assign different users to different servers. Alternatively, one can place a device called a load balancer at the front of the network that assigns incoming connection requests to an internal server and then takes care of setting up the appropriate forwarding. Forwarding can be at the IP layer (that is, via routing), or at the TCP layer, or at the application layer. The load balancer can be thought of as NAT-like ( 9.7 Network Address Translation ) in that it maintains a table of associations between external-user connections and a internal servers; once a user connects, the association with the chosen server remains in place for a period of time. One advantage of the SDN approach described here is that the individual front-line servers need no special conÔ¨Åguration; all of the load-sharing awareness is contained within the SDN network. Furthermore, the SDN switches do virtually no additional work beyond ordinary forwarding; they need only involve the controller when the 86 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 Ô¨Årst new TCP packet of each connection arrives. 3.4.3.3 Key-Value Store The packet Ô¨Åelds matched by OpenFlow do not have to be part of packet headers; data Ô¨Åelds can be matched as well. This makes it possible to implement some forms of key-value cache using SDN. Imagine a datalookup protocol that uses a userid, for example, to look up a hashed password on server S. Once a controller has seen key K (the userid) and corresponding value V (the hashed password), during a Ô¨Årst full exchange, an OpenFlow switch can be instructed to return V directly whenever a later request packet containing key K is seen. Matching on other Ô¨Åelds can be used to verify that the request is indeed directed to S. The later request packet need not be sent on to S at all. This is potentially much faster than having S process repeated requests. 3.5 Epilog As datacenters have grown, so has Ethernet. While A complex Ethernet can now handle tens of thousands of nodes, the ineluctable growth in broadcast trafÔ¨Åc remains a serious limiting factor. At some point, the IP approach to scaling of large networks, by division into multiple subnets ( 9.6 IPv4 Subnets ), wins out. That said, the tools in this chapter have enabled an awful lot of very large Ethernets, both campus-wide and within datacenters. To some degree, this is a matter of cost: fast Ethernet switches are often cheaper than fast IP routers. Software-deÔ¨Åned networking has been heralded as the future of networking. Someday that future may arrive. While SDN has been very successful in some speciÔ¨Åc areas, Ethernet installations are still dominant. 3.6 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.3 Solutions for Advanced Ethernet. 1.0. The following network is like that of 3.1.1 Spanning Tree Example 1: Switches Only, except that the switches are numbered differently. Again, the ID of switch Sn is n, so S1 will be the root. Which links end up ‚Äúpruned‚Äù by the spanning-tree algorithm, and why? Diagram the network formed by the surviving links. S1 S4 S6 S3 S5 S2 2.0. Consider the network below, consisting of just the Ô¨Årst two rows from the datacenter diagram in 3.4.1 OpenFlow Switches: 3.5 Epilog 87
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S5 S4 S3 (a).‚ô¢Give network of surviving links after application of the spanning-tree algorithm. Assume the ID of switch Sn is n. In this network, what is the path of trafÔ¨Åc from S2 to S5? (b). Do the same as part (a) except assuming S4 has ID 0, and so will be the root, while the ID for the other Sn remains n. What will be the path of trafÔ¨Åc from S1 to S5? 3.0. Suppose you want to develop a new protocol so that Ethernet switches participating in a VLAN all keep track of the VLAN ‚Äúcolor‚Äù associated with every destination in their forwarding tables. Assume that each switch knows which of its ports (interfaces) connect to other switches and which may connect to hosts, and in the latter case knows the color assigned to that port. (a). Suggest a way by which switches might propagate this destination-color information to other switches. (b). What must be done if a port formerly reserved for connection to another switch is now used for a host? 4.0. (This exercise assumes some familiarity with Distance-Vector routing as in 13 Routing-Update Algorithms .) (a). Suppose switches are able to identify the non-switch hosts that are directly connected, that is, reachable without passing through another switch. Explain how the algorithm of 13.1 Distance-Vector Routing-Update Algorithm could be used to construct optimal Ethernet forwarding tables even if loops were present in the network topology. (b). Suppose switches are allowed to ‚Äúmark‚Äù packets; all packets are initially unmarked. Give a mechanism that allows switches to detect which non-switch hosts are directly connected. (c). Explain why Ethernet broadcast (and multicast) would still be a problem. 5.0.‚ô¢Consider the following arrangement of three hosts h1, h2, h3 and one OpenFlow switch S with ports 1, 2 and 3 and controller C (not shown) 88 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 h1 S h2 h312 3 Four packets are then transmitted: (a). h1√ëh2 (b). h2√ëh1 (c). h3√ëh1 (d). h2√ëh3 Assume that S reports to C all packets with unknown destination, that is, all packets for which S does not have a forwarding entry for that packet‚Äôs destination. Packet reports include the source and destination addresses and the arrival port. On receiving a report, if the source address is previously unknown then C installs on S a forwarding-table entry for that source address. At that point S uses its forwarding table (including any new entries) to forward the packet, if a suitable entry exists. Otherwise S Ô¨Çoods the packet as usual. For the four packets above, indicate i. whether S reports the packet to C ii. if so, any new forwarding entry C installs on S iii. whether S is then able to forward the packet using its table, or must fall back to Ô¨Çooding. (If S does not report the packet to C then S must have had a forwarding-table entry for that destination, and so S is able to forward the packet normally.) 6.0. Consider again the arrangement of exercise 12.0 of three hosts h1, h2, h3 and one OpenFlow switch S with ports 1, 2 and 3 and controller C (not shown) h1 S h2 h312 3 The same four packets are transmitted: (a). h1√ëh2 (b). h2√ëh1 (c). h3√ëh1 (d). h2√ëh3 3.6 Exercises 89
An Introduction to Computer Networks, Release 2.0.11 This time, assume that S reports to C all packets with unknown destination or unknown source (that is, S does not have a forwarding entry for either the packet‚Äôs source or destination address). For the four packets above, indicate i. whether S reports the packet to C ii. if so, any new forwarding entry C installs on S iii. whether S is then able to forward the packet using its table, or must fall back to Ô¨Çooding. As before, packet reports include the source and destination addresses and the arrival port. On receiving a report, if the source address is previously unknown then C installs on S a forwarding-table entry for that source address. At that point S uses its forwarding table (including any new entries) to forward the packet, if a suitable entry exists. Otherwise S Ô¨Çoods the packet as usual. Again, if S does not report a packet to C then S must have had a forwarding-table entry for that destination, and so is able to forward the packet normally. 7.0 Consider the following arrangement of three switches S1-S3, three hosts h1-h3 and one OpenFlow controller C. h1 h2 h3 S1 S2 S3 C As with exercise 5.0, assume that the switches report packets to C only if they do not already have a forwarding-table entry for the packet‚Äôs destination. After each report, C installs a forwarding-table entry on the reporting switch for reaching the packet‚Äôs source address via the arrival port. At that point the switch Ô¨Çoods the packet (as the destination must not have been known). If a switch can forward a packet without reporting to C, no new forwarding entries are installed. Packets are now sent as follows: h1√ëh2 h2√ëh1 h1√ëh3 h3√ëh1 h2√ëh3 h3√ëh2 At the end, what are the forwarding tables on S1 ‚ô¢, S2 and S3? 8.0 Here are the switch rules for the multiple-Ô¨Çow-table example in 3.4.2 Learning Switches in OpenFlow: Table match Ô¨Åeld match action no-match default T0 destaddr forward and send to T 1Ô¨Çood and send to T 1 T1 srcaddr do nothing send to controller 90 3 Advanced Ethernet
An Introduction to Computer Networks, Release 2.0.11 Give a similar table where the matches are reversed; that is, T 0matches the srcaddr Ô¨Åeld and T 1matches the destaddr Ô¨Åeld. 3.6 Exercises 91
An Introduction to Computer Networks, Release 2.0.11 92 3 Advanced Ethernet
4 WIRELESS LANS Ethernet is ubiquitous in the datacenter, and in some large corporate desktop networks. Yet many residential and employee laptops never touch an Ethernet cable; Ethernet connections for smartphones and similar devices are almost unheard of. It would be difÔ¨Åcult to imagine contemporary laptop networking, let alone mobile devices, without wireless networking. In both homes and ofÔ¨Åces, Wi-Fi connectivity is the norm. Mobile networking is ubiquitous. A return to being tethered by wires is unthinkable. 4.1 Adventures in Radioland For this chapter we leave wires (and Ô¨Åber) behind, and contemplate the transmission of packets via radio, freeing nodes from their cable tethers. Wi-Ô¨Å ( 4.2 Wi-Fi ) and mobile wireless ( 4.3 WiMAX and LTE ) are now ubiquitous. But radio is not quite like wire, and wireless transmission of packets brings several changes. 4.1.1 Privacy It‚Äôs hard to tap into wired Ethernet, especially if you are locked out of the building. But anyone can receive wireless transmissions, often from a considerable distance. The data breach at TJX Corporation was achieved by attackers parking outside a company building and pointing a directional antenna at it; encryption was used but it was weak (see 28 Security and28.7.7 Wi-Fi WEP Encryption Failure ). Similarly, Internet caf√© visitors generally don‚Äôt want other patrons to read their email. Radio communication needs strong encryption. 4.1.2 Collisions Ethernet-like collision detection is no longer feasible over radio. To some extent, this has to do with the relative signal strength of the remote signal at the local transmitter. Along a wire-based Ethernet the remote signal might be as weak as 1/100 of the transmitted signal but that 1% received signal is still detectable during transmission. However, with radio the remote signal might easily be as little as 1/1,000,000 of the transmitted signal (-60 dB), as measured at the transmitting station, and it is simply overwhelmed during transmission. Even if signal strength could be resolved, there is also the ‚Äúhidden-node problem‚Äù, in the following section and also at 4.2.1.4 Hidden-Node Problem ): perhaps nodes A and B can both communicate directly with C, but there is a radio-opaque wall blocking any direct communication between A and B. If A and B now transmit simultaneously, C will encounter a collision, but there is no way for A or B to detect this collision directly. As a result, wireless protocols must be constructed appropriately. We will look at how Wi-Fi handles this in its most common mode of operation in 4.2.1 Wi-Fi and Collisions. Wi-Fi also supports its PCF mode (4.2.7 Wi-Fi Polling Mode ) that involves fewer (but not zero) collisions through the use of central-point polling. Finally, WiMAX and LTE switch from polling to scheduling to further reduce collisions, though the potential for collisions is still inevitable when new stations join the network. 93
An Introduction to Computer Networks, Release 2.0.11 It is also worth pointing out that, while an Ethernet collision affects every station in the physical Ethernet (the ‚Äúcollision domain‚Äù), wireless collisions are local, occuring at the receiver. Two stations can transmit at the same time, and in range of one another, but without a collision! This can happen if each of the relevant receivers is in range of only one of the two transmitting stations. As an example, suppose three stations are arranged linearly, A‚ÄìC‚ÄìB, with the A‚ÄìC and C‚ÄìB distances just under the maximum effective range. When A and B both transmit there is indeed a collision at C. But when C and B transmit simultaneously, A may receive C‚Äôs signal just Ô¨Åne, as B‚Äôs is too weak to interfere. 4.1.3 Hidden Nodes In wireless communication, two nodes A and B that are not in range of one another ‚Äì and thus cannot detect one another ‚Äì may still have their signals interfere at a third node C. Fig. 20:: If A and B transmit simultaneously, there is a collision at C, reachable by both, but not at D This creates an additional complication to collision handling: if A and B transmit simultaneously then there will be a collsion at C, but neither A nor B can possibly detect this. See 4.2.1.4 Hidden-Node Problem. 4.1.4 Band Width To radio engineers, ‚Äúband width‚Äù (two words) means the frequency range used by a signal, not the data transmission rate. No information can be conveyed using a single frequency; even signaling by switching a carrier frequency off and on at a low rate ‚Äúblurs‚Äù the carrier into a band of nonzero width. In keeping with this we will for the remainder of this chapter use the term ‚Äúdata rate‚Äù for what we have previously called ‚Äúbandwidth‚Äù. We will use the terms ‚Äúchannel width‚Äù or ‚Äúwidth of the frequency band‚Äù for the frequency range, as ‚Äúband width‚Äù is easily mistaken for ‚Äúbandwidth‚Äù. All else being equal, the data rate achievable with a radio signal is proportional to the channel width. The constant of proportionality is limited by the Shannon-Hartley theorem: the maximum data rate divided by the width of the frequency band is log 2(1+SNR), where SNR is the signal to noise power ratio. If SNR is 127, for example, and the width of the frequency band is 1 MHz, then the maximum theoretical data rate is 94 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 7 Mbps, where 7 = log 2(128). If the signal power S drops by about half so SNR=63, the data rate falls to 6 Mbps, as 6 = log 2(64); the relationship between signal power and data rate is logarithmic. The Shannon-Hartley theorem assumes that noise has a speciÔ¨Åc statistical form known as Gaussian white noise. It is theoretically possible that the Shannon-Hartley limit might be surpassed for some other, very speciÔ¨Åc source of noise. 4.1.4.1 OFDM The actual data rate achievable, for a given channel width and SNR, depends on the signal encoding, or modulation, mechanism. Most newer modulation mechanisms use ‚Äúorthogonal frequency-division multiplexing‚Äù, OFDM, or some variant. A central feature of OFDM is that one wider frequency band is divided into multiple narrow subchannels; each subchannel then carries a proportional fraction of the total information signal, modulated onto a subchannel-speciÔ¨Åc carrier. All the subchannels can be allocated to one transmission at a time (time-division multiplexing, 6.2 Time-Division Multiplexing ), or disjoint sets of subchannels can be allocated to different transmissions that can then proceed (at proportionally lower data rates) in parallel. The latter is known as frequency-division multiplexing. In many settings OFDM comes reasonably close to the Shannon-Hartley limit. Perhaps more importantly, OFDM also performs reasonably well with multipath interference, below, which is endemic in urban and building-interior environments with their many reÔ¨Çective surfaces. Multipath interference is, however, not necessarily comparable to the Gaussian noise assumed by the Shannon-Hartley theorem. We will not address further technical details of OFDM here, except to note that implementation usually requires some form of digital signal processing. The OFDMA variant, with the MA standing for Multiple Access, allows multiple users to use nonoverlapping sets of subchannels, thus allowing simultaneous transmission. It is an option available in 802.11ax. 4.1.5 Cost Another fundamental issue is that everyone shares the same radio spectrum. For mobile wireless providers, this constraint has driven prices to remarkable levels; the 2014-15 FCC AWS-3 auction raised almost $45 billion for 65 MHz (usable throughout the entire United States). This works out to somewhat over $2 per megahertz per phone. The corresponding issue for Wi-Fi users in a dense area is that all the available Wi-Fi capacity may be in use. Inside busy buildings one can often see dozens of Wi-Fi access points competing for the same Wi-Fi channel; the result is that no user will be getting close to the nominal data rates of 4.2 Wi-Fi. Higher data rates require wider frequency bands. To reduce costs in the face of Ô¨Åxed demand, the usual strategy is to make the coverage zones smaller, either by reducing power (and adding more access points as appropriate), or by using directional antennas, or both. 4.1.6 Multipath While a radio signal generally covers a wide area ‚Äì even with ordinary directional antennas ‚Äì it does so in surprisingly non-uniform ways. A signal may reach a receiver through a line-of-sight path and also several 4.1 Adventures in Radioland 95
An Introduction to Computer Networks, Release 2.0.11 reÔ¨Çected paths, possibly of varying length. In addition to reÔ¨Çection, the signal may be subject to reÔ¨Çectionlikescattering anddiffraction. All of this together is known as multipath interference (or, if analog audio is involved, multipath distortion; in the analog TV era this was ghosting ). A B Fig. 21:: Left: Line-of-sight and reÔ¨Çected signals; Right: Resulting superposition of encoded data The picture above shows two transmission paths from A to B. The respective carrier paths may interfere with or supplement one another. The longer delay of the reÔ¨Çecting path (red) will also delay its encoded signal. The result, shown at right, is that the line-of-sight and reÔ¨Çected data symbols may overlap and interfere with each other; this is known as intersymbol interference. Multipath interference may even change the meaning of the data symbol as seen by the receiver; for example, the red and black low datasignal peaks above at the point of the vertical dashed line may sum together so as to be received as a higher peak (assuming the underlying carriers are in sync). Multipath interference tends to lead to wide Ô¨Çuctuations in signal intensity with a period of about half a wavelength; this phenomenon is known as multipath fading. As an example, the wavelength of FM radio stations in the United States is about 3 meters; in fringe reception areas it is not uncommon to pull a car forward a quarter wavelength and have a station go from clear to indecipherable, or even for reception to switch to another station (on the same frequency but transmitted from another location) altogether. The picture above is from a mathematical simulation intended to illustrate multipath fading. The walls of the room reÔ¨Çect 40% of the signal from the transmitter located in the orange ball at the lower left. The transmitter transmits an unmodulated carrier signal, which may be reÔ¨Çected off the walls any number of times; at any point in the room the total signal intensity is the sum over all possible reÔ¨Çection paths. On the right-hand side, the small-scale blue ripples represent the received carrier strength variation due to multipath interference between the line-of-sight and all the reÔ¨Çected paths. Note that the ripple size is about half a wavelength. In comparison to this simulated intensity map, real walls tend to have a lower reÔ¨Çectivity, real rooms are not two-dimensional, and real carriers are modulated. However, real rooms also introduce scattering, diffraction and shadowing from objects within, and signiÔ¨Åcant (3 to 10) multipath-fading signal-strength variations are common in actual wireless settings. Multipath fading can be either Ô¨Çat‚Äì affecting all frequencies more or less equally ‚Äì or selective ‚Äì affecting some frequencies differently than others. It is quite possible for an OFDM channel ( 4.1.4.1 OFDM ) to encounter selective fading of only some of its subchannel frequencies. Generally, multipath interference is a problem that engineers go to great lengths to overcome. However, as we shall see in 4.2.3 Multiple Spatial Streams, multipath interference can sometimes be put to positive 96 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 Fig. 22:: Signal-intensity map (simulated) in a room with walls with 40% reÔ¨Çectivity 4.1 Adventures in Radioland 97
An Introduction to Computer Networks, Release 2.0.11 use by allowing almost-adjacent antennas to transmit and receive independent signals, thus increasing the effective throughput. For an alternative example of multipath interference in which the signal strength has no ripples, see exercise 6.0. 4.1.7 Power If you are cutting the network cable and replacing it with wireless, there is a good chance you will also want to cut the power cable as well and replace it with batteries. This tends to make power consumption a very important issue. The Wi-Fi standard has provisions for minimizing power usage by allowing a device to ‚Äúdoze‚Äù most of the time, waking periodically to check if any packets are ready to be sent to it (see 4.2.4.1 Joining a Network ). The 6LoWPAN project (IPv6 Low-power Wireless Personal Area Network) is intended to support very low-power devices; see RFC 4919 andRFC 6282. 4.1.8 Tangle Wireless is also used simply to replace cords and their attendant tangle, and, of course, the problem of incompatible connectors. The low-power Bluetooth wireless standard is commonly used as a cable alternative for things like computer mice and telephone headsets. Bluetooth is also a low-power network; for many applications the working range is about 10 meters. ZigBee is another low-power small-scale network. 4.2 Wi-Fi Wi-Fi is a trademark of the Wi-Fi Alliance denoting any of several IEEE wireless-networking protocols in the 802.11 family, speciÔ¨Åcally 802.11a, 802.11b, 802.11g, 802.11n, 802.11ac and 802.11ax. (Strictly speaking, these are all amendments to the original 802.11 standard, but they are also de facto standards in their own right.) Like classic Ethernet, Wi-Fi must deal with collisions; unlike Ethernet, however, WiFi is unable to detect collisions in progress, complicating the backoff and retransmission algorithms. See 4.1.2 Collisions above. Unlike any wired LAN protocol we have considered so far, in addition to normal data packets Wi-Fi also uses control andmanagement packets that exist entirely within the Wi-Fi LAN layer; these are not initiated by or delivered to higher network layers. Control packets are used to compensate for some of the infelicities of the radio environment, such as the lack of collision detection. Putting radio-essential control and management protocols within the Wi-Fi layer means that the IP layer can continue to interact with the Wi-Fi LAN exactly as it did with Ethernet; no changes are required. Wi-Fi is designed to interoperate freely with Ethernet at the logical LAN layer. Wi-Fi MAC (physical) addresses have the same 48-bit size as Ethernet‚Äôs and the same internal structure ( 2.1.3 Ethernet Address Internal Structure ). They also share the same namespace: one is never supposed to see an Ethernet and a Wi-Fi interface with the same address. As a result, data packets can be forwarded by switches between Ethernet and Wi-Fi; in many respects a Wi-Fi LAN attached to an Ethernet LAN looks like an extension of the Ethernet LAN. See 4.2.4 Access Points. 98 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 Microwave Ovens and Wi-Fi The impact of a running microwave oven on Wi-Fi signals is quite evident if the oven is between the sender and receiver. For other conÔ¨Ågurations the effect may vary. Most ovens transmit only during one half of the A/C cycle, that is, they are on 1/60 sec and then off 1/60 sec; this may allow intervening transmission time. See also here. Traditionally, Wi-Fi used the 2.4 GHz ISM (Industrial, ScientiÔ¨Åc and Medical) band used also by microwave ovens, though 802.11a used a 5 GHz band, 802.11n supports that as an option and the new 802.11ac has returned to using 5 GHz exclusively. The 5 GHz band has reduced ability to penetrate walls, often resulting in a lower effective range (though in ofÔ¨Åces and multi-unit housing this can be an advantage). The 5 GHz band provides many more usable channels than the 2.4 GHz band, resulting in much less interference in ‚Äúcrowded‚Äù environments. Wi-Fi radio spectrum is usually unlicensed, meaning that no special permission is needed to transmit but also that others may be trying to use the same frequency band simultaneously. The availability of unlicensed channels in the 5 GHz band continues to improve. The table below summarizes the different Wi-Fi versions. All data bit rates assume a single spatial stream; channel widths are nominal. The names in the far-right column have been introduced by the Wi-Fi Alliance as a more convenient designation for the newer versions. IEEE name maximum bit rate frequency channel width new name 802.11a 54 Mbps 5 GHz 20 MHz 802.11b 11 Mbps 2.4 GHz 20 MHz 802.11g 54 Mbps 2.4 GHz 20 MHz 802.11n 65-150 Mbps 2.4/5 GHz 20-40 MHz Wi-Fi 4 802.11ac 78-867 Mbps 5 GHz 20-160 MHz Wi-Fi 5 802.11ax Up to 1200 Mbps 2.4/5+ GHz 20-160 MHz Wi-Fi 6 The maximum bit rate is seldom achieved in practice. The effective bit rate must take into account, at a minimum, the time spent in the collision-handling mechanism. More signiÔ¨Åcantly, all the Wi-Fi variants above use dynamic rate scaling, below; the bit rate is reduced up to tenfold (or more) in environments with higher error rates, which can be due to distance, obstructions, competing transmissions or radio noise. All this means that, as a practical matter, getting 150 Mbps out of 802.11n requires optimum circumstances; in particular, no competing senders and unimpeded line-of-sight transmission. 802.11n lower-end performance can be as little as 10 Mbps, though 40-100 Mbps (for a 40 MHz channel) may be more typical. The 2.4 GHz ISM band is divided by international agreement into up to 14 ofÔ¨Åcially designated (and mostly adjacent) channels, each about 5 MHz wide, though in the United States use may be limited to the Ô¨Årst 11 channels. The 5 GHz band is similarly divided into 5 MHz channels. One Wi-Fi sender, however, needs several of these ofÔ¨Åcial channels; the typical 2.4 GHz 802.11g transmitter uses an actual frequency range of up to 22 MHz, or up to Ô¨Åve ofÔ¨Åcial channels. As a result, to avoid signal overlap Wi-Fi use in the 2.4 GHz band is often restricted to ofÔ¨Åcial channels 1, 6 and 11. The end result is that there are generally only three available Wi-Fi bands in the 2.4 GHz range, and so Wi-Fi transmitters can and do interact with and interfere with each other. There are almost 200 5 MHz channels in the 5 GHz band. The United States requires users of the this band to avoid interfering with weather and military applications in the same frequency range; this may 4.2 Wi-Fi 99
An Introduction to Computer Networks, Release 2.0.11 involve careful control of transmission power (under the IEEE 802.11h amendment) and so-called ‚Äúdynamic frequency selection‚Äù to choose channels with little interference, and to switch to such channels if interference is detected later. Even so, there are many more channels than at 2.4 GHz; the larger number of channels is one of the reasons (arguably the primary reason) that 802.11ac can run faster (below). The number of channels available for Wi-Fi use has been increasing, often as conÔ¨Çicts with existing 5 GHz weather systems are resolved. 802.11ax has preliminary support for additional frequency bands in the 6-7 GHz range, though these are still awaiting (in the US) Ô¨Ånal FCC approval. Wi-Fi designers can improve throughput through a variety of techniques, including 1. improved radio modulation techniques 2. improved error-correcting codes 3. smaller guard intervals between symbols 4. increasing the channel width 5. allowing multiple spatial streams via multiple antennas The Ô¨Årst two in this list seem now to be largely tapped out; OFDM modulation ( 4.1.4.1 OFDM ) is close enough to the Shannon-Hartley limit that there is limited room for improvement, though 802.11ax saw Ô¨Åt to move to OFDMA. The third reduces the range (because there is less protection from multipath interference) but may increase the data rate by ~10%; 802.11ax introduced support for dynamic changing of guard-interval and symbol size. The largest speed increases are obtained the last two items in the list. The channel width is increased by adding additional 5 MHz channels. For example, the 65 Mbps bit rate above for 802.11n is for a nominal frequency range of 20 MHz, comparable to that of 802.11g. However, in areas with minimal competition from other signals, 802.11n supports using a 40 MHz frequency band; the bit rate then goes up to 135 Mbps (or 150 Mbps if a smaller guard interval is used). This amounts to using two of the three available 2.4 GHz Wi-Fi bands. Similarly, the wide range in 802.11ac bit rates reÔ¨Çects support for using channel widths ranging from 20 MHz up to 160 MHz (32 5-MHz ofÔ¨Åcial channels). Using multiple spatial streams is the newest data-rate-improvement technique; see 4.2.3 Multiple Spatial Streams. For all the categories in the table above, additional bits are used for error-correcting codes. For 802.11g operating at 54 Mbps, for example, the actual raw bit rate is (4/3) 54 = 72 Mbps, sent in symbols consisting of six bits as a unit. 4.2.1 Wi-Fi and Collisions We looked extensively at the 10 Mbps Ethernet collision-handling mechanisms in 2.1 10-Mbps Classic Ethernet, only to conclude that with switches and full-duplex links, Ethernet collisions are rapidly becoming a thing of the past. Wi-Fi, however, has brought collisions back from obscurity. An Ethernet sender will discover a collision, if one occurs, during the Ô¨Årst slot time, by monitoring for faint interference with its own transmission. However, as mentioned in 4.1.2 Collisions, Wi-Fi transmitting stations simply cannot detect collisions in progress. If another station transmits at the same time, a Wi-Fi sender will see nothing amiss although its signal will not be received. While there isa largely-collision-free mode for Wi-Fi operation 100 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 (4.2.7 Wi-Fi Polling Mode ), it is not commonly used, and collision management has a signiÔ¨Åcant impact on ordinary Wi-Fi performance. 4.2.1.1 Link-Layer ACKs The Ô¨Årst problem with Wi-Fi collisions is even detecting them. Because of the inability to detect collisions directly, the Wi-Fi protocol adds link-layer ACK packets, at least for unicast transmission. These ACKs are our Ô¨Årst example of Wi-Fi control packets and are unrelated to the higher-layer TCP ACKs. The reliable delivery of these link-layer ACKs depends on careful timing. There are three time intervals applicable (numeric values here are for 802.11b/g in the 2.4 GHz band). The value we here call IFS is more formally known as DIFS (D for ‚Äúdistributed‚Äù; see 4.2.7 Wi-Fi Polling Mode ). 
- slot time: 20 ¬µsec 
- IFS, the ‚Äúnormal‚Äù InterFrame Spacing: 50 ¬µsec 
- SIFS, the short IFS: 10 ¬µsec For comparison, note that the RTT between two Wi-Fi stations 100 meters apart is less than 1 ¬µsec. At 11 Mbps, one IFS time is enough to send about 70 bytes; at 54 Mbps it is enough to send almost 340 bytes. Once a station, say C, has received a data packet addressed to it, say from A, it waits for time SIFS and sends its ACK. In a small Wi-Fi domain in which every station‚Äôs signal is clearly visible to every other station, at this point in time C will be the only station authorized to send, because, as we will see in the next section, all other stations (including those on someone else‚Äôs overlapping Wi-Fi) that have seen A‚Äôs data transmission will be required to wait the longer IFS period following the end of that transmission. These other stations will detect C‚Äôs ACK before the IFS time has elapsed and will thus not interfere with it. Unfortunately the hidden-note problem complicates this picture. Only stations that have seen A‚Äôs data transmission will wait the IFS interval. But there might be ‚Äì and often are ‚Äì other stations not within range of A and so unaware of the IFS-wait requirement, but which are still capable of interfering with C. (Somewhat surprisingly, sometimes everything works out for the best even so: the other station‚Äôs transmission might collide with C‚Äôs ACK at C, but might be too weak to interfere with C‚Äôs ACK at the point it has reached A, which is where it matters.) See exercise 1.0. Long-distance Wi-Fi There exists considerable interest in the transmission of stock Wi-Fi signals over extremely long distances. The current record-holder appears to be CISAR, who created a link of 304 km between Tuscany and Sardinia. Special antennas were used at each end. Links of such length require some tweaking of the protocol, to address link-layer ACKs. These are normally sent 10 ¬µsec (802.11b/g SIFS) after receipt, and time out after 50 ¬µsec (IFS), which will happen when the round-trip propagation delay reaches 40 ¬µsec. This corresponds to a distance of 20 light-microseconds, or 6 km; at newer Wi-Fi levels the distance is even smaller. The CISAR propagation RTT is around 2 milliseconds, and as their reported throughput of up to 175 Mbps vastly exceeds one packet every 2ms, it appears that link-layer ACKs were disabled. If a packet is involved in a collision, the receiver will send no ACK, so the sender will know something went awry. Unfortunately, the sender will notbe able to tell whether the problem was due to a collision, or electromagnetic interference, or signal blockage, or excessive distance, or the receiver‚Äôs being powered 4.2 Wi-Fi 101
An Introduction to Computer Networks, Release 2.0.11 off. But as a collision is usually the most likely cause, and as assuming the lost packet was involved in a collision results in, at worst, a slight additional delay in retransmission, a collision will always be assumed. Link-Layer ACKs contain no information ‚Äì such as a sequence number ‚Äì that identiÔ¨Åes the packet being acknowledged. These ACKs simply acknowledge the most recent transmission, the one that ended one SIFS earlier. In the Wi-Fi context, this is unambiguous. It may be compared, however, to 8.1 Building Reliable Transport: Stop-and-Wait, where at least one bit of packet sequence numbering is required. 4.2.1.2 Collision Avoidance and Backoff The Ethernet collision-management algorithm was known as CSMA/CD, where CD stood for Collision Detection. The corresponding Wi-Fi mechanism is CSMA/ CA, where CA stands for Collision Avoidance. A collision is presumed to have occurred if the link-layer ACK is not received. As with Ethernet, there is an exponential-backoff mechanism as well, though it is scaled somewhat differently. Any sender wanting to send a new data packet waits the IFS time after Ô¨Årst sensing the medium to see if it is idle. If no other trafÔ¨Åc is seen in this interval, the station may then transmit immediately. However, if other trafÔ¨Åc issensed, the sender must do an exponential backoff even for its Ô¨Årst transmission attempt; other stations, after all, are likely also waiting, and avoiding an initial collision is strongly preferred. The initial backoff is to choose a random k<25= 32 (recall that classic Ethernet in effect chooses an initial backoff of k<20= 1; iek=0). The prospective sender then waits k slot times. While waiting, the sender continues to monitor for other trafÔ¨Åc; if any other transmission is detected, then the sender ‚Äúsuspends‚Äù the backoff-wait clock. The clock resumes when the other transmission has completed and one followup idle interval of length IFS has elapsed. Note that, under these rules, data-packet senders always wait for at least one idle interval of length IFS before sending, thus ensuring that they never collide with an ACK sent after an idle interval of only SIFS. On an Ethernet, if two stations are waiting for a third to Ô¨Ånish before they transmit, they will both transmit as soon as the third is Ô¨Ånished and so there will always be an initial collision. With Wi-Fi, because of the larger initial k<32 backoff range, such initial collisions are unlikely. If a Wi-Fi sender believes there has been a collision, it retries its transmission, after doubling the backoff range to 64, then 128, 256, 512, 1024 and again 1024. If these seven attempts all fail, the packet is discarded and the sender starts over. In one slot time, radio signals move 6,000 meters; the Wi-Fi slot time ‚Äì unlike that for Ethernet ‚Äì has nothing to do with the physical diameter of the network. As with Ethernet, though, the Wi-Fi slot time represents the fundamental unit for backoff intervals. Finally, we note that, unlike Ethernet collisions, Wi-Fi collisions are a local phenomenon: if A and B transmit simultaneously, a collision occurs at node C only if the signals of A and B are both strong enough at C to interfere with one another. It is possible that a collision occurs at station C midway between A and B, but not at station D that is close to A. We return to this below in 4.2.1.4 Hidden-Node Problem. 4.2.1.3 Wi-Fi RTS/CTS Wi-Fi stations optionally also use a request-to-send/clear-to-send ( RTS/CTS ) protocol, again negotiated with designated control packets. Usually this is used only for larger data packets; often, the RTS/CTS 102 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 ‚Äúthreshold‚Äù (the size of the largest packet notsent using RTS/CTS) is set (as part of the Access Point conÔ¨Åguration, 4.2.4 Access Points ) to be the maximum packet size, effectively disabling this feature. The idea behind RTS/CTS is that a large packet that is involved in a collision represents a signiÔ¨Åcant waste of potential throughput; for large packets, we should ask Ô¨Årst. The RTS control packet ‚Äì which is small ‚Äì is sent through the normal procedure outlined above; this packet includes the identity of the destination and the size of the data packet the station desires to transmit. The destination station then replies with CTS after the SIFS wait period, effectively preventing any other transmission after the RTS. The CTS packet also contains the data-packet size. The original sender then waits for SIFS after receiving the CTS, and sends the packet. If all other stations can hear both the RTS and CTS messages, then once the RTS and CTS are sent successfully no collisions should occur during packet transmission, again because the only idle times are of length SIFS and other stations should be waiting for time IFS. 4.2.1.4 Hidden-Node Problem Consider the diagram below. Each station has a 100-meter range. Stations A and B are 150 meters apart and so cannot hear one another at all; each is 75 meters from C. If A is transmitting and B senses the medium in preparation for its own transmission, as part of collision avoidance, then B will conclude that the medium is idle and will go ahead and send. Fig. 23:: If A and B transmit simultaneously, there is a collision at C, reachable by both, but not at D However, C is within range of both A and B. If A and B transmit simultaneously, then from C‚Äôs perspective a collision occurs. C receives nothing usable. We will call this a hidden-node collision as the senders A and B are hidden from one another; the general scenario is known as the hidden-node problem. Note that node D receives only A‚Äôs signal, and so no collision occurs at D. The hidden-node problem can also occur if A and B cannot receive one another‚Äôs transmissions due to a physical obstruction such as a radio-impermeable wall: One of the rationales for the RTS/CTS protocol is the prevention of hidden-node collisions. Imagine that, instead of transmitting its data packet, A sends an RTS packet, and C responds with CTS. B has not heard the RTS packet from A, but does hear the CTS from C. A will begin transmitting after a SIFS interval, but B 4.2 Wi-Fi 103
An Introduction to Computer Networks, Release 2.0.11 C BA Fig. 24:: Another setting where A and B cannot detect one another, but their signals can collide at C will not hear A‚Äôs transmission. However, B will still wait, because the CTS packet contained the data-packet size and thus, implicitly, the length of time all other stations should remain idle. Because RTS packets are quite short, they are much less likely to be involved in collisions themselves than data packets. 4.2.1.5 Wi-Fi Fragmentation Conceptually related to RTS/CTS is Wi-Fi fragmentation. If error rates or collision rates are high, a sender can send a large packet as multiple fragments, each receiving its own link-layer ACK. As we shall see in 7.3.1 Error Rates and Packet Size, if bit-error rates are high then sending several smaller packets often leads to fewer total transmitted bytes than sending the same data as one large packet. Wi-Fi packet fragments are reassembled by the receiving node, which may or may not be the Ô¨Ånal destination. As with the RTS/CTS threshold, the fragmentation threshold is often set to the size of the maximum packet. Adjusting the values of these thresholds is seldom necessary, though might be appropriate if monitoring revealed high collision or error rates. Unfortunately, it is essentially impossible for an individual station to distinguish between reception errors caused by collisions and reception errors caused by other forms of noise, and so it is hard to use reception statistics to distinguish between a need for RTS/CTS and a need for fragmentation. 4.2.2 Dynamic Rate Scaling Wi-Fi senders, if they detect transmission problems, are able to reduce their transmission bit rate in a process known as rate scaling orrate control. The idea is that lower bit rates will have fewer noise-related errors, and so as the error rate becomes unacceptably high ‚Äì perhaps due to increased distance ‚Äì the sender should fall back to a lower bit rate. For 802.11g, the standard rates are 54, 48, 36, 24, 18, 12, 9 and 6 Mbps. Senders attempt to Ô¨Ånd the transmission rate that maximizes throughput; for example, 36 Mbps with a packet loss rate of 25% has an effective throughput of 36 75% = 27 Mbps, and so is better than 24 Mbps with no losses. Senders may update their bit rate on a per-packet basis; senders may also choose different bit rates for different recipients. For example, if a sender sends a packet and receives no conÔ¨Årming link-layer ACK, the sender may fall back to the next lower bit rate. The actual bit-rate-selection algorithm lives in the particular Wi-Fi driver in use; different nodes in a network may use different algorithms. 104 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 The earliest rate-scaling algorithm was Automatic Rate Fallback, or ARF, [KM97]. The rate decreases after two consecutive transmission failures (that is, the link-layer ACK is not received), and increases after ten transmission successes. A signiÔ¨Åcant problem for rate scaling is that a packet loss may be due either to low-level random noise (white noise, or thermal noise) or to a collision (which is also a form of noise, but less random); only in the Ô¨Årst case is a lower transmission rate likely to be helpful. If a larger number of collisions is experienced, the longer packet-transmission times caused by the lower bit rate may increase the frequency of hidden-node collisions. In fact, a higher transmission rate (leading to shorter transmission times) may help; enabling the RTS/CTS protocol may also help. Signal Strength Most Wi-Fi drivers report the received signal strength. Newer drivers use the IEEE Received Channel Power Indicator convention; the RCPI is an 8-bit integer proportional to the absolute power received by the antenna as measured in decibel-milliwatts (dBm). Wi-Fi values range from -10 dBm to -90 dBm and below. For comparison, the light from the star Polaris delivers about -97 dBm to one eye on a good night; Venus typically delivers about -73 dBm. A GPS satellite might deliver -127 dBm to your phone. (Inspired by Wikipedia on DBm.) A variety of newer rate-scaling algorithms have been proposed; see [JB05] for a summary. One, ReceiverBased Auto Rate (RBAR, [HVB01]), attempts to incorporate the signal-to-noise ratio into the calculation of the transmission rate. This avoids the confusion introduced by collisions. Unfortunately, while the signalto-noise ratio has a strong theoretical correlation with the transmission bit-error rate, most Wi-Fi radios will report to the host system the received signal strength. This is not the same as the signal-to-noise ratio, which is harder to measure. As a result, the RBAR approach has not been quite as effective in practice as might be hoped. With the Collision-Aware Rate Adaptation algorithm (CARA, [KKCQ06]), a transmitting station attempts (among other things) to infer that its packet was lost to a collision rather than noise if, after one SIFS interval following the end of its packet transmission, no link-layer ACK has been received andthe channel is still busy. This will detect collisions only when the colliding packet is longer than the station‚Äôs own packet, and only when the hidden-node problem isn‚Äôt an issue. Because the actual data in a Wi-Fi packet may be sent at a rate not every participant is close enough to receive correctly, every Wi-Fi transmission begins with a brief preamble at the minimum bit rate. Link-layer ACKs, too, are sent at the minimum bit rate. 4.2.3 Multiple Spatial Streams The latest innovation in improving Wi-Fi (and other wireless) data rates is to support multiple simultaneous data streams, through an antenna technique known as multiple-input-multiple-output, or MIMO. To use N streams, both sender and receiver must have N antennas; all the antennas use the same frequency channels but each transmitter antenna sends a different data stream. At Ô¨Årst glance, any signiÔ¨Åcant improvement in throughput might seem impossible, as the antenna elements in the respective sending and receiving groups are each within about half a wavelength of each other; indeed, in clear space MIMO is not possible. The reason MIMO works in most everyday settings is that it puts multipath interference to positive use. 4.2 Wi-Fi 105
An Introduction to Computer Networks, Release 2.0.11 Consider again at the right-hand side of the Ô¨Ånal image of 4.1.6 Multipath, in which the signal strength varies according to the blue ripples; the peaks and valleys have a period of about half a wavelength. We will assume initially that the signal strength is low enough that reception in the darkest blue areas is no longer viable; a single antenna with the misfortune to be in one of these ‚Äúdead zones‚Äù may receive nothing. We will start with two simpler cases: SIMO (single-input-multiple-output) and MISO (multiple-inputsingle-output). In SIMO, the receiver has multiple antennas; in MISO, the transmitter. Assume for the moment that the multiple-antenna side has two antennas. In the simplest implementation of SIMO, the receiver picks the stronger of the two received signals and uses that alone; as long as at least one antenna is not in a ‚Äúdead zone‚Äù, reception is successful. With two antennas under half a wavelength apart, the odds are that at least one of them will be located outside a dead zone, and will receive an adequate signal. Similarly, in simple MISO, the transmitter picks whichever of its antennas that gets a stronger signal to the receiver. The receiver is unlikely to be in a dead zone for both transmitter antennas. Note that for MISO the sender must get some feedback from the receiver to know which antenna to use. We can do quite a bit better if signal-processing techniques are utilized so the two sender or two receiver antennas can be used simultaneously (though this complicates the mathematics considerably). Such signalprocessing is standard in 802.11n and above; the Wi-Fi header, to assist this process, includes added management packets and Ô¨Åelds for reporting MIMO-related information. One station may, for example, send the other a sequence of training symbols for discerning the response of the antenna system. MISO with these added techniques is sometimes called beamforming: the sender coordinates its multiple antennas to maximize the signal reaching one particular receiver. In our simplistic description of SIMO and MIMO above, in which only one of the multiple-antenna-side antennas is actually used, we have suggested that the idea is to improve marginal reception. At least one antenna on the multiple-antenna side can successfully communicate with the single antenna on the other side. MIMO, on the other hand, can be thought of as applying when transmission conditions are quite good all around, and every antenna on one side can reach every antenna on the other side. The key point is that, in an environment with a signiÔ¨Åcant degree of multipath interference, the antenna-to-antenna paths may all beindependent, oruncorrelated. At least one receiving antenna must be, from the perspective of at least one transmitting antenna, in a multipath-interference ‚Äúgray zone‚Äù of reduced signal strength. Typically the distance from a ‚Äúgray zone‚Äù to a peak zone is half a wavelength, or, for 2.4 GHz, about 6 cm. A1 A2 B2B1 A B Fig. 25:: MIMO antennas As a speciÔ¨Åc example, consider the diagram above, with two sending antennas A 1and A 2at the left and two receiving antennas B 1and B 2at the right. Antenna A 1transmits signal S 1and A 2transmits S 2. There are thus four physical signal paths: A 1-to-B 1, A1-to-B 2, A2-to-B 1and A 2-to-B 2. If we assume that the signal along the A 1-to-B 2path (dashed and blue) arrives with half the strength of the other three paths (solid and black), then we have signal received by B 1: S 1+ S 2 signal received by B 2: S 1/2 + S 2 106 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 From these, B can readily solve for the two independent signals S 1and S 2. These signals are said to form two spatial streams, though the spatial streams are abstract and do not correspond to any of the four physical signal paths. The antennas are each more-or-less omnidirectional; the signal-strength variations come from multipath interference and not from physical aiming. Similarly, while the diagonal paths A 1-to-B 2and A 2-to-B 1are slightly longer than the horizontal paths A 1-to-B 1and A 2-to-B 2, the difference is not nearly enough to allow B to solve for the two signals. In practice, overall data-rate improvement over a single antenna can be considerably less than a factor of 2 (or than N, the number of antennas at each end). The 802.11n standard allows for up to four spatial streams, for a theoretical maximum bit rate of 600 Mbps. 802.11ac allows for up to eight spatial streams, for an even-more-theoretical maximum of close to 7 Gbps. MIMO support is sometimes described with an A BC notation, eg332, where A and B are the number of transmitting and receiving antennas and C ¬§min(A,B) is the number of spatial streams. 4.2.4 Access Points There are two standard Wi-Fi conÔ¨Ågurations: infrastructure andad hoc. The former involves connection to a designated access point; the latter includes individual Wi-Fi-equipped nodes communicating informally. For example, two laptops can set up an ad hoc connection to transfer data at a meeting. Ad hoc connections are often used for very simple networks notproviding Internet connectivity. Complex ad hoc networks are, however, certainly possible; see 4.2.8 MANETs. Theinfrastructure conÔ¨Åguration is much more common. Stations in an infrastructure network communicate directly only with their access point, which, in turn, communicates with the outside world. If Wi-Fi nodes B and C share access point AP, and B wishes to send a packet to C, then B Ô¨Årst forwards the packet to AP and AP then forwards it to C. While this introduces a degree of inefÔ¨Åciency, it does mean that the access point and its associated nodes automatically act as a true LAN: every node can reach every other node. (It is also often the case that most trafÔ¨Åc is between Wi-Fi nodes and the outside world.) In an ad hoc network, by comparison, it is common for two nodes to be able to reach each other only by forwarding through an intermediate third node; this is in fact a form of the hidden-node scenario. Wi-Fi access points are generally identiÔ¨Åed by their SSID (‚ÄúService Set IDentiÔ¨Åer‚Äù), an administratively deÔ¨Åned human-readable string such as ‚Äúlinksys‚Äù or ‚Äúloyola‚Äù. Ad hoc networks also have SSIDs; these are generated pseudorandomly at startup and look like (but are not) 48-bit MAC addresses. Portable Access Points Being a Wi-Fi access point is a very speciÔ¨Åc job; Wi-Fi-enabled ‚Äústation‚Äù devices like phones and workstations do not generally act as access points. However, it is often possible to for a station device to become an access point if the access-point mode is supported by the underlying radio hardware, and if suitable drivers can be found. The Linux hostapd package is one option. The FCC may or may not bestow its blessing. Many access points can support multiple SSIDs simultaneously. For example, an access point might support SSID ‚Äúguest‚Äù with limited authentication (below), and also SSID ‚Äúsecure‚Äù with much stronger authentication. 4.2 Wi-Fi 107
An Introduction to Computer Networks, Release 2.0.11 Finally, Wi-Fi is by design completely interoperable with Ethernet; if station A is associated with access point AP, and AP also connects via (cabled) Ethernet to station B, then if A wants to send a packet to B it sends it using AP as the Wi-Fi destination but with B also included in the header as the ‚Äúactual‚Äù destination. Once it receives the packet by wireless, AP acts as an Ethernet switch and forwards the packet to B. While this forwarding is transparent to senders, the Ethernet and Wi-Fi LAN header formats are quite different. dest addr src addr type data CRC receiver addrtransmit addrdest addr src addr data CRC frame controldurationseq controlEthernet Wi-Fi Data Fig. 26:: Top: Ethernet packet format; Bottom: Wi-Fi packet format (typical) The above diagram illustrates an Ethernet header and the Wi-Fi header for a typical data packet (not using Wi-Fi quality-of-service features). The Ethernet type Ô¨Åeld usually moves to an IEEE Logical Link Control header in the Wi-Fi region labeled ‚Äúdata‚Äù. The receiver and transmitter addresses are the MAC addresses of the nodes receiving and transmitting the (unicast) packet; these may each be different from the ultimate destination and source addresses. If station B wants to send a packet to station C in the same network, the source and destination are B and C but the transmitter and receiver are B and the access point. In infrastructure mode either the receiver or transmitter address is always the access point; in typical situations either the receiver is the destination or the sender is the transmitter. In ad hoc mode, if LAN-layer routing is used then all four addresses may be distinct; see 4.2.8.1 Routing in MANETs. 4.2.4.1 Joining a Network To join the network, an individual station must Ô¨Årst discover its access point, and must associate and then authenticate to that access point before general communication can begin. (Older forms of authentication ‚Äì so-called ‚Äúopen‚Äù authentication and the now-deprecated WEP authentication ‚Äì came before association, but newer authentication protocols such as WPA2, WPA2-Personal and WPA2-Enterprise ( 4.2.5 Wi-Fi Security ) come after.) We can summarize the stages in the process as follows: 
- scanning (or active probing) 
- open-authentication and association 
- true authentication 
- DHCP (getting an IP address, 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ) The association and authentication processes are carried out by an exchange of special management packets, which are conÔ¨Åned to the Wi-Fi LAN layer. Occasionally stations may re-associate to their Access Point, egif they wish to communicate some status update. Access points periodically broadcast their SSID in special beacon packets (though for pseudo-security reasons the SSID in the beacon packets can be suppressed). Beacon packets are one of several Wi-Fi-layer-only 108 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 management packets; the default beacon-broadcast interval is 100 ms. These broadcasts allow stations to see a list of available networks; the beacon packets also contain other Wi-Fi network parameters such as radio-modulation parameters and available data rates. Another use of beacons is to support the power-management doze mode. Some stations may elect to enter this power-conservation mode, in which case they inform the access point, record the announced beacontransmission time interval and then wake up brieÔ¨Çy to receive each beacon. Beacons, in turn, each contain a list (in a compact bitmap form) of each dozing station for which the access point has a packet to deliver. Ad hoc networks have beacon packets as well; all nodes participate in the regular transmission of these via a distributed algorithm. A connecting station may either wait for the next scheduled beacon, or send a special probe-request packet to elicit a beacon-like probe-response packet. These operations may involve listening to or transmitting on multiple radio channels, sequentially, as the station does not yet know the correct channel to use. Unconnected stations often send probe-request packets at regular intervals, to keep abreast of available networks; it is these probe packets that allow tracking by the station‚Äôs MAC address. See 4.2.4.2 MAC Address Randomization. (The number of probe requests can be startlingly high; [HSBS15] measured thousands per minute in some circumstances. This is more than sufÔ¨Åcient to have a serious impact on both battery life and on available throughput.) Once the beacon is received, the station initiates an association process. There is still a vestigial openauthentication process that comes before association, but once upon a time this could also be ‚Äúshared WEP key‚Äù authentication (below). Later, support for a wide range of authentication protocols was introduced, via the 802.1X framework; we return to this in 4.2.5 Wi-Fi Security. For our purposes here, we will include open authentication as part of the association process. Wi-Fi Drivers Even in 2015, 100%-open-source Wi-Fi drivers are available only for selected hardware, and even then not all operations may be supported. Something as simple in principle as changing one‚Äôs source Wi-Fi MAC address is sometimes not possible, though see 4.2.4.2 MAC Address Randomization. Using separate MAC addresses for a host and multiple embedded virtual machines is also sometimes not possible. In open authentication the station sends an authentication request to the access point and the access point replies. About all the station needs to know is the SSID of the access point, though it is usually possible to conÔ¨Ågure the access point to restrict admission to stations with MAC (physical) addresses on a predetermined list. Stations sometimes evade MAC-address checking by changing their MAC address to an acceptable one, though some Wi-Fi drivers do not support this. Because the SSID plays something of the role of a password here, some Wi-Fi access points are conÔ¨Ågured so that beacon packets does not contain the SSID; such access points are said to be hidden. Unfortunately, access points hidden this way are easily unmasked: Ô¨Årst, the SSID is sent in the clear by any other stations that need to authenticate, and second, an attacker can often transmit forged deauthentication or disassociation requests to force legitimate stations to retransmit the SSID. (See ‚Äúmanagement frame protection‚Äù in 4.2.5 Wi-Fi Security for a Ô¨Åx to this last problem.) The shared-WEP-key authentication was based on the (obsolete) WEP encryption mechanism ( 4.2.5 Wi-Fi Security ). It involved a challenge-response exchange by which the station proved to the access point that it knew the shared WEP key. Actual WEP encryption would then start slightly later. 4.2 Wi-Fi 109
An Introduction to Computer Networks, Release 2.0.11 Once the open-authentication step is done, the next step in an infrastructure network is for the station to associate to the access point. This involves an association request from station to access point, and an association response in return. The primary goal of the association exchange is to ensure that the access point knows (by MAC address) what stations it can reach. This tells the access point how to deliver packets to the associating station that come from other stations or the outside world. Association is not necessary in an ad hoc network. The entire connection process (including secure authentication, below, and DHCP, 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ), often takes rather longer than expected, sometimes several seconds. See [PWZMTQ17] for a discussion of some of the causes. Some station and access-point pairs appear not to work as well together as other pairs. 4.2.4.2 MAC Address Randomization Most Wi-Fi-enabled devices are conÔ¨Ågured to transmit Wi-Fi probe requests at regular intervals (and on all available channels), at least when not connected. These probe requests identify available Wi-Fi networks, but they also reveal the device‚Äôs MAC address. This allows sites such as stores to track customers by their device. To prevent such tracking, some devices now support MAC address randomization, proposed in [GG03]: the use at appropriate intervals of a new MAC address randomly selected by the device. Probe requests are generally sent when the device is not joined to a network. To prevent tracking via probe requests, the simplest approach is to change the MAC address used for probes at regular, frequent intervals. A device might even change its MAC address on every probe. Changing the MAC address used for actually joining a network is also important to prevent tracking, but introduces some complications. RFC 7844 suggests these options for selecting new random addresses: 
- At regular time intervals 
- Per connection: each time the device connects to a Wi-Fi network, it will select a new MAC address 
- Per network: like the above, except that if the device reconnects to the same network (identiÔ¨Åed by SSID), it will use the same MAC address The Ô¨Årst option, changing the joined MAC address at regular time intervals, breaks things. First, it will likely result in assignment of a new IP address to the device, terminating all existing connections. Second, many sites still authenticate ‚Äì at least in part ‚Äì based on the MAC address. The per-connection option prevents the Ô¨Årst problem. The per-network option prevents both, but allows a site at which the device actually joins the network to track repeat connections. (ConÔ¨Åguring the device to ‚Äúforget‚Äù the connection between successive joins will usually prevent this, but may not be convenient.) Another approach to the tracking problem is to disable probe requests entirely, except on explicit demand. Wi-Fi MAC address randomization is, unfortunately, not a complete barrier to device tracking; there are other channels through which devices may leak information. For example, probe requests also contain device-capability data known as Information Elements; these values are often distinctive enough that they allow at least partial Ô¨Ångerprinting. Additionally, it is possible to track many Wi-Fi devices based on minute variations in the modulated signals they transmit. MAC address randomization does nothing to prevent such ‚Äúradiometric identiÔ¨Åcation‚Äù. Access points can also impersonate other popular access points, and thus trick devices into initiating a connection with their real MAC addresses. See [BBGO08] and [VMCCP16] for these and other examples. 110 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 Finally, MAC address randomization may have applications for Ethernet as well as Wi-Fi. For example, in the original IPv6 speciÔ¨Åcation, IPv6 addresses embedded the MAC address, and thus introduced the possibility of tracking a device by its IPv6 address. MAC address randomization can prevent this form of tracking as well. However, other techniques implementable solely in the IPv6 layer appear to be more popular; see 11.2.1 Interface identiÔ¨Åers. 4.2.4.3 Roaming Large installations with multiple access points can create ‚Äúroaming‚Äù access by assigning all the access points the same SSID. An individual station will stay with the access point with which it originally associated until the signal strength falls below a certain level (as determined by the station), at which point it will seek out other access points with the same SSID and with a stronger signal. In this way, a large area can be carpeted with multiple Wi-Fi access points, so as to look like one large Wi-Fi domain. The access points are often connected via a wired LAN, known as the distribution system, though the use of Wi-Fi itself to provide interconnection between access points is also an option ( 4.2.4.4 Mesh Networks ). At any one time, a station may be associated to only one access point. In 802.11 terminology, a multiple-access-point conÔ¨Åguration with a single SSID is known as an ‚Äúextended service set‚Äù or ESS. In order for such a large-area network to work, trafÔ¨Åc toa wireless station, egB, must Ô¨Ånd that station‚Äôs current access point, egAP. To help the distribution system track B‚Äôs current location, B is required, at the time it moves from AP oldto AP, to send to AP a reassociation request, containing AP old‚Äôs address. This sets in motion a number of things; one of them is that AP contacts AP oldto verify (and terminate) the former association. This reassociation process also gives AP an opportunity ‚Äì not spelled out in detail in the standard ‚Äì to notify the distribution system of B‚Äôs new location. If the distribution system is a switched Ethernet supporting the usual learning mechanism ( 2.4 Ethernet Switches ), one simple approach to roaming stations is to handle this the same way as, in a wired Ethernet, trafÔ¨Åc Ô¨Ånds a laptop that has been unplugged, carried to a new location, and plugged in again. Suppose our wireless node B has been exchanging packets via the distribution system with wired node C (perhaps a router connecting B to the Internet). When B moves from AP oldto AP, all it has to do is send any packet over the LAN to C, and the Ethernet switches on the path from B to C will then learn the route through the switched Ethernet from C back to B‚Äôs new AP, and thus to B. It is also possible for B‚Äôs new AP to send this switch-updating packet, perhaps as part of its reassociation response. This process may leave other switches in the distribution system still holding in their forwarding tables the old location for B. This is not terribly serious, as it will be Ô¨Åxed for any one switch as soon as B sends a packet to a destination reached by that switch. The problem can be avoided entirely if, after moving, B (or, again, its new AP) sends out an Ethernet broadcast packet. Running Ethernet cable to remote access points can easily dwarf the cost of the access point itself. As a result, there is considerable pressure to Ô¨Ånd ways to allow the Wi-Fi network itself to form the distribution system. We return to this below, in 4.2.4.4 Mesh Networks. The IEEE 802.11r amendment introduced the standardization of fast handoffs from one access point to another within the same ESS. It allows the station and new access point to reuse the same pairwise master keys (below) that had been negotiated between the station and the old access point. It also slightly streamlines the reassociation process. Transitions must, however, still be initiated by the station. The amendment is limited to handoffs; it does not address Ô¨Ånding the access point to which a particular station is associated, or routing between access points. 4.2 Wi-Fi 111
An Introduction to Computer Networks, Release 2.0.11 Because handoffs must be initiated by the station, sometimes all does not quite work out smoothly. Within an ESS, most newer devices (2018) are quite good at initiating handoffs. However, this is not always the case for older devices, and is usually still not the case for many mobile-station devices moving from one ESS to another (that is, where there is a change in the SSID). Such devices may cling to their original access point well past the distance at which the original signal ceases to provide reasonable throughput, as long as it does not vanish entirely. Many Wi-Fi ‚Äúrepeaters‚Äù or ‚Äúextenders‚Äù (below) sold for residential use do require a second SSID, and so will often do a poor job at supporting roaming. Some access points support proprietary methods for dealing with older mobile stations that are reluctant to transfer to a closer access point within the same ESS, though these techniques are now seldom necessary. By communicating amongst themselves, the access points can detect when a station‚Äôs signal is weak enough that a handoff would be appropriate. One approach involves having the original access point initiate a dissociation. At that point the station will reconnect to the ESS but should now connect to an access point within the ESS that has a stronger signal. Another approach involves having the access points all use the same MAC address, so they are indistinguishable. Whichever access point receives the strongest signal from the station is the one used to transmit tothe station. 4.2.4.4 Mesh Networks Being able to move freely around a multiple-access-point Wi-Fi installation is very important for many users. When such a Wi-Fi installation is created in a typical ofÔ¨Åce building pre-wired for Ethernet, the access points all plug into the Ethernet, which becomes the distribution network. However, in larger-scale residential settings, and even many ofÔ¨Åces, running Ethernet cable may be prohibitively expensive. In such cases it makes sense to have the access points interconnect via Wi-Fi itself. If Alice associates to access point A and sends a packet destined for the outside world, then perhaps A will have to forward the packet to Wi-Fi node B, which will in turn forward it to C, before delivery can be complete. This is sometimes easier said than done, however, as the original Wi-Fi standards did not provide for the use of Wi-Fi access points as ‚Äúrepeaters‚Äù; there was no standard mechanism for a Wi-Fi-based distribution network. One inexpensive approach is to use devices sometimes sold as Wi-Fi ‚Äúextenders‚Äù. Such devices typically set up a new SSID, and forward all trafÔ¨Åc to the original SSID. Multi-hop conÔ¨Ågurations are possible, but must usually be conÔ¨Ågured manually. Because the original access point and the extender have different SSIDs, many devices will not automatically connect to whichever is closer, preferring to stick to the SSID originally connected to until that signal essentially disappears completely. This is, for many mobile users, reason enough to give up on this strategy. The desire for a Wi-Fi-based distribution network has led to multiple proprietary solutions. It is possible to purchase a set of Wi-Fi ‚Äúmesh routers‚Äù (2018), often sold at a considerable premium over ‚Äústandard‚Äù routers. After they are set up, these generally work quite well: they present a single SSID, and support fast handoffs from one access point to another, without user intervention. To the user, coverage appears seamless. The downside of a proprietary mechanism, however, is that once you buy into one solution, equipment from other vendors will seldom interoperate. This has led to pressure for standardization. The IEEE introduced ‚Äúmesh networking‚Äù in its 802.11s amendment, Ô¨Ånalized as part of the 2012 edition of the full 802.11 standard; it was slow to catch on. The Wi-Fi Alliance introduced the Wi-Fi EasyMesh solution in 2018, based on 802.11s, but, as of the initial rollout, no vendors were yet on board. We will assume, for the time being, that Wi-Fi mesh networking is restricted to the creation of a distribution 112 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 network interconnecting the access points; ordinary stations do not participate in forwarding other users‚Äô packets through the mesh. Common implementations often take this approach, but in fact the 802.11s amendment allows more general approaches. In creating a mesh network with a Wi-Fi distribution system ‚Äì proprietary or 802.11s ‚Äì the participating access points must address the following issues: 
- They must authenticate to one another 
- They must identify the correct access point to reach a given station B 
- They must correctly handle station B‚Äôs movement to a different access point 
- They must agree on how to route, through the mesh of access points, between the station and the connection to the Internet Eventually the routing issue becomes the same routing problem faced by MANETs ( 4.2.8 MANETs ), although restricted to the (simpler) case where all nodes are under common management. Routing is not trivial; the path A √ëB√ëC might be shorter than the alternative path A √ëD√ëE√ëC, but support a lower data rate. The typical 802.11s solution is to have the multiple access points participate in a mesh BSS. This allows all the access points to appear to be on a single LAN. In this setting, the mesh BSS is separate from the ESS seen by the user stations, and is only used for inter-access-point communication. One (or more) access points are typically connected to the Internet; these are referred to as root mesh stations. In the 802.11s setting, mesh discovery is achieved via initial conÔ¨Åguration of a mesh SSID, together with a WPA3 passphrase. Mutual authentication is then via WPA3, below; it is particularly important that each pair of stations authenticate symmetrically to one another. If station B associates to access point AP, then AP uses the mesh BSS to deliver packets sent by B to the root mesh station (or to some other AP). For reverse trafÔ¨Åc, B‚Äôs reassociation request sent to AP gives AP an opportunity to interact with the mesh BSS to update B‚Äôs new location. The act of B‚Äôs sending a packet via AP will also tell the mesh BSS how to Ô¨Ånd B. Routing through the mesh BSS is handled via the HWMP protocol, 13.4.3 HWMP. This protocol typically generates a tree of station-to-station links (that is, a subset of all links that contains no loops), based at the root station. This process uses a routing metric that is tuned to the wireless environment, so that highthroughput and low-error links are preferred. If a packet is routed through the mesh BSS from station A to station B, then more addresses are needed in the packet header. The ultimate source and destination are A and B, and the transmitter and receiver correspond to the speciÔ¨Åc hop, but the packet also needs a source and destination within the mesh, perhaps corresponding to the two access points to which A and B connect. 802.11s handles this by adding a mesh control Ô¨Åeld consisting of some management Ô¨Åelds (such as TTL and sequence number) and a variablelength block of up to three additional addresses. It is also possible for ordinary stations to join the 802.11s mesh BSS directly, rather than restricting the mesh BSS to the access points. This means that the stations will participate in the mesh as routing nodes. It is hard to predict, in 2018, how popular this will become. The EasyMesh standard of the Wi-Fi Alliance is not exactly the same as the IEEE 802.11s standard. For one thing, the EasyMesh standard speciÔ¨Åes that one access point ‚Äì the one connected to the Internet ‚Äì will be a 4.2 Wi-Fi 113
An Introduction to Computer Networks, Release 2.0.11 ‚ÄúMulti-AP‚Äù Controller; the other access points are called Agents. The EasyMesh standard also incorporates parts of the IEEE 1905.1 standard for home networks, which simpliÔ¨Åes initial conÔ¨Åguration. 4.2.5 Wi-Fi Security Unencrypted Wi-Fi trafÔ¨Åc is visible to anyone nearby with an appropriate receiver; this eavesdropping zone can be expanded by use of a larger antenna. Because of this, Wi-Fi security is important, and Wi-Fi supports several types of trafÔ¨Åc encryption. The original ‚Äì and now obsolete ‚Äì Wi-Fi encryption standard was Wired-Equivalent Privacy, or WEP. It involved a 5-byte key, later sometimes extended to 13 bytes. The encryption algorithm was based on RC4, 28.7.4.1 RC4. The key was a pre-shared key, manually conÔ¨Ågured into each station. Because of the speciÔ¨Åc way WEP made use of the RC4 cipher, it contained a fatal (and now-classic) Ô¨Çaw. Bytes of the key could be could be ‚Äúbroken‚Äù ‚Äì that is, guessed ‚Äì sequentially. Knowing bytes 0 through i‚Äì1 would allow an attacker to guess byte i with a relatively small amount of data, and so on through the entire key. See 28.7.7 Wi-Fi WEP Encryption Failure for details. WEP was replaced with Wi-Fi Protected Access, or WPA. This used the so-called TKIP encryption algorithm that, like WEP, was ultimately based on RC4, but which was immune to the sequential attack that made WEP so vulnerable. WPA was later replaced by WPA2 as part of the IEEE 802.11i amendment, which uses the presumptively stronger AES encryption ( 28.7.2 Block Ciphers ); the variant used by WPA2 is known as CCMP. WPA2 encryption is believed to be quite secure, although there was a vulnerability in the associated Wi-Fi Protected Setup protocol. In the 802.11i standard, WPA2 is known as the robust security network protocol. Access points supporting WPA or WPA2 declare this in their beacon and probe-response packets; these packets also include a list of acceptable ciphers. WPA2 (and WPA) comes in two Ô¨Çavors: WPA2-Personal andWPA2-Enterprise. These use the same AES encryption, but differ in how keys are managed. WPA2-Personal, appropriate for many smaller sites, uses a pre-shared master key, known as the PSK. This key must be entered into the Access Point (ideally not over the air) and into each connecting station. The key is usually a secure hash ( 28.6 Secure Hashes ) of a passphrase. The use of a single key for multiple stations makes changing the key, or revoking the key for a particular user, difÔ¨Åcult. In 2018, the IEEE introduced WPA3, intended to Ô¨Åx a host of accumulated issues. Perhaps the most important change is that WPA3-Personal switches from the WPA2 four-way handshake to the SAE mutualpassword-authentication mechanism, 28.8.2 Simultaneous Authentication of Equals. We return to WPA3 below, at 4.2.5.3 WPA3. 4.2.5.1 WPA2 Four-way handshake In any secure Wi-Fi authentication protocol, the station must authenticate to the access point andthe access point must authenticate to the station; without the latter part, stations might inadvertently connect to rogue access points, which would then likely gain at least partial access to private data. This bidirectional authentication is achieved through the so-called four-way handshake, which also generates a session key, known as the pairwise transient key or PTK, that is independent of the master key. Compromise of the PTK should not allow an attacker to determine the master key. To further improve security, the PTK is used to generate the temporal key, TK, used to encrypt data messages, a separate message-signing key used in the MIC code, below, and some management-packet keys. 114 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 In WPA2-Personal, the master key is the pre-shared key (PSK); in WPA2-Enterprise, below, the master key is the negotiated ‚Äúpairwise master key‚Äù, or PMK. The four-way handshake begins immediately after association and, for WPA2-Enterprise, the selection of the PMK. None of the four packets that are part of the handshake are encrypted. Both station and access point begin by each selecting a random string, called a nonce, typically 32 bytes long. In the diagram below, the access point (authenticator) has chosen ANonce and the station (supplicant) has chosen SNonce. The PTK will be a secure hash of the master key, both nonces, and both MAC addresses. The Ô¨Årst packet of the four-way handshake is sent by the access point to the station, and contains its nonce, unencrypted. This packet also contains a replay counter, RC; the access point assigns these sequentially and the station echoes them back. Station Access Point ANonce, RC= r SNonce, RC= r, MICStation knows PTK Access Point knows PTK RC=r+1, GTK, MIC ACK, RC=r+1, MIC Install PTKInstall PTK Fig. 27:: Four-way WPA2 handshake; RC represent the replay counter At this point the station has enough information to compute the PTK; in the second message of the handshake it now sends its own nonce to the access point. The nonce is again sent in the clear, but this second message also includes a digital signature. This signature is sometimes called a Message Integrity Code, or MIC, and in the 802.11i standard is ofÔ¨Åcially named Michael. It is calculated in a manner similar to the HMAC mechanism of 28.6.1 Secure Hashes and Authentication, and uses its own key derived from the PTK. Upon receipt of the station‚Äôs nonce, the access point too is able to compute the PTK. With the PTK now in hand, the access point veriÔ¨Åes the attached signature. If it checks out, that proves to the access point that the station did in fact know the master key, as a valid signature could not have been constructed without it. The station has now authenticated itself to the access point. For the third stage of the handshake, the access point, now also in possession of the PTK, sends a signed message to the station. The replay counter is incremented, and an optional group temporal key, GTK, may be included for encrypting non-unicast messages. If the GTK is included, it is encrypted with the PTK, though the entire message is not encrypted. When this third message is received and veriÔ¨Åed, the access point has authenticated itself to the station. The fourth and Ô¨Ånal step is simply an acknowledgment from the client. Four-way-handshake packets are sent in the EAPOL format, described in the following section. This format 4.2 Wi-Fi 115
An Introduction to Computer Networks, Release 2.0.11 can be used to identify the handshake packets in WireShark scans. One signiÔ¨Åcant vulnerability of the four-way handshake when WPA2-Personal is used is that if an eavesdropper records the messages, then it can attempt an ofÔ¨Çine brute-force attack on the key. Different values of the passphrase used to generate the PSK can be tried until the MIC values computed for the second and third packets match the values in the corresponding recorded packets. At this point the attacker can not only authenticate to the network, but can also decrypt packets. This attack is harder with WPA2-Enterprise, as each user has a different key. Other WPA2-Personal stations on the same network can also eavesdrop, given that all stations share the same PSK, and that the PTK is generated from the PSK and information transmitted without encryption. The DifÔ¨Åe-Hellman-Merkle key-exchange mechanism, 28.8 DifÔ¨Åe-Hellman-Merkle Exchange, would avoid this difÔ¨Åculty; keys produced this way are noteasily determined by an eavesdropper, even one with inside information about master keys. However, this was not used, in part because WPA needed to be rushed into service after the failure of WEP. 4.2.5.1.1 KRACK Attack The purpose of the replay counter, RC in the diagram above, is to prevent an attacker from reusing an old handshake packet. Despite this effort, replayed or regenerated instances of the third handshake packet can sometimes be used to seriously weaken the underlying encryption. The attack, known as the Key Reinstallation Attack, or KRACK, is documented in [VP17]. The attack has several variations, some of which address a particular implementation‚Äôs interpretation of the IEEE standard, and some of which address other Wi-Fi keys ( egthe group temporal key) and key handshakes ( egthe handshake used by 4.2.4.3 Roaming ). We consider only the most straightforward form here. The ciphers used by WPA2 are all ‚Äústream‚Äù ciphers ( 28.7.4 Stream Ciphers ), meaning that, for each packet, the key is used to generate a keystream of pseudorandom bits, the same length as the packet; the packet is then XORed with this keystream to encrypt it. It is essential for this scheme‚Äôs security that the keystreams of different packets are unrelated; to achieve this, the keystream algorithm incorporates an encryption nonce, initially 1 and incremented for each successive packet. The core observation of KRACK is that, whenever the station installs or reinstalls the PTK, it also resets this encryption nonce to 1. This has the effect of resetting the keystream, so that, for a while, each new packet will be encrypted with exactly the same keystream as an earlier packet. This key reinstallation at the station side occurs whenever an instance of the third handshake packet arrives. Because of the possibility of lost packets, the handshake protocol must allow duplicates of any packet. The basic strategy of KRACK is now to force key reinstallation, by arranging for either the access point or the attacker to deliver duplicates of the third handshake packet to the station. In order to interfere with packet delivery, the attacker must be able to block and delay packets from the access point to the station, and be able to send its own packets to the station. The easiest way to accomplish this is for the attacker to be set up as a ‚Äúclone‚Äù of the real access point, with the same MAC address, but operating on a different Wi-Fi channel. The attacker receives messages from the real access point on the original channel, and is able to selectively retransmit them to the station on the new channel. This can be described as a channel-based man-in-the-middle attack; cf 29.3 Trust and the Man in the Middle. Alternatively, the attacker may also be able to selectively jam messages from the access point. 116 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 If the attacker can block the fourth handshake packet, from station to access point, then the access point will eventually time out and retransmit a duplicate third packet, complete with properly updated replay counter. The attacker can delay the duplicate third packet, if desired, in order to prolong the interval of keystream reuse. The station‚Äôs response to this duplicate third packet will be encrypted, but the attacker can usually generate a forged, unencrypted version. Forcing reuse of the keystream does not automatically break the encryption. However, in many cases the plaintext of a few packets can be guessed by context, and hence, by XORing, the keystream used to encrypt the packet can be determined. This allows trivial decryption of any later packet encrypted with the same keystream. Other possibilities depend on the cipher. When the TKIP cipher is used, a vulnerability in the MIC algorithm may allow determination of the key used in the MIC; this in turn would allow the attacker to inject new packets, properly signed, into the connection. These new packets can be encrypted with one of the broken keystreams. This strategy does not work with AES (CCMP) encryption. The KRACK vulnerability was Ô¨Åxed in wpa_supplicant by disallowing reinstallation of the same key. That is, if a retransmission of the third handshake packet is received, it is ignored; the encryption nonce is not reset. 4.2.5.2 WPA2-Enterprise The WPA2-Enterprise alternative allows each station to have its own separate key. In fact, it largely separates the encryption mechanisms from the Wi-Fi protocols, allowing sites great freedom in choosing the former. Despite the ‚Äúenterprise‚Äù in the name, it is also well suited for smaller sites. WPA2-Enterprise is based rather closely on the 802.1X framework, which supports arbitrary authentication protocols as plug-in modules. In principle, the only improvement WPA2-Enterprise offers over WPA2-Personal is the ability to assign individual Wi-Fi passwords. In practice, this is an enormously important feature. It prevents, for example, one user from easily decrypting packets sent by another user. The keys are all held by a single common system known as the authentication server, usually unrelated to the access point. The client node (that is, the Wi-Fi station) is known as the supplicant, and the access point is known as the authenticator. To begin the authentication process, the supplicant contacts the authenticator using the Extensible Authentication Protocol, or EAP, with what amounts to a request to authenticate to that access point. EAP is a generic message framework meant to support multiple speciÔ¨Åc types of authentication; see RFC 3748 and RFC 5247. The EAP request is forwarded to the authentication server, which may exchange (via the authenticator) several challenge/response messages with the supplicant. No secret credentials should be sent in the clear. EAP is usually used in conjunction with the RADIUS (Remote Authentication Dial-In User Service) protocol ( RFC 2865 ), which is a speciÔ¨Åc (but Ô¨Çexible) authentication-server protocol. WPA2-Enterprise is sometimes known as 802.1X mode, EAP mode or RADIUS mode (though WPA2-Personal is also based on 802.1X, and uses EAP in its four-way handshake). EAP communication takes place before the supplicant is given an IP address; thus, a mechanism must be provided to support exchange of EAP packets between supplicant and authenticator. This mechanism is known as EAPOL, for EAP Over LAN. EAP messages between the authenticator and the authentication 4.2 Wi-Fi 117
An Introduction to Computer Networks, Release 2.0.11 server, on the other hand, can travel via IP; in fact, sites may choose to have the authentication server hosted remotely. SpeciÔ¨Åc protocols using the EAP/RADIUS framework often use packet formats other than EAPOL, but EAPOL will be used in the concluding four-way handshake. Once the authentication server ( egRADIUS server) is set up, speciÔ¨Åc per-user authentication methods can be entered. This can amount to xusername,password ypairs (below), or some form of security certiÔ¨Åcate, or sometimes both. The authentication server will generally allow different encryption protocols to be used for different supplicants, thus allowing for the possibility that there is not a common protocol supported by all stations. In WPA2-Enterprise, the access point no longer needs to know anything about what authentication protocol is actually used; it is simply the middleman forwarding EAP packets between the supplicant and the authentication server. In particular, the access point does not need to support any speciÔ¨Åc authentication protocol. The access point allows the supplicant to connect to the network once it receives permission to do so from the authentication server. At the end of the authentication process, the supplicant and the authentication server will, as part of that process, also have established a shared secret. In WPA2-Enterprise terminology this is known as the pairwise master key or PMK. The authentication server then communicates the PMK securely to the access point (using any standard protocol; see 29.5 SSH and TLS ). The next step is for the supplicant and the access point to negotiate their session key. This is done using the four-way-handshake mechanism of the previous section, with the PMK as the master key. The resultant PTK is, as with WPA2-Personal, used as the session key. WPA2-Enterprise authentication typically does require that the access point have an IP address, in order to be able to contact the authentication server. An access point using WPA2-Personal authentication does not need an IP address, though it may have one simply to enable conÔ¨Åguration. 4.2.5.2.1 Enabling WPA2-Enterprise ConÔ¨Åguring a Wi-Fi network to use WPA2-Enterprise authentication is relatively straightforward, as long as an authentication server running RADIUS is available. We here give an outline of setting up WPA2Enterprise authentication using FreeRADIUS (version 2.1.12, 2018). We want to enable per-user passwords, butnotper-user certiÔ¨Åcates. Passwords will be stored on the server using SHA-1 hashing ( 28.6 Secure Hashes ). This is not necessarily strong enough for production use; see 28.6.2 Password Hashes for other options. Because passwords will be hashed, the client will have to communicate the actual password to the authentication server; authentication methods such as those in 28.6.3 CHAP are not an option. The Ô¨Årst step is to set up the access point. This is generally quite straightforward; WPA2-Enterprise is supported even on inexpensive access points. After selecting the option to enable WPA2-Enterprise security, we will need to enter the IP address of the authentication server, and also a ‚Äúshared secret‚Äù password for authenticating messages between the access point and the server (see 28.6.1 Secure Hashes and Authentication for message-authentication techniques). ConÔ¨Åguration of the RADIUS server is a bit more complex, as both RADIUS and EAP are both quite general; both were developed long before 802.1X, and both are used in many other settings as well. Because we have decided to use hashed passwords ‚Äì which implies the client station will send the plaintext password to the authentication server ‚Äì we will need to use an authentication method that creates an encrypted tunnel. The Protected EAP method is well-suited here; it encrypts its trafÔ¨Åc using TLS ( 29.5.2 TLS, though here 118 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 without TCP). (There is also an EAP TLS method, using TLS directly and traditionally requiring client-side certiÔ¨Åcates, and a TTLS method, for Tunneled TLS.) Within the PEAP encrypted tunnel, we want to use plaintext password authentication. Here we want the Password Authentication Protocol, PAP, which basically just asks for the username and password. FreeRADIUS does not allow PAP to run directly within PEAP, so we interpose the Generic Token Card protocol, GTC. (There is no ‚Äútoken card‚Äù device anywhere in sight, but GTC is indeed quite generic.) We probably also have to tell the RADIUS server the IP address of the access point. The access point here must have an IP address, speciÔ¨Åcally for this communication. We enable all these things by editing the eap.conf Ô¨Åle so as to contain the following entries: default_eap_type = peap ... peap { default_eap_type = gtc ... } ... gtc { auth_type = PAP ... } The next step is to create a (username, hashed_password) credential pair on the server. To keep things simple, we will store credentials in the users Ô¨Åle. The username will be ‚Äúalice‚Äù, with password ‚Äúsnorri‚Äù. Per the FreeRADIUS rules, we need to convert the password to its SHA-1 hash, encoded using base64. There are several ways to do this; we will here make use of the OpenSSL command library: echo -n "snorri" | openssl dgst -binary -sha1 | openssl base64 This returns the string 7E6FbhrN2TYOkrBti+8W8weC2W8= which we then enter into the users Ô¨Åle as follows: alice SHA1-Password := "7E6FbhrN2TYOkrBti+8W8weC2W8=" Other options include Cleartext-Password ,MD5-Password andSSHA1-Password, with the latter being for salted passwords (which are recommended). With this approach, Alice will have difÔ¨Åculty changing her password, unless she is administrator of the authentication server. This is not necessarily worse than WPA2-Personal, where Alice shares her password with other users. However, if we want to support user-controlled password changing, we can conÔ¨Ågure the RADIUS server to look for the (username, hashed_password) credentials in a database instead of the users Ô¨Åle. It is then relatively straightforward to create a web-based interface for allowing users to change their passwords. Now, Ô¨Ånally, we try to connect. Any 802.1X client should ask for the username and password, before communication with the authentication server begins. Some may also ask for a preferred authentication method 4.2 Wi-Fi 119
An Introduction to Computer Networks, Release 2.0.11 (though our RADIUS server here is only offering one), an optional certiÔ¨Åcate (which we are not using), or an ‚Äúanonymous identity‚Äù, which is a way for a client to specify a particular authentication server if there are several. If all goes well, connection should be immediate. If not, FreeRADIUS has an authentication-testing tool, and copious debugging output. 4.2.5.3 WPA3 In 2018 the Wi-Fi Alliance introduced WPA3, a replacement for WPA2. The biggest change is that, when both parties are WPA3-aware, the WPA2 four-way handshake is replaced with SAE, 28.8.2 Simultaneous Authentication of Equals. The advantage of SAE here is that an eavesdropper can get nowhere with an ofÔ¨Çine, dictionary-based, brute-force password attack; recall from the end of 4.2.5.1 WPA2 Four-way handshake that WPA2 is quite vulnerable in this regard. An attacker can still attempt an online brute-force attack on WPA3, egby parking a van within Wi-Fi range and trying one password after another, but this is slow. Another consequence of SAE is forward secrecy ( 29.2 Forward Secrecy ). This means that if an attacker obtains the encryption key for one session, it will not help decrypt older (or newer) sessions. In fact, even if an attacker obtains the master password, it will not be able to obtain any session keys (although the attacker willbe able to connect to the network). Under WPA2, if an attacker obtains the PMK, then all session keys can be calculated from the nonce values exchanged in the four-way handshake. As with WPA2, WPA3 requires that both the station and the access point maintain the password cleartext (or at least the key derived from the password). Because each side must authenticate to the other, it is hard to see how this could be otherwise. WPA3 encrypts even connections to ‚Äúopen‚Äù access points, through what is called Opportunistic Wireless Encryption; see RFC 8110. WPA3 also introduces longer key lengths, and adds some new ciphers. Although it is not strictly part of WPA3, the EasyConnect feature was announced at the same time. This allows easier connection of devices that lack screens or keyboards, which makes entering a password difÔ¨Åcult. The EasyConnect device should come with a QR code; scanning the code allows the device to be connected. Finally, WPA3 contains an ofÔ¨Åcial Ô¨Åx to the KRACK attack. 4.2.5.4 Encryption Coverage Originally, encryption covered only the data packets. A common attack involved forging management packets,egto force stations to disassociate from their access point. Sometimes this was done continuously as a denial-of-service attack; it might also be done to force a station to reassociate and thus reveal a hidden SSID, or to reveal key information to enable a brute-force decryption attack. The 2009 IEEE 802.11w amendment introduced the option for a station and access point to negotiate management frame protection, which encrypts (and digitally signs) essential management packets exchanged after the authentication phase is completed. This includes those station-to-access-point packets requesting deauthentication or disassociation, effectively preventing the above attacks. However, management frame protection is (as of 2015) seldom enabled by default by consumer-grade Wi-Fi access points, even when data encryption is in effect. 120 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 4.2.6 Wi-Fi Monitoring Again depending on ones driver, it is sometimes possible to monitor all Wi-Fi trafÔ¨Åc on a given channel. Special tools exist for this, including aircrack-ng and kismet, but often plain WireShark will sufÔ¨Åce if one can get the underlying driver into so-called ‚Äúmonitor‚Äù mode. On Linux systems the command iwconfig wlan0 mode monitor should do this (where wlan0 is the name of the wireless network interface). It may be necessary to Ô¨Årst kill other processes that have the wlan0 interface open, eg withservice NetworkManager stop. It may also be necessary to bring the interface down, with ifconfig wlan0 down, in which case the interface needs to be brought back upafter entering monitor mode. Finally, the receive channel can be set with, eg,iwconfig wlan0 channel 6. (On some systems the interface name may change after the transition to monitor mode.) After the mode and channel are set, Wireshark will report the 802.11 management-frame headers, and also the so-called radiotap header containing information about the transmission data rate, channel, and received signal strength. One useful experiment is to begin monitoring and then to power up a Wi-Fi enabled device. The WireShark display Ô¨Ålter wlan.addr == device-MAC-address helps focus on the relevant packets(or, better yet, the capture Ô¨Ålter ether host device-MAC-address ). The WireShark screenshot below is an example. we see node SamsungE_03:3f:ad broadcast a probe request, which is answered by the access point CiscoLi_d1:24:40. The next two packets represent the open-authentication process, followed by two packets representing the association process. The last four packets, of type EAPOL, represent the WPA2-Personal four-way authentication handshake. 4.2.7 Wi-Fi Polling Mode Wi-Fi also includes a ‚Äúpolled‚Äù mechanism, where one station (the Access Point) determines which stations are allowed to send. While it is not often used, it has the potential to greatly reduce collisions, or even eliminate them entirely. This mechanism is known as ‚ÄúPoint Coordination Function‚Äù, or PCF, versus the collision-oriented mechanism which is then known as ‚ÄúDistributed Coordination Function‚Äù. The PCF name refers to the fact that in this mode it is the Access Point that is in charge of coordinating which stations get to send when. The PCF option offers the potential for regular trafÔ¨Åc to receive improved throughput due to fewer collisions. However, it is often seen as intended for real-time Wi-Fi trafÔ¨Åc, such as voice calls over Wi-Fi. The idea behind PCF is to schedule, at regular intervals, a contention-free period, or CFP. During this period, the Access Point may 
- send Data packets to any receiver 
- send Poll packets to any receiver, allowing that receiver to reply with its own data packet 4.2 Wi-Fi 121
An Introduction to Computer Networks, Release 2.0.11 
- send a combination of the two above (not necessarily to the same receiver) 
- send management packets, including a special packet marking the end of the CFP None of these operations can result in a collision (unless an unrelated but overlapping Wi-Fi domain is involved). Stations receiving data from the Access Point send the usual ACK after a SIFS interval. A data packet from the Access Point addressed to station B may also carry, piggybacked in the Wi-Fi header, a Poll request to another station C; this saves a transmission. Polled stations that send data will receive an ACK from the Access Point; this ACK may be combined in the same packet with the Poll request to the next station. At the end of the CFP, the regular ‚Äúcontention period‚Äù or CP resumes, with the usual CSMA/CA strategy. The time interval between the start times of consecutive CFP periods is typically 100 ms, short enough to allow some real-time trafÔ¨Åc to be supported. During the CFP, all stations normally wait only the Short IFS, SIFS, between transmissions. This works because normally there is only one station designated to respond: the Access Point or the polled station. However, if a station is polled and has nothing to send, the Access Point waits for time interval PIFS (PCF Inter-Frame Spacing), of length midway between SIFS and IFS above (our previous IFS should now really be known as DIFS, for DCF IFS). At the expiration of the PIFS, any non-Access-Point station that happens to be unaware of the CFP will continue to wait the full DIFS, and thus will not transmit. An example of such a CFP-unaware station might be one that is part of an entirely different but overlapping Wi-Fi network. The Access Point generally maintains a polling list of stations that wish to be polled during the CFP. Stations request inclusion on this list by an indication when they associate or (more likely) reassociate to the Access Point. A polled station with nothing to send simply remains quiet. PCF mode is not supported by many lower-end Wi-Fi routers, and often goes unused even when it is available. Note that PCF mode is collision-free, so long as no other Wi-Fi access points are active and within range. While the standard has some provisions for attempting to deal with the presence of other Wi-Fi networks, these provisions are somewhat imperfect; at a minimum, they are not always supported by other access points. The end result is that polling is not quite as useful as it might be. 4.2.8 MANETs The MANET acronym stands for mobile ad hoc network; in practice, the term generally applies to ad hoc wireless networks of sufÔ¨Åcient complexity that some internal routing mechanism is needed to enable full connectivity. A mesh network in the sense of 4.2.4.4 Mesh Networks qualiÔ¨Åes as a MANET, though MANETs also include networks with much less centralized control, and in which the routing nodes may be highly mobile. MANETs are also potentially much larger, with some designs intended to handle many hundreds of routing nodes, while a typical Wi-Fi mesh network may have only a handful of access points. While MANETs be built with any wireless mechanism, we will assume here that Wi-Fi is used. MANET nodes communicate by radio signals with a Ô¨Ånite range, as in the diagram below. Each node‚Äôs radio range is represented by a circle centered about that node. In general, two MANET nodes may be able to communicate only by relaying packets through intermediate nodes, as is the case for nodes A and G in the diagram above. Finding the optimal route through those intermediate nodes is a signiÔ¨Åcant problem. 122 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 ABC DE FG Fig. 28:: Typical MANET in which the radio range for each node is represented by a circle around that node. Node A can reach Node G either by the route A‚ÄìB‚ÄìC‚ÄìD‚ÄìG or by A‚ÄìB‚ÄìD‚ÄìF‚ÄìG (or by other routes) MANETs for the People In the early years of MANETs, many designs focused on a decentralized, communitarian approach, eg wireless community networks. During the 2010 Arab Spring, MANETs were often proposed (in conjunction with a few users having satellite-Internet access) as a way to bypass government censorship of the Internet. Fast forward to 2018, and much press discussion of ‚Äúmesh networks‚Äù is oriented towards those with exceptionally large private residences. Nothing endures but change. In the Ô¨Åeld, the radio range of each node may not be very circular at all, due to among other things signal reÔ¨Çection and blocking from obstructions. An additional complication arises when the nodes (or even just obstructions) are moving in real time (hence the ‚Äúmobile‚Äù of MANET); this means that a working route may stop working a short time later. For this reason, and others, routing within MANETs is a good deal more complex than routing in an Ethernet. A switched Ethernet, for example, is required to be loop-free, so there is never a choice among multiple alternative routes. Note that, without successful LAN-layer routing, a MANET does not have full node-to-node connectivity and thus does not meet the deÔ¨Ånition of a LAN given in 1.9 LANs and Ethernet. With either LAN-layer or IP-layer routing, one or more MANET nodes may serve as gateways to the Internet. Note also that MANETs in general do not support broadcast or multicast, unless the forwarding of broadcast and multicast messages throughout the MANET is built in to the routing mechanism. This can complicate the operation of IPv4 and IPv6 networks, even assuming that the MANET routing mechanism replaces the need for broadcast/multicast protocols like IPv4‚Äôs ARP ( 10.2 Address Resolution Protocol: ARP ) and IPv6‚Äôs Neighbor Discovery ( 11.6 Neighbor Discovery ) that otherwise play important roles in local packet delivery. For example, the common IPv4 address-assignment mechanism we will describe in 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) relies on broadcast and so often needs adaptation. Similarly, IPv6 4.2 Wi-Fi 123
An Introduction to Computer Networks, Release 2.0.11 relies on multicast for several ancillary services, including address assignment ( 11.7.3 DHCPv6 ) and duplicate address detection ( 11.7.1 Duplicate Address Detection ). MANETs are simplest when all the nodes are under common, coordinated management, as in the mesh Wi-Fi described above. Life is much more complicated when nodes are individually owned, and each owner wishes to place limits on the amount of ‚Äútransit trafÔ¨Åc‚Äù ‚Äì trafÔ¨Åc passing through the owner‚Äôs node ‚Äì that is acceptable. Yet this is often the situation faced by schemes to offer Wi-Fi-based community Internet access. Finally, we observe that while MANETs are of great theoretical interest, their practical impact has been modest; they are almost unknown, for example, in corporate environments, beyond the mesh networks of 4.2.4.4 Mesh Networks. They appear most useful in emergency situations, rural settings, and settings where the conventional infrastructure network has failed or been disabled. 4.2.8.1 Routing in MANETs Routing in MANETs can be done either at the LAN layer, using physical addresses, or at the IP layer with some minor bending (below) of the rules. Either way, nodes must Ô¨Ånd out about the existence of other nodes, and appropriate routes must then be selected. Route selection can use any of the mechanisms we describe later in 13 Routing-Update Algorithms. Routing at the LAN layer is much like routing by Ethernet switches; each node will construct an appropriate forwarding table. Unlike Ethernet, however, there may be multiple paths to a destination, direct connectivity between any particular pair of nodes may come and go, and negotiation may be required even to determine which MANET nodes will serve as forwarders. Routing at the IP layer involves the same issues, but at least IP-layer routing-update algorithms have always been able to handle multiple paths. There are some minor issues, however. When we initially presented IP forwarding in 1.10 IP - Internet Protocol, we assumed that routers made their decisions by looking only at the network preÔ¨Åx of the address; if another node had the same network preÔ¨Åx it was assumed to be reachable directly via the LAN. This model usually fails badly in MANETs, where direct reachability has nothing to do with addresses. At least within the MANET, then, a modiÔ¨Åed forwarding algorithm must be used where every address is looked up in the forwarding table. One simple way to implement this is to have the forwarding tables contain only host-speciÔ¨Åc entries as were discussed in 5.1 Virtual Private Networks. Multiple routing algorithms have been proposed for MANETs. Performance of a given algorithm may depend on the following factors: 
- The size of the network 
- How many nodes have agreed to serve as routers 
- The degree of node mobility, especially of routing-node mobility if applicable 
- Whether the nodes (especially routing nodes) are under common administration, and thus may agree to defer their own transmission interests for the common good 
- per-node storage and power availability 124 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 4.3 WiMAX and LTE WiMAX and LTE are both wireless network technologies suitable for data connections to mobile (and sometimes stationary) devices. WiMAX is an IEEE standard, 802.16; its original name is WirelessMAN (for Metropolitan Area Network), and this name appears intermittently in the IEEE standards. In its earlier versions it was intended for stationary subscribers (802.16d), but was later expanded to support mobile subscribers (802.16e). The stationarysubscriber version is often used to provide residential Internet connectivity, in both urban and rural areas. LTE (the acronym itself stands for Long Term Evolution) is a product of the mobile telecom world; it was designed for mobile subscribers from the beginning. Its ofÔ¨Åcial name ‚Äì at least for its radio protocols ‚Äì isEvolved UTRA, or E-UTRA, where UTRA in turn stands for UMTS Terrestrial Radio Access. UMTS stands for Universal Mobile Telecommunications System, a core mobile-device data-network mechanism with standards dating from the year 2000. 4G Capacity A medium-level wireless data plan often comes with a 5 GB monthly cap. At the 100 Mbps 4G data rate, that allotment can be downloaded in under six minutes. Data rate isn‚Äôt everything. Both LTE and the mobile version of WiMAX are often marketed as fourth generation (or 4G) networking technology. The ITU has a speciÔ¨Åc deÔ¨Ånition for 4G developed in 2008, ofÔ¨Åcially named IMT-Advanced and including a 100 Mbps download rate to moving devices and a 1 Gbps download rate to more-or-lessstationary devices. Neither WiMAX nor LTE quite qualiÔ¨Åed technically, but to marketers that was no impediment. In any event, in December 2010 the ITU issued a statement in which it ‚Äúrecognized that [the term 4G], while undeÔ¨Åned, may also be applied to the forerunners of these technologies, LTE and WiMax‚Äù. So-called Advanced LTE and WiMAX2 are true IMT-Advanced protocols. As in 4.1.4 Band Width we will use the term ‚Äúdata rate‚Äù for what is commonly called ‚Äúbandwidth‚Äù to avoid confusion with the radio-speciÔ¨Åc meaning of the latter term. WiMAX can use unlicensed frequencies, like Wi-Fi, but its primary use is over licensed radio spectrum; LTE is used almost exclusively over licensed spectrum. WiMAX and LTE both support a number of options for the width of the frequency band; the wider the band, the higher the data rate. Downlink (base station to subscriber) data rates can be well over 100 Mbps (uplink rates are usually smaller). Most LTE bands are either in the range 700-900 MHz or are above 1700 MHz; the lower frequencies tend to be better at penetrating trees and walls. Like Wi-Fi, WiMAX and LTE subscriber stations connect to a central access point. The WiMAX standard prefers the term base station which we will use henceforth for both protocols; LTE ofÔ¨Åcially prefers the term ‚Äúevolved NodeB‚Äù or eNB. The coverage radius for LTE and mobile-subscriber WiMAX might be one to ten kilometers, versus less (sometimes much less) than 100 meters for Wi-Fi. Stationary-subscriber WiMAX can operate on a larger scale; the coverage radius can be several tens of kilometers. As distances increase, the data rate is reduced. Large-radius base stations are typically mounted in towers; smaller-radius base-stations, generally used only in areas densely populated with subscribers, may use lower antennas integrated discretely into the local 4.3 WiMAX and LTE 125
An Introduction to Computer Networks, Release 2.0.11 architecture. Subscriber stations are not expected to be able to hear other stations; they interact only with the base station. 4.3.1 Uplink Scheduling As distances increase, the subscriber-to-base RTT becomes non-negligible. At 10 kilometers, this RTT is 66 ¬µsec, based on the speed of light of about 300 m/¬µsec. At 100 Mbps this is enough time to send 800 bytes, making it a priority to reduce the number of RTTs. To this end, it is no longer practical to use Wi-Fi-style collisions to resolve access contention; it is not even practical to use the Wi-Fi PCF mode of 4.2.7 Wi-Fi Polling Mode because polling requires additional RTTs. Instead, WiMAX and LTE rely on base-station-regulated scheduling of transmissions. The base station has no difÔ¨Åculty scheduling downlink transmissions, from base to subscriber: the base station simply sends the packets sequentially (or in parallel on different sets of subcarriers if OFDM is used). If beamforming MISO antennas are used, or multiple physically directional antennas, the base station will take this into account. It is the uplink transmissions ‚Äì from subscriber to base ‚Äì that are more complicated to coordinate. Once a subscriber station completes the network entry process to connect to a base station ( 4.3.3 Network Entry ), it is assigned regular transmission slots, including times and frequencies. These transmission slots may vary in size over time; the base station may regularly issue new transmission schedules. Each subscriber station is told in effect that it may transmit on its assigned frequencies starting at an assigned time and for an assigned length; LTE lengths start at 1 ms and WiMAX lengths at 2 ms. The station synchronizes its clock with that of the base station as part of the network entry process. Each subscriber station is scheduled to transmit so that one transmission Ô¨Ånishes arriving at the base station just before the next station‚Äôs same-frequency transmission begins arriving. Only minimal ‚Äúguard intervals‚Äù need be included between consecutive transmissions. Two (or more) consecutive uplink transmissions may in fact be ‚Äúin the air‚Äù simultaneously, as far-away stations need to begin transmitting early so their signals will arrive at the base station at the expected time. The diagram above illustrates this for stations separated by relatively large physical distances (as may be typical for long-range WiMAX). This strategy for uplink scheduling eliminates the full RTT that Wi-Fi polling mode ( 4.2.7 Wi-Fi Polling Mode ) entails. Scheduled timeslots may be periodic (as is would be appropriate for voice) or may occur at varying intervals. Quality-of-Service requests may also enter into the schedule; LTE focuses on end-to-end QoS while WiMAX focuses on subscriber-to-base QoS. When a station has data to send, it may include in its next scheduled transmission a request for a longer transmission interval; if the request is granted, the station may send its data (or at least some of its data) in itsnext scheduled transmission slot. When a station is done transmitting, its timeslot may shrink back to the minimum, and may be scheduled less frequently as well, but it does not disappear. Stations without data to send remain connected to the base station by sending ‚Äúempty‚Äù messages during these slots. 4.3.2 Ranging The uplink scheduling of the previous section requires that each subscriber station know the distance to the base station. If a subscriber station is to transmit so that its message arrives at the base station at a 126 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 BaseSta1Sta2 Sta3 Fig. 29:: Three packets in transit from stations Sta1, Sta2 and Sta3. The packets propagate outwards from the stations at the speed of light, like ripples, spatially conÔ¨Åned between two concentric dot-dash circles (circles around Sta3 are not shown). The packet portion along the straight line from the station to the Base is represented as a heavy arrow. The three packets will arrive at Base sequentially and without overlap. certain time, it must actually begin transmission early by an amount equal to the one-way station-to-base propagation delay. This distance/delay measurement process is called ranging. Ranging can be accomplished through any RTT measurement. Any base-station delay in replying, once a subscriber message is received, simply needs to be subtracted from the total RTT. Of course, that base-station delay needs also to be communicated back to the subscriber. The distance to the base station is used not only for the subscriber station‚Äôs transmission timing, but also to determine its power level; signals from each subscriber station, no matter where located, should arrive at the base station with about the same power. 4.3.3 Network Entry The scheduling process eliminates the potential for collisions between normal data transmissions. But there remains the issue of initial entry to the network. If a handoff is involved, the new base station can be informed by the old base station, and send an appropriate schedule to the moving subscriber station. But if the subscriber station was just powered on, or is arriving from an area without LTE/WiMAX coverage, potential transmission collisions are unavoidable. Fortunately, network entry is infrequent, and so collisions are even less frequent. A subscriber station begins the network-entry connection process to a base station by listening for the base station‚Äôs transmissions; these message streams contain regular management messages containing, among other things, information about available data rates in each direction. Also included in the base station‚Äôs message stream is information about when network-entry attempts can be made. 4.3 WiMAX and LTE 127
An Introduction to Computer Networks, Release 2.0.11 In WiMAX these entry-attempt timeslots are called ranging intervals; the subscriber station waits for one of these intervals and sends a ‚Äúrange-request‚Äù message to the base station. These ranging intervals are open to all stations attempting network entry, and if another station transmits at the same time there will be a collision. An Ethernet/Wi-Fi-like exponential-backoff process is used if a collision does occur. In LTE the entry process is known as RACH, for Random Access CHannel. The base station designates certain 1 ms timeslots for network entry. During one of these slots an entry-seeking subscriber chooses at random one of up to 64 predetermined random access preambles (some preambles may be reserved for a second, contention-free form of RACH), and transmits it. The 1-ms timeslot corresponds to 300 kilometers, much larger than any LTE cell, so the fact that the subscriber does not yet know its distance to the base does not matter. The preambles are mathematically ‚Äúorthogonal‚Äù, in such a way that as long as no two RACH-participating subscribers choose the same preamble, the base station can decode overlapping preambles and thus receive thesetof all preambles transmitted during the RACH timeslot. The base station then sends a reply, listing the preambles received and, in effect, an initial schedule indexed by preamble of when each newly entering subscriber station can transmit actual data. This reply is sent to a special temporary multicast address known as aradio network temporary identiÔ¨Åer, or RNTI, as the base station does not yet know the actual identity of any new subscriber. Those identities are learned as the new subscribers transmit to the base station according to this initial schedule. A collision occurs when two LTE subscriber stations have the misfortune of choosing the same preamble in the same RACH timeslot, in which case the chosen preamble will not appear in the initial schedule transmitted by the base station. As for WiMAX, collisions are rare because network entry is rare. Subscribers experiencing a collision try again during the next RACH timeslot, choosing at random a new preamble. For both WiMAX and LTE, network entry is the only time when collisions can occur; afterwards, all subscriber-station transmissions are scheduled by the base station. If there is no collision, each subscriber station is able to use the base station‚Äôs initial-response transmission to make its Ô¨Årst ranging measurement. Subscribers must have a ranging measurement in hand before they can send any scheduled transmission. 4.3.4 Mobility There are some signiÔ¨Åcant differences between stationary and mobile subscribers. First, mobile subscribers will likely expect some sort of handoff from one base station to another as the subscriber moves out of range of the Ô¨Årst. Second, moving subscribers mean that the base-to-subscriber ranging information may change rapidly; see exercise 4.0. If the subscriber does not update its ranging information often enough, it may transmit too early or too late. If the subscriber is moving fast enough, the Doppler effect may also alter frequencies. 4.4 Fixed Wireless This category includes all wireless-service-provider systems where the subscriber‚Äôs location does not change. Often, but not always, the subscriber will have an outdoor antenna for improved reception and range. Fixed-wireless systems can involve relay through satellites, or can be terrestrial. 128 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 4.4.1 Terrestrial Wireless Terrestrial wireless ‚Äì also called terrestrial broadband or Ô¨Åxed-wireless broadband ‚Äì involves direct (nonsatellite) radio communication between subscribers and a central access point. Access points are usually tower-mounted and serve multiple subscribers, though single-subscriber point-to-point ‚Äúmicrowave links‚Äù also exist. A multi-subscriber access point may serve an area with radius up to several tens of miles, depending on the technology, though more common ranges are under ten miles. WiMAX 802.16d is one form of terrestrial wireless, but there are several others. Frequencies may be either licensed or unlicensed. Unlicensed frequency bands are available at around 900 MHz, 2.4 GHz, and 5 GHz. Nominally all three bands require that line-of-sight transmission be used, though that requirement becomes stricter as the frequency increases. Lower frequencies tend to be better at ‚Äúseeing‚Äù through trees and other obstructions. Trees vs Signal 4.4 Fixed Wireless 129
An Introduction to Computer Networks, Release 2.0.11 Photo of the author attempting to improve his 2.4 GHz terrestrial-wireless signal via tree trimming. Terrestrial Ô¨Åxed wireless was originally popularized for rural areas, where residential density is too low for economical cable connections. However, some Ô¨Åxed-wireless ISPs now operate in urban areas, often using WiMAX. One advantage of terrestrial Ô¨Åxed-wireless in remote areas is that the antennas covers a much smaller geographical area than a satellite, generally meaning that there is a higher data rate available per user and the cost per megabyte is much lower. Outdoor subscriber antennas often use a parabolic dish to improve reception; sizes range from 10 to 50 cm in diameter. The size of the dish may depend on the distance to the central tower. While there are standardized Ô¨Åxed-wireless systems, such as WiMAX, there are also a number of proprietary alternatives, including systems from Trango and Canopy. Fixed-wireless systems might, in fact, be considered one of the last bastions of proprietary LAN protocols. This lack of standardization is due to a variety of factors; two primary ones are the relatively modest overall demand for this service and the the fact that most antennas need to be professionally installed by the ISP to ensure that they are ‚Äúproperly mounted, aligned, grounded and protected from lightning‚Äù. 4.4.2 Satellite Internet An extreme case of Ô¨Åxed wireless is satellite Internet, in which signals pass through a satellite in geosynchronous orbit (35,786 km above the earth‚Äôs surface). Residential customers have parabolic antennas typically from 70 to 100 cm in diameter, larger than those used for terrestrial wireless but smaller than the dish antennas used at access points. The geosynchronous satellite orbit means that the antennas need to be pointed only once, at installation. Transmitter power is typically 1-2 watts, remarkably low for a signal that travels 35,786 km. The primary problem associated with satellite Internet is very long RTTs. The the speed-of-light round-trip propagation delay is about 500 ms to which must be added queuing delays for the often-backlogged access point (my own personal experience suggested that RTTs of close to 1,000 ms were the norm). These long delays affect real-time trafÔ¨Åc such as V oIP and gaming, but as we shall see in 21.8 The Satellite-Link TCP Problem bulk TCP transfers also perform poorly with very long RTTs. To provide partial compensation for the TCP issue, many satellite ISPs provide some sort of ‚Äúacceleration‚Äù for bulk downloads: a web page, for example, would be downloaded rapidly by the access point and streamed to the satellite and back down to the user via a proprietary mechanism. Acceleration, however, cannot help interactive connections such as VPNs. Another common feature of satellite Internet is a low daily utilization cap, typically in the hundreds of megabytes. Utilization caps are directly tied to the cost of maintaining satellites, but also to the fact that one satellite covers a great deal of ground, and so its available capacity is shared by a large number of users. The delay issues associated with satellite Internet would go away if satellites were in so-called low-earth orbits, a few hundred km above the earth. RTTs would then be comparable with terrestrial Internet. Fixeddirection antennas could no longer be used. A large number of satellites would need to be launched to provide 24-hour coverage even at one location. To data (2016), such a network of low-earth satellites has been proposed, but not yet launched. 130 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 4.5 Epilog Wireless networking has revolutionized the world. In wealthy countries, everyone is connected all the time. Even in poorer regions of the world, though, wireless has brought at least partial access to the Internet to huge numbers of people. Of course, this wireless revolution has led to conÔ¨Çicts in the allocation of radiofrequency spectrum. 4.6 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.4 Solutions for Wireless LANs. 1.0. A seemingly important part of the IEEE 801.11 Wi-Fi standard is that stations do not transmit when another station is transmitting; this is meant to reduce collisions. And yet the standard states ‚Äútransmission of the ACK frame shall commence after a SIFS period, without regard to the busy/idle state of the medium‚Äù; that is, the ACK sender does notlisten Ô¨Årst for an idle network. Give a scenario in which the transmission of an ACK while the medium is notidle does notresult in a collision! That is, station A has just Ô¨Ånished transmitting a packet to station C, but before C can begin sending its ACK, another station B starts transmitting. Hint: this is another example of the hidden-node problem, 4.2.1.4 Hidden-Node Problem, with station C again the ‚Äúmiddle‚Äù station. Recall also that simultaneous transmission results in a collision only if some node fails to be able to read either signal as a result. (Also note that, if C does notsend its ACK, despite B, the packet just sent from A has to all intents and purposes been lost.) 2.0.‚ô¢Give an example of a three-sender hidden-node collision ( 4.2.1.4 Hidden-Node Problem ); that is, three nodes A, B and C, no two of which can see one another, where all can reach a fourth node D. Can you do this for more than three sending nodes? 3.0. Suppose the average contention interval in a Wi-Fi network (802.11g) is 64 SlotTimes. The average packet size is 1 kB, and the data rate is 54 Mbps. At that data rate, it takes about (8 1024)/54 = 151 ¬µsec to transmit a packet. (a). How long is the average contention interval, in ¬µsec? (b)‚ô¢. What fraction of the total potential data rate is lost to contention? (See 2.1.11 Analysis of Classic Ethernet for a similar example). 4.0. WiMAX and LTE subscriber stations are not expected to hear one another at all. For Wi-Fi non-accesspoint stations in an infrastructure (access-point) setting, on the other hand, listening to other non-accesspoint transmissions is encouraged. List some ways in which such stations might respond to packets sent by other non-access-point stations. Try to come up with examples for stations associated to the same access point, and also for stations associated to different access points. The responses need not be in the form of transmissions. (Wi-Fi stations cannot be required to make such responses because reception of the relevant packets is often quite spotty.) 4.5 Epilog 131
An Introduction to Computer Networks, Release 2.0.11 5.0. Suppose WiMAX subscriber stations can be moving, at speeds of up to 33 meters/sec (the maximum allowed under 802.16e). (a). How much earlier (or later) can one subscriber packet arrive? Assume that the ranging process updates the station‚Äôs propagation delay once a minute. The speed of light is about 300 meters/¬µsec. (b). With 5000 senders per second, how much time out of each second must be spent on ‚Äúguard intervals‚Äù accommodating the early/late arrivals above? You will need to double the time from part (a), as the base station cannot tell whether the signal from a moving subscriber will arrive earlier or later. 6.0. In this exercise we outline the two-ray ground model of wireless transmission in which the signal power is inversely proportional to the fourth power of the distance, rather than following the usual inversesquare law. Some familiarity with trigonometric (or complex-exponential) manipulations is necessary. Suppose the signal near the transmitter is A sin(2 ùúãft), where A is the amplitude, f the frequency and t the time. Signal power is proportional to A2. At distance r ¬•1, the amplitude is reduced by a factor of 1/r (so the power is reduced by 1/r2) and the signal is delayed by a time r/c, where c is the speed of light, giving (A/r)sin(2 ùúãf(t ‚Äì r/c)) = (A/r)sin(2 ùúãft ‚Äì 2 ùúãùúÜr) where ùúÜ= f/c is the wavelength. The received signal is the superposition of the line-of-sight signal path and its reÔ¨Çection from the ground, as in the following diagram: height hline-of-sight r reflected path rr/2 r/2arriving signals Sender and receiver are shown at equal heights above the ground, for simplicity. We assume 100% ground reÔ¨Çectivity (this is reasonable for very shallow angles). The phase of the ground signal is reversed 180¬∞ by the reÔ¨Çection, and then is delayed slightly more by the slightly longer path. (a). Find a formula for the length of the reÔ¨Çected-signal path r1, in terms of r and h. Eliminate the square root, using the approximation (1+x)1/21 + x/2 for small x. (You will need to factor r out of the square-root expression for r1Ô¨Årst.) (b). Simplify the difference (because of the 180¬∞ reÔ¨Çection phase-reversal) of the line-of-sight and reÔ¨Çectedsignal paths. Use the approximation sin(X) ‚Äì sin(Y) = 2 sin((X‚ÄìY)/2) cos((X+Y)/2) (X-Y) cos((X+Y)/2) (X‚ÄìY) cos(X), for X Y (or else use complex exponentials, noting sin(X) is the real part of i eiX). You may assume r1‚Äìr is smaller than the wavelength ùúÜ. Start with (A/r)sin(2 ùúãft ‚Äì 2 ùúãùúÜr) ‚Äì (A/r1)sin(2 ùúãft ‚Äì 2ùúãùúÜr1); it helps to isolate the r √ër1change to one subexpression at a time by writing this as follows (adding and subtracting the identical middle terms): ((A/r)sin(2 ùúãft ‚Äì 2 ùúãùúÜr) ‚Äì (A/r1)sin(2 ùúãft ‚Äì 2 ùúãùúÜr)) + ((A/r1)sin(2 ùúãft ‚Äì 2 ùúãùúÜr) ‚Äì (A/r1)sin(2 ùúãft ‚Äì 2 ùúãùúÜr1)) = (A/r ‚Äì A/r1)sin(2 ùúãft ‚Äì 2 ùúãùúÜr) + (A/r1)(sin(2 ùúãft ‚Äì 2 ùúãùúÜr) ‚Äì sin(2 ùúãft ‚Äì 2 ùúãùúÜr1)) 132 4 Wireless LANs
An Introduction to Computer Networks, Release 2.0.11 (c). Show that the approximate amplitude of this difference is proportional to 1/r2, making the relative power proportional to 1/r4. 7.0. In the four-way handshake of 4.2.5 Wi-Fi Security, suppose station B (for Bad) records the successful handshake of station A and the access point. A then leaves the network, and B attempts a replay attack: B uses A‚Äôs packets in the handshake. At exactly what point does the handshake break down? 4.6 Exercises 133
An Introduction to Computer Networks, Release 2.0.11 134 4 Wireless LANs
5 OTHER LANS In the wired era, one could get along quite well with nothing but Ethernet and the occasional long-haul point-to-point link joining different sites. However, there are important wired alternatives out there. Some, like token ring, are mostly of historical importance; others, like virtual circuits, are of great conceptual importance but ‚Äì so far ‚Äì of relatively modest day-to-day signiÔ¨Åcance (though see MPLS, 25.12 MultiProtocol Label Switching (MPLS) ). 5.1 Virtual Private Networks Suppose you want to connect to your workplace network from home. Your workplace, however, has a security policy that does not allow ‚Äúoutside‚Äù IP addresses to access essential internal resources. How do you proceed, without leasing a dedicated telecommunications line to your workplace? A virtual private network, or VPN, provides a solution; it supports creation of virtual links that join farÔ¨Çung nodes via the Internet. Your home computer creates an ordinary Internet connection (TCP or UDP) to a workplace VPN server (IP-layer packet encapsulation can also be used, and avoids the timeout problems sometimes created by sending TCP packets within another TCP stream; see 9.9 Mobile IP ). Each end of the connection is typically associated with a software-created virtual network interface; each of the two virtual interfaces is assigned an IP address. (Virtual interfaces are not essential; VPNs created with IPsec, 29.6 IPsec, generally omit them.) When a packet is to be sent along the virtual link, it is actually encapsulated and sent along the original Internet connection to the VPN server, wending its way through the commodity Internet; this process is called tunneling. To all intents and purposes, the virtual link behaves like any other physical link. Tunneled packets are often encrypted as well as encapsulated, though that is a separate issue. One relatively easy-to-implement example of a tunneling mechanism is to treat a TCP home-workplace connection as a serial line and send packets over it back-to-back, using PPP with HDLC; see 6.1.5.1 HDLC andRFC 1661 (though this can lead to the above-mentioned TCP-in-TCP timeout problems). At the workplace side, the virtual network interface in the VPN server is attached to a router or switch; at the home user‚Äôs end, the virtual network interface can now be assigned an internal workplace IP address. The home computer is now, for all intents and purposes, part of the internal workplace network. In the diagram below, the user‚Äôs regular Internet connection is via hardware interface eth0. A connection is established to Site A‚Äôs VPN server; a virtual interface tun0 is created on the user‚Äôs machine which appears to be a direct link to the VPN server. The tun0 interface is assigned a Site-A IP address. Packets sent via thetun0 interface in fact travel over the original connection via eth0 and the Internet. 135
An Introduction to Computer Networks, Release 2.0.11 Internet Site A 200.0.1/24User at homeeth0 VPN serverSite A private tun0 200.0.1.37 VPN: blue link represents tunnel. Actual connection is made via eth0 The tun0 interface is a virtual network interface with a Site-A address After the VPN is set up, the home host‚Äôs tun0 interface appears to be locally connected to Site A, and thus the home host is allowed to connect to the private area within Site A. The home host‚Äôs forwarding table will be conÔ¨Ågured so that trafÔ¨Åc to Site A‚Äôs private addresses is routed via interface tun0. VPNs are also commonly used to connect entire remote ofÔ¨Åces to headquarters. In this case the remote-ofÔ¨Åce end of the tunnel will be at that ofÔ¨Åce‚Äôs local router, and the tunnel will carry trafÔ¨Åc for all the workstations in the remote ofÔ¨Åce. Other applications of VPNs include trying to appear geographically to be at another location, and bypassing Ô¨Årewall rules blocking speciÔ¨Åc TCP or UDP ports. To improve security, it is common for the residential (or remote-ofÔ¨Åce) end of the VPN connection to use the VPN connection as the default route for all trafÔ¨Åc except that needed to maintain the VPN itself. This may require a so-called host-speciÔ¨Åc forwarding-table entry at the residential end to allow the packets that carry the VPN tunnel trafÔ¨Åc to be routed correctly via eth0. This routing strategy means that potential intruders cannot access the residential host ‚Äì and thus the workplace internal network ‚Äì through the original residential Internet access. A consequence is that if the home worker downloads a large Ô¨Åle from a nonworkplace site, it will travel Ô¨Årst to the workplace, then back out to the Internet via the VPN connection, and Ô¨Ånally arrive at the home. To improve congestion response, IP packets are sometimes marked by routers that are experiencing congestion; see 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN). If such marking is done to the outer, encapsulating, packet, and the marks are not transferred at the remote endpoint of the VPN to the inner, encapsulat ed, packet, then the marks are lost. Congestion response may suffer. RFC 6040 spells out a proper re-marking strategy in general; RFC 7296 deÔ¨Ånes re-marking for IPsec ( 29.6 IPsec ). Older VPN protocols, however, may not support congestion re-marking. 5.2 Carrier Ethernet Carrier Ethernet is a leased-line point-to-point link between two sites, where the subscriber interface at each end of the line looks like Ethernet (in some Ô¨Çavor). The physical path in between sites, however, need not 136 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 have anything to do with Ethernet; it may be implemented however the carrier wishes. In particular, it will be (or at least appear to be) full-duplex, it will be collision-free, and its length may far exceed the maximum permitted by any IEEE Ethernet standard. As we have seen, Ethernet itself has become a kind of generic virtual interface for a wide variety of physical implementations and speeds. Carrier Ethernet takes this view to its logical conclusion: it is Ethernet only in terms of the endpoint interfaces; the intervening network can be anything at all. Bandwidth can be purchased in whatever increments the carrier has implemented, and uplink bandwidth can be very different from downlink. It is perfectly reasonable for the endpoint Ethernet interfaces to be much faster than the bandwidth provided, egGigabit Ethernet interfaces for a 40 Mbps link. The point of carrier Ethernet is to provide a layer of abstraction between the customers, who need only install a commodity Ethernet interface, and the provider, who can upgrade the link implementation at will without requiring change at the customer end. In a sense, carrier Ethernet is similar to the widespread practice of provisioning residential DSL and cable routers with an Ethernet interface for customer interconnection; again, the actual link technologies may not look anything like Ethernet, but the interface will. A carrier Ethernet connection looks like a virtual VPN link, but runs on top of the provider‚Äôs internal network rather than the Internet at large. Carrier Ethernet connections often provide the primary Internet connectivity for one endpoint, unlike Internet VPNs which assume both endpoints already have full Internet connectivity. 5.3 Token Ring A signiÔ¨Åcant part of the previous chapter was devoted to classic Ethernet‚Äôs collision mechanism for supporting shared media access. After that, it may come as a surprise that there is a simple multiple-access mechanism that is not only collision-free, but which supports fairness in the sense that if N stations wish to send then each will receive 1/N of the opportunities. That method is Token Ring. Actual implementations come in several forms, from Fiber-Distributed Data Interface (FDDI) to so-called ‚ÄúIBM Token Ring‚Äù. The central idea is that stations are connected in a ring: F DCB EA Packets will be transmitted in one direction (clockwise in the ring above). Stations in effect forward most packets around the ring, although they can also remove a packet. (It is perhaps more accurate to think of the forwarding as representing the default cable connectivity; non-forwarding represents the station‚Äôs momentarily breaking that connectivity.) When the network is idle, all stations agree to forward a special, small packet known as a token. When a station, say A, wishes to transmit, it must Ô¨Årst wait for the token to arrive at A. Instead of forwarding the 5.3 Token Ring 137
An Introduction to Computer Networks, Release 2.0.11 token, A then transmits its own packet; this travels around the network and is then removed by A. At that point (or in some cases at the point when A Ô¨Ånishes transmitting its data packet) A then forwards the token. In a small ring network, the ring circumference may be a small fraction of one packet. Ring networks become ‚Äúlarge‚Äù at the point when some packets may be entirely in transit on the ring. Slightly different solutions apply in each case. (It is also possible that the physical ring exists only within the token-ring switch, and that stations are connected to that switch using the usual point-to-point wiring.) If all stations have packets to send, then we will have something like the following: 
- A waits for the token 
- A sends a packet 
- A sends the token to B 
- B sends a packet 
- B sends the token to C 
- C sends a packet 
- C sends the token to D 
-. .. All stations get an equal number of chances to transmit, and no bandwidth is wasted on collisions. (A station constantly sending smaller packets will send the same number of packets as a station constantly sending larger packets, but the bandwidth will be smaller in proportion to the smaller packet size.) One problem with token ring is that when stations are powered off it is essential that the packets continue forwarding; this is usually addressed by having the default circuit conÔ¨Åguration be to keep the loop closed. Another issue is that some station has to watch out in case the token disappears, or in case a duplicate token appears. Because of fairness and the lack of collisions, IBM Token Ring was once considered to be the premium LAN mechanism. As such, Token Ring hardware commanded a substantial price premium. But due to Ethernet‚Äôs combination of lower hardware costs and higher bitrates (even taking collisions into account), the latter eventually won out. There was also a much earlier collision-free hybrid of 10 Mbps Ethernet and Token Ring known as Token Bus: an Ethernet physical network (often linear) was used with a token-ring-like protocol layer above that. Stations were physically connected to the (linear) Ethernet but were assigned identiÔ¨Åers that logically arranged them in a (virtual) ring. Each station had to wait for the token and only then could transmit a packet; after that it would send the token on to the next station in the virtual ring. As with ‚Äúreal‚Äù Token Ring, some mechanisms need to be in place to monitor for token loss. Token Bus Ethernet never caught on. The additional software complexity was no doubt part of the problem, but perhaps the real issue was that it was not necessary. 5.4 Virtual Circuits Before we can get to our Ô¨Ånal LAN example, ATM, we need to detour brieÔ¨Çy through virtual circuits. 138 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 The Road Not Taken A close reading of Robert Frost‚Äôs poem referenced here reveals that the supposed great difference between the two roads exists only in the narrator‚Äôs retrospective imaginings; the roads were in fact ‚Äúreally about the same‚Äù. Perhaps this would also apply to datagram and virtual-circuit forwarding, though see below on per-connection billing. Virtual circuits are The Road Not Taken by IP. Virtual-circuit switching (or routing) is an alternative to datagram switching, which was introduced in . In datagram switching, routers know the next_hop to each destination, and packets are addressed by destination. In virtual-circuit switching, routers know about end-to-end connections, and packets are ‚Äúaddressed‚Äù by a connection ID. Before any data packets can be sent, a connection needs to be established Ô¨Årst. For that connection, the route is computed and then each link along the path is assigned a connection ID, traditionally called the VCI, for Virtual Circuit IdentiÔ¨Åer. In most cases, VCIs are only locally unique; that is, the same connection may use a different VCI on each link. The lack of global uniqueness makes VCI allocation much simpler. Although the VCI keeps changing along a path, the VCI can still be thought of as identifying the connection. To send a packet, the host marks the packet with the VCI assigned to the host‚Äìrouter1 link. Packets arrive at (and depart from) switches via one of several ports, which we will assume are numbered beginning at 0. Switches maintain a connection table indexed byxVCI,portypairs; unlike a forwarding table, the connection table has a record of every connection through that switch at that particular moment. As a packet arrives, its inbound VCI inand inbound port inare looked up in this table; this yields an outbound xVCI out,port outypair. The VCI Ô¨Åeld of the packet is then rewritten to VCI out, and the packet is sent via port out. Note that typically there is no source address information included in the packet (although the sender can be identiÔ¨Åed from the connection, which can be identiÔ¨Åed from the VCI at any point along the connection). Packets are identiÔ¨Åed by connection, not destination. Any node along the path (including the endpoints) can in principle look up the connection and Ô¨Ågure out the endpoints. Note also that each switch must rewrite the VCI. Datagram switches never rewrite addresses (though they do update hopcount/TTL Ô¨Åelds). The advantage to this rewriting is that VCIs need be unique only for a given link, greatly simplifying the naming. Datagram switches also do not make use of a packet‚Äôs arrival interface. As an example, consider the network below. Switch ports are numbered 0,1,2,3. Two paths are drawn in, one from A to F in red and one from B to D in green; each link is labeled with its VCI number in the same color. We will construct virtual-circuit connections between 
- A and F (shown above in red) 
- A and E 
- A and C 
- B and D (shown above in green) 5.4 Virtual Circuits 139
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S3 S4 S5A D B C EF0 12 0 12 0 0 0 1 11 2 23 346 4 8 5487 8 
- A and F again (a separate connection) The following VCIs have been chosen for these connections. The choices are made more or less randomly here, but in accordance with the requirement that they be unique to each link. Because links are generally taken to be bidirectional, a VCI used from S1 to S3 cannot be reused from S3 to S1 until the Ô¨Årst connection closes. 
- A to F: A 4S1 6S2 4S4 8S5 5F; this path goes from S1 to S4 via S2 
- A to E: A 5S1 6S3 3S4 8E; this path goes, for no particular reason, from S1 to S4 via S3, the opposite corner of the square 
- A to C: A 6S1 7S3 3C 
- B to D: B 4S3 8S1 7S2 8D 
- A to F: A 7S1 8S2 5S4 9S5 2F One may verify that on any one link no two different paths use the same VCI. We now construct the actual xVCI,portytables for the switches S1-S4, from the above; the table for S5 is left as an exercise. Note that either the xVCI in,port inyor thexVCI out,port outycan be used as the key; we cannot have the same pair in both the in columns and the out columns. It may help to display the port numbers for each switch, as in the upper numbers in following diagram of the above red connection from A to F (lower numbers are the VCIs): A S1 S2 S4 S5 F02 01 32 01 4 6 4 8 5 Switch S1: VCI inport inVCI out port out connection 4 0 6 2 A√ù√ëF #1 5 0 6 1 A√ù√ëE 6 0 7 1 A√ù√ëC 8 1 7 2 B√ù√ëD 7 0 8 2 A√ù√ëF #2 140 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 Switch S2: VCI inport inVCI out port out connection 6 0 4 1 A√ù√ëF #1 7 0 8 2 B√ù√ëD 8 0 5 1 A√ù√ëF #2 Switch S3: VCI inport inVCI out port out connection 6 3 3 2 A√ù√ëE 7 3 3 1 A√ù√ëC 4 0 8 3 B√ù√ëD Switch S4: VCI inport inVCI out port out connection 4 3 8 2 A√ù√ëF #1 3 0 8 1 A√ù√ëE 5 3 9 2 A√ù√ëF #2 The namespace for VCIs is small, and compact ( egcontiguous). Typically the VCI and port bitÔ¨Åelds can be concatenated to produce a xVCI,Portycomposite value small enough that it is suitable for use as an array index. VCIs work best as local identiÔ¨Åers. IP addresses, on the other hand, need to be globally unique, and thus are often rather sparsely distributed. Virtual-circuit switching offers the following advantages: 
- connections can get quality-of-service guarantees, because the switches are aware of connections and can reserve capacity at the time the connection is made 
- headers are smaller, allowing faster throughput 
- headers are small enough to allow efÔ¨Åcient support for the very small packet sizes that are optimal for voice connections. ATM packets, for instance, have 48 bytes of data; see below. Datagram forwarding, on the other hand, offers these advantages: 
- Routers have less state information to manage. 
- Router crashes and partial connection state loss are not a problem. 
- If a router or link is disabled, rerouting is easy and does not affect any connection state. (As mentioned in , this was Paul Baran‚Äôs primary concern in his 1962 paper introducing packet switching.) 
- Per-connection billing is very difÔ¨Åcult. The last point above may once have been quite important; in the era when the ARPANET was being developed, typical daytime long-distance rates were on the order of $1/minute. It is unlikely that early TCP/IP protocol development would have been as fertile as it was had participants needed to justify per-minute billing costs for every project. 5.4 Virtual Circuits 141
An Introduction to Computer Networks, Release 2.0.11 It is certainly possible to do virtual-circuit switching with globally unique VCIs ‚Äì say the concatenation of source and destination IP addresses and port numbers. The IP-based RSVP protocol ( 25.6 RSVP ) does exactly this. However, the fast-lookup and small-header advantages of a compact namespace are then lost. Multi-Protocol Label Switching ( 25.12 Multi-Protocol Label Switching (MPLS) ) is another IP-based application of virtual circuits. Note that virtual-circuit switching does notsuffer from the problem of idle channels still consuming resources, which is an issue with circuits using time-division multiplexing ( egshared T1 lines) 5.5 Asynchronous Transfer Mode: ATM ATM is a network mechanism intended to accommodate real-time trafÔ¨Åc as well as bulk data transfer. We present ATM here as a LAN layer, for which it is still sometimes used, but it was originally proposed as a replacement for the IP layer as well, and, to an extent, the Transport layer. These broader plans were not greeted with universal enthusiasm within the IETF. When used as a LAN layer, IP packets are transmitted over ATM as in 5.5.1 ATM Segmentation and Reassembly. A distinctive feature of ATM is its small packet size. ATM has its roots in the telephone industry, and was therefore particularly intended to support voice. A signiÔ¨Åcant source of delay in voice trafÔ¨Åc is the packet Ô¨Åll time: at DS0 speeds (64 kbps), voice data accumulates at 8 bytes/ms. If we are sending 1 kB packets, this means voice is delayed by about 1/8 second, meaning in turn that when one person stops speaking, the earliest they can hear the other‚Äôs response is 1/4 second later. Slightly smaller levels of voice delay can introduce an annoying echo. Smaller packets reduce the Ô¨Åll time and thus the delay: when voice is sent over IP (V oIP), one common method is to send 160 bytes every 20 ms. ATM took this small-packet strategy even further: packets have 48 bytes of data, plus 5 bytes of header. Such small packets are often called cells. To manage such a small header, virtual-circuit routing is a necessity. IP packets of such small size would likely consume more than 50% of the bandwidth on headers, if the LAN header were included. Aside from reduced voice Ô¨Åll-time, other beneÔ¨Åts to small cells are reduced store-and-forward delay and minimal queuing delay, at least for high-priority trafÔ¨Åc. Prioritizing trafÔ¨Åc and giving precedence to highpriority trafÔ¨Åc is standard, but high-priority trafÔ¨Åc is never allowed to interrupt transmission already begun of a low-priority packet. If you have a high-priority voice cell, and someone else has a 1500-byte packet just started, your cell has to wait about 30 cell times, because 1500 bytes is about 30 cells. However, if their low-priority trafÔ¨Åc is instead made up of 30 cells, you have only to wait for their Ô¨Årst cell to Ô¨Ånish; the delay is 1/30 as much. ATM also made the decision to require Ô¨Åxed-size cells. The penalty for one partially used cell among many is small. Having a Ô¨Åxed cell size simpliÔ¨Åes hardware design, and, in theory, allows it easier to design for parallelism. Unfortunately, the designers of ATM also chose to mandate no cell reordering. This means cells can use a smaller sequence-number Ô¨Åeld, but also makes parallel switches much harder to build. A typical parallel switch design might involve distributing incoming cells among any of several input queues; the queues would then handle the VCI lookups in parallel and forward the cells to the appropriate output queues. With such an architecture, avoiding reordering is difÔ¨Åcult. It is not clear to what extent the no-reordering decision was related to the later decline of ATM in the marketplace. 142 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 ATM cells have 48 bytes of data and a 5-byte header. The header contains up to 28 bits of VCI information, three ‚Äútype‚Äù bits, one cell-loss priority, or CLP, bit, and an 8-bit checksum over the header only. The VCI is divided into 8-12 bits of Virtual Path IdentiÔ¨Åer and 16 bits of Virtual Channel IdentiÔ¨Åer, the latter supposedly for customer use to separate out multiple connections between two endpoints. Forwarding is by full switching only, and there is no mechanism for physical (LAN) broadcast. 5.5.1 ATM Segmentation and Reassembly Due to the small packet size, ATM deÔ¨Ånes its own mechanisms for segmentation and reassembly of larger packets. Thus, individual ATM links in an IP network are quite practical. These mechanisms are called ATM Adaptation Layers, and there are four of them: AALs 1, 2, 3/4 and 5 (AAL 3 and AAL 4 were once separate layers, which merged). AALs 1 and 2 are used only for voice-type trafÔ¨Åc; we will not consider them further. The ATM segmentation-and-reassembly mechanism deÔ¨Åned here is intended to apply only to large data; no cells are ever further subdivided. Furthermore, segmentation is always applied at the point where the data enters the network; reassembly is done at exit from the ATM path. IPv4 fragmentation, on the other hand, applies conceptually to IP packets, and may be performed by routers within the network. For AAL 3/4, we Ô¨Årst deÔ¨Åne a high-level ‚Äúwrapper‚Äù for an IP packet, called the CS-PDU (Convergence Sublayer - Protocol Data Unit). This preÔ¨Åxes 32 bits on the front and another 32 bits (plus padding) on the rear. We then chop this into as many 44-byte chunks as are needed; each chunk goes into a 48-byte ATM payload, along with the following 32 bits worth of additional header/trailer: 
- 2-bit type Ô¨Åeld: ‚Äì10: begin new CS-PDU ‚Äì00: continue CS-PDU ‚Äì01: end of CS-PDU ‚Äì11: single-segment CS-PDU 
- 4-bit sequence number, 0-15, good for catching up to 15 dropped cells 
- 10-bit MessageID Ô¨Åeld 
- CRC-10 checksum. We now have a total of 9 bytes of header for 44 bytes of data; this is more than 20% overhead. This did not sit well with the IP-over-ATM community (such as it was), and so AAL 5 was developed. AAL 5 moved the checksum to the CS-PDU and increased it to 32 bits from 10 bits. The MID Ô¨Åeld was discarded, as no one used it, anyway (if you wanted to send several different types of messages, you simply created several virtual circuits). A bit from the ATM header was taken over and used to indicate: 
- 1: start of new CS-PDU 
- 0: continuation of an existing CS-PDU The CS-PDU is now chopped into 48-byte chunks, which are then used as the entire body of each ATM cell. With 5 bytes of header for 48 bytes of data, overhead is down to 10%. Errors are detected by the 5.5 Asynchronous Transfer Mode: ATM 143
An Introduction to Computer Networks, Release 2.0.11 CS-PDU CRC-32. This also detects lost cells (impossible with a per-cell CRC!), as we no longer have any cell sequence number. For both AAL3/4 and AAL5, reassembly is simply a matter of stringing together consecutive cells in order of arrival, starting a new CS-PDU whenever the appropriate bits indicate this. For AAL3/4 the receiver has to strip off the 4-byte AAL3/4 headers; for AAL5 the receiver has to verify the CRC-32 checksum once all cells are received. Note that this strategy can fail without the general ATM requirement of no cell reordering, above. Different cells from different virtual circuits can be jumbled together on the ATM ‚Äúbackbone‚Äù, but on any one virtual circuit the cells from one higher-level packet must be sent one right after the other. A typical IP packet divides into about 20 cells. For AAL 3/4, this means a total of 200 bits devoted to CRC codes, versus only 32 bits for AAL 5. It might seem that AAL 3/4 would be more reliable because of this, but, paradoxically, it was not! The reason for this is that errors are rare, and so we typically have one or at most two per CS-PDU. Suppose we have only a single error, iea single cluster of corrupted bits small enough that it is likely conÔ¨Åned to a single cell. In AAL 3/4 the CRC-10 checksum will fail to detect that error (that is, the checksum of the corrupted packet will by chance happen to equal the checksum of the original packet) with probability 1/210. The AAL 5 CRC-32 checksum, however, will fail to detect the error with probability 1/232. Even if there are enough errors that two cells are corrupted, the two CRC-10s together will fail to detect the error with probability 1/220; the CRC-32 is better. AAL 3/4 is more reliable only when we have errors in at least four cells, at which point we might do better to switch to an errorcorrecting code. Moral: one checksum over the entire message is often better than multiple shorter checksums over parts of the message. 5.6 Epilog There are not many wired LANs that are not called ‚ÄúEthernet‚Äù. While it is sometimes tempting (in the IP world at least) to write off ATM as a niche technology, virtual circuits are a serious conceptual alternative to datagram forwarding. As we shall see in 25 Quality of Service, IP has problems handling real-time trafÔ¨Åc, and virtual circuits offer a solution. The Internet has so far embraced only small steps towards virtual circuits (such as MPLS, 25.12 Multi-Protocol Label Switching (MPLS) ), but they remain a tantalizing strategy. 5.7 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.5 Solutions for Other LANs. 1.0. Suppose remote host A uses a VPN connection to connect to host B, with IP address 200.0.0.7. A‚Äôs normal Internet connection is via device eth0 with IP address 12.1.2.3; A‚Äôs VPN connection is via device ppp0 with IP address 10.0.0.44. Whenever A wants to send a packet via ppp0, it is encapsulated and forwarded over the connection to B at 200.0.0.7. (a). Suppose A‚Äôs IP forwarding table is set up so that all trafÔ¨Åc to 200.0.0.7 uses eth0 and all trafÔ¨Åc to anywhere else uses ppp0. What happens if an intruder M attempts to open a connection to A at 12.1.2.3? 144 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 What route will packets from A to M take? (b). Suppose A‚Äôs IP forwarding table is (mis)conÔ¨Ågured so that alloutbound trafÔ¨Åc uses ppp0. Describe what will happen when A tries to send a packet. 2.0. Suppose remote host A wishes to use a TCP-based VPN connection to connect to host B, with IP address 200.0.0.7. However, the VPN software is not available for host A. Host A is, however, able to run that software on a virtual machine V hosted by A; A and V have respective IP addresses 10.0.0.1 and 10.0.0.2 on the virtual network connecting them. V reaches the outside world through network address translation (9.7 Network Address Translation ), with A acting as V‚Äôs NAT router. When V runs the VPN software, it forwards packets addressed to B the usual way, through A using NAT. TrafÔ¨Åc to any other destination it encapsulates over the VPN. Can A conÔ¨Ågure its IP forwarding table so that it can make use of the VPN? If not, why not? If so, how? (If you prefer, you may assume V is a physical host connecting to a second interface on A; A still acts as V‚Äôs NAT router.) 3.0. Token Bus was a proprietary Ethernet-based network. It worked like Token Ring in that a small token packet was sent from one station to the next in agreed-upon order, and a station could transmit only when it had just received the token. (a). If the data rate is 10 Mbps and the token is 64 bytes long (the 10-Mbps Ethernet minimum packet size), what is the average wait to receive the token on an idle network with 40 stations? (The average number of stations the token must pass through is 40/2 = 20.) Ignore the propagation delay and the gap Ethernet requires between packets. (b)‚ô¢. Sketch a protocol by which stations can sort themselves out to decide the order of token transmission; that is, an order of the stations S 0.. . S n-1where station S isends the token to station S (i+1) mod n. 4.0. [SM90] contained a proposal for sending IP packets over ATM as N cells as in AAL-5, followed by one cell containing the XOR of all the previous cells. This way, the receiver can recover from the loss of any one cell. Suppose N=20 here; with the SM90 mechanism, each packet would require 21 cells to transmit; that is, we always send 5% more. Suppose the cellloss-rate is p (presumably very small). If we send 20 cells without the SM90 mechanism, we have a probability of about 20p that any one cell will be lost, and we will have to retransmit the entire 20 again. This gives an average retransmission amount of about 20p extra packets. For what value of p do the with-SM90 and the without-SM90 approaches involve about the same total number of cell transmissions? 5.0. In the example in 5.4 Virtual Circuits, give the VCI table for switch S5. 6.0. Suppose we have the following network: AS1 S2B CS3 S4D The virtual-circuit switching tables are below. Ports are identiÔ¨Åed by the node at the other end. Identify all 5.7 Exercises 145
An Introduction to Computer Networks, Release 2.0.11 the connections that begin at A (and so have their Ô¨Årst connection entry in the table for S1). Give the path for each connection and the VCI on each link of the path. Switch S1: VCI inport inVCI out port out 1 A 2 S3 2 A 2 S2 3 A 3 S2 Switch S2: VCI inport inVCI out port out 2 S4 1 B 2 S1 3 S4 3 S1 4 S4 Switch S3: VCI inport inVCI out port out 2 S1 2 S4 3 S4 2 C Switch S4: VCI inport inVCI out port out 2 S3 2 S2 3 S2 3 S3 4 S2 1 D 7.0.‚ô¢We have the same network as the previous exercise: AS1 S2B CS3 S4D The virtual-circuit switching tables are below. Ports are identiÔ¨Åed by the node at the other end. Identify all the connections. Give the path for each connection and the VCI on each link of the path. Switch S1: VCI inport inVCI out port out 1 A 2 S2 3 S3 2 A 146 5 Other LANs
An Introduction to Computer Networks, Release 2.0.11 Switch S2: VCI inport inVCI out port out 2 S1 3 S4 1 B 2 S4 Switch S3: VCI inport inVCI out port out 2 S1 2 S4 1 S4 3 S1 Switch S4: VCI inport inVCI out port out 3 S2 2 D 2 S2 1 S3 8.0. Suppose we have the following network: AS1 S2B CS3 S4D Give virtual-circuit switching tables for the following connections. Route via a shortest path. (a). A‚ÄìD (b). C‚ÄìB, via S4 (c). B‚ÄìD (d). A‚ÄìD, via whichever of S2 or S3 was notused in part (a) 9.0. Below is a set of switches S1 through S4. DeÔ¨Åne VCI-table entries so the virtual circuit from A to B follows the path A√ù√ëS1√ù√ëS2√ù√ëS4√ù√ëS3√ù√ëS1√ù√ëS2√ù√ëS4√ù√ëS3√ù√ëB That is, each switch is visited twice. AS1S2 BS3S4 5.7 Exercises 147
An Introduction to Computer Networks, Release 2.0.11 148 5 Other LANs
6 LINKS At the lowest (logical) level, network links look like serial lines. In this chapter we address how packet structures are built on top of serial lines, via encoding and framing. Encoding determines how bits and bytes are represented on a serial line; framing allows the receiver to identify the beginnings and endings of packets. We then conclude with the high-speed serial lines offered by the telecommunications industry, T-carrier and SONET, upon which almost all long-haul point-to-point links that tie the Internet together are based. 6.1 Encoding and Framing A typical serial line is ultimately a stream of bits, not bytes. How do we identify byte boundaries? This is made slightly more complicated by the fact that, beneath the logical level of the serial line, we generally have to avoid transmitting long runs of identical bits, because the receiver may simply lose count; this is theclock synchronization problem (sometimes called the clock recovery problem). This means that, one way or another, we cannot always just send the desired bits sequentially; for example, extra bits are often inserted to break up long runs. Exactly how we do this is the encoding mechanism. Once we have settled the transmission of bits, the next step is to determine how the receiver identiÔ¨Åes the start of each new packet. Ethernet packets are separated by physical gaps, but for most other link mechanisms packets are sent end-to-end, with no breaks. How we tell when one packet stops and the next begins is the framing problem. To summarize: 
- encoding: correctly recognizing all the bits in a stream 
- framing: correctly recognizing packet boundaries These are related, though not the same. For long (multi-kilometer) electrical serial lines, in addition to the clock-related serial-line requirements we also want the average voltage to be zero; that is, we want no DC component. We will mostly concern ourselves here, however, only with lines short enough for this not to be a major concern. 6.1.1 NRZ NRZ (Non-Return to Zero) is perhaps the simplest encoding; it corresponds to direct bit-by-bit transmission of the 0‚Äôs and 1‚Äôs in the data. We have two signal levels, loandhi, we set the signal to one or the other of these depending on whether the data bit is 0 or 1, as in the diagram below. Note that in the diagram the signal bits have been aligned with the start of the pulse representing that signal value. 149
An Introduction to Computer Networks, Release 2.0.11 0 0 1 0 1 1 0 1 1 11 0 0 0 00 1 NRZ Encoding: 1 = hi, 0 = lo NRZ replaces an earlier RZ (Return to Zero) encoding, in which hi and lo corresponded to +1 and -1, and between each pair of pulses corresponding to consecutive bits there was a brief return to the 0 level. One drawback to NRZ is that we cannot distinguish between 0-bits and a signal that is simply idle. However, the more serious problem is the lack of synchronization: during long runs of 0‚Äôs or long runs of 1‚Äôs, the receiver can ‚Äúlose count‚Äù, egif the receiver‚Äôs clock is running a little fast or slow. The receiver‚Äôs clock can and does resynchronize whenever there is a transition from one level to the other. However, suppose bits are sent at one per ¬µs, the sender sends Ô¨Åve 1-bits in a row, and the receiver‚Äôs clock is running 10% fast. The signal sent is a 5-¬µs hi pulse, but when the pulse ends the receiver‚Äôs clock reads 5.5 ¬µs due to the clock speedup. Should this represent Ô¨Åve 1-bits or six 1-bits? 6.1.2 NRZI An alternative that helps here (though not obviously at Ô¨Årst) is NRZI, or NRZ Inverted. In this encoding, we represent a 0-bit as no change, and a 1-bit as a transition from lo to hi or hi to lo: 0 0 1 0 1 1 0 1 1 11 0 0 0 00 1 NRZI Encoding: 1 = transition, 0 = no transition Now there is a signal transition aligned above every 1-bit; a 0-bit is represented by the lack of a transition. This solves the synchronization problem for runs of 1-bits, but does nothing to address runs of 0-bits. However, NRZI can be combined with techniques to minimize runs of 0-bits, such as 4B/5B (below). 6.1.3 Manchester Manchester encoding sends the data stream using NRZI, with the addition of a clock transition between each pair of consecutive data bits. This means that the signaling rate is now double the data rate, eg20 MHz for 10Mbps Ethernet (which does use Manchester encoding). The signaling is as if we doubled the bandwidth and inserted a 1-bit between each pair of consecutive data bits, removing this extra bit at the receiver: 150 6 Links
An Introduction to Computer Networks, Release 2.0.11 clock clock clock clock clock clock clock clock clock clock clock clock clock clock clock clock clock100 01101111000001 Manchester Encoding: NRZI alternating with clock transitions All these transitions mean that the longest the clock has to ‚Äúcount‚Äù is 1 bit-time; clock synchronization is essentially solved, at the expense of the doubled signaling rate. 6.1.4 4B/5B In 4B/5B encoding, for each 4-bit ‚Äúnybble‚Äù of data we actually transmit a designated 5-bit symbol, or code, selected to have ‚Äúenough‚Äù 1-bits. A symbol in this sense is a digital or analog transmission unit that decodes to a set of data bits; the data bits are not transmitted individually. (The transmission of symbols rather than individual bits is nearly universal for high-performance links, including all forms of Ethernet faster than 10Mbps and all Wi-Fi links.) SpeciÔ¨Åcally, every 5-bit symbol used by 4B/5B has at most one leading 0-bit and at most two trailing 0-bits. The 5-bit symbols corresponding to the data are then sent with NRZI, where runs of 1‚Äôs are safe. Note that the worst-case run of 0-bits has length three. Note also that the signaling rate here is 1.25 times the data rate. 4B/5B is used in 100-Mbps Ethernet, 2.2 100 Mbps (Fast) Ethernet. The mapping between 4-bit data values and 5-bit symbols is Ô¨Åxed by the 4B/5B standard: data symbol data symbol 0000 11110 1011 10111 0001 01001 1100 11010 0010 10100 1101 11011 0011 10101 1110 11100 0100 01010 1111 11101 0101 01011 IDLE 11111 0110 01110 HALT 00100 0111 01111 START 10001 1000 10010 END 01101 1001 10011 RESET 00111 1010 10110 DEAD 00000 There are more than sixteen possible symbols; this allows for some symbols to be used for signaling rather than data. IDLE, HALT, START, END and RESET are shown above, though there are others. These can be used to include control and status information without fear of confusion with the data. Some combinations of control symbols do lead to up to four 0-bits in sequence; HALT and RESET have two leading 0-bits. 10-Mbps and 100-Mbps Ethernet pads short packets up to the minimum packet size with 0-bytes, meaning that the next protocol layer has to be able to distinguish between padding and actual 0-byte data. Although 100-Mbps Ethernet uses 4B/5B encoding, it does not make use of special non-data symbols for packet padding. Gigabit Ethernet uses PAM-5 encoding ( 2.3 Gigabit Ethernet ), and does use special non-data 6.1 Encoding and Framing 151
An Introduction to Computer Networks, Release 2.0.11 symbols (inserted by the hardware) to pad packets; there is thus no ambiguity at the receiving end as to where the data bytes ended. The choice of 5-bit symbols for 4B/5B is in principle arbitrary; note however that for data from 0100 to 1101 we simply insert a 1 in the fourth position, and in the last two we insert a 0 in the fourth position. The Ô¨Årst four symbols (those with the most zeroes) follow no obvious pattern, though. 6.1.5 Framing How does a receiver tell when one packet stops and the next one begins, to keep them from running together? We have already seen the following techniques for addressing this framing problem: determining where packets end: 
- Interpacket gaps (as in Ethernet) 
- 4B/5B and special bit patterns Putting a length Ô¨Åeld in the header would also work, in principle, but seems not to be widely used. One problem with this technique is that restoring order after desynchronization can be difÔ¨Åcult. There is considerable overlap of framing with encoding; for example, the existence of non-data bit patterns in 4B/5B is due to an attempt to solve the encoding problem; these special patterns can also be used as unambiguous frame delimiters. 6.1.5.1 HDLC HDLC (High-level Data Link Control) is a general link-level packet format used for a number of applications, including Point-to-Point Protocol (PPP) (which in turn is used for PPPoE ‚Äì PPP over Ethernet ‚Äì which is how a great many Internet subscribers connect to their ISP), and Frame Relay, still used as the low-level protocol for delivering IP packets to many sites via telecommunications lines. HDLC supports the following two methods for frame separation: 
- HDLC over asynchronous links: byte stufÔ¨Ång 
- HDLC over synchronous links: bit stufÔ¨Ång The basic encapsulation format for HDLC packets is to begin and end each frame with the byte 0x7E, or, in binary, 0111 1110. The problem is that this byte may occur in the data as well; we must make sure we don‚Äôt misinterpret such a data byte as the end of the frame. Asynchronous serial lines are those with some sort of start/stop indication, typically between bytes; such lines tend to be slower. Over this kind of line, HDLC uses the byte 0x7D as an escape character. Any data bytes of 0x7D and 0x7E are escaped by preceding them with an additional 0x7D. (Actually, they are transmitted as 0x7D followed by (original_byte xor 0x20).) This strategy is fundamentally the same as that used by C-programming-language character strings: the string delimiter is ‚Äúand the escape character is \. Any occurrences of ‚Äúor\within the string are escaped by preceding them with \. Over synchronous serial lines (typically faster than asynchronous), HDLC generally uses bit stufÔ¨Ång. The underlying bit encoding involves, say, the reverse of NRZI, in which transitions denote 0-bits and lack of transitions denote 1-bits. This means that long runs of 1‚Äôs are now the problem and runs of 0‚Äôs are safe. 152 6 Links
An Introduction to Computer Networks, Release 2.0.11 Whenever Ô¨Åve consecutive 1-bits appear in the data, eg011111, a 0-bit is then inserted, or ‚Äústuffed‚Äù, by the transmitting hardware (regardless of whether or not the next data bit is also a 1). The HDLC frame byte of 0x7E = 0111 1110 thus can never appear as encoded data, because it contains six 1-bits in a row. If we had 0x7E in the data, it would be transmitted as 0111 11 010. The HDLC receiver knows that 
- six 1-bits in a row marks the end of the packet 
- when Ô¨Åve 1-bits in a row are seen, followed by a 0-bit, the 0-bit is removed Example: Data: 011110 0111110 01111110 Sent as: 011110 011111 00 011111 010 (stuffed bits in bold ) Note that bit stufÔ¨Ång is used by HDLC to solve two unrelated problems: the synchronization problem where long runs of the same bit cause the receiver to lose count, and the framing problem, where the transmitted bit pattern 0111 1110 now represents a Ô¨Çag that can never be mistaken for a data byte. 6.1.5.2 B8ZS While insertion of an occasional extra bit or byte is no problem for data delivery, it is anathema to voice engineers; extra bits upset the precise 64 kbps DS-0 rate. As a result, long telecom lines prefer encodings that, like 4B/5B, do not introduce timing Ô¨Çuctuations. Very long (electrical) lines also tend to require encodings that guarantee a long-term average voltage level of 0 (versus 0.5 if half the bits are 1 v and half are 0 v in NRZ); that is, the signal must have no DC component. TheAMI (Alternate Mark Inversion) technique eliminates the DC component by using three voltage levels, nominally +1, 0 and -1; this ternary encoding is also known as bipolar. Zero bits are encoded by the 0 voltage, while 1-bits take on alternating values of +1 and -1 volts. Thus, the bits 011101 might be encoded as 0,+1,-1,+1,0,-1, or, more compactly, 0+‚Äì+0‚Äì. Over a long run, the +1‚Äôs and the ‚Äì1‚Äôs cancel out. Plain AMI still has synchronization problems with long runs of 0-bits. The solution used on North American T1 lines (1.544 Mbps) is known as B8ZS, for bipolar with 8-zero substitution. The sender replaces any run of 8 zero bits with a special bit-pattern, either 000+‚Äì0‚Äì+ or 000‚Äì+0+‚Äì. To decide which, the sender checks to see if the previous 1-bit sent was +1 or ‚Äì1; if the former, the Ô¨Årst pattern is substituted, if the latter then the second pattern is substituted. Either way, this leads to two instances of violation of the rule that consecutive 1-bits have opposite sign. For example, if the previous bit were +, the receiver sees +000+0+ B8ZS Encoding. The bits in the box were originally all zeros. Arrows link 1-bit alternating-sign violations This double-violation is the clue to the receiver that the special pattern is to be removed and replaced with the original eight 0-bits. 6.1 Encoding and Framing 153
An Introduction to Computer Networks, Release 2.0.11 6.2 Time-Division Multiplexing Classical circuit switching means a separate wire for each connection. This is still in common use for residential telephone connections: each subscriber has a dedicated wire to the Central OfÔ¨Åce. But a separate physical line for each connection is not a solution that scales well. Once upon a time it was not uncommon to link computers with serial lines, rather than packet networks. This was most often done for Ô¨Åle transfers, but telnet logins were also done this way. The problem with this approach is that the line had to be dedicated to one application (or one user) at a time. Packet switching naturally implements multiplexing (sharing) on links; the demultiplexer is the destination address. Port numbers allow demultiplexing of multiple streams to same destination host. There are other ways for multiple channels to share a single wire. One approach is frequency-division multiplexing, or putting each channel on a different carrier frequency. Analog cable television did this. Some Ô¨Åber-optic protocols also do this, calling it wavelength -division multiplexing. But perhaps the most pervasive alternative to packets is the voice telephone system‚Äôs time division multiplexing, or TDM, sometimes preÔ¨Åxed with the adjective synchronous. The idea is that we decide on a number of channels, N, and the length of a timeslice, T, and allow each sender to send over the channel for time T, with the senders taking turns in round-robin style. Each sender gets to send for time T at regular intervals of NT, thus receiving 1/N of the total bandwidth. The timeslices consume no bandwidth on headers or addresses, although sometimes there is a small amount of space dedicated to maintaining synchronization between the two endpoints. Here is a diagram of sending with N=8: A B C D E F G H A B C D E F G H A B... ... Time-Division Multiplexing Note, however, that if a sender has nothing to send, its timeslice cannot be used by another sender. Because so much data trafÔ¨Åc is bursty, involving considerable idle periods, TDM has traditionally been rejected for data networks. 6.2.1 T-Carrier Lines TDM, however, works extremely well for voice networks. It continues to work when the timeslice T is small, when packet-based approaches fail because the header overhead becomes unacceptable. Consider for a moment the telecom Digital Signal hierarchy. A single digitized voice line in North America is one 8-bit sample every 1/8,000 second, or 64 kbps; this is known as a DS0 channel. A T1line ‚Äì the lowest level of the T-carrier hierarchy and known at the logical level as a DS1 line ‚Äì represents 24 DS0 lines multiplexed via TDM, where each channel sends a single byte at a time. Thus, every 1/8,000 of a second a T1 line carries 24 bytes of user data, one byte per channel (plus one bitfor framing), for a total of 193 bits. This gives a raw line speed of 1.544 Mbps. Note that the per-channel frame size here is a single byte. There is no efÔ¨Åcient way to send single-byte packets. The advantage to the single-byte approach is that it greatly reduces the latency across the line. The biggest source of delay in packet-based digital voice lines is the packet Ô¨Åll time at the sender‚Äôs end: the sender generates voice data at a rate of 8 bytes/ms, and a packet cannot be sent until it is full. For a 1 kB 154 6 Links
An Introduction to Computer Networks, Release 2.0.11 packet, that‚Äôs about a quarter second. For standard V oice-over-IP or VoIP channels, RTP is used with 160 bytes of data sent every 20 ms; for ATM, a 48-byte packet is sent every 6 ms. But the Ô¨Åll-time delay for a call sent over a T1 line is 0.125 ms, which is negligible (to be fair, 6 ms and even 20 ms turn out to be pretty negligible in terms of call quality). The T1 one-byte-at-a-time strategy also means that T1 multiplexers need to do essentially no buffering, which might have been important back in 1962 when T-carrier was introduced. The next most common T-carrier / Digital Signal line is perhaps T3/DS3; this represents the TDM multiplexing of 28 DS1 signals. The problem is that some individual DS1s may run a little slow, so an elaborate pulse stufÔ¨Ång protocol has been developed. This allows extra bits to be inserted at speciÔ¨Åc points, if necessary, in such a way that the original component T1s can be exactly recovered even if there are clock irregularities. The pulse-stufÔ¨Ång solution did not scale well, and so T-carrier levels past T3 were very rarely used. While T-carrier was originally intended as a way of bundling together multiple DS0 channels on a single high-speed line, it also allows providers to offer leased digital point-to-point links with data rates in almost any multiple of the DS0 rate. 6.2.2 SONET SONET stands for Synchronous Optical NETwork; it is the telecommunications industry‚Äôs standard mechanism for very-high-speed TDM over optical Ô¨Åber. While there is now Ô¨Çexibility regarding the the ‚Äúoptical‚Äù part, the ‚Äúsynchronous‚Äù part is taken quite seriously indeed, and SONET senders and receivers all use very precisely synchronized clocks (often atomic). The actual bit encoding is NRZI. Due to the frame structure, below, the longest possible run of 0-bits is ~250 bits (~30 bytes), but is usually much less. Accurate reception of 250 0-bits requires a clock accurate to within (at a minimum) one part in 500, which is potentially within reach. However, SONET also has a ‚Äúbit-scrambling‚Äù feature, involving XOR with a Ô¨Åxed bit pattern, to ensure in most cases that there is a 1-bit every byte or less. The primary reason for SONET‚Äôs accurate clocking, however, is not the clock-synchronization problem as we have been using the term, but rather the problem of demultiplexing and remultiplexing multiple component bitstreams in a setting in which some of the streams may run slow. One of the primary design goals for SONET was to allow such multiplexing without the need for ‚Äúpulse stufÔ¨Ång‚Äù, as is used in the Digital Signal hierarchy. SONET tributary streams are in effect not allowed to run slow (although SONET does provide for occasional very small byte slips, below). Furthermore, as multiple SONET streams are demultiplexed at a switching center and then remultiplexed into new SONET streams, synchronization means that none of the streams falls behind or gets ahead. The basic SONET format is known as STS-1. Data is organized as a 9x90 byte grid. The Ô¨Årst 3 bytes of each row (that is, the Ô¨Årst three columns) form the frame header. Frames are not addressed; SONET is a point-to-point protocol and a node sends a continuous sequence of frames to each of its neighbors. When the frames reach their destination, in principle they need to be fully demultiplexed for the data to be forwarded on. In practice, there are some shortcuts to full demultiplexing. 6.2 Time-Division Multiplexing 155
An Introduction to Computer Networks, Release 2.0.11 The actual bytes sent are scrambled: the data is XORed with a standard, Ô¨Åxed pseudorandom pattern before transmission. This introduces many 1-bits, on which clock resynchronization can occur, with a high degree of probability. There are two other special columns in a frame, each guaranteed to contain at least one 1-bit, so the maximum run of data bytes is limited to ~30; this is thus the longest run of possible 0‚Äôs. The Ô¨Årst two bytes of each frame are 0xF628. SONET‚Äôs frame-synchronization check is based on verifying these byte values at the start of each frame. If the receiver is ever desynchronized, it begins a frame resynchronization procedure: the receiver searches for those 0xF628 bytes at regular 810-byte (6480-bit) spacing. After a few frames with 0xF628 in the right place, the receiver is ‚Äúvery sure‚Äù it is looking at the synchronization bytes and not at a data-byte position. Note that there is no evident byte boundary to a SONET frame, so the receiver must check for 0xF628 beginning at every bitposition. SONET frames are transmitted at a rate of 8,000 frames/second. This is the canonical byte sampling rate for standard voice-grade (‚ÄúDS0‚Äù, or 64 kbps) lines. Indeed, the classic application of SONET is to transmit multiple DS0 voice calls using TDM: within a frame, each data byte position is given over to one voice channel. The same byte position in consecutive frames constitutes one byte every 1/8000 seconds. The basic STS-1 data rate of 51.84 Mbps is exactly 810 bytes/frame 8 bits/byte8000 frames/sec. To a customer who has leased a SONET-based channel to transmit data, a SONET link looks like a very fast bitstream. There are several standard ways of encoding data packets over SONET. One is to encapsulate the data as ATM cells, and then embed the cells contiguously in the bitstream. Another is to send IP packets encoded in the bitstream using HDLC-like bit stufÔ¨Ång, which means that the SONET bytes and the IP bytes may no longer correspond. The advantage of HDLC encoding is that it makes SONET re-synchronization vanishingly infrequent. Most IP backbone trafÔ¨Åc today travels over SONET links. Within the 990-byte STS-1 frame, the payload envelope is the 9 87 region nominally following the three header columns; this payload region has its own three reserved columns meaning that there are 84 columns (984 bytes) available for data. This 9 87-byte payload envelope can ‚ÄúÔ¨Çoat‚Äù within the physical 9 90byte frame; that is, if the input frames are running slow then the output physical frames can be transmitted at the correct rate by letting the payload frames slip ‚Äúbackwards‚Äù, one byte at a time. Similarly, if the input frames are arriving slightly too fast, they can slip ‚Äúforwards‚Äù by up to one byte at a time; the extra byte is stored in a reserved location in the three header columns of the 9 90 physical frame. Faster SONET streams are made by multiplexing slower ones. The next step up is STS-3, an STS-3 frame is three STS-1 frames, for 9 270 bytes. STS-3 (or, more properly, the physical layer for STS-3) is also called OC-3, for Optical Carrier. Beyond STS-3, faster lines are multiplexed combinations of four of the 156 6 Links
An Introduction to Computer Networks, Release 2.0.11 next-slowest lines. Here are some of the higher levels: STS STM line rate STS-1 STM-0 51.84 Mbps STS-3 STM-1 155.52 Mbps STS-12 STM-4 622.08 Mbps (=12*51.84, exactly) STS-48 STM-16 2.48832 Gbps STS-192 STM-64 9.953 Gbps STS-768 STM-256 39.8 Gbps Usable capacity is typically 84/90 of the above line rates, as six of the 90 columns of an STS-1 frame are for overhead. SONET provides a wide variety of leasing options at various bandwidths. High-volume customers can lease an entire STS-1 or larger unit. Alternatively, the 84 data columns of an STS-1 frame can be divided into seven virtual tributary groups, each of twelve columns; these groups can be leased individually or in multiples, or be further divided into as few as three columns (which works out to be just over the T1 data rate). 6.2.3 Optical Transport Network The Optical Transport Network, or OTN, is an ITU speciÔ¨Åcation for data transmission over optical Ô¨Åber; the primary standard is G.709. A preliminary version of G.709 was published in 1988, but version 1.0 was released in 2001. OTN abandons SONET‚Äôs voice-oriented frame rate of 8000 frames/sec; while OTN is still widely used for voice, transmission no longer quite so perfectly Ô¨Åts the time-division-multiplexing model. The standard OTN frame is as diagrammed below; each frame is arranged in four rows. It can be helpful to view the 4,080 columns as divided into 255 16-byte-wide ‚Äúsupercolumns‚Äù: one for the frame and payload overhead, 238 for the payload itself, and 16 for error correction. 1 1416 3824 4080 = 16√ó255 256 bytes 3824 = 16√ó239 bytes Payload: 3808 = 16√ó238 bytes Payload Overhead OTN Frame Format: 4080 columns √ó 4 rowsError Correction Frame Overhead The portion available for carrying customer data is highlighted in blue; unlike SONET, the payload portion of a frame is not allowed to ‚ÄúÔ¨Çoat‚Äù. The 256 columns at the end are for error-correcting codes ( 7.4.2 ErrorCorrecting Codes ); the addition of such error correction (below) is a major advantage of OTN over SONET. OTN comes in three primary rates, OTN1, OTN2 and OTN3; there are also a few variants. The OTN1 rate is chosen so that a SONET STS-48/STM-16 stream ‚Äì 2.48832 Gbps ‚Äì exactly Ô¨Åts in the blue payload portion 6.2 Time-Division Multiplexing 157
An Introduction to Computer Networks, Release 2.0.11 of the frame. This means that the OTN1 line rate must be (255/238) 2.488322.6661 Gbps. The frame rate, in turn, is therefore about 20.42 frames/ms. Four OTN1 streams can be multiplexed into a single OTN2 stream; four OTN2 streams can be multiplexed into a single OTN3 stream. The frame layout does not change, however; it is the frame rate that increases. The data rate does not increase by an exact multiple of 4; the OTN2 rate is chosen so that four OTN1 payloads and an additional 16 columns per frame can be carried in the payload portion of the OTN2 frames. The additional 16 columns serve to specify details about the interleaving of the four OTN1 frames; this leaves 3792 = 16 237 columns for the OTN1-stream data. The end result is that the OTN2 stream must carry data at a line rate of 4 238/237 times the OTN1 rate, or 4 255/237 times the STS-48/STM-16 rate. This works out to be about 10.709 Gbps. In order to handle multiplexing without the need for T-carrier-style pulse stufÔ¨Ång, OTN streams must have a long-term bit-rate accuracy of 20 parts per million, which works out to be one byte every 3-4 frames. The frame-overhead region takes care of the slack. The last 256 columns of each frame are devoted to error correction; SONET has nothing comparable. By default, these columns are used for so-called Reed-Solomon codes; speciÔ¨Åcally, in a formulation where 16 bytes of codes are used for each 239 bytes of payload (this is exactly consistent with the relative sizes of the payload and error-correction columns). Such codes can correct up to 8 bytes of error. They can be used to increase the length of cable runs before regeneration is needed. Alternatively, their use can reduce the bit error rate as much as a hundredfold, which will be important in 21.6 The High-Bandwidth TCP Problem. OTN also includes, at the physical layer, extensive support for dense wavelength-division multiplexing (DWDM, a form of frequency-division multiplexing), meaning that multiple independent OTN streams can be carried on relatively close wavelengths. This greatly increases the overall capacity of a given run of Ô¨Åber. DWDM works at a lower network layer than the frame organization outlined above, and in principle SONET could implement DWDM as well. In practice, though, DWDM has been integrated with OTN from the beginning. 6.2.4 Other Optical Fiber Optical Fiber and Lightning One of the advantages of of optical Ô¨Åber (particularly at mountaintop observatories) is its resistance to damage and interference from lightning. Some Ô¨Åber-optic cables do, however, have metal jackets to add strength or to resist animals; lightning resistance must be researched carefully. SONET and OTN are primarily, though not exclusively, used by telecommunications carriers. There are also multiple Ô¨Åber-optic alternatives for smaller-scale operations. These are often part of the Ethernet family although they may have little except their bitrate in common with one another or with Ethernet over copper wire. Another standard that supports optical Ô¨Åber links is so-called ‚ÄúFibre Channel‚Äù, though that too also now supports copper. Distances supported by Ô¨Åber-optic Ethernet range from hundreds of meters to tens of kilometers. Generally, the longer-haul links require the use of full duplex to avoid the collision-detection (slot time) requirement that a sender continue to transmit for one full Ethernet-segment RTT, but full duplex is common at high Ethernet speeds even for short links. For 100 Mbps Ethernet, Ô¨Åber-optic standards include 100BASE-FX, 158 6 Links
An Introduction to Computer Networks, Release 2.0.11 100BASE-SX, 100BASE-LX and 100BASE-BX. The latter supports distances up to 40 km; the limiting factor is the need for signal regeneration. 1000-Gbit Ethernet optical standards include 1000BASE-SX, 1000BASE-LX, 1000BASE-BX and 1000BASE-EX; the latter again supports up to 40 km. These forms of Ethernet are often deployed in residential ‚ÄúÔ¨Åber Internet‚Äù connections, although in some cases the last hundred meters or so may still involve copper. At speeds of 10 Gbps, long-range optical Ô¨Åber alternatives include 10GBASE-ER and 10GBASE-ZR, supporting 40 and 80 km respectively; 10GBASE-ZR is based on SONET STM-64 standards. It is worth noting that Ô¨Åber is often used for short links as well at 1 Gbps and 10 Gbps speeds; at 10 Gbps some copper-link standards support distances of only a few meters. As of 2014, several 10-Gbps Ethernet physical-layer standards come from industry consortiums rather than the IEEE. 6.3 Epilog This completes our discussion of common physical links. Perhaps the main takeaway point is that transmitting bits over any distance is not quite as simple as it may appear; simple NRZ transmission is noteffective. 6.4 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of future exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.6 Solutions for Links. 1.0. What is encoded by the following NRZI signal? The Ô¨Årst two bits are shown. 0 1 2.0. Argue that sending 4 0-bits via NRZI requires a clock accurate to within 1 part in 8. Assume that the receiver resynchronizes its clock whenever a 1-bit transition is received, but that otherwise it attempts to sample a bit in the middle of the bit‚Äôs timeslot. 3.0.(a) What bits are encoded by the following Manchester-encoded sequence? (b). Why is there no ambiguity as to whether the Ô¨Årst transition is a clock transition or a data (1-bit) transition? (c). Give an example of a signal pattern consisting of an NRZI encoding of 0-bits and 1-bits that does not contain two consecutive 0-bits and which is nota valid Manchester encoding of data. Such a pattern could thus could be used as a special non-data marker. 6.3 Epilog 159
An Introduction to Computer Networks, Release 2.0.11 4.0.‚ô¢What is the 4B/5B encoding for the 3-byte string ‚ÄúNet‚Äù? (Hint: ‚ÄòN‚Äô is 0100 1110.) 5.0. What three ASCII letters (bytes) are encoded by the following 4B/5B pattern? (Be careful about uppercase vs lower-case.) 010110101001110101010111111110 6.0.(a) Suppose a device is forwarding SONET STS-1 frames. How much clock drift, as a percentage, on the incoming line would mean that the output payload envelopes must slip backwards by one byte per three physical frames? | (b). In 6.2.2 SONET it was claimed that sending 250 0-bits required a clock accurate to within 1 part in 500. Describe how a SONET clock might meet the requirement of part (a) above, and yet fail at this second requirement. (Hint: in part (a) the requirement is a long-term average). 160 6 Links
7 PACKETS In this chapter we address a few abstract questions about packets, and take a close look at transmission times. We also consider how big packets should be, and how to detect transmission errors. These issues are independent of any particular set of protocols. 7.1 Packet Delay There are several contributing sources to the delay encountered in transmitting a packet. On a LAN, the most signiÔ¨Åcant is usually what we will call bandwidth delay: the time needed for a sender to get the packet onto the wire. This is simply the packet size divided by the bandwidth, after everything has been converted to common units (either all bits or all bytes). For a 1500-byte packet on 100 Mbps Ethernet, the bandwidth delay is 12,000 bits / (100 bits/¬µsec) = 120 ¬µsec. There is also propagation delay, relating to the propagation of the bits at the speed of light (for the transmission medium in question). This delay is the distance divided by the speed of light; for 1,000 m of Ethernet cable, with a signal propagation speed of about 230 m/¬µsec, the propagation delay is about 4.3 ¬µsec. That is, if we start transmitting the 1500 byte packet of the previous paragraph at time T=0, then the Ô¨Årst bit arrives at a destination 1,000 m away at T = 4.3 ¬µsec, and the last bit is transmitted at 120 ¬µsec, and the last bit arrives at T = 124.3 ¬µsec. Minimizing Delay Back in the last century, gamers were sometimes known to take advantage of players with slow (as in dialup) links; an opponent could be eliminated literally before he or she could respond. As an updated take on this, some Ô¨Ånancial-trading Ô¨Årms have set up microwave-relay links between trading centers, say New York and Chicago, in order to reduce delay. In computerized trading, milliseconds count. A direct line of sight from New York to Chicago ‚Äì which we round off to 1200 km ‚Äì takes about 4 ms in air, where signals propagate at essentially the speed of light c = 300 km/ms. But Ô¨Åber is slower; even an absolutely straight run would take 6 ms at glass Ô¨Åber‚Äôs propagation speed of 200 km/ms. In the presence of high-speed trading, this 2 ms savings is of considerable Ô¨Ånancial signiÔ¨Åcance. Bandwidth delay, in other words, tends to dominate within a LAN. But as networks get larger, propagation delay begins to dominate. This also happens as networks get faster: bandwidth delay goes down, but propagation delay remains unchanged. An important difference between bandwidth delay and propagation delay is that bandwidth delay is proportional to the amount of data sent while propagation delay is not. If we send two packets back-to-back, then the bandwidth delay is doubled but the propagation delay counts only once. The introduction of switches leads to store-and-forward delay, that is, the time spent reading in the entire packet before any of it can be retransmitted. Store-and-forward delay can also be viewed as an additional bandwidth delay for the second link. 161
An Introduction to Computer Networks, Release 2.0.11 Finally, a switch may or may not also introduce queuing delay; this will often depend on competing trafÔ¨Åc. We will look at this in more detail in 20 Dynamics of TCP, but for now note that a steady queuing delay ( eg due to a more-or-less constant average queue utilization) looks to each sender more like propagation delay than bandwidth delay, in that if two packets are sent back-to-back and arrive that way at the queue, then the pair will experience only a single queuing delay. 7.1.1 Delay examples Case 1: A B 
- Propagation delay is 40 ¬µsec 
- Bandwidth is 1 byte/¬µsec (1 MB/sec, 8 Mbit/sec) 
- Packet size is 200 bytes (200 ¬µsec bandwidth delay) Then the total one-way transmit time for one packet is 240 ¬µsec = 200 ¬µsec + 40 ¬µsec. To send two backto-back packets, the time rises to 440 ¬µsec: we add one more bandwidth delay, but not another propagation delay. Case 2: A B Like the previous example except that the propagation delay is increased to 4 ms The total transmit time for one packet is now 4200 ¬µsec = 200 ¬µsec + 4000 ¬µsec. For two packets it is 4400 ¬µsec. Case 3: A R B We now have two links, each with propagation delay 40 ¬µsec; bandwidth and packet size as in Case 1. The total transmit time for one 200-byte packet is now 480 ¬µsec = 240 + 240. There are two propagation delays of 40 ¬µsec each; A introduces a bandwidth delay of 200 ¬µsec and R introduces a store-and-forward delay (or second bandwidth delay) of 200 ¬µsec. Case 4: A R B The same as 3, but with data sent as two 100-byte packets The total transmit time is now 380 ¬µsec = 3x100 + 2x40. There are still two propagation delays, but there is only 3/4 as much bandwidth delay because the transmission of the Ô¨Årst 100 bytes on the second link overlaps with the transmission of the second 100 bytes on the Ô¨Årst link. 162 7 Packets
An Introduction to Computer Networks, Release 2.0.11 T=40 T=240T=40 T=240 T=280 T=480 Case 1: one link Case 3: two links Bandwidth delay 200 ¬µsec, per-link propagation delay 40 ¬µsecT=40 T=240 T=280 T=380 Case 4: two half-sized packetsT=140 T=180 These ladder diagrams represent the full transmission; a snapshot state of the transmission at any one instant can be obtained by drawing a horizontal line. In the middle, case 3, diagram, for example, at no instant are both links active. Note that sending two smaller packets is faster than one large packet. We expand on this important point below. Now let us consider the situation when the propagation delay is the most signiÔ¨Åcant component. The crosscontinental US roundtrip delay is typically around 50-100 ms (propagation speed 200 km/ms in cable, 5,00010,000 km cable route, or about 3-6000 miles); we will use 100 ms in the examples here. At a bandwidth of 1.0 Mbps, 100ms is about 12 kB, or eight full-sized Ethernet packets. At this bandwidth, we would have four packets and four returning ACKs strung out along the path. At 1.0 Gbit/s, in 100ms we can send 12,000 kB, or 800 Ethernet packets, before the Ô¨Årst ACK returns. At most non-LAN scales, the delay is typically simpliÔ¨Åed to the round-trip time, orRTT: the time between sending a packet and receiving a response. Different delay scenarios have implications for protocols: if a network is bandwidth-limited then protocols are easier to design. Extra RTTs do not cost much, so we can build in a considerable amount of back-andforth exchange. However, if a network is delay-limited, the protocol designer must focus on minimizing extra RTTs. As an extreme case, consider wireless transmission to the moon (0.3 sec RTT), or to Jupiter (1 hour RTT). At my home I formerly had satellite Internet service, which had a roundtrip propagation delay of ~600 ms. This is remarkably high when compared to purely terrestrial links. When dealing with reasonably high-bandwidth ‚Äúlarge-scale‚Äù networks ( egthe Internet), propagation delay is usually much larger than bandwidth delay, and so to a Ô¨Årst approximation bandwidth delay can be ignored. This means the bandwidth and the total delay are effectively independent. Only when propagation delay is small are the two interrelated. Because propagation delay dominates at this scale, we can often make simpliÔ¨Åcations when diagramming. In the illustration below, A sends a data packet to B and receives a small ACK in return. In (a), we show the data packet traversing several switches; in (b) we show the data packet as if it were sent along one long unswitched link, and in (c) we introduce the idealization that bandwidth 7.1 Packet Delay 163
An Introduction to Computer Networks, Release 2.0.11 delay (and thus the width of the packet line) no longer matters. (Most later ladder diagrams in this book are of this type.) 7.1.2 Bandwidth Delay The bandwidthdelay product (usually involving round-trip delay, or RTT), represents how much we can send before we hear anything back, or how much is ‚Äúpending‚Äù in the network at any one time if we send continuously. Note that, if we use RTT instead of one-way time, then half the ‚Äúpending‚Äù packets will be returning ACKs. Here are a few approximate values, where 100 ms can be taken as a typical intercontinental-distance RTT: RTT bandwidth bandwidthdelay 1 ms 10 Mbps 1.2 kB 100 ms 1.5 Mbps 20 kB 100 ms 600 Mbps 8 MB 100 ms 1.5 Gbps 20 MB 7.2 Packet Delay Variability For many links, the bandwidth delay and the propagation delay are rigidly Ô¨Åxed quantities, the former by the bandwidth and the latter by the speed of light. This leaves queuing delay as the major source of variability. This state of affairs lets us deÔ¨Åne RTT noLoad to be the time it takes to transmit a packet from A to B, and receive an acknowledgment back, with no queuing delay. While this is often a reasonable approximation, it is not necessarily true that RTT noLoad is always a Ô¨Åxed quantity. There are several possible causes for RTT variability. On Ethernet and Wi-Fi networks there is an initial ‚Äúcontention period‚Äù before transmission actually begins. Although this delay is related to waiting for other senders, it is not exactly queuing delay, and a packet may encounter considerable delay here even if it 164 7 Packets
An Introduction to Computer Networks, Release 2.0.11 ends up being the Ô¨Årst to be sent. For Wi-Fi in particular, the uncertainty introduced by collisions into packet delivery times ‚Äì even with no other senders competing ‚Äì can complicate higher-level delay measurements. It is also possible that different packets are routed via slightly different paths, leading to (hopefully) minor variations in travel time, or are handled differently by different queues of a parallel-processing switch. A link‚Äôs bandwidth, too, can vary dynamically. Imagine, for example, a T1 link comprised of the usual 24 DS0 channels, in which all channels not currently in use by voice calls are consolidated into a single data channel. With eight callers, the data bandwidth would be cut by a third from 24 DS0 to 16DS0. Alternatively, perhaps routers are allowed to reserve a varying amount of bandwidth for high-priority trafÔ¨Åc, depending on demand, and so the bandwidth allocated to the best-effort trafÔ¨Åc can vary. Perceived link bandwidth can also vary over time if packets are compressed at the link layer, and some packets are able to be compressed more than others. Finally, if mobile nodes are involved, then the distance and thus the propagation delay can change. This can be quite signiÔ¨Åcant if one is communicating with a wireless device that is being taken on a cross-continental road trip. Despite these sources of Ô¨Çuctuation, we will usually assume that RTT noLoad is Ô¨Åxed and well-deÔ¨Åned, especially when we wish to focus on the queuing component of delay. 7.3 Packet Size How big should packets be? Should they be large ( eg64 kB) or small ( eg48 bytes)? The Ethernet answer to this question had to do with equitable sharing of the line: large packets would not allow other senders timely access to transmit. In any network, this issue remains a concern. On the other hand, large packets waste a smaller percentage of bandwidth on headers. However, in most of the cases we will consider, this percentage does not exceed 10% (the V oIP/RTP example in 1.3 Packets is an exception). It turns out that if store-and-forward switches are involved, smaller packets have much better throughput. The links on either side of the switch can be in use simultaneously, as in Case 4 of 7.1.1 Delay examples. This is a very real effect, and has put a damper on interest in support for IP ‚Äújumbograms‚Äù. The ATM protocol (intended for both voice and data) pushes this to an extreme, with packets with only 48 bytes of data and 5 bytes of header. As an example of this, consider a path from A to B with four switches and Ô¨Åve links: A R1 R2 R3 R4 B Suppose we send either one big packet or Ô¨Åve smaller packets. The relative times from A to B are illustrated in the following Ô¨Ågure: 7.3 Packet Size 165
An Introduction to Computer Networks, Release 2.0.11 R2 R4 B R3 R4 R2 R1 A B R3 R1 A One large packet over five links Five smaller packets over five links The point is that we can take advantage of parallelism: while the R4‚ÄìB link above is handling packet 1, the R3‚ÄìR4 link is handling packet 2 and the R2‚ÄìR3 link is handling packet 3 and so on. The Ô¨Åve smaller packets would have Ô¨Åve times the header capacity, but as long as headers are small relative to the data, this is not a signiÔ¨Åcant issue. The sliding-windows algorithm, used by TCP, uses this idea as a continuous process: the sender sends a continual stream of packets which travel link-by-link so that, in the full-capacity case, all links may be in use at all times. 7.3.1 Error Rates and Packet Size Packet size is also inÔ¨Çuenced, to a modest degree, by the transmission error rate. For relatively high error rates, it turns out to be better to send smaller packets, because when an error does occur then the entire packet containing it is lost. Small error rates Generally, if the bit error rate p is small, we can approximate the probability of error in an N-bit packet as pN, rather than working out the exact answer (assuming bit-error independence) of 1 ‚Äì (1‚Äìp)N. This approximation works best if p N is also small. For the 1000-bit example here with p=1/10,000, the exact value of the success rate is 90.4833% versus the p N approximation of 90%. For the 10,000-bit packet, though, the pN approximation predicts a 100% chance of error, which is not very helpful at all. For example, suppose that 1 bit in 10,000 is corrupted, at random, so the probability that a single bit is 166 7 Packets
An Introduction to Computer Networks, Release 2.0.11 transmitted correctly is 0.9999 (this is much higher than the error rates encountered on real networks). For a 1000-bit packet, the probability that every bit in the packet is transmitted correctly is (0.9999)1000, or about 90.5%. For a 10,000-bit packet the probability is (0.9999)10,00037%. For 20,000-bit packets, the success rate is below 14%. Now suppose we have 1,000,000 bits to send, either as 1000-bit packets or as 20,000-bit packets. Nominally this would require 1,000 of the smaller packets, but because of the 90% packet-success rate we will need to retransmit 10% of these, or 100 packets. Some of the retransmissions may also be lost; the total number of packets we expect to need to send is about 1,000/90% 1,111, for a total of 1,111,000 bits sent. Next, let us try this with the 20,000-bit packets. Here the success rate is so poor that each packet needs to be sent on average seven times; lossless transmission would require 50 packets but we in fact need 7 50 = 350 packets, or 7,000,000 bits. Moral: choose the packet size small enough that most packets do not encounter errors. To be fair, very large packets can be sent reliably on most cable links ( egTDM and SONET). Wireless, however, is more of a problem. 7.3.2 Packet Size and Real-Time TrafÔ¨Åc There is one other concern regarding excessive packet size. As we shall see in 25 Quality of Service, it is common to commingle bulk trafÔ¨Åc on the same links with real-time trafÔ¨Åc. It is straightforward to give priority to the real-time trafÔ¨Åc in such a mix, meaning that a router does not begin forwarding a bulk-trafÔ¨Åc packet if there are any real-time packets waiting (we do need to be sure in this case that real-time trafÔ¨Åc will not amount to so much as to starve the bulk trafÔ¨Åc). However, once a bulk-trafÔ¨Åc packet has begun transmission, it is impractical to interrupt it. Therefore, one component of any maximum-delay bound for real-time trafÔ¨Åc is the transmission time for the largest bulk-trafÔ¨Åc packet; we will call this the largest-packet delay. As a practical matter, most IPv4 packets are limited to the maximum Ethernet packet size of 1500 bytes, but IPv6 has an option for so-called ‚Äújumbograms‚Äù up to 2 MB in size. Transmitting one such packet on a 100 Mbps link takes about 1/6 of a second, which is likely too large for happy coexistence with real-time trafÔ¨Åc. 7.4 Error Detection The basic strategy for packet error detection is to add some extra bits ‚Äì formally known as an error-detection code ‚Äì that will allow the receiver to determine if the packet has been corrupted in transit. A corrupted packet will then be discarded by the receiver; higher layers do not distinguish between lost packets and those never received. While packets lost due to bit errors occur much less frequently than packets lost due to queue overÔ¨Çows, it is essential that data be received accurately. Intermittent packet errors generally fall into two categories: low-frequency bit errors due to things like cosmic rays, and interference errors, typically generated by nearby electrical equipment. Errors of the latter type generally occur in bursts, with multiple bad bits per packet. Occasionally, a malfunctioning network device will introduce bursty errors as well. 7.4 Error Detection 167
An Introduction to Computer Networks, Release 2.0.11 Networks v Refrigerators At Loyola we once had a workstation used as a mainframe terminal that kept losing its connection. We eventually noticed that the connection dropped every time the ofÔ¨Åce refrigerator kicked on. Sure enough, the cable ran directly behind the fridge; rerouting it solved the problem. The simplest error-detection mechanism is a single parity bit; this will catch all one-bit errors. There is, however, no straightforward generalization to N bits! That is, there is no N-bit error code that catches all N-bit errors; see exercise 11.0. The so-called Internet checksum, used by IP, TCP and UDP, is formed by taking the ones-complement sum of the 16-bit words of the message. Ones-complement is an alternative way of representing signed integers in binary; if one adds two positive integers and the sum does not overÔ¨Çow the hardware word size, then ones-complement and the now-universal twos-complement are identical. To form the ones-complement sum of 16-bit words A and B, Ô¨Årst take the ordinary twos-complement sum A+B. Then, if there is an overÔ¨Çow bit, add it back in as low-order bit. Thus, if the word size is 4 bits, the ones-complement sum of 0101 and 0011 is 1000 (no overÔ¨Çow). Now suppose we want the ones-complement sum of 0101 and 1100. First we take the ‚Äúexact‚Äù sum and get 1|0001, where the leftmost 1 is an overÔ¨Çow bit past the 4-bit wordsize. Because of this overÔ¨Çow, we add this bit back in, and get 0010. The 4-bit ones-complement numeric representation has two forms for zero: 0000 and 1111 (it is straightforward to verify that any 4-bit quantity plus 1111 yields the original quantity; in twos-complement notation 1111 represents -1, and an overÔ¨Çow is guaranteed, so adding back the overÔ¨Çow bit cancels the -1 and leaves us with the original number). It is a fact that the ones-complement sum is never 0000 unless all bits of all the summands are 0; if the summands add up to zero by coincidence, then the actual binary representation will be 1111. This means that we can use 0000 in the checksum to represent ‚Äúchecksum not calculated‚Äù, which the UDP protocol still allows over IPv4 for efÔ¨Åciency reasons. Over IPv6, UDP packets must include a calculated checksum ( RFC 2460, ¬ß8.1). What is stored in the packet‚Äôs checksum Ô¨Åeld is the complement of the sum above, or 0xFFFF ‚Äì sum. This means that when the receiver calculates the checksum over the same range of bytes, the value obtained should be zero. Ones‚Äô complement Long ago, before Loyola had any Internet connectivity, I wrote a primitive UDP/IP stack to allow me to use the Ethernet to back up one machine that did not have TCP/IP to another machine that did. We used ‚Äúprivate‚Äù IP addresses of the form 10.0.0.x. I set as many header Ô¨Åelds to zero as I could. I paid no attention to how to implement ones-complement addition; I simply used twos-complement, for the IP header only, and did not use a UDP checksum at all. Hey, it worked. Then we got a real Class B address block 147.126.0.0/16, and changed IP addresses. My software no longer worked! It turned out that, in the original version, the IP header bytes were all small enough that when I added up the 16-bit words there were no carries, and so ones-complement was the same as twoscomplement. With the new addresses, this was no longer true. As soon as I Ô¨Ågured out how to implement ones-complement addition properly, my backups worked again. Ones-complement addition has a few properties that make numerical calculations simpler. First, when 168 7 Packets
An Introduction to Computer Networks, Release 2.0.11 Ô¨Ånding the ones-complement sum of a series of 16-bit values, we can defer adding in the overÔ¨Çow bits until the end. SpeciÔ¨Åcally, we can Ô¨Ånd the ones-complement sum of the values by adding them using ordinary (twos-complement) 32-bit addition, and then forming the ones-complement sum of the upper and lower 16-bit half-words. The upper half-word here represents the accumulated overÔ¨Çow. See exercise 10.0. We can also Ô¨Ånd the ones-complement sum of a series of 16-bit values by concatenating them pairwise into 32-bit values, taking the 32-bit ones-complement sum of these, and then, as in the previous paragraph, forming the ones-complement sum of the upper and lower 16-bit half-words. Somewhat surprisingly, when calculating the 16-bit ones-complement sum of a series of bytes taken two at a time, it does not matter whether we convert the pairs of consecutive bytes to integers using big-endian or little-endian byte order ( 16.1.5 Binary Data ). The overÔ¨Çow from the low-order bytes is added to the highorder bytes by virtue of ordinary carries in addition, and the overÔ¨Çow from the high-order bytes is added to the low-order bytes by the ones-complement rule. See exercise 10.5. Finally, there is another way to look at the (16-bit) ones-complement sum: it is in fact the remainder upon dividing the message (seen as a very long binary number) by 2161, provided we replace a remainder of 0 with the equivalent ones-complement zero value consisting of sixteen 1-bits. This is similar to the decimal ‚Äúcasting out nines‚Äù rule: if we add up the digits of a base-10 number, and repeat the process until we get a single digit, then that digit is the remainder upon dividing the original number by 10-1 = 9. The analogy here is that the message is looked at as a very large number written in base-216, where the ‚Äúdigits‚Äù are the 16-bit words. The process of repeatedly adding up the ‚Äúdigits‚Äù until we get a single ‚Äúdigit‚Äù amounts to taking the ones-complement sum of the words. This remainder approach to ones-complement addition isn‚Äôt very practical, but it does provide a useful way to analyze ones-complement checksums mathematically. A weakness of any error-detecting code based on sums is that transposing words leads to the same sum, and the error is not detected. In particular, if a message is fragmented and the fragments are reassembled in the wrong order, the ones-complement sum will likely not detect it. While some error-detecting codes are better than others at detecting certain kinds of systematic errors (for example, CRC, below, is usually better than the Internet checksum at detecting transposition errors), ultimately the effectiveness of an error-detecting code depends on its length. Suppose a packet P1 is corrupted randomly into P2, but still has its original N-bit error code EC(P1). This N-bit code will failto detect the error that has occurred if EC(P2) is, by chance, equal to EC(P1). The probability that two random N-bit codes will match is 1/2N(though a small random change in P1 might not lead to a uniformly distributed random change in EC(P1); see the tail end of the CRC section below). This does not mean, however, that one packet in 2Nwill be received incorrectly, as most packets are errorfree. If we use a 16-bit error code, and only 1 packet in 100,000 is actually corrupted, then the rate at which corrupted packets will sneak by is only 1 in 100,000 65536, or about one in 6 109. If packets are 1500 bytes, you have a good chance (90+%) of accurately transferring a terabyte, and a 37% chance (1/e) at ten terabytes. 7.4.1 Cyclical Redundancy Check: CRC The CRC error code is based on long division of polynomials, where the coefÔ¨Åcients are integers modulo 2. The use of polynomials tends to sound complicated but in fact it eliminates the need for carries or borrowing in addition and subtraction. Together with the use of modulo-2 coefÔ¨Åcients, this means that addition and subtraction become equivalent to XOR. We treat the message, in binary, as a giant polynomial m(X), using the bits of the message as successive coefÔ¨Åcients ( eg10011011 = X7+ X4+ X3+ X + 1). We standardize 7.4 Error Detection 169
An Introduction to Computer Networks, Release 2.0.11 a divisor polynomial p(X) of degree N (N=32 for CRC32codes); the full speciÔ¨Åcation of a given CRC code requires giving this polynomial. (A full speciÔ¨Åcation also requires spelling out the bit order within bytes.) We append N 0-bits to m(X) (this is the polynomial XNm(X)), and divide the result by p(X). The ‚Äúchecksum‚Äù is the remainder r(X), of maximum degree N‚Äì1 (that is, N bits). This is a reasonably secure hash against real-world network corruption, in that it is very hard for systematic errors to result in the same hash code. However, CRC is not secure against intentional corruption; given an arbitrary message msg, there are straightforward algebraic means for tweaking the last bytes of msg so that the CRC code of the result is equal to any predetermined value in the appropriate range. As an example of CRC, suppose that the CRC divisor is 1011 (making this a CRC-3 code) and the message is 1001 1011 1100. Here is the division; we repeatedly subtract (using XOR) a copy of the divisor 1011, shifted so the leading 1 of the divisor lines up with the leading 1 of the previous difference. A 1 then goes on the quotient line, lined up with the last digit of the shifted divisor; otherwise a 0. There are several online calculators for this sort of thing, eghere. Note that an extra 000 has been appended to the dividend. 1 0100 1101 011 10111001 1011 1100 000 1011 010 1011 1100 000 10 11 00 0111 1100 000 101 1 010 0100 000 10 11 00 1000 000 1011 0011 000 10 11 01 110 1 011 0 101 The remainder, at the bottom, is 101; this is the N-bit CRC code. We then append the code to the original message, that is, without the added zeroes: 1001 1011 1100 101; algebraically this is XNm(X) + r(X). This is what is actually transmitted; if converted to a polynomial, it yields a remainder of zero upon division by p(X). This slightly simpliÔ¨Åes the receiver‚Äôs job of validating the CRC code: it just has to check that the remainder is zero. CRC is easily implemented in hardware, using bit-shifting registers. Fast software implementations are also possible, usually involving handling the bits one byte at a time, with a precomputed lookup table with 256 entries. If we randomly change enough bits in packet P1 to create P2, then CRC(P1) and CRC(P2) are effectively independent random variables, with probability of a match 1 in 2Nwhere N is the CRC length. However, if 170 7 Packets
An Introduction to Computer Networks, Release 2.0.11 we change just a fewbits then the change is notso random. In particular, for many CRC codes (that is, for many choices of the underlying polynomial p(X)), changing up to three bits in P1 to create a new message P2 guarantees that CRC(P1) CRC(P2). For the Internet checksum, this is not guaranteed even if we know only two bits were changed. Finally, there are also secure hashes, such as MD-5 and SHA-1 and their successors ( 28.6 Secure Hashes ). Nobody knows (or admits to knowing) how to produce two messages with same hash here. However, these secure-hash codes are generally not used in network error-correction as they are much slower to calculate than CRC; they are generally used only for secure authentication and other higher-level functions. 7.4.2 Error-Correcting Codes If a link is noisy, we can add an errorcorrection code (also called forward error correction ) that allows the receiver in many cases to Ô¨Ågure out which bits are corrupted, and Ô¨Åx them. This has the effect of improving the bit error rate at a cost of reducing throughput. Error-correcting codes tend to involve many more bits than are needed for error detection. Typically, if a communications technology proves to have an unacceptably high bit-error rate (such as wireless), the next step is to introduce an error-correcting code to the protocol. This generally reduces the ‚Äúvirtual‚Äù bit-error rate (that is, the error rate as corrected) to acceptable levels. Perhaps the easiest error-correcting code to visualize is 2-D parity, for which we need O(N1/2) additional bits. We take N N data bits and arrange them into a square; we then compute the parity for every column, for every row, and for the entire square; this is 2N+1 extra bits. Here is a diagram with N=4, and with even parity; the column-parity bits (in blue) are in the bottom (Ô¨Åfth) row and the row-parity bits (also in blue) are in the rightmost (Ô¨Åfth) column. The parity bit for the entire 4 4 data square is the light-blue bit in the bottom right corner. 0 1100 0 1111 1 0100 1 1110 0 1001 Now suppose one bit is corrupted; for simplicity, assume it is one of the data bits. Then exactly one columnparity bit will be incorrect, and exactly one row-parity bit will be incorrect. These two incorrect bits mark the column and row of the incorrect data bit, which we can then Ô¨Çip to the correct state. We can make N large, but an essential requirement here is that there be only a single corrupted bit per square. We are thus likely either to keep N small, or to choose a different code entirely that allows correction of multiple bits. Either way, the addition of error-correcting codes can easily increase the size of a packet signiÔ¨Åcantly; some codes double or even triple the total number of bits sent. 7.4 Error Detection 171
An Introduction to Computer Networks, Release 2.0.11 7.4.2.1 Hamming Codes The Hamming code is another popular error-correction code; it adds O(log N) additional bits, though if N is large enough for this to be a material improvement over the O(N1/2) performance of 2-D parity then errors must be very infrequent. If we have 8 data bits, let us number the bit positions 0 through 7. We then write each bit‚Äôs position as a binary value between 000 and 111; we will call these the position bits of the given data bit. We now add four code bits as follows: 1. a parity bit over all 8 data bits 2. a parity bit over those data bits for which the Ô¨Årst digit of the position bits is 1 (these are positions 4, 5, 6 and 7) 3. a parity bit over those data bits for which the second digit of the position bits is 1 (these are positions 010, 011, 110 and 111, or 2, 3, 6 and 7) 4. a parity bit over those data bits for which the third digit of the position bits is 1 (these are positions 001, 011, 101, 111, or 1, 3, 5 and 7) We can tell whether or not an error has occurred by the Ô¨Årst code bit; the remaining three code bits then tell us the respective three position bits of the incorrect bit. For example, if the #2 code bit above is correct, then the Ô¨Årst digit of the position bits is 0; otherwise it is one. With all three position bits, we have identiÔ¨Åed the incorrect data bit. As a concrete example, suppose the data word is 10110010. The four code bits are thus 1. 0, the (even) parity bit over all eight bits 2. 1, the parity bit over the second half, 1011 0010 3. 1, the parity bit over the bold bits: 10 110010 4. 1, the parity bit over these bold bits: 1 0110010 If the received data+code is now 1011 1010 0111, with the bold bit Ô¨Çipped, then the fact that the Ô¨Årst code bit is wrong tells the receiver there was an error. The second code bit is also wrong, so the Ô¨Årst bit of the position bits must be 1. The third code bit is right, so the second bit of the position bits must be 0. The fourth code bit is also right, so the third bit of the position bits is 0. The position bits are thus binary 100, or 4, and so the receiver knows that the incorrect bit is in position 4 (counting from 0) and can be Ô¨Çipped to the correct state. 7.5 Epilog The issues presented here are perhaps not very glamorous, and often play a supporting, behind-the-scenes role in protocol design. Nonetheless, their inÔ¨Çuence is pervasive; we may even think of them as part of the underlying ‚Äúphysics‚Äù of the Internet. As the early Internet became faster, for example, and propagation delay became the dominant limiting factor, protocols were often revised to limit the number of back-and-forth exchanges. A classic example is the Simple Mail Transport Protocol (SMTP), amended by RFC 1854 to allow multiple commands to be sent together ‚Äì called pipelining ‚Äì instead of individually. 172 7 Packets
An Introduction to Computer Networks, Release 2.0.11 While there have been periodic calls for large-packet support in IPv4, and IPv6 protocols exist for ‚Äújumbograms‚Äù in excess of a megabyte, these are very seldom used, due to the store-and-forward costs of large packets as described in 7.3 Packet Size. Almost every LAN-level protocol, from Ethernet to Wi-Fi to point-to-point links, incorporates an errordetecting code chosen to reÔ¨Çect the underlying transportation reliability. Ethernet includes a 32-bit CRC code, for example, while Wi-Fi includes extensive error-correcting codes due to the noisier wireless environment. The Wi-Fi fragmentation option ( 4.2.1.5 Wi-Fi Fragmentation ) is directly tied to 7.3.1 Error Rates and Packet Size. 7.6 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.7 Solutions for Packets. 1.0. Suppose a link has a propagation delay of 20 ¬µsec and a bandwidth of 2 bytes/¬µsec. (a). How long would it take to transmit a 600-byte packet over such a link? (b). How long would it take to transmit the 600-byte packet over two such links, with a store-and-forward switch in between? 2.0. Suppose the path from A to B has a single switch S in between: A S B. Each link has a propagation delay of 60 ¬µsec and a bandwidth of 2 bytes/¬µsec. (a). How long would it take to send a single 600-byte packet from A to B? (b). How long would it take to send two back-to-back 300-byte packets from A to B? (c). How long would it take to send three back-to-back 200-byte packets from A to B? 3.0.‚ô¢Repeat parts (a) and (b) of the previous exercise, except change the per-link propagation delay from 60 ¬µsec to 600 ¬µsec. 4.0. Suppose the path from A to B has a single switch S in between: A S B. The propagation delays on the A‚ÄìS and S‚ÄìB are 24 ¬µsec and 35 ¬µsec respectively. The per-packet bandwidth delays on the A‚ÄìS and S‚ÄìB links are 103 ¬µsec and 157 ¬µsec respectively. The ladder diagram below describes the sending of two consecutive packets from A to B. Label the time intervals (a) through (e) at the right edge, and give the total time for the packets to be sent. 7.6 Exercises 173
An Introduction to Computer Networks, Release 2.0.11 A S B packet 1 packet 2 packet 1 packet 2(a) (b) (c) (d) (e) 5.0. Now assume that the delays are the same as in exercise 4.0, except that the bandwidth delays are reversed: the A‚ÄìS bandwidth delay is 157 ¬µsec and the S‚ÄìB bandwidth delay is 103 ¬µsec. The propagation delays are unchanged. Draw the corresponding ladder diagram, and also the total time for two consecutive packets to be sent. (Hint: the total time should not change.) 6.0. Again suppose the path from A to B has a single switch S in between: A S B. The per-link bandwidth and propagation delays are as follows: link bandwidth propagation delay AS5 bytes/¬µsec 24 ¬µsec SB 3 bytes/¬µsec 13 ¬µsec (a). How long would it take to send a single 600-byte packet from A to B? (b). How long would it take to send two back-to-back 300-byte packets from A to B? Note that, because the S B link is slower, packet 2 arrives at S from A before S has Ô¨Ånished transmitting packet 1 to B. 7.0. Suppose in the previous exercise, the A‚ÄìS link has the smaller bandwidth of 3 bytes/¬µsec and the S‚ÄìB link has the larger bandwidth of 5 bytes/¬µsec. The propagation delays are unchanged. Now how long does it take to send two back-to-back 300-byte packets from A to B? 8.0. Suppose we have Ô¨Åve links, A R1 R2 R3 R4 B. Each link has a bandwidth of 100 bytes/ms. Assume we model the per-link propagation delay as 0. (a). How long would it take a single 1500-byte packet to go from A to B? (b). How long would it take Ô¨Åve consecutive 300-byte packets to go from A to B? 174 7 Packets
An Introduction to Computer Networks, Release 2.0.11 The diagram in 7.3 Packet Size may help. 9.0. Suppose there are N equal-bandwidth links on the path between A and B, as in the diagram below, and we wish to send M consecutive packets. A S1. .. SN-1 B Let BD be the bandwidth delay of a single packet on a single link, and assume the propagation delay on each link is zero. Show that the total (bandwidth) delay is (M+N-1) BD. Hint: the total time is the sum of the time A takes to begin transmitting the last packet, and the time that last packet (or any other packet) takes to travel from A to B. Show that the former is (M-1) BD and the latter is N BD. Note that no packets ever have to wait at any S ibecause the ith packet takes exactly as long to arrive as the (i-1)th packet takes to depart. 10.0. Repeat the analysis in 7.3.1 Error Rates and Packet Size to compare the probable total number of bytes that need to be sent to transmit 107bytes using (a). 1,000-byte packets (b). 10,000-byte packets Assume the bit error rate is 1 in 16 105, making the error rate per byte about 1 in 2105. 11.0. In the text it is claimed ‚Äúthere is no N-bit error code that catches all N-bit errors‚Äù for N ¬•2 (for N=1, a parity bit works). Prove this claim for N=2. Hint: pick a length M, and consider all M-bit messages with a single 1-bit. Any such message can be converted to any other with a 2-bit error. Show, using the Pigeonhole Principle, that for large enough M two messages m 1and m 2must have the same error code, that is, e(m 1) = e(m 2). If this occurs, then the error code fails to detect the error that converted m 1into m 2. 12.0. Consider the following four-bit numbers, with decimal values in parentheses: 1000 (8) 1011 (11) 1101 (13) 1110 (14) The ones-complement sum of these can be found using the division method by treating these as a four-digit hex number 0x8bde and taking the remainder mod 15; the result is 1. (a). Find this ones-complement sum via three 4-bit ones-complement additions. To get started, note that the (exact) sum of 1000 and 1011 is 1|0011, and adding the carry bit to the low-order 4 bits gives a ones-complement sum of the Ô¨Årst pair of 0100. (b). The exact (and 8-bit twos-complement) sum of the values above is 46, or 10|1110 in binary. Find the ones-complement sum of the values by taking this exact sum and then forming the ones-complement sum of the 4-bit high and low halves. Note that this is not the same as the twos-complement sum of the halves. 13.0. Let [a,b] denote a pair of bytes a and b. The 16-bit integer corresponding to [a,b] using big-endian conversion is a 256 + b; using little-endian conversion it is a + 256 b. 7.6 Exercises 175
An Introduction to Computer Networks, Release 2.0.11 (a). Find the ones-complement sum of [200,150] and [90,230] by using big-endian conversion to the respective 16-bit integers 51,350 and 23,270. Convert back to two bytes, again using big-endian conversion, at the end. (b). Do the same using little-endian conversion, in which case the 16-bit integers are 38,600 and 58,970. 13.5. In the description in the text of the Internet checksum, the overÔ¨Çow bit was added back in after each ones-complement addition. Show that the same Ô¨Ånal result will be obtained if we add up the 16-bit words using 32-bit twos-complement arithmetic (twos-complement is the normal arithmetic on all modern hardware), and then add the upper 16 bits of the sum to the lower 16 bits. (If there is an overÔ¨Çow at this last step, we have to add that back in as well.) 14.0. Suppose a message is 110010101. Calculate the CRC-3 checksum using the polynomial X3+ 1, that is, Ô¨Ånd the 3-bit remainder using divisor 1001. 15.0. The CRC algorithm presented above requires that we process one bit at a time. It is possible to do the algorithm N bits at a time ( egN=8), with a precomputed lookup table of size 2N. Complete the steps in the following description of this strategy for N=3 and polynomial X3+ X + 1, or 1011. 16.0. Consider the following set of bits sent with 2-D even parity; the data bits are in the 4 4 upper-left block and the parity bits are in the rightmost column and bottom row. Which bit is corrupted? 11011 01001 11111 10010 11101 17.0. (a) Show that 2-D parity can detect any three errors. | (b). Find four errors that cannot be detected by 2-D parity. | (c). Show that that 2-D parity cannot correct all two-bit errors. Hint: put both bits in the same row or column. 18.0. Each of the following 8-bit messages with 4-bit Hamming code contains a single error. Correct the message. (a)‚ô¢. 10100010 0111 (b). 10111110 1011 19.0. (a) What happens in 2-D parity if the corrupted bit is in the parity column or parity row? (b). In the following 8-bit message with 4-bit Hamming code, there is an error in the code portion. How can this be determined? 176 7 Packets
An Introduction to Computer Networks, Release 2.0.11 1001 1110 0100 7.6 Exercises 177
An Introduction to Computer Networks, Release 2.0.11 178 7 Packets
8 ABSTRACT SLIDING WINDOWS In this chapter we take a general look at how to build reliable data-transport layers on top of unreliable lower layers. This is achieved through a retransmit-on-timeout policy; that is, if a packet is transmitted and there is no acknowledgment received during the timeout interval then the packet is resent. As a class, protocols where one side implements retransmit-on-timeout are known as ARQ protocols, for Automatic Repeat reQuest. In addition to reliability, we also want to keep as many packets in transit as the network can support. The strategy used to achieve this is known as sliding windows. It turns out that the sliding-windows algorithm is also the key to managing congestion; we return to this in 19 TCP Reno and Congestion Management. TheEnd-to-End principle, 17.1 The End-to-End Principle, suggests that trying to achieve a reliable transport layer by building reliability into a lower layer is a misguided approach; that is, implementing reliability at the endpoints of a connection ‚Äì as is described here ‚Äì is in fact the correct mechanism. 8.1 Building Reliable Transport: Stop-and-Wait Retransmit-on-timeout generally requires sequence numbering for the packets, though if a network path is guaranteed not to reorder packets then it is safe to allow the sequence numbers to wrap around surprisingly quickly (for stop-and-wait, a single-bit sequence number will work; see exercise 8.5). However, as the no-reordering hypothesis does not apply to the Internet at large, we will assume conventional numbering. Data[N] will be the Nth data packet, acknowledged by ACK[N]. In the stop-and-wait version of retransmit-on-timeout, the sender sends only one outstanding packet at a time. If there is no response, the packet may be retransmitted, but the sender does not send Data[N+1] until it has received ACK[N]. Of course, the receiving side will not send ACK[N] until it has received Data[N]; each side has only one packet in play at a time. In the absence of packet loss, this leads to the following: 179
An Introduction to Computer Networks, Release 2.0.11 Stop and WaitData 1 Ack 1 Data 2 Data 3 Ack 3Ack 2 Data 4 Ack 4 8.1.1 Packet Loss Lost packets, however, are a reality. The left half of the diagram below illustrates a lost Data packet, where the sender is the host sending Data and the Receiver is the host sending ACKs. The receiver is not aware of the loss; it sees Data[N] as simply slow to arrive. 180 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 Sender Sender Receiver Receiver Timeout Lost Data Lost ACKData[N] Data[N] ACK[N]Data[N] ACK[N] TimeoutData[N] ACK[N] The right half of the diagram, by comparison, illustrates the case of a lost ACK. The receiver has received aduplicate Data[N]. We have assumed here that the receiver has implemented a retransmit-on-duplicate strategy, and so its response upon receipt of the duplicate Data[N] is to retransmit ACK[N]. As a Ô¨Ånal example, note that it is possible for ACK[N] to have been delayed (or, similarly, for the Ô¨Årst Data[N] to have been delayed) longer than the timeout interval. Not every packet that times out is actually lost! Sender Receiver Timeout Late ACKData[N] Data[N]ACK[N] ACK[N]Data[N+1] Expecting ACK[N+1] In this case we see that, after sending Data[N], receiving a delayed ACK[N] (rather than the expected ACK[N+1]) must be considered a normal event. In principle, either side can implement retransmit-on-timeout if nothing is received. Either side can also implement retransmit-on-duplicate; this was done by the receiver in the second example above but notby the sender in the third example (the sender received a second ACK[N] but did not retransmit Data[N+1]). 8.1 Building Reliable Transport: Stop-and-Wait 181
An Introduction to Computer Networks, Release 2.0.11 At least one side must implement retransmit-on-timeout; otherwise a lost packet leads to deadlock as the sender and the receiver both wait forever. The other side must implement at least one of retransmit-onduplicate or retransmit-on-timeout; usually the former alone. If both sides implement retransmit-on-timeout with different timeout values, generally the protocol will still work. 8.1.2 Sorcerer‚Äôs Apprentice Bug Sorcerer‚Äôs Apprentice The Sorcerer‚Äôs Apprentice bug is named for the legend in which the apprentice casts a spell on a broom to carry water, one bucket at a time. When the basin is full, the apprentice chops the broom in half to stop it, only to Ô¨Ånd both halves still carrying water. An animated version of this appears in Disney‚Äôs _Fantasia_, set to the music of Paul Dukas. I used to post a YouTube link here to the video, but Disney has blocked it. It may still be Ô¨Åndable online, though. Mickey Mouse chops the broom about Ô¨Åve and a half minutes in from the start of the music. A strange thing happens if one side implements retransmit-on-timeout but both sides implement retransmiton-duplicate, as can happen if the implementer takes the naive view that retransmitting on duplicates is ‚Äúsafer‚Äù; the moral here is that too much redundancy can be the Wrong Thing. Let us imagine that an implementation uses this strategy (with the sender retransmitting on timeouts), and that the initial ACK[3] is delayed until after Data[3] is retransmitted on timeout. In the following diagram, the only packet retransmitted due to timeout is the second Data[3]; all the other duplications are due to the bilateral retransmit-onduplicate strategy. 182 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 sender receiver First Data[3] Second Data[3]First ACK[3], delayed TIMEOUT First Data[4]Second ACK[3] First ACK[4] Second Data[4] Second ACK[4] First Data[5] Second Data[5]First ACK[5]First ACK[4] Second ACK[5] ... The Sorcerer's Apprentice bug First transmissions are in black Second transmissions are in blue All packets are sent twice from Data[3] on. The transfer completes normally, but takes double the normal bandwidth. The usual Ô¨Åx is to have one side (usually the sender) retransmit on timeout only. TCP does this; see18.12 TCP Timeout and Retransmission. See also exercise 2.0. 8.1.3 Flow Control Stop-and-wait also provides a simple form of Ô¨Çow control to prevent data from arriving at the receiver faster than it can be handled. Assuming the time needed to process a received packet is less than one RTT, the stop-and-wait mechanism will prevent data from arriving too fast. If the processing time is slightly larger than RTT, all the receiver has to do is to wait to send ACK[N] until Data[N] has not only arrived but also been processed, and the receiver is ready for Data[N+1]. For modest per-packet processing delays this works quite well, but if the processing delays are long it introduces a new problem: Data[N] may time out and be retransmitted even though it has successfully been received; the receiver cannot send an ACK until it has Ô¨Ånished processing. One approach is to have twokinds of ACKs: ACK WAIT[N] meaning that Data[N] has arrived but the receiver is not yet ready for Data[N+1], and ACK GO[N] meaning that the sender may now send Data[N+1]. The receiver will send ACK WAIT[N] when Data[N] arrives, and ACK GO[N] when it is done processing it. Presumably we want the sender not to time out and retransmit Data[N] after ACK WAIT[N] is received, as a retransmission would be unnecessary. This introduces a new problem: if the subsequent ACK GO[N] is lost and neither side times out, the connection is deadlocked. The sender is waiting for ACK GO[N], which 8.1 Building Reliable Transport: Stop-and-Wait 183
An Introduction to Computer Networks, Release 2.0.11 is lost, and the receiver is waiting for Data[N+1], which the sender will not send until the lost ACK GO[N] arrives. One solution is for the receiver to switch to a timeout model, perhaps until Data[N+1] is received. TCP has a Ô¨Åx to the Ô¨Çow-control problem involving sender-side polling; see 18.10 TCP Flow Control. 8.2 Sliding Windows Stop-and-wait is reliable but it is not very efÔ¨Åcient (unless the path involves neither intermediate switches nor signiÔ¨Åcant propagation delay; that is, the path involves a single LAN link). Most links along a multi-hop stop-and-wait path will be idle most of the time. During a Ô¨Åle transfer, ideally we would like zero idleness (at least along the slowest link; see 8.3 Linear Bottlenecks ). We can improve overall throughput by allowing the sender to continue to transmit, sending Data[N+1] (and beyond) without waiting for ACK[N]. We cannot, however, allow the sender get toofar ahead of the returning ACKs. Packets sent too fast, as we shall see, simply end up waiting in queues, or, worse, dropped from queues. If the links of the network have sufÔ¨Åcient bandwidth, packets may also be dropped at the receiving end. Now that, say, Data[3] and Data[4] may be simultaneously in transit, we have to revisit what ACK[4] means: does it mean that the receiver has received only Data[4], or does it mean both Data[3] and Data[4] have arrived? We will assume the latter, that is, ACKs are cumulative: ACK[N] cannot be sent until Data[K] has arrived for all K ¬§N. With this understanding, if ACK[3] is lost then a later-arriving ACK[4] makes up for it; without it, if ACK[3] is lost the only recovery is to retransmit Data[3]. The sender picks a window size, winsize. The basic idea of sliding windows is that the sender is allowed to send this many packets before waiting for an ACK. More speciÔ¨Åcally, the sender keeps a state variable last_ACKed, representing the last packet for which it has received an ACK from the other end; if data packets are numbered starting from 1 then initially last_ACKed = 0. Window Size In this chapter we will assume winsize does not change. TCP, however, varies winsize up and down with the goal of making it as large as possible without introducing congestion; we will return to this in 19 TCP Reno and Congestion Management. At any instant, the sender may send packets numbered last_ACKed + 1 through last_ACKed + winsize; this packet range is known as the window. Generally, if the Ô¨Årst link in the path is not the slowest one, the sender will most of the time have sent all these. If ACK[N] arrives with N > last_ACKed (typically N = last_ACKed+1), then the window slides forward; we set last_ACKed = N. This also increments the upper edge of the window, and frees the sender to send more packets. For example, with winsize = 4 and last_ACKed = 10, the window is [11,12,13,14]. If ACK[11] arrives, the window slides forward to [12,13,14,15], freeing the sender to send Data[15]. If instead ACK[13] arrives, then the window slides forward to [14,15,16,17] (recall that ACKs are cumulative), and three more packets become eligible to be sent. If there is no packet reordering and no packet losses (and every packet is ACKed individually) then the window will slide forward in units of one packet at a time; the next arriving ACK will always be ACK[last_ACKed+1]. 184 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 Note that the rate at which ACKs are returned will always be exactly equal to the rate at which the slowest link is delivering packets. That is, if the slowest link (the ‚Äúbottleneck‚Äù link) is delivering a packet every 50 ms, then the receiver will receive those packets every 50 ms and the ACKs will return at a rate of one every 50 ms. Thus, new packets will be sent at an average rate exactly matching the delivery rate; this is the sliding-windows self-clocking property. Self-clocking has the effect of reducing congestion by automatically reducing the sender‚Äôs rate whenever the available fraction of the bottleneck bandwidth is reduced. Below is a video of sliding windows in action, with winsize = 5. (A link is here, if the embedded video does not display properly, which will certainly be the case with non-html formats.) The nodes are labeled 0, 1 and 2. The second link, 1‚Äì2, has a capacity of Ô¨Åve packets in transit either way, so one ‚ÄúÔ¨Çight‚Äù (windowful) of Ô¨Åve packets can exactly Ô¨Åll this link. The 0‚Äì1 link has a capacity of one packet in transit either way. The video was prepared using the network animator, ‚Äúnam‚Äù, described further in 31 Network Simulations: ns-2. The Ô¨Årst Ô¨Çight of Ô¨Åve data packets leaves node 0 just after T=0, and leaves node 1 at around T=1 (in video time). Subsequent Ô¨Çights are spaced about seven seconds apart. The tiny packets moving leftwards from node 2 to node 0 represent ACKs; at the very beginning of the video one can see Ô¨Åve returning ACKs from the previous windowful. At any moment (except those instants where packets have just been received) there are in principle Ô¨Åve packets in transit, either being transmitted on a link as data, or being transmitted as an ACK, or sitting in a queue (this last does not happen in this video). Due to occasional video artifacts, in some frames not all the ACK packets are visible. 8.2.1 Bandwidth Delay As indicated previously ( 7.1 Packet Delay ), the bandwidth RTT product represents the amount of data that can be sent before the Ô¨Årst response is received. It plays a large role in the analysis of transport protocols. In the literature the bandwidth delay product is often abbreviated BDP. The bandwidth RTT product is generally the optimum value for the window size. There is, however, one catch: if a sender chooses winsize larger than this, then the RTT simply grows ‚Äì due to queuing delays ‚Äì to the point that bandwidth RTT matches the chosen winsize. That is, a connection‚Äôs own trafÔ¨Åc can inÔ¨Çate RTT actual to well above RTT noLoad; see 8.3.1.3 Case 3: winsize = 6 below for an example. For this reason, a sender is often more interested in bandwidth RTT noLoad, or, at the very least, the RTT before the sender‚Äôs own packets had begun contributing to congestion. We will sometimes refer to the bandwidth RTT noLoad product as the transit capacity of the route. As will become clearer below, a winsize smaller than this means underutilization of the network, while a larger winsize means each packet spends time waiting in a queue somewhere. Below are simpliÔ¨Åed diagrams for sliding windows with window sizes of 1, 4 and 6, each with a path bandwidth of 6 packets/RTT (so bandwidth RTT = 6 packets). The diagram shows the initial packets sent as a burst; these then would be spread out as they pass through the bottleneck link so that, after the Ô¨Årst burst, packet spacing is uniform. (Real sliding-windows protocols such as TCP generally attempt to avoid such initial bursts.) 8.2 Sliding Windows 185
An Introduction to Computer Networks, Release 2.0.11 Sliding Windows, bandwidth 6 packets/RTTWinSize = 1 WinSize = 4 WinSize = 6Data 1-4 Ack 1 / Data 5 Ack 2 / Data 6 Ack 3 / Data 7 Ack 4 / Data 8 Ack 5 / Data 9 Ack 6 / Data 10 Ack 7 / Data 11 Ack 8 / Data 12Data 1-6 Data 1 Ack 1 Data 2 Data 3 Ack 3Ack 2Ack 1 / Data 7 Ack 2 / Data 8 Ack 3 / Data 9 Ack 4 / Data 10 Ack 5 / Data 11 Ack 6 / Data 12 Ack 7 / Data 13 Ack 8 / Data 14 Ack 9 / Data 15 Ack 10 / Data 16 With winsize=1 we send 1 packet per RTT; with winsize=4 we always average 4 packets per RTT. To put this another way, the three window sizes lead to bottle-neck link utilizations of 1/6, 4/6 and 6/6 = 100%, respectively. While it is tempting to envision setting winsize to bandwidth RTT, in practice this can be complicated; neither bandwidth nor RTT is constant. Available bandwidth can Ô¨Çuctuate in the presence of competing trafÔ¨Åc. As for RTT, if a sender sets winsize too large then the RTT is simply inÔ¨Çated to the point that bandwidthRTT matches winsize; that is, a connection‚Äôs own trafÔ¨Åc can inÔ¨Çate RTT actual to well above RTT noLoad. This happens even in the absence of competing trafÔ¨Åc. 8.2.2 The Receiver Side Perhaps surprisingly, sliding windows can work pretty well with the receiver assuming that winsize=1, even if the sender is in fact using a much larger value. Each of the receivers in the diagrams above receives Data[N] and responds with ACK[N]; the only difference with the larger sender winsize is that the Data[N] arrive faster. 186 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 If we are using the sliding-windows algorithm over single links, we may assume packets are never reordered, and a receiver winsize of 1 works quite well. Once switches are introduced, however, life becomes more complicated (though some links may do link-level sliding-windows for per-link throughput optimization). If packet reordering is a possibility, it is common for the receiver to use the same winsize as the sender. This means that the receiver must be prepared to buffer a full window full of packets. If the window is [11,12,13,14,15,16], for example, and Data[11] is delayed, then the receiver may have to buffer Data[12] through Data[16]. Like the sender, the receiver will also maintain the state variable last_ACKed, though it will not be completely synchronized with the sender‚Äôs version. At any instant, the receiver is willing to accept Data[last_ACKed+1] through Data[last_ACKed+winsize]. For any but the Ô¨Årst of these, the receiver must buffer the arriving packet. If Data[last_ACKed+1] arrives, then the receiver should consult its buffers and send back the largest cumulative ACK it can for the data received; for example, if the window is [11-16] and Data[12], Data[13] and Data[15] are in the buffers, then on arrival of Data[11] the correct response is ACK[13]. Data[11] Ô¨Ålls the ‚Äúgap‚Äù, and the receiver has now received everything up through Data[13]. The new receive window is [14-19], and as soon as the ACK[13] reaches the sender that will be the new send window as well. 8.2.3 Loss Recovery Under Sliding Windows Suppose winsize = 4 and packet 5 is lost. It is quite possible that packets 6, 7, and 8 may have been received. However, the only (cumulative) acknowledgment that can be sent back is ACK[4]; the sender does not know how much of the windowful made it through. Because of the possibility that only Data[5] (or more generally Data[last_ACKed+1]) is lost, and because losses are usually associated with congestion, when we most especially do notwish to overburden the network, the sender will usually retransmit only the Ô¨Årst lost packet, egpacket 5. If packets 6, 7, and 8 were also lost, then after retransmission of Data[5] the sender will receive ACK[5], and can assume that Data[6] now needs to be sent. However, if packets 6-8 did make it through, then after retransmission the sender will receive back ACK[8], and so will know 6-8 do not need retransmission and that the next packet to send is Data[9]. Normally Data[6] through Data[8] would time out shortly after Data[5] times out. After the Ô¨Årst timeout, however, sliding windows protocols generally suppress further timeout/retransmission responses until recovery is more-or-less complete. Once a full timeout has occurred, usually the sliding-windows process itself has ground to a halt, in that there are usually no packets remaining in Ô¨Çight. This is sometimes described as pipeline drain. After recovery, the sliding-windows process will have to start up again. Most implementations of TCP, as we shall see later, implement a mechanism (‚Äúfast recovery‚Äù) for early detection of packet loss, before the pipeline has fully drained. 8.3 Linear Bottlenecks Consider the simple network path shown below, with bandwidths shown in packets/ms. The minimum bandwidth, or path bandwidth, is 3 packets/ms. 8.3 Linear Bottlenecks 187
An Introduction to Computer Networks, Release 2.0.11 A R110 pkts/ms R26 pkts/ms R3 R4 B3 pkts/ms 3 pkts/ms 8 pkts/ms The slow links are R2‚ÄìR3 and R3‚ÄìR4. We will refer to the slowest link as the bottleneck link; if there are (as here) ties for the slowest link, then the Ô¨Årst such link is the bottleneck. The bottleneck link is where the queue will form. If trafÔ¨Åc is sent at a rate of 4 packets/ms from A to B, it will pile up in an ever-increasing queue at R2. TrafÔ¨Åc will notpile up at R3; it arrives at R3 at the same rate by which it departs. Furthermore, if sliding windows is used (rather than a Ô¨Åxedratesender), trafÔ¨Åc will eventually not queue up at any router other than R2: data cannot reach B faster than the 3 packets/ms rate, and so B will not return ACKs faster than this rate, and so A will eventually not send data faster than this rate. At this 3 packets/ms rate, trafÔ¨Åc will not pile up at R1 (or R3 or R4). There is a signiÔ¨Åcant advantage in speaking in terms of winsize rather than transmission rate. If A sends to B at any rate greater than 3 packets/ms, then the situation is unstable as the bottleneck queue grows without bound and there is no convergence to a steady state. There is no analogous instability, however, if A uses sliding windows, even if the winsize chosen is quite large (although a large-enough winsize will overÔ¨Çow the bottleneck queue). If a sender speciÔ¨Åes a sending window size rather than a rate, then the network will converge to a steady state in relatively short order; if a queue develops it will be steadily replenished at the same rate that packets depart, and so will be of Ô¨Åxed size. 8.3.1 Simple Ô¨Åxed-window-size analysis We will analyze the effect of window size on overall throughput and on RTT. Consider the following network path, with bandwidths now labeled in packets/ second. A R1infinitely fast R21 pkt/sec R3 R4 B1 pkt/sec 1 pkt/sec 1 pkt/sec We will assume that in the backward B √ù√ëA direction, all connections are inÔ¨Ånitely fast, meaning zero delay; this is often a good approximation because ACK packets are what travel in that direction and they are negligibly small. In the A √ù√ëB direction, we will assume that the A √ù√ëR1 link is inÔ¨Ånitely fast, but the other four each have a bandwidth of 1 packet/second (and no propagation-delay component). This makes the R1√ù√ëR2 link the bottleneck link; any queue will now form at R1. The ‚Äúpath bandwidth‚Äù is 1 packet/second, and the RTT is 4 seconds. As a roughly equivalent alternative example, we might use the following: C S1infinitely fast S21 pkt/sec D1 pkt/sec with the following assumptions: the C‚ÄìS1 link is inÔ¨Ånitely fast (zero delay), S1 √ù√ëS2 and S2√ù√ëD each take 1.0 sec bandwidth delay (so two packets take 2.0 sec, per link, etc), and ACKs also have a 1.0 sec 188 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 bandwidth delay in the reverse direction. In both scenarios, if we send one packet, it takes 4.0 seconds for the ACK to return, on an idle network. This means that the no-load delay, RTT noLoad, is 4.0 seconds. (These models will change signiÔ¨Åcantly if we replace the 1 packet/sec bandwidth delay with a 1-second propagation delay; in the former case, 2 packets take 2 seconds, while in the latter, 2 packets take 1 second. See exercise 6.0.) We assume a single connection is made; iethere is no competition. Bandwidth delay here is 4 packets (1 packet/sec4 sec RTT) 8.3.1.1 Case 1: winsize = 2 In this case winsize < bandwidth delay (where delay = RTT). The table below shows what is sent by A and each of R1-R4 for each second. Every packet is acknowledged 4 seconds after it is sent; that is, RTT actual = 4 sec, equal to RTT noLoad; this will remain true as the winsize changes by small amounts ( egto 1 or 3). Throughput is proportional to winsize: when winsize = 2, throughput is 2 packets in 4 seconds, or 2/4 = 1/2 packet/sec. During each second, two of the routers R1-R4 are idle. The overall path will have less than 100% utilization. Time A R1 R1 R2 R3 R4 B T sends queues sends sends sends sends ACKs 0 1,2 2 1 1 2 1 2 2 1 3 2 1 4 3 3 2 1 5 4 4 3 2 6 4 3 7 4 3 8 5 5 4 3 9 6 6 5 4 Note the brief pile-up at R1 (the bottleneck link!) on startup. However, in the steady state, there is no queuing. Real sliding-windows protocols generally have some way of minimizing this ‚Äúinitial pileup‚Äù. 8.3.1.2 Case 2: winsize = 4 When winsize=4, at each second all four slow links are busy. There is again an initial burst leading to a brief surge in the queue; RTT actual for Data[4] is 7 seconds. However, RTT actual for every subsequent packet is 4 seconds, and there are no queuing delays (and nothing in the queue) after T=2. The steady-state connection throughput is 4 packets in 4 seconds, ie1 packet/second. Note that overall path throughput now equals the bottleneck-link bandwidth, so this is the best possible throughput. 8.3 Linear Bottlenecks 189
An Introduction to Computer Networks, Release 2.0.11 TA sends R1 queues R1 sends R2 sends R3 sends R4 sends B ACKs 01,2,3,4 2,3,4 1 1 3,4 2 1 2 4 3 2 1 3 4 3 2 1 45 5 4 3 2 1 56 6 5 4 3 2 67 7 6 5 4 3 78 8 7 6 5 4 89 9 8 7 6 5 At T=4, R1 has just Ô¨Ånished sending Data[4] as Data[5] arrives from A; R1 can begin sending packet 5 immediately. No queue will develop. Case 2 is the ‚Äúcongestion knee‚Äù of Chiu and Jain [CJ89], deÔ¨Åned here in 1.7 Congestion. 8.3.1.3 Case 3: winsize = 6 T A sends R1 queues R1 sends R2 sends R3 sends R4 sends B ACKs 0 1,2,3,4,5,6 2,3,4,5,6 1 1 3,4,5,6 2 1 2 4,5,6 3 2 1 3 5,6 4 3 2 1 4 7 6,7 5 4 3 2 1 5 8 7,8 6 5 4 3 2 6 9 8,9 7 6 5 4 3 7 10 9,10 8 7 6 5 4 8 11 10,11 9 8 7 6 5 9 12 11,12 10 9 8 7 6 10 13 12,13 11 10 9 8 7 Note that packet 7 is sent at T=4 and the acknowledgment is received at T=10, for an RTT of 6.0 seconds. All later packets have the same RTT actual. That is, the RTT has risen from RTT noLoad = 4 seconds to 6 seconds. Note that we continue to send one windowful each RTT; that is, the throughput is still winsize/RTT, but RTT is now 6 seconds. One might initially conjecture that if winsize is greater than the bandwidth RTT noLoad product, then the entire window cannot be in transit at one time. In fact this is not the case; the sender does usually have the entire window sent and in transit, but RTT has been inÔ¨Çated so it appears to the sender that winsize equals the bandwidthRTT product. In general, whenever winsize > bandwidth RTT noLoad, what happens is that the extra packets pile up at a router somewhere along the path (speciÔ¨Åcally, at the router in front of the bottleneck link). RTT actual is inÔ¨Çated by queuing delay to winsize/bandwidth, where bandwidth is that of the bottleneck link; this means winsize = bandwidth RTT actual. Total throughput is equal to that bandwidth. Of the 6 seconds of RTT actual in the example here, a packet spends 4 of those seconds being transmitted on one link or another because 190 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 RTT noLoad =4. The other two seconds, therefore, must be spent in a queue; there is no other place for packets wait. Looking at the table, we see that each second there are indeed two packets in the queue at R1. If the bottleneck link is the very Ô¨Årst link, packets may begin returning before the sender has sent the entire windowful. In this case we may argue that the full windowful has at least been queued by the sender, and thus has in this sense been ‚Äúsent‚Äù. Suppose the network, for example, is A R11 pkt/sec R2 R3 B1 pkt/sec 1 pkt/sec 1 pkt/sec where, as before, each link transports 1 packet/sec from A to B and is inÔ¨Ånitely fast in the reverse direction. Then, if A sets winsize = 6, a queue of 2 packets will form at A. 8.3.2 RTT Calculations We can make some quantitative observations of sliding windows behavior, and about queue utilization. First, we note that RTT noLoad is the physical ‚Äútravel‚Äù time (subject to the limitations addressed in 7.2 Packet Delay Variability ); any time in excess of RTT noLoad is spent waiting in a queue somewhere. Therefore, the following holds regardless of competing trafÔ¨Åc, and even for individual packets: 1. queue_time = RTT actual ‚Äì RTT noLoad When the bottleneck link is saturated, that is, is always busy, the number of packets actually in transit (not queued) somewhere along the path will always be bandwidth RTT noLoad. Second, we always send exactly one windowful per actual RTT, assuming no losses and each packet is individually acknowledged. This is perhaps best understood by examining the diagrams above, but here is a simple non-visual argument: if we send Data[N] at time T D, and ACK[N] arrives at time T A, then RTT = T A ‚Äì T D, by deÔ¨Ånition. At time T Athe sender is allowed to send Data[N+winsize], so during the RTT interval TD¬§T < T Athe sender must have sent Data[N] through Data[N+winsize-1]; that is, winsize many packets in time RTT. Therefore (whether or not there is competing trafÔ¨Åc) we always have 2. throughput = winsize/RTT actual where ‚Äúthroughput‚Äù is the rate at which the connection is sending packets. This relationship holds even if winsize or the bottleneck bandwidth changes suddenly, though in that case RTT actual might change from one packet to the next, and the throughput here must be seen as a measurement averaged over the RTT of one speciÔ¨Åc packet. If the sender doubles its winsize, those extra packets will immediately end up in a queue somewhere (perhaps a queue at the sender itself, though this is why in examples it is often clearer if the Ô¨Årst link has inÔ¨Ånite bandwidth so as to prevent this). If the bottleneck bandwidth is cut in half without changing winsize, eventually the RTT must rise due to queuing. See exercise 17.0. In the sliding windows steady state, where throughput and RTT actual are reasonably constant, the average number of packets in the queue is just throughput queue_time (where throughput is measured in packets/sec): 3. queue_usage = throughput (RTT actual ‚Äì RTT noLoad ) = winsize(1 ‚Äì RTT noLoad /RTT actual) 8.3 Linear Bottlenecks 191
An Introduction to Computer Networks, Release 2.0.11 To give a little more detail making the averaging perhaps clearer, each packet spends time (RTT actual ‚Äì RTT noLoad ) in the queue, from equation 1 above. The total time spent by a windowful of packets is winsize (RTT actual ‚Äì RTT noLoad ), and dividing this by RTT actual thus gives the average number of packets in the queue over the RTT interval in question. In the presence of competing trafÔ¨Åc, the throughput referred to above is simply the connection‚Äôs current share of the total bandwidth. It is the value we get if we measure the rate of returning ACKs. If there is nocompeting trafÔ¨Åc and winsize is below the congestion knee ‚Äì winsize < bandwidth RTT noLoad ‚Äì then winsize is the limiting factor in throughput. Finally, if there is no competition and winsize ¬•bandwidth RTT noLoad then the connection is using 100% of the capacity of the bottleneck link and throughput is equal to the bottleneck-link physical bandwidth. To put this another way, 4. RTT actual = winsize/bottleneck_bandwidth queue_usage = winsize ‚Äì bandwidth RTT noLoad Dividing the Ô¨Årst equation by RTT noLoad, and noting that bandwidth RTT noLoad = winsize - queue_usage = transit_capacity, we get 5. RTT actual/RTT noLoad = winsize/transit_capacity = (transit_capacity + queue_usage) / transit_capacity Regardless of the value of winsize, in the steady state the sender never sends faster than the bottleneck bandwidth. This is because the bottleneck bandwidth determines the rate of packets arriving at the far end, which in turn determines the rate of ACKs arriving back at the sender, which in turn determines the continued sending rate. This illustrates the self-clocking nature of sliding windows. We will return in 20 Dynamics of TCP to the issue of bandwidth in the presence of competing trafÔ¨Åc. For now, suppose a sliding-windows sender has winsize > bandwidth RTT noLoad, leading as above to a Ô¨Åxed amount of queue usage, and no competition. Then another connection starts up and competes for the bottleneck link. The Ô¨Årst connection‚Äôs effective bandwidth will thus decrease. This means that bandwidth  RTT noLoad will decrease, and hence the connection‚Äôs queue usage will increase. 8.3.3 Graphs at the Congestion Knee Consider the following graphs of winsize versus 1. throughput 2. delay 3. queue utilization 192 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 winsize winsize winsizedelayqueue utilization Graphs of winsize versus throughput, delay and queue utilization. Vertical dashed line represents winsize = bandwidth x no-load delayT h r o u g h p u t The critical winsize value is equal to bandwidth RTT noLoad; this is known as the congestion knee. For winsize below this, we have: 
- throughput is proportional to winsize 
- delay is constant 
- queue utilization in the steady state is zero For winsize larger than the knee, we have 
- throughput is constant (equal to the bottleneck bandwidth) 
- delay increases linearly with winsize 
- queue utilization increases linearly with winsize Ideally, winsize will be at the critical knee. However, the exact value varies with time: available bandwidth changes due to the starting and stopping of competing trafÔ¨Åc, and RTT changes due to queuing. Standard TCP makes an effort to stay well above the knee much of the time, presumably on the theory that maximizing throughput is more important than minimizing queue use. Thepower of a connection is deÔ¨Åned to be throughput/RTT. For sliding windows below the knee, RTT is constant and power is proportional to the window size. For sliding windows above the knee, throughput is constant and delay is proportional to winsize; power is thus proportional to 1/winsize. Here is a graph, akin to those above, of winsize versus power: power winsize 8.3 Linear Bottlenecks 193
An Introduction to Computer Networks, Release 2.0.11 8.3.4 Simple Packet-Based Sliding-Windows Implementation Here is a pseudocode outline of the receiver side of a sliding-windows implementation, ignoring lost packets and timeouts. We abbreviate as follows: W: winsize LA: last_ACKed Thus, the next packet expected is LA+1 and the window is [LA+1,. .. , LA+W]. We have a data structure EarlyArrivals in which we can place packets that cannot yet be delivered to the receiving application. Upon arrival of Data[M]: if M¬§LA or M>LA+W, ignore the packet if M>LA+1, put the packet into EarlyArrivals. if M==LA+1: deliver the packet (that is, Data[LA+1]) to the application LA = LA+1 (slide window forward by 1) while (Data[LA+1] is in EarlyArrivals) { output Data[LA+1] LA = LA+1 } send ACK[LA] A possible implementation of EarlyArrivals is as an array of packet objects, of size W. We always put packet Data[M] into position M % W. At any point between packet arrivals, Data[LA+1] is not in EarlyArrivals, but some later packets may be present. For the sender side, we begin by sending a full windowful of packets Data[1] through Data[W], and setting LA=0. When ACK[M] arrives, LA<M ¬§LA+W, the window slides forward from [LA+1.. . LA+W] to [M+1.. . M+W], and we are now allowed to send Data[LA+W+1] through Data[M+W]. The simplest case is M=LA+1. Upon arrival of ACK[M]: if M¬§LA or M>LA+W, ignore the packet otherwise: set K = LA+W+1, the Ô¨Årst packet just above the old window set LA = M, just below the bottom of the new window for (i=K; i¬§LA+W; i++) send Data[i] Note that new ACKs may arrive while we are in the loop at the last line. We assume here that the sender stolidly sends what it may send and only after that does it start to process additional arriving ACKs. Some implementations may take a more asynchronous approach, perhaps with one thread processing arriving ACKs and incrementing LA and another thread sending everything it is allowed to send. To add support for timeout and retransmission, each transmitted packet would need to be stored, together with the time it was sent. Periodically this collection of stored packets must then be scanned, looking for 194 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 packets for which send_time +timeout_interval ¬§current_time; those packets get retransmitted. When a packet Data[N] is acknowledged (perhaps by an ACK[M] for M>N), it can be deleted. 8.4 Epilog This completes our discussion of the sliding-windows algorithm in the abstract setting. We will return to concrete implementations of this in 16.4.1 TFTP and the Sorcerer (stop-and-wait) and in 18.7 TCP Sliding Windows; the latter is one of the most important mechanisms on the Internet. 8.5 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.8 Solutions for Sliding Windows. 1.0. Sketch a ladder diagram for stop-and-wait if Data[3] is lost the Ô¨Årst time it is sent, assuming no sender timeout (but the sender retransmits on duplicate), and a receiver timeout of 2 seconds. Continue the diagram to the point where Data[4] is successfully transmitted. Assume an RTT of 1 second. 2.0. Re-draw the Sorcerer‚Äôs Apprentice diagram of 8.1.2 Sorcerer‚Äôs Apprentice Bug, assuming the sender now does notretransmit on duplicates, though the receiver still does. ACK[3] is, as before, delayed until the sender retransmits Data[3]. 3.0. Suppose a stop-and-wait receiver has an implementation Ô¨Çaw. When Data[1] arrives, ACK[1] and ACK[2] are sent, separated by a brief interval; after that, the receiver transmits ACK[N+1] when Data[N] arrives, rather than the correct ACK[N]. (a). Draw a diagram, including at least three RTTs. Assume no packets are lost. (b). What is the average throughput, in data packets per RTT? (For normal stop-and-wait, the average throughput is 1.) (c). Is there anything the sender can do to detect this receiver behavior before the Ô¨Ånal packet, assuming no packets are lost and that the sender must respond to each ACK as soon as it arrives? (Note that if a Data packet islost, the receiver may already have acknowledged it, which creates a problem.) 4.0.‚ô¢Consider the alternative model of 8.3.1 Simple Ô¨Åxed-window-size analysis: C S1infinitely fast S21 pkt/sec D1 pkt/sec (a). Using the formulas of 8.3.2 RTT Calculations, calculate the steady-state queue usage for a window size of 6. 8.4 Epilog 195
An Introduction to Computer Networks, Release 2.0.11 (b). Again for a window size of 6, create a table like those in 8.3.1 Simple Ô¨Åxed-window-size analysis up through T=8 seconds. 5.0. Create a table as in 8.3.1 Simple Ô¨Åxed-window-size analysis for the original A R1 R2 R3 R4 B network with winsize = 8. As in the text examples, assume 1 packet/sec bandwidth delay for the R1 √ù√ëR2, R2√ù√ëR3, R3√ù√ëR4 and R4√ù√ëB links. The A‚ÄìR1 link and all reverse links (from B to A) are inÔ¨Ånitely fast. Carry out the table for 10 seconds. 6.0. Create a table as in 8.3.1 Simple Ô¨Åxed-window-size analysis for a network A R1 R2 B. The A‚ÄìR1 ink is inÔ¨Ånitely fast; the R1‚ÄìR2 and R2‚ÄìB each have a 1-second propagation delay, in each direction, and zero bandwidth delay (that is, one packet takes 1.0 sec to travel from R1 to R2; two packets also take 1.0 sec to travel from R1 to R2). Assume winsize=6. Carry out the table for 8 seconds. Note that with zero bandwidth delay, multiple packets sent together will remain together until the destination; propagation delay behaves very differently from bandwidth delay! 7.0. Suppose RTT noLoad = 4 seconds and the bottleneck bandwidth is 1 packet every 2 seconds. (a). What window size is needed to remain just at the knee of congestion? (b). Suppose winsize=6. What is the eventual value of RTT actual? (c). Again with winsize=6, how many packets are in the queue at the steady state? 8.0. Create a table as in 8.3.1 Simple Ô¨Åxed-window-size analysis for a network A R1 R2 R3 B. The A‚ÄìR1 link is inÔ¨Ånitely fast. The R1‚ÄìR2 and R3‚ÄìB links have a bandwidth delay of 1 packet/second with no additional propagation delay. The R2‚ÄìR3 link has a bandwidth delay of 1 packet / 2 seconds, and no propagation delay. The reverse B √ù√ëA direction (for ACKs) is inÔ¨Ånitely fast. Assume winsize = 6. (a). Carry out the table for 10 seconds. Note that you will need to show the queue for both R1 and R2. (b). Continue the table at least partially until T=18, in sufÔ¨Åcient detail that you can verify that RTT actual for packet 8 is as calculated in the previous exercise. To do this you will need more than 10 packets, but fewer than 16; the use of hex labels A, B, C for packets 10, 11, 12 is a convenient notation. Hint: The column for ‚ÄúR2 sends‚Äù (or, more accurately, ‚ÄúR2 is in the process of sending‚Äù) should look like this: T R2 sends 0 1 1 2 1 3 2 4 2 5 3 6 3. .. .. . 196 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 9.0. Create a table as in 8.3.1 Simple Ô¨Åxed-window-size analysis for a network A R1 R2 B. The A‚ÄìR1 link is inÔ¨Ånitely fast. The R1‚ÄìR2 link has a bandwidth delay of 1 packet / 2 seconds and the R2‚ÄìB link has a bandwidth delay of 1 packet/second, each with no additional propagation delay. The reverse B √ù√ëA direction (for ACKs) is inÔ¨Ånitely fast. Assume winsize = 4. Recommended columns are Time, ‚ÄúA sends‚Äù, ‚ÄúR1 queues‚Äù, ‚ÄúR1 sends‚Äù, ‚ÄúR2 sends‚Äù and ‚ÄúB Acks‚Äù. Hint: the ‚ÄúR1 sends‚Äù column will look like the ‚ÄúR2 sends‚Äù column for the hint in the previous problem, except it will start at T=0 rather than T=1. Note that bandwidth RTT noLoad = 1/23 = 1.5, and so, by 8.3.2 RTT Calculations equation 4, the queue utilization will be 4 ‚Äì 1.5 = 2.5. (a). Carry out the table for 8 seconds. (b). In the steady state, does R1 have 2.5 packets in the queue? If so, what is meant by half a packet? 10.0 Argue that, if A sends to B using sliding windows, and in the path from A to B the slowest link is not the Ô¨Årst link out of A, then eventually A will have the entire window outstanding (except at the instant just after each new ACK comes in). 11.0.‚ô¢Suppose RTT noLoad is 100 ms and the available bandwidth is 1,000 packets/sec. Sliding windows is used. (a). What is the transit capacity for the connection? (b). If RTT actual rises to 130 ms (due to use of a larger winsize), how many packets are in a queue at any one time? (c). If winsize increases by 50, what is RTT actual? 12.0. Suppose RTT noLoad is 50 ms and the available bandwidth is 2,000 packets/sec (2 packets/ms). Sliding windows is used for transmission. (a). What window size is needed to remain just at the knee of congestion? (b). If RTT actual rises to 60 ms (due to use of a larger winsize), how many packets are in a queue at any one time? (c). What value of winsize would lead to RTT actual = 60 ms? (d). What value of winsize would make RTT actual rise to 100 ms? 13.0. Suppose stop-and-wait is used (winsize=1), and assume that while packets may be lost, they are never reordered (that is, if two packets P1 and P2 are sent in that order, and both arrive, then they arrive in that order). Show that at the point the receiver is waiting for Data[N], the only two packet arrivals possible are Data[N] and Data[N-1]. (A consequence is that, in the absence of reordering, stop-and-wait can make do with 1-bit packet sequence numbers.) Hint: if the receiver is waiting for Data[N], it must have just received Data[N-1] and sent ACK[N-1]. Also, once the sender has sent Data[N], it will never transmit a Data[K] with K<N. 8.5 Exercises 197
An Introduction to Computer Networks, Release 2.0.11 14.0.‚ô¢Suppose winsize=4 in a sliding-windows connection, and assume that while packets may be lost, they are never reordered (that is, if two packets P1 and P2 are sent in that order, and both arrive, then they arrive in that order). Show that if Data[8] is in the receiver‚Äôs window (meaning that everything up through Data[4] has been received and acknowledged), then it is no longer possible for even a late Data[0] to arrive at the receiver. (A consequence of the general principle here is that ‚Äì in the absence of reordering ‚Äì we can replace the packet sequence number with (sequence_number) mod (2 winsize+1) without ambiguity.) 15.0. Suppose winsize=4 in a sliding-windows connection, and assume as in the previous exercise that while packets may be lost, they are never reordered. Give an example in which Data[8] is in the receiver‚Äôs window (so the receiver has presumably sent ACK[4]), and yet Data[1] legitimately arrives. (Thus, the late-packet bound in the previous exercise is the best possible.) 16.0. Suppose the network is A R1 R2 B, where the A‚ÄìR1 ink is inÔ¨Ånitely fast and the R1‚ÄìR2 link has a bandwidth of 1 packet/second each way, for an RTT noLoad of 2 seconds. Suppose also that A begins sending with winsize = 6. By the analysis in 8.3.1.3 Case 3: winsize = 6, RTT should rise to winsize/bandwidth = 6 seconds. Give the RTTs of the Ô¨Årst eight packets. How long does it take for RTT to rise to 6 seconds? 17.0. In this exercise we look at the relationship between bottleneck bandwidth and winsize/RTT actual when the former changes suddenly. Suppose the network is as follows A R1 R2 R3 B The A R1 link is inÔ¨Ånitely fast. The R1 √ëR2 link has a 1 packet/sec bandwidth delay in the R1 √ëR2 direction. The remaining links R2 √ëR3 and R3√ëB have a 1 sec bandwidth delay in the direction indicated. ACK packets, being small, travel instantaneously from B back to A. A sends to B using a winsize of three. Three packets P0, P1 and P2 are sent at times T=0, T=1 and T=2 respectively. At T=3, P0 arrives at B. At this instant the R1 √ëR2 bandwidth is suddenly halved to 1 packet / 2 sec; P3 is transmitted at T=3 and arrives at R2 at T=5. It will arrive at B at T=7. (a). Complete the following table of packet arrival times T A sends R1‚Äôs queue R1 sends R2 sends R3 sends B recvs/ACKs 2 P2 P2 P1 P0 3 P3 P3 P2 P1 P0 4 P4 P4 P3 cont P2 P1 5 P5 P5 P4 P3 P2 6 P5 P4 cont P3 7 P6 P3 8 9 P7 10 11 P8 198 8 Abstract Sliding Windows
An Introduction to Computer Networks, Release 2.0.11 (b). For each of P2, P3, P4 and P5, calculate the througput given by winsize/RTT over the course of that packet‚Äôs round trip. Obtain each packet‚Äôs RTT from the completed table above. (c). Once the steady state is reached in which RTT actual = 6, how much time does each packet spend in transit? How much time does each packet spend in R1‚Äôs queue? 8.5 Exercises 199
An Introduction to Computer Networks, Release 2.0.11 200 8 Abstract Sliding Windows
9 IP VERSION 4 There are multiple LAN protocols below the IP layer and multiple transport protocols above, but IP itself stands alone. The Internet is the IP Internet. If you want to run your own LAN protocol somewhere, or if you want to run your own transport protocol, the Internet backbone will still work just Ô¨Åne for you. But if you want to change the IP layer, you will encounter difÔ¨Åculty. (Just talk to the IPv6 people, or the IP-multicasting or IP-reservations groups.) In this chapter we discuss the original core IP protocol ‚Äì known as version 4, or IPv4, and with a 32-bit address size. Most of the Internet today (2020) still uses IPv4, though IPv6 is making inroads. We will see how the IP layer enables efÔ¨Åcient, scalable routing. In the following chapter we discuss some companion protocols: ICMP, ARP, DHCP and DNS. Despite its ubiquity, IPv4 faces an unsettled future: the Internet has run out of new large blocks of IPv4 addresses ( 1.10 IP - Internet Protocol ). There is therefore increasing pressure to convert to IPv6, with its 128-bit address size. Progress has been slow, however, and delaying tactics such as IPv4-address markets and NAT ( 9.7 Network Address Translation ) ‚Äì by which multiple hosts can share a single public IPv4 address ‚Äì have allowed IPv4 to continue. Aside from the major change in address structure, there are relatively few differences in the routing models of IPv4 and IPv6. We will study IPv4 in this and the following chapters, and IPv6 in 11 IPv6; at points where the IPv4/IPv6 difference doesn‚Äôt much matter we will simply write ‚ÄúIP‚Äù. IPv4 (and IPv6) is, in effect, a universal routing and addressing protocol. Routing and addressing are developed together; every node has an IP address and every router knows how to handle IP addresses. IP was originally seen as a way to inter connect multiple LANs, but it may make more sense now to view IP as a virtual LAN overlaying all the physical LANs. A crucial aspect of IP is its scalability. As of 2019 the Internet had over 109hosts; this estimate is probably low. However, at the same time the size of the largest forwarding tables was still under 106(15.5 BGP Table Size ). Ethernet, in comparison, scales poorly, as the forwarding tables need one entry for every active host. How many IPv4 hosts are there? Counting the size of the Internet is not easy. The Internet Systems Consortium used to run a survey based on DNS; it reached one billion hosts in 2012 and leveled off there. But not all devices get a DNS entry; behind NAT routers, few do. According to InternetLiveStats.com, as of 2019 there were four billion Internet users, about 50% of the world‚Äôs population, counting an ‚ÄúInternet user‚Äù as someone who could access the Internet in their own home. Facebook reported 2.4 billion monthly active users in June 2019. Furthermore, IP, unlike Ethernet, offers excellent support for multiple redundant links. If the network below were an IP network, each node would communicate with each immediate neighbor via their shared direct link. If, on the other hand, this were an Ethernet network with the spanning-tree algorithm, then one of the four links would simply be disabled completely. 201
An Introduction to Computer Networks, Release 2.0.11 A C DB The IP network service model is to act like a giant LAN. That is, there are no acknowledgments; delivery is generally described as best-effort. This design choice is perhaps surprising, but it has also been quite fruitful. If you want to provide a universal service for delivering any packet anywhere, what else do you need besides routing and addressing? Every network (LAN) needs to be able to carry any packet. The protocols spell out the use of octets (bytes), so the only possible compatibility issue is that a packet is too large for a given network. IPv4 handles this by supporting fragmentation: a network may break a too-large packet up into units it can transport successfully. While IPv4 fragmentation is inefÔ¨Åcient and clumsy, it does guarantee that any packet can potentially be delivered to any node. (Note, however, that IPv6 has given up on universal fragmentation; 11.5.4 IPv6 Fragment Header .) 9.1 The IPv4 Header The IPv4 Header needs to contain the following information: 
- destination and source addresses 
- indication of ipv4 versus ipv6 
- a Time To Live (TTL) value, to prevent inÔ¨Ånite routing loops 
- a Ô¨Åeld indicating what comes next in the packet ( egTCP v UDP) 
- Ô¨Åelds supporting fragmentation and reassembly. The header is organized as a series of 32-bit words as follows: Version Total Length IHL Fragment Offset Time to LiveECN Identification FlagsDS field Header Checksum Protocol Source Address Destination Address IPv4 Options (0-10 rows) Padding8 16 0 24 32 The IPv4 header, and basics of IPv4 protocol operation, were originally deÔ¨Åned in RFC 791; some minor changes have since occurred. Most of these changes were documented in RFC 1122, though the DS Ô¨Åeld 202 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 was deÔ¨Åned in RFC 2474 and the ECN bits were Ô¨Årst proposed in RFC 2481. TheVersion Ô¨Åeld is, for IPv4, the number 4: 0100. The IHL Ô¨Åeld represents the total IPv4 Header Length, in 32-bit words; an IPv4 header can thus be at most 15 words long. The base header takes up Ô¨Åve words, so the IPv4 Options can consist of at most ten words. If one looks at IPv4 packets using a packet-capture tool that displays the packets in hex, the Ô¨Årst byte will most often be 0x45. The Differentiated Services (DS) Ô¨Åeld is used by the Differentiated Services suite to specify preferential handling for designated packets, egthose involved in V oIP or other real-time protocols. The Explicit Congestion NotiÔ¨Åcation bits are there to allow routers experiencing congestion to mark packets, thus indicating to the sender that the transmission rate should be reduced. We will address these in 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN). These two Ô¨Åelds together replace the old 8-bit Type of Service Ô¨Åeld. TheTotal Length Ô¨Åeld is present because an IPv4 packet may be smaller than the minimum LAN packet size (see Exercise 1) or larger than the maximum (if the IPv4 packet has been fragmented over several LAN packets. The IPv4 packet length, in other words, cannot be inferred from the LAN-level packet size. Because the Total Length Ô¨Åeld is 16 bits, the maximum IPv4 packet size is 216bytes. This is probably much too large, even if fragmentation were not something to be avoided (though see IPv6 ‚Äújumbograms‚Äù in 11.5.1 Hop-by-Hop Options Header ). The second word of the header is devoted to fragmentation, discussed below at 9.4 Fragmentation. TheTime-to-Live (TTL) Ô¨Åeld is decremented by 1 at each router; if it reaches 0, the packet is discarded. A typical initial value is 64; it must be larger than the total number of hops in the path. In most cases, a value of 32 would work. The TTL Ô¨Åeld is there to prevent routing loops ‚Äì always a serious problem should they occur ‚Äì from consuming resources indeÔ¨Ånitely. Later we will look at various IP routing-table update protocols and how they minimize the risk of routing loops; they do not, however, eliminate it. By comparison, Ethernet headers have no TTL Ô¨Åeld, but Ethernet also disallows cycles in the underlying topology. TheProtocol Ô¨Åeld contains a value to identify the contents of the packet body. A few of the more common values are 
- 1: an ICMP packet, 10.4 Internet Control Message Protocol 
- 4: an encapsulated IPv4 packet, 9.9.1 IP-in-IP Encapsulation 
- 6: a TCP packet 
- 17: a UDP packet 
- 41: an encapsulated IPv6 packet, 12.6 IPv6 Connectivity via Tunneling 
- 50: an Encapsulating Security Payload, 29.6 IPsec A list of assigned protocol numbers is maintained by the IANA. TheHeader Checksum Ô¨Åeld is the ‚ÄúInternet checksum‚Äù applied to the header only, not the body. Its only purpose is to allow the discarding of packets with corrupted headers. When the TTL value is decremented the router must update the header checksum. This can be done ‚Äúalgebraically‚Äù by adding a 1 in the correct place to compensate, but it is not hard simply to re-sum the 8 halfwords of the average header. The header checksum must also be updated when an IPv4 packet header is rewritten by a NAT router. TheSource andDestination Address Ô¨Åelds contain, of course, the IPv4 addresses. These would normally be updated only by NAT Ô¨Årewalls. 9.1 The IPv4 Header 203
An Introduction to Computer Networks, Release 2.0.11 The source-address Ô¨Åeld is supposed to be the sender‚Äôs IPv4 address, but hardly any ISP checks that trafÔ¨Åc they send out has a source address matching one of their customers, despite the call to do so in RFC 2827. As a result, IP spooÔ¨Ång ‚Äì the sending of IP packets with a faked source address ‚Äì is straightforward. For some examples, see 18.3.1 ISNs and spooÔ¨Ång, and SYN Ô¨Çooding at 17.3 TCP Connection Establishment. IP-address spooÔ¨Ång also facilitates an all-too-common IP-layer denial-of-service attack in which a server is Ô¨Çooded with a huge volume of trafÔ¨Åc so as to reduce the bandwidth available to legitimate trafÔ¨Åc to a trickle. This Ô¨Çooding trafÔ¨Åc typically originates from a large number of compromised machines. Without spooÔ¨Ång, even a lengthy list of sources can be blocked, but, with spooÔ¨Ång, this becomes quite difÔ¨Åcult. One IPv4 option is the Record Route option, in which routers are to insert their own IPv4 address into the IPv4 header option area. Unfortunately, with only ten words available, there is not enough space to record most longer routes (but see 10.4.1 Traceroute and Time Exceeded, below). The Timestamp option is related; intermediate routers are requested to mark packets with their address and a local timestamp (to save space, the option can request only timestamps). There is room for only four xaddress,timestamp ypairs, but addresses can be prespeciÔ¨Åed; that is, the sender can include up to four IPv4 addresses and only those routers will Ô¨Åll in a timestamp. Another option, now deprecated as security risk, is to support source routing. The sender would insert into the IPv4 header option area a list of IPv4 addresses; the packet would be routed to pass through each of those IPv4 addresses in turn. With strict source routing, the IPv4 addresses had to represent adjacent neighbors; no router could be used if its IPv4 address were not on the list. With loose source routing, the listed addresses did not have to represent adjacent neighbors and ordinary IPv4 routing was used to get from one listed IPv4 address to the next. Both forms are essentially never used, again for security reasons: if a packet has been source-routed, it may have been routed outside of the at-least-somewhat trusted zone of the Internet backbone. Finally, the IPv4 header was carefully laid out with memory alignment in mind. The 4-byte address Ô¨Åelds are aligned on 4-byte boundaries, and the 2-byte Ô¨Åelds are aligned on 2-byte boundaries. All this was once considered important enough that incoming packets were stored following two bytes of padding at the head of their containing buffer, so the IPv4 header, starting after the 14-byte Ethernet header, would be aligned on a 4-byte boundary. Today, however, the architectures for which this sort of alignment mattered have mostly faded away; alignment is a non-issue for ARM and Intel x86 processors. 9.2 Interfaces IP addresses (both IPv4 and IPv6) are, strictly speaking, assigned not to hosts or nodes, but to interfaces. In the most common case, where each node has a single LAN interface, this is a distinction without a difference. In a room full of workstations each with a single Ethernet interface eth0 (or perhaps Ethernet adapter Local Area Connection ), we might as well view the IP address assigned to the interface as assigned to the workstation itself. Each of those workstations, however, likely also has a loopback interface (at least conceptually), providing a way to deliver IP packets to other processes on the same machine. On many systems, the name ‚Äúlocalhost‚Äù resolves to the IPv4 loopback address 127.0.0.1 (the IPv6 address ::1 is also used); see 9.3 Special Addresses. Delivering packets to the loopback interface is simply a form of interprocess communication; a functionally similar alternative is named pipes. Loopback delivery avoids the need to use the LAN at all, or even the need to have a LAN. For simple 204 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 client/server testing, it is often convenient to have both client and server on the same machine, in which case the loopback interface is a convenient (and fast) standin for a ‚Äúreal‚Äù network interface. On unix-based machines the loopback interface represents a genuine logical interface, commonly named lo. On Windows systems the ‚Äúinterface‚Äù may not represent an actual operating-system entity, but this is of practical concern only to those interested in ‚ÄúsnifÔ¨Ång‚Äù all loopback trafÔ¨Åc; packets sent to the loopback address are still delivered as expected. Workstations often have special other interfaces as well. Most recent versions of Microsoft Windows have a Teredo Tunneling pseudo-interface and an Automatic Tunneling pseudo-interface; these are both intended (when activated) to support IPv6 connectivity when the local ISP supports only IPv4. The Teredo protocol is documented in RFC 4380. When VPN connections are created, as in 5.1 Virtual Private Networks, each end of the logical connection typically terminates at a virtual interface (one of these is labeled tun0 in the diagram of 5.1 Virtual Private Networks ). These virtual interfaces appear, to the systems involved, to be attached to a point-to-point link that leads to the other end. When a computer hosts a virtual machine, there is almost always a virtual network to connect the host and virtual systems. The host will have a virtual interface to connect to the virtual network. The host may act as a NAT router for the virtual machine, ‚Äúhiding‚Äù that virtual machine behind its own IP address, or it may act as an Ethernet switch, in which case the virtual machine will need an additional public IP address. What‚Äôs My IP Address? This simple-seeming question is in fact not very easy to answer, if by ‚Äúmy IP address‚Äù one means the IP address assigned to the interface that connects directly to the Internet. One strategy is to Ô¨Ånd the address of the default router, and then iterate through all interfaces ( egwith the Java NetworkInterface class) to Ô¨Ånd an IP address with a matching network preÔ¨Åx; a Python3 example of this approach appears in30.5.1 Multicast Programming. Unfortunately, Ô¨Ånding the default router (to identify the primary interface) is hard to do in an OS-independent way, and even then this approach can fail if the Wi-Fi and Ethernet interfaces both are assigned IP addresses on the same network, but only one is actually connected. Routers always have at least two interfaces on two separate IP networks. Generally this means a separate IP address for each interface, though some point-to-point interfaces can be used without being assigned any IP address ( 9.8 Unnumbered Interfaces ). 9.2.1 Multihomed hosts A non-router host with multiple non-loopback network interfaces is often said to be multihomed. Many laptops, for example, have both an Ethernet interface and a Wi-Fi interface. Both of these can be used simultaneously, with different IP addresses assigned to each. On residential networks the two interfaces will often be on the same IP network ( egthe same bridged Wi-Fi/Ethernet LAN); at more security-conscious sites the Ethernet and Wi-Fi interfaces are often on quite different IP networks (though see 10.2.5 ARP and multihomed hosts ). Multiple physical interfaces are not actually needed here; it is usually possible to assign multiple IP addresses to a single interface. Sometimes this is done to allow two IP networks (two distinct preÔ¨Åxes) to 9.2 Interfaces 205
An Introduction to Computer Networks, Release 2.0.11 share a single physical LAN; in this case the interface would be assigned one IP address for each IP network. Other times a single interface is assigned multiple IP addresses on the same IP network; this is often done so that one physical machine can act as a server ( ega web server) for multiple distinct IP addresses corresponding to multiple distinct domain names. Multihoming raises some issues with packets addressed to one interface, A, with IP address A IP, but which arrive via another interface, B, with IP address B IP. Strictly speaking, such arriving packets should be discarded unless the host is promoted to functioning as a router. In practice, however, the strict interpretation often causes problems; a typical user understanding is that the IP address A IPshould work to reach the host even if the physical connection is to interface B. A related issue is whether the host receiving such a packet addressed to A IPon interface B is allowed to send its reply with source address A IP, even though the reply must be sent via interface B. RFC 1122, ¬ß3.3.4, deÔ¨Ånes two alternatives here: 
- The Strong End-System model: IP addresses ‚Äì incoming and outbound ‚Äì must match the physical interface. 
- The Weak End-System model: A match is not required: interface B can accept packets addressed to AIP, and send packets with source address A IP. Linux systems generally use the weak model by default. See also 10.2.5 ARP and multihomed hosts. While it is important to be at least vaguely aware of the special cases that multihoming presents, we emphasize again that in most ordinary contexts each end-user workstation has one IP address that corresponds to a LAN connection. 9.3 Special Addresses A few IPv4 addresses represent special cases. While the standard IPv4 loopback address is 127.0.0.1, any IPv4 address beginning with 127 can serve as a loopback address. Logically they all represent the current host. Most hosts are conÔ¨Ågured to resolve the name ‚Äúlocalhost‚Äù to 127.0.0.1. However, any loopback address ‚Äì eg127.255.37.59 ‚Äì should work, egwith ping. For an example using 127.0. 1.0, see 10.1 DNS. Private addresses are IPv4 addresses intended only for site internal use, egeither behind a NAT Ô¨Årewall or intended to have no Internet connectivity at all. If a packet shows up at any non-private router ( egat an ISP router), with a private IPv4 address as either source or destination address, the packet should be dropped. Three standard private-address blocks have been deÔ¨Åned: 
- 10.0.0.0/8 
- 172.16.0.0/12 
- 192.168.0.0/16 The last block is the one from which addresses are most commonly allocated by DHCP servers ( 10.3.1 NAT, DHCP and the Small OfÔ¨Åce ) built into NAT routers. There are subtle issues with private addresses. First of all, when organizations merge, wholesale privateaddress renumbering is usually necessary. Second, suppose Alice uses 10.0.0.0/8 for her home network, where her laptop is 10.0.0.23. Suppose also that Alice connects via a VPN to work, and her server at work 206 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 is also 10.0.0.23. Connection between laptop and server will then fail. It would also fail if the server were 10.0.0.24, as Alice‚Äôs laptop will think that should be on the local subnet, and it is not. There are other potential conÔ¨Çicts as well. Perhaps the main reason these problems are not worse than they are is that most home networks use 192.168.0.0/16, and most corporate networks use one of the other two private-address blocks. There is an additional problem with mobile-phone networks. Most phones get a mobile-network IPv4 address from the carrier, and also a Wi-Fi IPv4 address from whatever Wi-Fi network the phone owner is currently connected to. If the carrier uses any of the above private-address blocks, there is a fair chance that, at some Wi-Fi-providing establishment, someone‚Äôs carrier-assigned mobile IPv4 address will conÔ¨Çict with their Wi-Fi address. The addresses may even be the same. Because of this, RFC 6598 has established the following special address block for mobile-device carriers, known as a shared-address block: 
- 100.64.0.0/10 It is exactly like a private-address block, except no one is to use it except mobile-device carriers. Broadcast addresses are a special form of IPv4 address intended to be used in conjunction with LAN-layer broadcast. The most common forms are ‚Äúbroadcast to this network‚Äù, consisting of all 1-bits, and ‚Äúbroadcast to network D‚Äù, consisting of D‚Äôs network-address bits followed by all 1-bits for the host bits. If you try to send a packet to the broadcast address of a remote network D, the odds are that some router involved will refuse to forward it, and the odds are even higher that, once the packet arrives at a router actually on network D, that router will refuse to broadcast it. Even addressing a broadcast to one‚Äôs own network will fail if the underlying LAN does not support LAN-level broadcast ( egATM). The highly inÔ¨Çuential early Unix implementation Berkeley 4.2 BSD used 0-bits for the broadcast bits, instead of 1‚Äôs. As a result, to this day host bits cannot be all 1-bits or all 0-bits in order to avoid confusion with the IPv4 broadcast address. One consequence of this is that a Class C network has 254 usable host addresses, not 256. 9.3.1 Multicast addresses Finally, IPv4 multicast addresses remain as the last remnant of the Class A/B/C strategy: multicast addresses are Class D, with Ô¨Årst byte beginning 1110 (meaning that the Ô¨Årst byte is, in decimal, 224-239). Multicasting means delivering to a speciÔ¨Åed setof addresses, preferably by some mechanism more efÔ¨Åcient than sending to each address individually. A reasonable goal of multicast would be that no more than one copy of the multicast packet traverses any given link. Support for IPv4 multicast requires considerable participation by the backbone routers involved. For example, if hosts A, B and C each connect to different interfaces of router R1, and A wishes to send a multicast packet to B and C, then it is up to R1 to receive the packet, Ô¨Ågure out that B and C are the intended recipients, and forward the packet twice, once for B‚Äôs interface and once for C‚Äôs. R1 must also keep track of what hosts have joined the multicast group and what hosts have left. Due to this degree of router participation, backbone router support for multicasting has not been entirely forthcoming. A discussion of IPv4 multicasting appears in 25 Quality of Service. 9.3 Special Addresses 207
An Introduction to Computer Networks, Release 2.0.11 9.4 Fragmentation If you are trying to interconnect two LANs (as IP does), what else might be needed besides Routing and Addressing? IPv4 (and IPv6) explicitly assumes all packets are composed on 8-bit bytes (something not universally true in the early days of IP; to this day the RFCs refer to ‚Äúoctets‚Äù to emphasize this requirement). IP also deÔ¨Ånes bit-order within a byte, and it is left to the networking hardware to translate properly. Neither byte size nor bit order, therefore, can interfere with packet forwarding. There is one more feature IPv4 must provide, however, if the goal is universal connectivity: it must accommodate networks for which the maximum packet size, or Maximum Transfer Unit, MTU, is smaller than the packet that needs forwarding. Otherwise, if we were using IPv4 to join Token Ring (MTU = 4kB, at least originally) to Ethernet (MTU = 1500B), the token-ring packets might be too large to deliver to the Ethernet side, or to traverse an Ethernet backbone en route to another Token Ring. (Token Ring, in its day, did commonly offer a conÔ¨Åguration option to allow Ethernet interoperability.) So, IPv4 must support fragmentation, and thus also reassembly. There are two potential strategies here: per-link fragmentation and reassembly, where the reassembly is done at the opposite end of the link (as in ATM), and path fragmentation and reassembly, where reassembly is done at the far end of the path. The latter approach is what is taken by IPv4, partly because intermediate routers are too busy to do reassembly (this is as true today as it was in 1981 when RFC 791 was published), partly because there is no absolute guarantee that all fragments will go to the same next-hop router, and partly because IPv4 fragmentation has always been seen as the strategy of last resort. An IPv4 sender is supposed to use a different value for the IDENT Ô¨Åeld for different packets, at least up until the Ô¨Åeld wraps around. When an IPv4 datagram is fragmented, the fragments keep the same IDENT Ô¨Åeld, so this Ô¨Åeld in effect indicates which fragments belong to the same packet. After fragmentation, the Fragment Offset Ô¨Åeld marks the start position of the data portion of this fragment within the data portion of the original IPv4 packet. Note that the start position can be a number up to 216, the maximum IPv4 packet length, but the FragOffset Ô¨Åeld has only 13 bits. This is handled by requiring the data portions of fragments to have sizes a multiple of 8 (three bits), and left-shifting the FragOffset value by 3 bits before using it. As an example, consider the following network, where MTUs are excluding the LAN header: A R1 R3 R2 A BMTU 1500 MTU 1000 MTU 400 MTU 1500 Suppose A addresses a packet of 1500 bytes to B, and sends it via the LAN to the Ô¨Årst router R1. The packet contains 20 bytes of IPv4 header and 1480 of data. R1 fragments the original packet into two packets of sizes 20+976 = 996 and 20+504=524. Having 980 bytes of payload in the Ô¨Årst fragment would Ô¨Åt, but violates the rule that the sizes of the data portions be divisible by 8. The Ô¨Årst fragment packet has FragOffset = 0; the second has FragOffset = 976. R2refragments the Ô¨Årst fragment into three packets as follows: 
- Ô¨Årst: size = 20+376=396, FragOffset = 0 
- second: size = 20+376=396, FragOffset = 376 208 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 
- third: size = 20+224 = 244 (note 376+376+224=976), FragOffset = 752. R2 refragments the second fragment into two: 
- Ô¨Årst: size = 20+376 = 396, FragOffset = 976+0 = 976 
- second: size = 20+128 = 148, FragOffset = 976+376=1352 R3 then sends the fragments on to B, without reassembly. Note that it would have been slightly more efÔ¨Åcient to have fragmented into four fragments of sizes 376, 376, 376, and 352 in the beginning. Note also that the packet format is designed to handle fragments of different sizes easily. The algorithm is based on multiple fragmentation with reassembly only at the Ô¨Ånal destination. Each fragment has its IPv4-header Total Length Ô¨Åeld set to the length of that fragment. We have not yet discussed the three Ô¨Çag bits. The Ô¨Årst bit is reserved, and must be 0. The second bit is theDon‚Äôt Fragment, or DF, bit. If it is set to 1 by the sender then a router must notfragment the packet and must drop it instead; see 18.6 Path MTU Discovery for an application of this. The third bit, the More Fragments bit, is set to 1 for all fragments except the Ô¨Ånal one (this bit is thus set to 0 if no fragmentation has occurred). The third bit tells the receiver where the fragments stop. That Ô¨Årst Ô¨Çag bit RFC 3514 proposes the use of the Ô¨Årst Ô¨Çag bit here as a security Ô¨Çag: it would be set to 0 for legitimate trafÔ¨Åc, while senders of malicious and hacking trafÔ¨Åc were supposed to set the bit to 1. This would make detection and Ô¨Årewalling of malicious trafÔ¨Åc much easier. The publication date of this RFC is April 1, or April Fool‚Äôs Day; there is a long tradition of RFC humor released that day. Another example is RFC 2324. The receiver must take the arriving fragments and reassemble them into a whole packet. The fragments may not arrive in order ‚Äì unlike in ATM networks ‚Äì and may have unrelated packets interspersed. The reassembler must identify when different arriving packets are fragments of the same original, and must Ô¨Ågure out how to reassemble the fragments in the correct order; both these problems were essentially trivial for ATM. Fragments are considered to belong to the same packet if they have the same IDENT Ô¨Åeld and also the same source and destination addresses and same protocol. As all fragment sizes are a multiple of 8 bytes, the receiver can keep track of whether all fragments have been received with a bitmap in which each bit represents one 8-byte fragment chunk. A 1 kB packet could have up to 128 such chunks; the bitmap would thus be 16 bytes. If a fragment arrives that is part of a new (and fragmented) packet, a buffer is allocated. While the receiver cannot know the Ô¨Ånal size of the buffer, it can usually make a reasonable guess. Because of the FragOffset Ô¨Åeld, the fragment can then be stored in the buffer in the appropriate position. A new bitmap is also allocated, and a reassembly timer is started. As subsequent fragments arrive, not necessarily in order, they too can be placed in the proper buffer in the proper position, and the appropriate bits in the bitmap are set to 1. 9.4 Fragmentation 209
An Introduction to Computer Networks, Release 2.0.11 If the bitmap shows that all fragments have arrived, the packet is sent on up as a completed IPv4 packet. If, on the other hand, the reassembly timer expires, then all the pieces received so far are discarded. TCP connections usually engage in Path MTU Discovery, and Ô¨Ågure out the largest packet size they can send that will notentail fragmentation ( 18.6 Path MTU Discovery ). But it is not unusual, for example, for UDP protocols to use fragmentation, especially over the short haul. In the Network File System (NFS) protocol, for example, UDP is used to carry 8 kB disk blocks. These are often sent as a single 8+ kB IPv4 packet, fragmented over Ethernet to Ô¨Åve full packets and a fraction. Fragmentation works reasonably well here because most of the time the packets do not leave the Ethernet they started on. Note that this is an example of fragmentation done by the sender, not by an intermediate router. Finally, any given IP link may provide its own link-layer fragmentation and reassembly; we saw in 5.5.1 ATM Segmentation and Reassembly that ATM does just this. Such link-layer mechanisms are, however, generally invisible to the IP layer. 9.5 The Classless IP Delivery Algorithm Recall from that any IPv4 address can be divided into a net portion IP netand a host portion IP host; the division point was determined by whether the IPv4 address was a Class A, a Class B, or a Class C. We also indicated in that the division point was not always so clear-cut; we now present the delivery algorithm, for both hosts and routers, that does notassume a globally predeclared division point of the input IPv4 address into net and host portions. We will, for the time being, punt on the question of forwarding-table lookup and assume there is a lookup() method available that, when given a destination address, returns the next_hop neighbor. Instead of class-based divisions, we will assume that each of the IPv4 addresses assigned to a node‚Äôs interfaces is conÔ¨Ågured with an associated length of the network preÔ¨Åx; following the slash notation of 1.10 IP - Internet Protocol, if B is an address and the preÔ¨Åx length is k = k Bthen the preÔ¨Åx itself is B/k. As usual, an ordinary host may have only one IP interface, while a router will always have multiple interfaces. Let D be the given IPv4 destination address; we want to decide if D is local ornonlocal. The host or router involved may have multiple IP interfaces, but for each interface the length of the network portion of the address will be known. For each network address B/k assigned to one of the host‚Äôs interfaces, we compare the Ô¨Årst k bits of B and D; that is, we ask if D matches B/k. 
- If one of these comparisons yields a match, delivery is local; the host delivers the packet to its Ô¨Ånal destination via the LAN connected to the corresponding interface. This means looking up the LAN address of the destination, if applicable, and sending the packet to that destination via the interface. 
- If there is no match, delivery is nonlocal, and the host passes D to the lookup() routine of the forwarding table and sends to the associated next_hop (which must represent a physically connected neighbor). It is now up to lookup() routine to make any necessary determinations as to how D might be split into D netand D host; the split cannot be made outside of lookup(). The forwarding table is, abstractly, a set of network addresses ‚Äì now also with lengths ‚Äì each of the form B/k, with an associated next_hop destination for each. The lookup() routine will, in principle, compare D with each table entry B/k, looking for a match (that is, equality of the Ô¨Årst k = k Bbits). As with the local-delivery interfaces check above, the net/host division point (that is, k) will come from the table entry; it will not be inferred from D or from any other information borne by the packet. There is, in fact, no place in the IPv4 header to store a net/host division point, and furthermore different routers along the path may use different 210 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 values of k with the same destination address D. Routers receive the preÔ¨Åx length /k for a destination B/k as part of the process by which they receive xdestination,next_hop ypairs; see 13 Routing-Update Algorithms. In14 Large-Scale IP Routing we will see that in some cases multiple matches in the forwarding table may exist, eg147.0.0.0/8 and 147.126.0.0/16. The longest-match rule will be introduced for such cases to pick the best match. Here is a simple example for a router with immediate neighbors A-E: destination next_hop 10.3.0.0/16 A 10.4.1.0/24 B 10.4.2.0/24 C 10.4.3.0/24 D 10.3.37.0/24 E The IPv4 addresses 10.3.67.101 and 10.3.59.131 both route to A. The addresses 10.4.1.101, 10.4.2.157 and 10.4.3.233 route to B, C and D respectively. Finally, 10.3.37.103 matches both A and E, but the E match is longer so the packet is routed that way. The forwarding table may also contain a default entry for the next_hop, which it may return in cases when the destination D does not match any known network. We take the view here that returning such a default entry is a valid result of the routing-table lookup() operation, rather than a third option to the algorithm above; one approach is for the default entry to be the next_hop corresponding to the destination 0.0.0.0/0, which does indeed match everything (use of this would deÔ¨Ånitely require the above longest-match rule, though). Default routes are hugely important in keeping leaf forwarding tables small. Even backbone routers sometimes expend considerable effort to keep the network address preÔ¨Åxes in their forwarding tables as short as possible, through consolidation. At a site with a single ISP and with no Internet customers (that is, which is not itself an ISP for others), the top-level forwarding table usually has a single external route: its default route to its ISP. If a site has more than one ISP, however, the top-level forwarding table can expand in a hurry. For example, Internet2 is a consortium of research sites with very-high-bandwidth internal interconnections, acting as a sort of ‚Äúparallel Internet‚Äù. Before Internet2, Loyola‚Äôs top-level forwarding table had the usual single external default route. After Internet2, we in effect had a second ISP and had to divide trafÔ¨Åc between the commercial ISP and the Internet2 ISP. The default route still pointed to the commercial ISP, but Loyola‚Äôs top-level forwarding table now had to have an entry for every individual Internet2 site, so that trafÔ¨Åc to any of these sites would be forwarded via the Internet2 ISP. See exercise 5.0. Routers may also be conÔ¨Ågured to allow passing quality-of-service information to the lookup() method, as mentioned in , to support different routing paths for different kinds of trafÔ¨Åc ( egbulk Ô¨Åle-transfer versus real-time). For a modest exception to the local-delivery rule described here, see below in 9.8 Unnumbered Interfaces. 9.5 The Classless IP Delivery Algorithm 211
An Introduction to Computer Networks, Release 2.0.11 9.5.1 EfÔ¨Åcient Forwarding-Table Lookup Fast implementation of the lookup() operation above is tricky, especially in the presence of destination entries that may not result in unique matches, such as 10.0.0.0/8 and 10.11.0.0/16, which both match 10.11.12.13. Straightforward hashing, in particular, is out, as the preÔ¨Åx-length value k is not available to the call oflookup(). The simplest approach is a trie, a form of tree in which the child nodes are labeled with bit strings; the concatenated node labels on a branch represent an address preÔ¨Åx. Node labels can represent single address bits or larger bit groups. A trie allows straightforward implementation of the longest-match rule by requiring that we descend in the trie until no further matches are possible. As an example, we construct a trie for the following forwarding table: destination next_hop 1.10.0.0/16 A 1.10.104.0/24 B 1.10.105.0/24 C 1.11.0.0/16 D 1.12.116.0/24 E 1.12.117.0/24 F As all the preÔ¨Åx lengths are multiples of 8 bits, we build the trie using bytes as node labels: 104 105 116 11712 11 101root Forwarding-table trie Heavy blue nodes represent matches; light nodes represent non-matches. There is one heavy node for each entry in the table above. To look up 1.10.105.213 in the trie, we view the address as a list x1,10,105,213y. From the root, we traverse nodes 1 and 10, arriving at a heavy node representing the destination 1.10.0.0/16. However, the next element of the address list is 105, and so we continue down to child node 105, thus matching 1.10.105.0/24. There are no further child nodes, so this match is as long as possible. A straightforward trie implementation for arbitrary preÔ¨Åx lengths requires single-bit node labels. However, 212 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 the Lule√• algorithm, [DBCP97], implements lookup with a trie of only three levels, representing address bits 0-15, 16-23 and 24-31. At the top of the trie, an array of size 216helps determine the Ô¨Årst child node; similar supplemental data structures assist with the child-node lookups at subsequent levels. Finally, high-performance switches often implement the lookup() operation using content-addressable memory. IP forwarding requires the ternary form of this memory, TCAM, described in the Ô¨Ånal paragraph of3.2.1 Switch Hardware. If A/k is an IP address preÔ¨Åx of length k, then A goes in the TCAM memory register, and the corresponding mask register is used to indicate that only the Ô¨Årst k bits matter. When an IPv4 address is presented to the TCAM for lookup, there may now be multiple matches of differing lengths. To implement the longest-match rule, we Ô¨Årst need to make sure that, in the TCAM sequence of registers, shorter preÔ¨Åxes come before longer ones. The TCAM encoder circuit then converts the matching register‚Äôs position in the sequence to an address corresponding to the register, with which the rest of the routing information can be retrieved. This encoder must be designed so that it always prefers longer preÔ¨Åxes (higher TCAM sequence positions). A longest-match lookup can then be performed in a single memorylookup cycle. When backbone IPv4 routers experience acute difÔ¨Åculties due to growth of the forwarding table, it is often because the existing table has outgrown the space available in the TCAM hardware. 9.6 IPv4 Subnets Subnets were the Ô¨Årst step away from Class A/B/C routing: a large network ( ega class A or B) could be divided into smaller IPv4 networks called subnets. Consider, for example, a typical Class B network such as Loyola University‚Äôs (originally 147.126.0.0/16); the underlying assumption is that any packet can be delivered via the underlying LAN to any internal host. This would require a rather large LAN, and would require that a single physical LAN be used throughout the site. What if our site has more than one physical LAN? Or is really too big for one physical LAN? It did not take long for the IP world to run into this problem. Subnets were Ô¨Årst proposed in RFC 917, and became ofÔ¨Åcial with RFC 950. Getting a separate IPv4 network preÔ¨Åx for each subnet is bad for routers: the backbone forwarding tables now must have an entry for every subnet instead of just for every site. What is needed is a way for a site to appear to the outside world as a single IP network, but for further IP-layer routing to be supported inside the site. This is what subnets accomplish. Subnets introduce hierarchical routing: Ô¨Årst we route to the primary network, then inside that site we route to the subnet, and Ô¨Ånally the last hop delivers to the host. Routing with subnets involves in effect moving the IP netdivision line rightward. (Later, when we consider CIDR, we will see the complementary case of moving the division line to the left.) For now, observe that moving the line rightward within a site does not affect the outside world at all; outside routers are not even aware of site-internal subnetting. In the following diagram, the outside world directs trafÔ¨Åc addressed to 147.126.0.0/16 to the router R. Internally, however, the site is divided into subnets. The idea is that trafÔ¨Åc from 147.126.1.0/24 to 147.126.2.0/24 is routed, not switched; the two LANs involved may not even be compatible (for example, the ovals might represent Token Ring while the lines represent Ethernet). Most of the subnets shown are of size /24, meaning that the third byte of the IPv4 address has become part of the network portion of the subnet‚Äôs address; 9.6 IPv4 Subnets 213
An Introduction to Computer Networks, Release 2.0.11 one /20 subnet is also shown. RFC 950 would have disallowed the subnet with third byte 0, but having 0 for the subnet bits generally does work. R R2147.126.0.0/24147.126.1.0/24147.126.2.0/24 147.126.16.0/20147.126.3.0/24Internet A D What we want is for the internal routing to be based on the extended network preÔ¨Åxes shown, while externally continuing to use only the single routing entry for 147.126.0.0/16. To implement subnets, we divide the site‚Äôs IPv4 network into some combination of physical LANs ‚Äì the subnets ‚Äì, and assign each a subnet address: an IPv4 network address which has the site‚Äôs IPv4 network address as preÔ¨Åx. To put this more concretely, suppose the site‚Äôs IPv4 network address is A, and consists of n network bits (so the site address may be written with the slash notation as A/n); in the diagram above, A/n = 147.126.0.0/16. A subnet address is an IPv4 network address B/k such that: 
- The address B/k is within the site: the Ô¨Årst n bits of B are the same as A/n‚Äôs 
- B/k extends A/n: k ¬•n An example B/k in the diagram above is 147.126.1.0/24. (There is a slight simpliÔ¨Åcation here in that subnet addresses do not absolutely have to be preÔ¨Åxes; see below.) We now have to Ô¨Ågure out how packets will be routed to the correct subnet. For incoming packets we could set up some proprietary protocol at the entry router to handle this. However, the more complicated situation is all those existing internal hosts that, under the class A/B/C strategy, would still believe they can deliver via the LAN to any site host, when in fact they can now only do that for hosts on their own subnet. We need a more general solution. We proceed as follows. For each subnet address B/k, we create a subnet mask for B consisting of k 1-bits followed by enough 0-bits to make a total of 32. We then make sure that every host and router in the site knows the subnet mask for every one of its interfaces. Hosts usually Ô¨Ånd their subnet mask the same way they Ô¨Ånd their IP address (by static conÔ¨Åguration if necessary, but more likely via DHCP, below). Hosts and routers now apply the IP delivery algorithm of the previous section, with the proviso that, if a subnet mask for an interface is present, then the subnet mask is used to determine the number of address bits 214 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 rather than the Class A/B/C mechanism. That is, we determine whether a packet addressed to destination D is deliverable locally via an interface with subnet address B/k and corresponding mask M by comparing D&M with B&M, where &represents bitwise AND; if the two match, the packet is local. This will generally involve a match of more bits than if we used the Class A/B/C strategy to determine the network portion of addresses D and B. As stated previously, given an address D with no other context, we will notbe able to determine the network/host division point in general ( egfor outbound packets). However, that division point is not in fact what we need. All that isneeded is a way to tell if a given destination host address D belongs to the current subnet, say B; that is, we need to compare the Ô¨Årst k bits of D and B where k is the (known) length of B. In the diagram above, the subnet mask for the /24 subnets would be 255.255.255.0; bitwise ANDing any IPv4 address with the mask is the same as extracting the Ô¨Årst 24 bits of the IPv4 address, that is, the subnet portion. The mask for the /20 subnet would be 255.255.240.0 (240 in binary is 1111 0000). In the diagram above none of the subnets overlaps or conÔ¨Çicts: the subnets 147.126.0.0/24 and 147.126.1.0/24 are disjoint. It takes a little more effort to realize that 147.126.16.0/20 does not overlap with the others, but note that an IPv4 address matches this network preÔ¨Åx only if the Ô¨Årst four bits of the third byte are 0001, so the third byte itself ranges from decimal 32 to decimal 63 = binary 0001 1111. Note also that if host A = 147.126.0.1 wishes to send to destination D = 147.126.1.1, and A is notsubnetaware, then delivery will fail: A will infer that the interface is a Class B, and therefore compare the Ô¨Årst two bytes of A and D, and, Ô¨Ånding a match, will attempt direct LAN delivery. But direct delivery is now likely impossible, as the subnets are not joined by a switch. Only with the subnet mask will A realize that its network is 147.126.0.0/ 24while D‚Äôs is 147.126.1.0/24 and that these are not the same. A would still be able to send packets to its own subnet. In fact A would still be able to send packets to the outside world: it would realize that the destination in that case does not match 147.126.0.0/16 and will thus forward to its router. Hosts on other subnets would be the only unreachable ones. Properly, the subnet address is the entire preÔ¨Åx, eg147.126.65.0/24. However, it is often convenient to identify the subnet address with just those bits that represent the extension of the site IPv4-network address; we might thus say casually that the subnet address here is 65. The class-based IP-address strategy allowed any host anywhere on the Internet to properly separate any address into its net and host portions. With subnets, this division point is now allowed to vary; for example, the address 147.126.65.48 divides into 147.126 | 65.48 outside of Loyola, but into 147.126.65 | 48 inside. This means that the net-host division is no longer an absolute property of addresses, but rather something that depends on where the packet is on its journey. Technically, we also need the requirement that given any two subnet addresses of different, disjoint subnets, neither is a proper preÔ¨Åx of the other. This guarantees that if A is an IP address and B is a subnet address with mask M (so B = B&M), then A&M = B implies A does not match any other subnet. Regardless of the net/host division rules, we cannot possibly allow subnet 147.126.16.0/20 to represent one LAN while 147.126.16.0/24 represents another; the second subnet address block is a subset of the Ô¨Årst. (We can, and sometimes do, allow the Ô¨Årst LAN to correspond to everything in 147.126.16.0/20 that is not also in 147.126.16.0/24; this is the longest-match rule.) The strategy above is actually a slight simpliÔ¨Åcation of what the subnet mechanism actually allows: subnet address bits do not in fact have to be contiguous, and masks do not have to be a series of 1-bits followed by 0-bits. The mask can be anybit-mask; the subnet address bits are by deÔ¨Ånition those where there is a 1 in the mask bits. For example, we could at a Class-B site use the fourth byte as the subnet address, and the third byte as the host address. The subnet mask would then be 255.255.0.255. While this generality was once 9.6 IPv4 Subnets 215
An Introduction to Computer Networks, Release 2.0.11 sometimes useful in dealing with ‚Äúlegacy‚Äù IPv4 addresses that could not easily be changed, life is simpler when the subnet bits precede the host bits. 9.6.1 Subnet Example As an example of having different subnet masks on different interfaces, let us consider the division of a class-C network into subnets of size 70, 40, 25, and 20. The subnet addresses will of necessity have different lengths, as there is not room for four subnets each able to hold 70 hosts. 
- A: size 70 
- B: size 40 
- C: size 25 
- D: size 20 Because of the different subnet-address lengths, division of a local IPv4 address LA into net versus host on subnets cannot be done in isolation, without looking at the host bits. However, that division is not in fact what we need. All that is needed is a way to tell if the local address LA belongs to a given subnet, say B; that is, we need to compare the Ô¨Årst n bits of LA and B, where n is the length of B‚Äôs subnet mask. We do this by comparing LA&M to B&M, where M is the mask corresponding to n. LA&M is not necessarily the same as LA net, if LA actually belongs to one of the other subnets. However, if LA&M = B&M, then LA must belong subnet B, in which case LA&M is in fact LA net. We will assume that the site‚Äôs IPv4 network address is 200.0.0.0/24. The Ô¨Årst three bytes of each subnet address must match 200.0.0. Only some of the bits of the fourth byte will be part of the subnet address, so we will switch to binary for the last byte, and use both the /n notation (for total number of subnet bits) and also add a vertical bar | to mark the separation between subnet and host. Example: 200.0.0.10 | 00 0000 / 26 Note that this means that the 0-bit following the 1-bit in the fourth byte is ‚ÄúsigniÔ¨Åcant‚Äù in that for a subnet to match, it must match this 0-bit exactly. The remaining six 0-bits are part of the host portion. To allocate our four subnet addresses above, we start by Ô¨Åguring out just how many host bits we need in each subnet. Subnet sizes are always powers of 2, so we round up the subnets to the appropriate size. For subnet A, this means we need 7 host bits to accommodate 27= 128 hosts, and so we have a single bit in the fourth byte to devote to the subnet address. Similarly, for B we will need 6 host bits and will have 2 subnet bits, and for C and D we will need 5 host bits each and will have 8-5=3 subnet bits. We now start choosing non-overlapping subnet addresses. We have one bit in the fourth byte to choose for A‚Äôs subnet; rather arbitrarily, let us choose this bit to be 1. This means that every other subnet address must have a 0 in the Ô¨Årst bit position of the fourth byte, or we would have ambiguity. Now for B‚Äôs subnet address. We have two bits to work with, and the Ô¨Årst bit must be 0. Let us choose the second bit to be 0 as well. If the fourth byte begins 00, the packet is part of subnet B, and the subnet addresses for C and D must therefore notbegin 00. Finally, we choose subnet addresses for C and D to be 010 and 011, respectively. We thus have 216 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 subnet size address bits in fourth byte host bits in 4th byte decimal range A 128 1 7 128-255 B 64 00 6 0-63 C 32 010 5 64-95 D 32 011 5 96-127 As desired, none of the subnet addresses in the third column is a preÔ¨Åx of any other subnet address. The end result of all of this is that routing is now hierarchical: we route on the site IP address to get to a site, and then route on the subnet address within the site. 9.6.2 Links between subnets Suppose the Loyola CS department subnet (147.126.65.0/24) and a department at some other site, we will say 147.100.100.0/24, install a private link. How does this affect routing? Each department router would add an entry for the other subnet, routing along the private link. TrafÔ¨Åc addressed to the other subnet would take the private link. All other trafÔ¨Åc would go to the default router. TrafÔ¨Åc from the remote department to 147.126.6 4.0/24 would take the long route, and Loyola trafÔ¨Åc to 147.100.10 1.0/24 would take the long route. Subnet anecdote A long time ago I was responsible for two hosts, abel and borel. One day I was informed that machines in computer lab 1 at the other end of campus could not reach borel, though they could reach abel. Machines in lab 2, adjacent to lab 1, however, could reach both borel and abel just Ô¨Åne. What was the difference? It turned out that borel had a bad (/16 instead of /24) subnet mask, and so it was attempting local delivery to the labs. This should have meant it could reach neither of the labs, as both labs were on a different subnet from my machines; I was still perplexed. After considerably more investigation, it turned out that between abel/borel and the lab building was a bridge-router: a hybrid device that properly routed subnet trafÔ¨Åc at the IP layer, but which also forwarded Ethernet packets directly, the latter feature apparently for the purpose of backwards compatibility. Lab 2 was connected directly to the bridge-router and thus appeared to be on the same LAN as borel, despite the apparently different subnet; lab 1 was connected to its own router R1 which in turn connected to the bridge-router. Lab 1 was thus, at the LAN level, isolated from abel and borel. Moral 1: Switching and routing are both great ideas, alone. But switching at one layer mixed with routing at another is not. Moral 2: Test thoroughly! The reason the problem wasn‚Äôt noticed earlier was that previously borel communicated only with other hosts on its own subnet and with hosts outside the university entirely. Both of these worked with the bad subnet mask; it was different-subnet local hosts that were the problem. How would nearby subnets at either endpoint decide whether to use the private link? Classical link-state or distance-vector theory ( 13 Routing-Update Algorithms ) requires that they be able to compare the privatelink route with the going-around-the-long-way route. But this requires a global picture of relative routing costs, which, as we shall see, almost certainly does not exist. The two departments are in different routing 9.6 IPv4 Subnets 217
An Introduction to Computer Networks, Release 2.0.11 domains; if neighboring subnets at either end want to use the private link, then manual conÔ¨Åguration is likely the only option. 9.6.3 Subnets versus Switching A frequent network design question is whether to have many small subnets or to instead have just a few (or even only one) larger subnet. With multiple small subnets, IP routing would be used to interconnect them; the use of larger subnets would replace much of that routing with LAN-layer communication, likely Ethernet switching. Debates on this route-versus-switch question have gone back and forth in the networking community, with one aphorism summarizing a common view: Switch when you can, route when you must This aphorism reÔ¨Çects the idea that switching is faster, cheaper and easier to conÔ¨Ågure, and that subnet boundaries should be drawn only where ‚Äúnecessary‚Äù. Ethernet switching equipment is indeed generally cheaper than routing equipment, for the same overall level of features and reliability. And traditional switching requires relatively little conÔ¨Åguration, while to implement subnets not only must the subnets be created by hand but one must also set up and conÔ¨Ågure the routing-update protocols. However, the price difference between switching and routing is not always signiÔ¨Åcant in the big picture, and the conÔ¨Åguration involved is often straightforward. Somewhere along the way, however, switching has acquired a reputation ‚Äì often deserved ‚Äì for being faster than routing. It is true that routers have more to do than switches: they must decrement TTL, update the header checksum, and attach a new LAN header. But these things are relatively minor: a larger reason many routers are slower than switches may simply be that they are inevitably asked to serve as Ô¨Årewalls. This means ‚Äúdeep inspection‚Äù of every packet, egcomparing every packet to each of a large number of Ô¨Årewall rules. The Ô¨Årewall may also be asked to keep track of connection state. All this drives down the forwarding rate, as measured in packets-per-second. Traditional switching scales remarkably well, but it does have limitations. First, broadcast packets must be forwarded throughout a switched network; they do not, however, pass to different subnets. Second, LAN networks do not like redundant links (that is, loops); while one can rely on the spanning-tree algorithm to eliminate these, that algorithm too becomes less efÔ¨Åcient at larger scales. The rise of software-deÔ¨Åned networking ( 3.4 Software-DeÔ¨Åned Networking ) has blurred the distinction between routing and switching. The term ‚ÄúLayer 3 switch‚Äù is sometimes used to describe routers that in effect do not support all the usual Ô¨Årewall bells and whistles. These are often SDN Ethernet switches (3.4 Software-DeÔ¨Åned Networking ) that are making forwarding decisions based on the contents of the IP header. Such streamlined switch/routers may also be able to do most of the hard work in specialized hardware, another source of speedup. But SDN can do much more than IP-layer forwarding, by taking advantage of site-speciÔ¨Åc layout information. One application, of a switch hierarchy for trafÔ¨Åc entering a datacenter, appears in 3.4.1 OpenFlow Switches. Other SDN applications include enabling Ethernet topologies with loops, ofÔ¨Çoading large-volume Ô¨Çows to alternative paths, and implementing policy-based routing as in 13.6 Routing on Other Attributes. Some SDN solutions involve site-speciÔ¨Åc programming, but others work more-or-less out of the box. Locations with switch-versus-route issues are likely to turn increasingly to SDN in the future. 218 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 9.7 Network Address Translation What do you do if your ISP assigns to you a single IPv4 address and you have two computers? The solution is Network Address Translation, or NAT. NAT‚Äôs ability to ‚Äúmultiplex‚Äù an arbitrarily large number of individual hosts behind a single IPv4 address (or small number of addresses) makes it an important tool in the conservation of IPv4 addresses. It also, however, enables an important form of Ô¨Årewall-based security. It is documented in RFC 3022, where this is called NAPT, or Network Address Port Translation. Another term in common use is IP masquerading. The basic idea is that, instead of assigning each host at a site a publicly visible IPv4 address, just one such address is assigned to a special device known as a NAT router. A NAT router sold for residential or smallofÔ¨Åce use is commonly simply called a ‚Äúrouter‚Äù, or (somewhat more precisely) a ‚Äúresidential gateway‚Äù. One side of the NAT router connects to the Internet; the other connects to the site‚Äôs internal network. Hosts on the internal network are assigned private IP addresses ( 9.3 Special Addresses ), typically of the form or 192.168.x.y or 10.x.y.z. Connections to internal hosts that originate in the outside world are banned. When an internal machine wants to connect to the outside, the NAT router intercepts the connection, and forwards the connection‚Äôs packets after rewriting the source address to make it appear they came from the NAT router‚Äôs own IP address, shown below as 200.1.2.37. InternetNAT routerhost Bhost A host C10.0.0.10 10.0.0.11 10.0.0.12200.1.2.37 10.0.0.1 The remote machine responds, sending its responses to the NAT router‚Äôs public IPv4 address. The NAT router remembers the connection, having stored the connection information in a special forwarding table, and forwards the data to the correct internal host, rewriting the destination-address Ô¨Åeld of the incoming packets. The NAT forwarding table also includes port numbers. That way, if two internal hosts attempt to connect to the same external host, the NAT router can tell which packets belong to which. For example, suppose internal hosts A and B each connect from port 3000 to port 80 on external hosts S and T, respectively. Here is what the NAT forwarding table might look like. No columns for the NAT router‚Äôs own IPv4 addresses are needed; we shall let NR denote the router‚Äôs external address. remote host remote port outside source port inside host inside port S 80 3000 A 3000 T 80 3000 B 3000 A packet to S from xA,3000ywould be rewritten so that the source was xNR,3000y. A packet from xS,80y addressed toxNR,3000ywould be rewritten and forwarded to xA,3000y. Similarly, a packet from xT,80y addressed toxNR,3000ywould be rewritten and forwarded to xB,3000y; the NAT table takes into account the source host and port as well as the destination. 9.7 Network Address Translation 219
An Introduction to Computer Networks, Release 2.0.11 Sometimes it is necessary for the NAT router to rewrite the internal-side port number as well; this happens if two internal hosts want to connect, each from the same port, to the same external host and port. For example, suppose B now opens a connection to xS,80y, also from inside port 3000. This time the NAT router must remap the port number, because that is the only way to distinguish between packets from xS,80yback to A and to B. With B‚Äôs second connection‚Äôs internal port remapped from 3000 to 3001, the new table is remote host remote port outside source port inside host inside port S 80 3000 A 3000 T 80 3000 B 3000 S 80 3001 B 3000 The NAT router does not create TCP connections between itself and the external hosts; it simply forwards packets (with rewriting). The connection endpoints are still the external hosts S and T and the internal hosts A and B. However, NR might very well monitor the TCP connections to know when they have closed (by looking for FIN packets, 17.8.1 Closing a connection ), and so can be removed from the table. For UDP connections, NAT routers typically remove the forwarding entry after some period of inactivity; see 16 UDP Transport, exercise 16.0. NAT still works for some trafÔ¨Åc without port numbers, such as network pings, though the above table is then not quite the whole story. See 10.4 Internet Control Message Protocol. Done properly, NAT improves the security of a site, by making it impossible for an external host to probe or to initiate a connection to any of the internal hosts. While this Ô¨Årewall feature is of great importance, essentially the same effect can be achieved without address translation, and with public IPv4 addresses for all internal hosts, by having the router refuse to forward incoming packets that are not part of existing connections. The router still needs to maintain a table like the NAT table above, in order to recognize such packets. The address translation itself, in other words, is not the source of the Ô¨Årewall security. That said, it is hard for a NAT router to ‚Äúfail open‚Äù; ieto fail in a way that lets outside connections in. It is much easier for a non-NAT Ô¨Årewall to fail open. For the common residential form of NAT router, see 10.3.1 NAT, DHCP and the Small OfÔ¨Åce. 9.7.1 NAT Problems NAT router‚Äôs refusal to allow inbound connections is a source of occasional frustration. We illustrate some of these frustrations here, using V oice-over-IP (V oIP) and the call-setup protocol SIP ( RFC 3261 ). The basic strategy is that each phone is associated with a remote phone server. These phone servers, because they have to be able to accept incoming connections from anywhere, must not be behind NAT routers. The phones themselves, however, usually will be: 220 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 Internet NAT1 NAT2 Server2Server1 phone1 phone2 For phone1 to call phone2, phone1 Ô¨Årst contacts Server1, which then contacts Server2. So far, all is well. The Ô¨Ånal step is for Server2 to contact phone2, which, however, cannot be done normally as NAT2 allows no inbound connections. One common solution is for phone2 to maintain a persistent connection to Server2 (and ditto for phone1 and Server1). By having these persistent phone-to-server connections, we can arrange for the phone to ring on incoming calls. As a second issue, somewhat particular to the SIP protocol, is that it is common for server and phone to prefer to use UDP port 5060 at both ends. For a single internal phone, it is likely that port 5060 will pass through without remapping, so the phone will appear to be connecting from the desired port 5060. However, if there are two phones inside (not shown above), one of them will appear to be connecting to the server from an alternative port. The solution here is to have the server tolerate such port remapping. V oIP systems run into a much more serious problem with NAT, however. Once the call between phone1 and phone2 is set up, the servers would prefer to step out of the loop, and have the phones exchange voice packets directly. The SIP protocol was designed to handle this by having each phone report to its respective server the UDP socket ( xIP address,port ypair) it intends to use for the voice exchange; the servers then report these phone sockets to each other, and from there to the opposite phones. This socket information is rendered incorrect by NAT, however, certainly the IP address and quite likely the port as well. If only one of the phones is behind a NAT Ô¨Årewall, it can initiate the voice connection to the other phone, but the other phone will see the voice packets arriving from a different socket than promised and will likely not recognize them as part of the call. If both phones are behind NAT Ô¨Årewalls, they may not be able to connect directly to one another at all. The common solution is for the V oIP server of a phone behind a NAT Ô¨Årewall to remain in the communications path, forwarding packets to its hidden partner. This works, but represents an unwanted server workload. If a site wants to make it possible to allow external connections to hosts behind a NAT router or other Ô¨Årewall, one option is tunneling. This is the creation of a ‚Äúvirtual LAN link‚Äù that runs on top of a TCP connection between the end user and one of the site‚Äôs servers; the end user can thus appear to be on one of the organization‚Äôs internal LANs; see 5.1 Virtual Private Networks. Another option is to ‚Äúopen up‚Äù a speciÔ¨Åc port: in essence, a static NAT-table entry is made connecting a speciÔ¨Åc port on the NAT router to a speciÔ¨Åc internal host and port (usually the same port). For example, all UDP packets to port 5060 on the NAT router might be forwarded to port 5060 on internal host A, even in the absence of any prior packet exchange. Gamers creating peer-to-peer game connections must also usually engage in some port-opening conÔ¨Åguration. The Port Control Protocol ( RFC 6887 ) is sometimes used for this. See also 9.7.3 NAT Traversal, below. NAT routers work very well when the communications model is of client-side TCP connections, originating from the inside and with public outside servers as destination. The NAT model works less well for peer-to9.7 Network Address Translation 221
An Introduction to Computer Networks, Release 2.0.11 peer networking, as with the gamers above, where two computers, each behind a different NAT router, wish to establish a connection. Most NAT routers provide at least limited support for ‚Äúopening‚Äù access to a given internalxhost,portysocket, by creating a semi-permanent forwarding-table entry. See also 17.10 Exercises, exercise 3.0. NAT routers also often have trouble with UDP protocols, due to the tendency for such protocols to have the public server reply from a different port than the one originally contacted. For example, if host A behind a NAT router attempts to use TFTP ( 16.2 Trivial File Transport Protocol, TFTP ), and sends a packet to port 69 of public server C, then C is likely to reply from some newport, say 3000, and this reply is likely to be dropped by the NAT router as there will be no entry there yet for trafÔ¨Åc from xC,3000y. 9.7.2 Middleboxes Firewalls and NAT routers are sometimes classed as middleboxes: network devices that block, throttle or modify trafÔ¨Åc beyond what is necessary for basic forwarding. Middleboxes play a very important role in network security, but they sometimes (as here with V oIP) break things. The word ‚Äúmiddlebox‚Äù (versus ‚Äúrouter‚Äù or ‚ÄúÔ¨Årewall‚Äù) usually has a perjorative connotation; middleboxes have, in some circles, acquired a rather negative reputation. NAT routers‚Äô interference with V oIP, above, is a direct consequence of their function: NAT handles connections from inside to outside quite well, but the NAT mechanism offers no support for connections from one inside to another inside. Sometimes, however, middleboxes block trafÔ¨Åc when there is no technical reason to do so, simply because correct behavior has not been widely implemented. As an example, the SCTP protocol, 18.15.2 SCTP, has seen very limited use despite some putative advantages over TCP, largely due to lack of NAT-router support. SCTP cannot be used by residential users because the middleboxes have not kept up. A third category of middlebox-related problems is overzealous blocking in the name of security. SCTP runs into this problem as well, though not quite as universally: a few routers simply drop all SCTP packets because they represent an ‚Äúunknown‚Äù ‚Äì and therefore suspect ‚Äì type of trafÔ¨Åc. There is a place for this block-by-default approach. If a datacenter Ô¨Årewall blocks all inbound TCP trafÔ¨Åc except to port 80 (the HTTP port), and if SCTP is not being used within the datacenter intentionally, it is hard to argue against blocking all inbound SCTP trafÔ¨Åc. But if the frontline router for home or ofÔ¨Åce users blocks all outbound SCTP trafÔ¨Åc, then the users cannot use SCTP. A consequence of overzealous blocking is that it becomes much harder to introduce new protocols. If a new protocol is blocked for even a small fraction of potential users, it is just not worth the effort. See also the discussion at 18.15.4 QUIC Revisited; the design of QUIC includes several elements to mitigate middlebox problems. For another example of overzealous blocking by middleboxes, with the added element of spoofed TCP RST packets, see the sidebar at 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN). 9.7.3 NAT Traversal If a server must be located behind a NAT router, the traditional way to make it visible to the outside internet is to ‚Äúopen up‚Äù one or more selected ports, using the Port Control Protocol, above. Surprisingly, it is often possible to arrange for a UDP connection between a client A and a server B, both behind different NAT 222 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 Ô¨Årewalls, without any special NAT-router cooperation. TCP connections can, if desired, then be tunneled over the UDP connection. See [MEGK10] and the pwnat package. We will Ô¨Årst assume that the server B knows the public IP address A public of A‚Äôs NAT router, and knows that A wishes to communicate. B then begins sending a series of UDP packets to an agreed-upon port at A public, using an agreed-upon source port; these packets might be sent at 10-second intervals. The pwnat package uses port 2222 for both endpoints; we will assume that here. These packets, of course, are dropped by A‚Äôs NAT router. A now sends a single UDP packet to B‚Äôs public IP address, from port 2222 and to port 2222. When A‚Äôs NAT router sees these packets, it will create a NAT-table entry as follows: remote host remote port outside source port inside host inside port Bpublic 2222 2222[?] A 2222 Assuming that A‚Äôs NAT router does not remap port 2222 as the outside source port, the existence of this NAT-table entry will allow B‚Äôs packets to be delivered to A. A can then respond, and the bidirectional connection is established. Remapping of port 2222 by A‚Äôs NAT router would be a serious problem, but is quite unlikely if no other host at A‚Äôs end is using port 2222. So far, we have assumed that server B knew about A‚Äôs interest in advance. A similar trick, using ICMP (below at 10.4 Internet Control Message Protocol ) allows A to notify B of its interest and existence, so that B can begin sending its series of UDP packets. The idea here is for B to send periodic ICMP Echo Request (ping-request) packets to a Ô¨Åxed IP address IP blackhole chosen because it is not in use. All these packets disappear somewhere, but they do create an ICMP opening in B‚Äôs NAT router. The client A is assumed to know B‚Äôs public IP address B public, and begins to send ICMP Time Exceeded packets to B public that are crafted to look like legitimate responses to B‚Äôs Echo Request packets. ICMP Time Exceeded messages are acceptable regardless of their source IP address; their intended use is by intermediate routers reporting a problem. B‚Äôs NAT router will now (hopefully) forward A‚Äôs arriving Time Exceeded packets to B. The ICMP Time Exceeded packet has error-message space for A‚Äôs public IP address A public; when received, this triggers B‚Äôs sending of UDP packets to xApublic,2222yas above. A and B do have to standardize on a speciÔ¨Åc ping query identiÔ¨Åer. How NAT routers handle ICMP replies does vary from implementation to implementation, and some higherend NAT devices do notice problems with B‚Äôs forged ICMP Time Exceeded packets, either at A‚Äôs end or at B‚Äôs. However, for most consumer-grade NAT devices, this strategy works quite well. 9.8 Unnumbered Interfaces We mentioned in 1.10 IP - Internet Protocol and9.2 Interfaces that some devices allow the use of pointto-point IP links without assigning IP addresses to the interfaces at the ends of the link. Such IP interfaces are referred to as unnumbered; they generally make sense only on routers. It is a Ô¨Årm requirement that the node ( ierouter) at each endpoint of such a link has at least one other interface that does have an IP address; otherwise, the node in question would be anonymous, and could not participate in the router-torouter protocols of 13 Routing-Update Algorithms. The diagram below shows a link L joining routers R1 and R2, which are connected to subnets 200.0.0.0/24 and 201.1.1.0/24 respectively. The endpoint interfaces of L, both labeled link0, are unnumbered. 9.8 Unnumbered Interfaces 223
An Introduction to Computer Networks, Release 2.0.11 R1 R2A B E 200.0.0.0/24 201.1.1.0/24 Two LANs joined by an unnumbered link LF 200.0.0.1 201.1.1.1 L eth0 link0 link0 eth0 The endpoints of L could always be assigned private IPv4 addresses ( 9.3 Special Addresses ), such as 10.0.0.1 and 10.0.0.2. To do this we would need to create a subnet; because the host bits cannot be all 0‚Äôs or all 1‚Äôs, the minimum subnet size is four ( eg10.0.0.0/30). Furthermore, the routing protocols to be introduced in 13 Routing-Update Algorithms will distribute information about the subnet throughout the organization or ‚Äúrouting domain‚Äù, meaning care must be taken to ensure that each link‚Äôs subnet is unique. Use of unnumbered links avoids this. If R1 were to originate a packet to be sent to (or forwarded via) R2, the standard strategy is for it to treat itslink0 interface as if it shared the IP address of its Ethernet interface eth0, that is, 200.0.0.1; R2 would do likewise. This still leaves R1 and R2 violating the IP local-delivery rule of 9.5 The Classless IP Delivery Algorithm; R1 is expected to deliver packets via local delivery to 201.1.1.1 but has no interface that is assigned an IP address on the destination subnet 201.1.1.0/24. The necessary dispensation, however, is granted by RFC 1812. All that is necessary by way of conÔ¨Åguration is that R1 be told R2 is a directly connected neighbor reachable via its link0 interface. On Linux systems this might be done with the ip route command on R1 as follows: ip route The Linuxip route command illustrated here was tested on a virtual point-to-point link created with ssh andpppd; the link interface name was in fact ppp0. While the command appeared to work as advertised, it was only possible to create the link if endpoint IP addresses were assigned at the time of creation; these were then removed with ip route del and then re-assigned with the command shown here. ip route add 201.1.1.1 dev link0 Because L is a point-to-point link, there is no destination LAN address and thus no ARP query. 9.9 Mobile IP In the original IPv4 model, there was a strong if implicit assumption that each IP host would stay put. One role of an IPv4 address is simply as a unique endpoint identiÔ¨Åer, but another role is as a locator: some preÔ¨Åx of the address ( egthe network part, in the class-A/B/C strategy, or the provider preÔ¨Åx) represents something about where the host is physically located. Thus, if a host moves far enough, it may need a new address. When laptops are moved from site to site, it is common for them to receive a new IP address at each location, egvia DHCP as the laptop connects to the local Wi-Fi. But what if we wish to support devices like smartphones that may remain active and communicating while moving for thousands of miles? Changing IP addresses requires changing TCP connections; life (and application development) might be simpler if a device had a single, unchanging IP address. 224 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 One option, commonly used with smartphones connected to some so-called ‚Äú3G‚Äù networks, is to treat the phone‚Äôs data network as a giant wireless LAN. The phone‚Äôs IP address need not change as it moves within this LAN, and it is up to the phone provider to Ô¨Ågure out how to manage LAN-level routing, much as is done in 4.2.4.3 Roaming. ButMobile IP is another option, documented in RFC 5944. In this scheme, a mobile host has a permanent home address and, while roaming about, will also have a temporary care-of address, which changes from place to place. The care-of address might be, for example, an IP address assigned by a local Wi-Fi network, and which in the absence of Mobile IP would be theIP address for the mobile host. (This kind of care-of address is known as ‚Äúco-located‚Äù; the care-of address can also be associated with some other device ‚Äì known as aforeign agent ‚Äì in the vicinity of the mobile host.) The goal of Mobile IP is to make sure that the mobile host is always reachable via its home address. To maintain connectivity to the home address, a Mobile IP host needs to have a home agent back on the home network; the job of the home agent is to maintain an IP tunnel that always connects to the device‚Äôs current care-of address. Packets arriving at the home network addressed to the home address will be forwarded to the mobile device over this tunnel by the home agent. Similarly, if the mobile device wishes to send packets from its home address ‚Äì that is, with the home address as IP source address ‚Äì it can use the tunnel to forward the packet to the home agent. The home agent may use proxy ARP ( 10.2.1 ARP Finer Points ) to declare itself to be the appropriate destination on the home LAN for packets addressed to the home (IP) address; it is then straightforward for the home agent to forward the packets. Anagent discovery process is used for the mobile host to decide whether it is mobile or not; if it is, it then needs to notify its home agent of its current care-of address. 9.9.1 IP-in-IP Encapsulation There are several forms of packet encapsulation that can be used for Mobile IP tunneling, but the default one is IP-in-IP encapsulation, deÔ¨Åned in RFC 2003. In this process, the entire original IP packet (with header addressed to the home address) is used as data for a new IP packet, with a new IP header (the ‚Äúouter‚Äù header) addressed to the care-of address. Original IP headerData Original packet Original IP headerData Outer header Encapsulated packet with outer header A value of 4 in the outer-IP-header Protocol Ô¨Åeld indicates that IPv4-in-IPv4 tunneling is being used, so the receiver knows to forward the packet on using the information in the inner header. The MTU of the tunnel will be the original MTU of the path to the care-of address, minus the size of the outer header. A very 9.9 Mobile IP 225
An Introduction to Computer Networks, Release 2.0.11 similar mechanism is used for IPv6-in-IPv4 encapsulation (that is, with IPv6 in the inner packet), except that the outer IPv4 Protocol Ô¨Åeld value is now 41. See 12.6 IPv6 Connectivity via Tunneling. IP-in-IP encapsulation presents some difÔ¨Åculties for NAT routers. If two hosts A and B behind a NAT router send out encapsulated packets, the packets may differ only in the source IP address. The NAT router, upon receiving responses, doesn‚Äôt know whether to forward them to A or to B. One partial solution is for the NAT router to support only one inside host sending encapsulated packets. If the NAT router knew that encapsulation was being used for Mobile IP, it might look at the home address in the inner header to determine the correct home agent to which to deliver the packet, but this is a big assumption. A fuller solution is outlined in RFC 3519. 9.10 Epilog At this point we have concluded the basic mechanics of IPv4. Still to come is a discussion of how IP routers build their forwarding tables. This turns out to be a complex topic, divided into routing within single organizations and ISPs ‚Äì 13 Routing-Update Algorithms ‚Äì and routing between organizations ‚Äì 14 Large-Scale IP Routing. But before that, in the next chapter, we compare IPv4 with IPv6, now twenty years old but still seeing limited adoption. The biggest issue Ô¨Åxed by IPv6 is IPv4‚Äôs lack of address space, but there are also several other less dramatic improvements. 9.11 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.9 Solutions for IPv4. 1.0. Suppose an Ethernet packet represents a TCP acknowledgment; that is, the packet contains the Ethernet header, the IPv4 header, a 20-byte TCP header, and a 4-byte CRC checksum. Is such a packet smaller than the 10-Mbit Ethernet minimum-packet size, and, if so, by how much? 2.0. How can a receiving host tell if an arriving IPv4 packet is unfragmented? Hint: such a packet will be both the ‚ÄúÔ¨Årst fragment‚Äù and the ‚Äúlast fragment‚Äù; how are these two states marked in the IPv4 header? 3.0. How long will it take the IDENT Ô¨Åeld of the IPv4 header to wrap around, if the sender host A sends a stream of packets to host B as fast as possible? Assume the packet size is 1500 bytes and the bandwidth is 600 Mbps. 4.0. The following diagram has routers A, B, C, D and E; E is the ‚Äúborder router‚Äù connecting the site to the Internet. All router-to-router connections are via Ethernet-LAN /24 subnets with addresses of the form 200.0.x. Give forwarding tables for each of A ‚ô¢, B, C and D. Each table should include each of the listed subnets and also a default entry that routes trafÔ¨Åc toward router E. Directly connected subnets may be listed with a next_hop of ‚Äúdirect‚Äù. 200.0.5 A200.0.6 B200.0.7 D200.0.8 EInternet 200.0.9 (continues on next page) 226 9 IP version 4
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) C 200.0.10 5.0. (This exercise is an attempt at modeling Internet-2 routing.) Suppose sites S 1.. . S neach have a single connection to the standard Internet, and each site S ihas a single IPv4 address block A i. Each site‚Äôs connection to the Internet is through a single router R i; each R i‚Äôs default route points towards the standard Internet. The sites also maintain a separate, higher-speed network among themselves; each site has a single link to this separate network, also through R i. Describe what the forwarding tables on each R iwill have to look like so that trafÔ¨Åc from one S ito another will always use the separate higher-speed network. 6.0. For each IPv4 network preÔ¨Åx given (with length), identify which of the subsequent IPv4 addresses are part of the same subnet. (a).10.0.130.0/23: 10.0.130.23, 10.0.129.1, 10.0.131.12, 10.0.132.7 (b).10.0.132.0/22: 10.0.130.23, 10.0.135.1, 10.0.134.12, 10.0.136.7 (c).10.0.64.0/18: 10.0.65.13, 10.0.32.4, 10.0.127.3, 10.0.128.4 (d).‚ô¢10.0.168.0/21: 10.0.166.1, 10.0.170.3, 10.0.174.5, 10.0.177.7 (e).10.0.0.64/26: 10.0.0.125, 10.0.0.66, 10.0.0.130, 10.0.0.62 7.0. Convert the following subnet masks to /k notation, and vice-versa: (a).‚ô¢255.255.240.0 (b). 255.255.248.0 (c). 255.255.255.192 (d).‚ô¢/20 (e). /22 (f). /27 8.0. Suppose that the subnet bits below for the following Ô¨Åve subnets A-E all come from the beginning of the fourth byte of the IPv4 address; that is, these are subnets of a /24 block. 
- A: 00 
- B: 01 
- C: 110 
- D: 111 
- E: 1010 (a). What are the sizes of each subnet, and the corresponding decimal ranges? Count the addresses with 9.11 Exercises 227
An Introduction to Computer Networks, Release 2.0.11 host bits all 0‚Äôs or with host bits all 1‚Äôs as part of the subnet. (b). How many IPv4 addresses in the class-C block do not belong to any of the subnets A, B, C, D and E? 228 9 IP version 4
10 IPV4 COMPANION PROTOCOLS IP is the keystone of the Internet, but it occupies that position with a little help from its friends. DNS translates human-readable host names, such as intronetworks.cs.luc.edu to IP addresses. ARP translates IPv4 addresses to Ethernet addresses, for destinations on the same LAN. DHCP assigns IPv4 addresses. And ICMP enables the transmission of IPv4-related error and status messages. These four are the subject of this chapter. The original DNS can be used with IPv6, with modest extensions, but for the other three, IPv6 has its own versions; see 11.6 Neighbor Discovery (replacing ARP), 11.7 IPv6 Host Address Assignment and 12.2 ICMPv6. 10.1 DNS TheDomain Name System, DNS, is an essential companion protocol to IPv4 (and IPv6); an overview can be found in RFC 1034. It is DNS that permits users the luxury of not needing to remember numeric IP addresses. Instead of 162.216.18.28, a user can simply enter intronetworks.cs.luc.edu, and DNS will take care of looking up the name and retrieving the corresponding address. DNS also makes it easy to move services from one server to another with a different IP address; as users will locate the service by DNS name and not by IP address, they do not need to be notiÔ¨Åed. While DNS supports a wide variety of queries, for the moment we will focus on queries for IPv4 addresses, or so-called Arecords. The AAAA record type is used for IPv6 addresses, and, internally, the NSrecord type is used to identify the ‚Äúname servers‚Äù that answer DNS queries. While a workstation can use TCP/IP without DNS, users would have an almost impossible time Ô¨Ånding anything, and so the core startup conÔ¨Åguration of an Internet-connected workstation almost always includes the IP address of its DNS server (see 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) below for how startup conÔ¨Ågurations are often assigned). It‚Äôs Always DNS That is a common, though exaggerated, sentiment among system administrators dealing with network problems. While DNS is on the whole remarkably reliable, its failures can be tricky to diagnose. Most DNS trafÔ¨Åc today is over UDP, although a TCP option exists. Due to the much larger response sizes, TCP is often necessary for DNSSEC ( 29.7 DNSSEC ). DNS is distributed, meaning that each domain is responsible for maintaining its own DNS servers to translate names to addresses. DNS, in fact, is a classic example of a highly distributed database where each node maintains a relatively small amount of data. That said, in days gone by it was common practice for each domain to maintain its own DNS server; today, domain registrars often provide DNS services for many of their domain customers. DNS is hierarchical as well; for the DNS name intronetworks.cs.luc.edu the levels of the hierarchy are 229
An Introduction to Computer Networks, Release 2.0.11 
- edu: the top-level domain (TLD) for educational institutions in the US 
- luc: Loyola University Chicago 
- cs: The Loyola Computer Science Department 
- intronetworks: a hostname associated to a speciÔ¨Åc IP address The hierarchy of DNS names (that is, the set of all names and sufÔ¨Åxes of names) forms a tree, but it is not only leaf nodes that represent individual hosts. In the example above, domain names luc.edu and cs.luc.edu happen to be valid hostnames as well. The DNS hierarchy is in a great many cases not very deep, particularly for DNS names assigned to commercial websites. Such domain names are often simply the company name (or a variant of it) followed by the top-level domain (often .com ). Still, internally most organizations have many individually named behindthe-scenes servers with three-level (or more) domain names; sometimes some of these can be identiÔ¨Åed by viewing the source of the web page and searching it for domain names. Top-level domains are assigned by ICANN. The original top-level domains were seven three-letter domains ‚Äì.com ,.net ,.org ,.int ,.edu ,.mil and.gov ‚Äì and the two-letter country-code domains ( eg .us, .ca, .mx). Now there are hundreds of non-country top-level domains, such as .aero ,.biz ,.info, and, apparently, .wtf. Domain names (and subdomain names) can also contain unicode characters, so as to support national alphabets. Some top-level domains are generic, meaning anyone can apply for a subdomain although there may be qualifying criteria. Other top-level domains are sponsored, meaning the sponsoring organization determines who can be assigned a subdomain, and so the qualifying criteria can be a little more arbitrary. ICANN still must approve all new top-level domains. Applications are accepted only during speciÔ¨Åc intervals; the application fee for the 2012 interval was US$185,000. The actual leasing of domain names to companies and individuals is done by organizations known as domain registrars who work under contract with ICANN. The full tree of all DNS names and preÔ¨Åxes is divided administratively into zones: a zone is an independently managed subtree, minus any sub-subtrees that have been placed ‚Äì by delegation ‚Äì into their own zone. Each zone has its own root DNS name that is a sufÔ¨Åx of every DNS name in the zone. For example, the luc. edu zone contains most of Loyola‚Äôs DNS names, but cs.luc.edu has been spun off into its own zone. A zone cannot be the disjoint union of two subtrees; that is, cs.luc.edu andmath.luc.edu must be two distinct zones, unless both remain part of their parent zone. A zone can deÔ¨Åne DNS names more than one level deep. For example, the luc.edu zone can deÔ¨Åne records for the luc.edu name itself, for names with one additional level such as www.luc.edu, and for names with two additional levels such as www.cs.luc.edu. That said, it is common for each zone to handle only one additional level, and to create subzones for deeper levels. Each zone has its own authoritative nameservers for the zone, which are charged with maintaining the records ‚Äì known as resource records, or RRs ‚Äì for that zone. Each zone must have at least two nameservers, for redundancy. IPv4 addresses are stored as so-called A records, for Address. Information about how to Ô¨Ånd sub-zones is stored as NS records, for Name Server. Additional resource-record types are discussed at 10.1.3 Other DNS Records. Each DNS record type also includes a time-to-live Ô¨Åeld for caching (below). An authoritative nameserver need not be part of the organization that manages the zone, and a single server can be the authoritative nameserver for multiple unrelated zones. For example, many domain registrars 230 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 maintain single nameservers that handle DNS queries for all their domain customers who do not wish to set up their own nameservers. Theroot nameservers handle the zone that is the root of the DNS tree; that is, that is represented by the DNS name that is the empty string. As of 2019, there are thirteen of them. The root nameservers contain only NS records, identifying the nameservers for all the immediate subzones. Each top-level domain is its own such subzone. The IP addresses of the root nameservers are widely distributed. Their DNS names (which are only of use if some DNS lookup mechanism is already present) are a.root.servers.net through m.root-servers.net. These names today correspond not to individual machines but to clusters of up to hundreds of servers. 10.1.1 DNS Resolvers We can now put together a Ô¨Årst draft of a DNS lookup algorithm. To Ô¨Ånd the IP address of intronetworks.cs.luc.edu, a host Ô¨Årst contacts a root nameserver (at a known address) to Ô¨Ånd the nameserver for the edu zone; this involves the retrieval of an NS record. The edu nameserver is then queried to Ô¨Ånd the nameserver for the luc.edu zone, which in turn supplies the NS record giving the address of the cs.luc.edu zone. This last has an A record for the actual host. (This example is carried out in detail below.) The system (or application) that executes these DNS lookups is known as a DNS resolver. Confusingly, resolvers are also sometimes known as ‚Äúnameservers‚Äù or, more precisely, non-authoritative nameservers. To reduce overall DNS trafÔ¨Åc, in particular to the root nameservers, it makes sense to cache intermediate (and Ô¨Ånal) results, so that in a later query for, say, uchicago.edu, the host can reuse the previously learned address of the edu nameserver. For still greater DNS efÔ¨Åciency, we can provide one DNS resolver to handle requests for a large pool of users. The idea is that, if one user has looked up youtube.com, or facebook.com, those addresses are in hand locally for the next user. The beneÔ¨Åt of this consolidation approach depends on the distribution of lookup requests, their lifetimes, and how likely it is that two users visit the same site. In recent years this caching beneÔ¨Åt has been getting smaller, at least for ‚Äúfull‚Äù DNS names, as the Internet becomes more diverse, and as cache lifetimes have been shrinking. (The caching beneÔ¨Åt for DNS ‚Äúpartial‚Äù names, such as .edu and .com, remains signiÔ¨Åcant.) Regardless of caching beneÔ¨Åts, such pooled-use DNS resolvers are almost universally used. Almost all ISPs and most companies, for example, provide a resolver to handle the DNS needs of their customers and employees. We will refer to these as site resolvers. The IP addresses of these site resolvers are generally supplied via DHCP options ( 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ); such resolvers are thus the default choice for DNS services. DNS Policing It is sometimes suggested that if a site is engaged in illegal activity or copyright infringement, such as thepiratebay.se, its domain name should be seized. The problem with this strategy is that it is straightforward for users to set up ‚Äúnonstandard‚Äù nameservers (for example the Gnu Name System, GNS) that continue to list the banned site. Sometimes, however, users elect to use a DNS resolver not provided by their ISP or company; there are a number of public DNS servers (that is, resolvers) available. Such resolvers generally serve much larger 10.1 DNS 231
An Introduction to Computer Networks, Release 2.0.11 areas. Common choices include OpenDNS, Google DNS (primary address 8.8.8.8), CloudÔ¨Çare (primary address 1.1.1.1) and the Gnu Name System mentioned in the sidebar above, though there are many others. Searching for ‚Äúpublic DNS server‚Äù turns up lists of them. In theory, one advantage of using a public DNS server is that your local ISP can no longer track your DNS queries. However, some ISPs still do record customer DNS queries, and may even intercept and modify them. If this is a concern, DNS encryption is necessary. There are two primary proposals: 
- DNS over TLS (DoT): RFC 7858 
- DNS over HTTPS (DoH): RFC 8484 TLS is an encryption protocol ( 29.5.2 TLS ). HTTPS ‚Äì secure HTTP ‚Äì uses TLS encryption, but the two are not the same. For one thing, eavesdroppers can still identify DoT trafÔ¨Åc as DNS trafÔ¨Åc (because it is sent to a special port), but DoH trafÔ¨Åc is indistinguishable from HTTPS web trafÔ¨Åc. Use of any DNS server, whether via plain DNS or DoT or DoH, does mean that the DNS server now has access to all your DNS queries. Ultimately, the choice depends on how much you trust your site resolver versus your selected public resolver. Both DoT and DoH often take some deliberate conÔ¨Åguration to enable as the standard system resolver. However, as of 2020 Mozilla has started enabling DoH by default in their Firefox browser, that is, without operating-system support, though disabling DoH and reverting back to the system DNS resolver is straightforward. IPv6 Google and CloudÔ¨Çare provide public DNS for IPv6 users too, though the addresses are much less easy to remember. Google‚Äôs is 2001:4860:4860:: 8888; CloudÔ¨Çare‚Äôs is 2606:4700:4700:: 1111. In setting up DoH for Firefox, the Mozilla foundation created what it calls its Trusted Recursive Resolver program. Participating providers ‚Äì initially CloudÔ¨Çare, and eventually others ‚Äì had to agree to contractual requirements. Among these requirements are that personal browsing history not be collected, and that Query Name Minimization ( 10.1.2.1 Query Name Minimization ) be supported. Ultimately, however, the choice of what DNS to trust belongs to the user. Some public DNS servers provide additional services, such as automatically Ô¨Åltering out domain names associated with security risks, or content inappropriate for young users. Sometimes there is a fee for this service. A common drawback to Ô¨Åltering content at the DNS level is that, unless the DNS server provides an alternative address pointing to an explanation page, users who attempt to access blocked content may have absolutely no idea what went wrong. Because of this, Ô¨Åltering content at the browser level rather than the DNS level is sometimes preferred, though Ô¨Åltering at the browser level has its own drawbacks. In October 2021, CloudÔ¨Çare (with public DNS 1.1.1.1, above) began offering two additional (and free) public DNS servers: 
- 1.1.1.2, blocking known malware 
- 1.1.1.3, blocking malware and adult content See blog.cloudÔ¨Çare.com/introducing-1-1-1-1-for-families for further details. As mentioned earlier, each DNS record comes with a time-to-live (TTL) value, used by resolvers as an 232 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 indication of how long they are supposed to keep that record in their caches. DNS TTL lifetimes can be up to several days; RFC 1035 recommends a minimum TTL of ‚Äúat least a day‚Äù. However, in recent years TTL values have been getting quite a bit smaller. Below, in 10.1.2 nslookup and dig, we retrieve the TTL values forfacebook.com andgoogle.com from their respective authoritative nameservers; in each case it is 300 seconds (5 minutes). Authoritative nameservers also provide a TTL value for lookup failures. According to RFC 2308, this is the TTL value speciÔ¨Åed in the SOA record. (Originally, the SOA TTL represented the default TTL for successful lookups.) Lookup-failure TTLs should usually be kept quite short; otherwise there is potential for large numbers of users to be locked out of a site. Consider, for example, the following scenario for updating a DNS record for host foo.com. Let us suppose that the lookup-failure TTL is one week: Site foo.com Site B (perhaps a large ISP) Delete foo.com A record Site B queries for foo.com Site B gets NXDOMAIN Immediately reinstall foo.com A record At this point, Site B may be telling its users for a week that foo.com is unavailable, and site foo.com will be unable to Ô¨Åx it. If I send a query to Loyola‚Äôs site resolver for google.com, it is almost certainly in the cache. If I send a query for the misspelling googel.com, this may not be in the cache, but the .com top-level nameserver almost certainly isin the cache. From that nameserver my local resolver Ô¨Ånds the nameserver for the googel.com zone, and from that Ô¨Ånds the IP address of the googel.com host. There are, as of 2019, around 1500 top-level domains. If, while still using Loyola‚Äôs site resolver, I send a query for a site in one of the more obscure top-level domains, there is a reasonable chance that the top-level domain will notbe in the cache. A consequence of this aspect of caching is that popular top-level domains are likelier to result in faster lookups. Applications almost always invoke DNS through library calls, such as Java‚Äôs InetAddress. getByName(). The library forwards the query to the system-designated resolver (though browsers sometimes offer other DNS options; see 29.7.4 DNS over HTTPS ). We will return to DNS library calls in 16.1.3.3 The Client and17.6.1 The TCP Client. On unix-based systems, traditionally the IPv4 addresses of the local DNS resolvers were kept in a Ô¨Åle / etc/resolv.conf. Typically this Ô¨Åle was updated with the addresses of the current resolvers by DHCP (10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ), at the time the system received its IPv4 address. It is possible, though not common, to create special entries in /etc/resolv.conf so that queries about different domains are sent to different resolvers, or so that single-level hostnames have a domain name appended to them before lookup. On Windows, similar functionality can be achieved through settings on the DNS tab within the Network Connections applet. Recent systems often run a small ‚Äústub‚Äù resolver locally ( egLinux‚Äôs dnsmasq); such resolvers are sometimes also called DNS forwarders. The entry in /etc/resolv.conf is then an IPv4 address of localhost (sometimes 127.0. 1.1 rather than 127.0. 0.1). Such a stub resolver would, of course, still need access to the addresses of site or public resolvers; sometimes these addresses are provided by static conÔ¨Åguration and sometimes by DHCP ( 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ). 10.1 DNS 233
An Introduction to Computer Networks, Release 2.0.11 If a system running a stub resolver then runs internal virtual machines, it is usually possible to conÔ¨Ågure everything so that the virtual machines can be given an IP address of the host system as their DNS resolver. For example, often virtual machines are assigned IPv4 addresses on a private subnet and connect to the outside world using NAT ( 9.7 Network Address Translation ). In such a setting, the virtual machines are given the IPv4 address of the host system interface that connects to the private subnet. It is then necessary to ensure that, on the host system, the local resolver accepts queries sent not only to the designated loopback address but also to the host system‚Äôs private-subnet address. (Generally, local resolvers do notaccept requests arriving from externally visible addresses.) When someone submits a query for a nonexistent DNS name, the resolver is supposed to return an error message, technically known as NXDOMAIN (Non eXistent Domain). Some resolvers, however, have been conÔ¨Ågured to return the IP address of a designated web server; this is particularly common for ISP-provided site resolvers. Sometimes the associated web page is meant to be helpful, and sometimes it presents an offer to buy the domain name from a registrar. Either way, additional advertising may be displayed. Of course, this is completely useless to users who are trying to contact the domain name in question via a protocol (ssh, smtp) other than http. At the DNS protocol layer, a DNS lookup query can be either recursive ornon-recursive. If A sends to B a recursive query to resolve a given DNS name, then B takes over the job until it is Ô¨Ånally able to return an answer to A. If The query is non-recursive, on the other hand, then if B is not an authoritative nameserver for the DNS name in question it returns either a failure notice or an NS record for the sub-zone that is the next step on the path. Almost all DNS requests from hosts to their site or public resolvers are recursive. A basic DNS response consists of an ANSWER section, an AUTHORITY section and, optionally, an ADDITIONAL section. Generally a response to a lookup of a hostname contains an ANSWER section consisting of a single A record, representing a single IPv4 address. If a site has multiple servers that are entirely equivalent, however, it is possible to give them all the same hostname by conÔ¨Åguring the authoritative nameserver to return, for the hostname in question, multiple A records listing, in turn, each of the server IPv4 addresses. This is sometimes known as round-robin DNS. It is a simple form of load balancing; see also 30.9.5 loadbalance31.py. Consecutive queries to the nameserver should return the list of A records in different orders; ideally the same should also happen with consecutive queries to a local resolver that has the hostname in its cache. It is also common for a single server, with a single IPv4 address, to be identiÔ¨Åed by multiple DNS names; see the next section. The response AUTHORITY section contains the DNS names of the authoritative nameservers responsible for the original DNS name in question. These records are often NS records, which point to the zone from its parent, though SOA records ‚Äì declaring a zone from within ‚Äì are also seen. The ADDITIONAL section contains information the sender thinks is related; for example, this section often contains A records for the authoritative nameservers. The Tor Project uses DNS-like names that end in ‚Äú.onion‚Äù. While these are not true DNS names in that they are not managed by the DNS hierarchy, they do work as such for Tor users; see RFC 7686. These names follow an unusual pattern: the next level of name is an 80-bit hash of the site‚Äôs RSA public key (29.1 RSA ), converted to sixteen ASCII bytes. For example, 3g2upl4pq6kufc4m.onion is apparently the Tor address for the search engine duckduckgo.com. Unlike DuckDuckGo, many sites try different RSA keys until they Ô¨Ånd one where at least some initial preÔ¨Åx of the hash looks more or less meaningful; for example, nytimes2tsqtnxek.onion. Facebook got very lucky in Ô¨Ånding an RSA key whose corresponding Tor address isfacebookcorewwwi.onion (though it is sometimes said that fortune is infatuated with the wealthy ). This naming strategy is a form of cryptographically generated addresses; for another example see11.6.4 Security and Neighbor Discovery. The advantage of this naming strategy is that you don‚Äôt need 234 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 a certiÔ¨Åcate authority ( 29.5.2.1 CertiÔ¨Åcate Authorities ) to verify a site‚Äôs RSA key; the site name does it for you. 10.1.2 nslookup and dig Let us trace a non-recursive lookup of intronetworks.cs.luc.edu, using the nslookup tool. The nslookup tool is time-honored, but also not completely up-to-date, so we also include examples using the dig utility (supposedly an acronym for ‚Äúdomain Internet groper‚Äù). Lines we type in nslookup ‚Äôs interactive mode begin below with the prompt ‚Äú>‚Äù; the shell prompt is ‚Äú#‚Äù. All dig commands are typed directly at the shell prompt. The Ô¨Årst step is to look up the IP address of the root nameserver a.root-servers.net. We can do this with a regular call to nslookup ordig, we can look this up in our nameserver‚Äôs conÔ¨Åguration Ô¨Åles, or we can search for it on the Internet. The address is 198.41.0.4. We now send our nonrecursive query to this address. The presence of the single hyphen in the nslookup command line below means that we want to use 198.41.0.4 as the nameserver rather than as the thing to be looked up; dig has places on the command line for both the nameserver (following the @) and the DNS name. For both commands, we use the norecurse option to send a nonrecursive query. # nslookup -norecurse - 198.41.0.4 > intronetworks.cs.luc.edu ***Can't find intronetworks.cs.luc.edu: No answer # dig @198.41.0.4 intronetworks.cs.luc.edu +norecurse These fail because by default nslookup anddig ask for an A record. What we want is an NS record: the name of the next zone down to ask. (We can tell the dig query failed to Ô¨Ånd an A record because there are zero records in the ANSWER section) > set query=ns > intronetworks.cs.luc.edu edu nameserver = a.edu-servers.net ... a.edu-servers.net internet address = 192.5.6.30 # dig @198.41.0.4 intronetworks.cs.luc.edu NS +norecurse ;; AUTHORITY SECTION: edu. 172800 IN NS b.edu-servers.net. ;; ADDITIONAL SECTION: b.edu-servers.net. 172800 IN A 192.33.14.30 The full responses in each case are a list of all nameservers for the .edu zone; we list only the Ô¨Årst response above. The INin the two response records shown above indicates these are InterNet records. Note that the full DNS name intronetworks.cs.luc.edu in the query here is notan exact match for the DNS name .edu in the resource record returned; the latter is a sufÔ¨Åx of the former. This has privacy implications; the root nameserver didn‚Äôt need to know we were searching for intronetworks.cs. luc.edu. We could have just asked for edu. We return to this in 10.1.2.1 Query Name Minimization. We send the next NS query to a.edu-servers.net (which does appear in the full dig answer) 10.1 DNS 235
An Introduction to Computer Networks, Release 2.0.11 # nslookup -query=ns -norecurse - 192.5.6.30 > intronetworks.cs.luc.edu ... Authoritative answers can be found from: luc.edu nameserver = bcdnswt1.it.luc.edu. bcdnswt1.it.luc.edu internet address = 147.126.64.64 # dig @192.5.6.30 intronetworks.cs.luc.edu NS +norecurse ;; AUTHORITY SECTION: luc.edu. 172800 IN NS bcdnsls1.it.luc.edu. ;; ADDITIONAL SECTION: bcdnsls1.it.luc.edu. 172800 IN A 147.126.1.217 (Again, we show only one of several luc.edu nameservers returned). The next step is to ask the luc.edu nameserver for the cs.luc.edu nameserver. # nslookup -query=ns - -norecurse 147.126.1.217 > cs.luc.edu ... cs.luc.edu nameserver = bcdnsls1.it.luc.edu. # dig @147.126.1.217 intronetworks.cs.luc.edu NS +norecurse ;; AUTHORITY SECTION: cs.luc.edu. 300 IN SOA bcdnsls1.it.luc.edu. √£√ëpostmaster.luc.edu. 589544360 1200 180 1209600 300 Desperately seeking Switzerland To look up the .edu nameservers with the default resolver, we might use dig edu NS. This works for any top-level domain ( egus,de,uk,aero ),except India (.in) and Switzerland (.ch). The command dig in NS returns the root servers; that is, inis interpreted as the DNS abbreviation for InterNet. A working query is dig in. IN NS (note the period). For Switzerland the problem is that chcollides with the DNS abbreviation for the now-obsolete ChaosNet; a working query is dig ch IN NS. For more, see here. Thenslookup command returns the same nameserver as before; the dig command does also, but at least indicates it is returning an SOA rather than an NS record. The Ô¨Årst data Ô¨Åeld of the SOA result ‚Äìbcdnsls1.it.luc.edu. ‚Äì is the primary nameserver for cs.luc.edu. All this is a somewhat roundabout way of saying that the same nameserver handles cs.luc.edu as handles luc.edu; that is, they are two zones that just happen to be handled by the same nameserver. Prior to 2019, cs.luc. edu was handled by a separate nameserver, but after a signiÔ¨Åcant outage it was folded back to the luc. edu nameserver. If we drop the intronetworks label in the last query above, that is, we run dig @147.126.1.217 cs.luc.edu NS +norecurse, we now get an ANSWER section (instead of AUTHORITY), which declares that bcdnsls1.it.luc.edu is indeed the authoritative nameserver for cs.luc.edu. In any event, we can now ask for the A record directly: 236 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 # nslookup -query=A - -norecurse 147.126.1.217 > intronetworks.cs.luc.edu ... Name: intronetworks.cs.luc.edu Address: 162.216.18.28 # dig @147.126.1.217 intronetworks.cs.luc.edu A +norecurse ;; ANSWER SECTION: intronetworks.cs.luc.edu. 600 IN A 162.216.18.28 This is the Ô¨Årst time we get an ANSWER section (versus the AUTHORITY section) Prior to 2019, the Ô¨Ånal result from nslookup was in fact this: intronetworks.cs.luc.edu canonical name = linode1.cs.luc.edu. Name: linode1.cs.luc.edu Address: 162.216.18.28 Here we received a canonical name, or CNAME, record. The server that hosts intronetworks.cs. luc.edu also hosts several other websites, with different names; for example, introcs.cs.luc.edu. This is known as virtual hosting. Rather than provide separate A records for each website name, DNS was set up to provide a CNAME record for each website name pointing to a single physical server name linode1. cs.luc.edu. Only one A record is then needed, for this server. Post-2019, this CNAME strategy is no longer used. Note that both the CNAME and the corresponding A record was returned, for convenience. The pre-2019 answer returned by dig (above) made no mention of CNAMEs, because they are often of little user interest; dig will return CNAMEs however if asked explicitly. Note that the IPv4 address here, 162.216.18.28, is unrelated to Loyola‚Äôs own IPv4 address block 147.126.0.0/16. The server hosting intronetworks.cs.luc.edu is managed by an external provider; there is no connection between the DNS name hierarchy and the IP address hierarchy. As another example of the use of dig, we can Ô¨Ånd the time-to-live values advertised by facebook.com andgoogle.com: dig facebook.com ;; ANSWER SECTION: facebook.com. 78 IN A 157.240.18.35 ;; AUTHORITY SECTION: facebook.com. 147771 IN NS b.ns.facebook.com. dig google.com ;; ANSWER SECTION: google.com. 103 IN A 172.217.9.78 ;; AUTHORITY SECTION: google.com. 141861 IN NS ns3.google.com. The TTLs are 78 and 103 seconds, respectively. But these are the TTLs coming from the local site resolver. To get the TTL values from facebook.com andgoogle.com directly, we can do this: dig @b.ns.facebook.com facebook.com ;; ANSWER SECTION: (continues on next page) 10.1 DNS 237
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) facebook.com. 300 IN A 157.240.18.35 dig @ns3.google.com google.com ;; ANSWER SECTION: google.com. 300 IN A 172.217.1.14 That is, both sites‚Äô authoritative nameservers advertise a TTL of 300 seconds (5 minutes). The TTL value of 78 received above means that our local resolver itself last asked about facebook.com 300‚Äì78 = 222 seconds ago. 10.1.2.1 Query Name Minimization In the example above in which we traced the lookup of DNS name intronetworks.cs.luc.edu starting from the root, we commented that the entire DNS name was sent to the root server. This was unnecessary, as the root nameservers only know how to reach the .edu nameservers; we could have sent an NS request just for .edu. There is a privacy issue here; the root nameservers don‚Äôt need to know everyone‚Äôs full queries. RFC 7816 proposes query name minimization, or QNAME minimization, as an alternative. The idea is to send the root server just a query for .edu, then the.edu nameserver just a query for luc.edu, and so on. After each NS query, one more DNS label is attached to the query name before proceeding to the next query to the next nameserver. This way, no nameserver learns more of the query than the absolute minimum. There is a potential catch, though, as not every level of the DNS name corresponds to a different nameserver; to put it another way, not every ‚Äò.‚Äô in a DNS name corresponds to a zone break. For example, it is now (2020) the case that the luc.edu nameserver is responsible for the formerly independent cs.luc.edu name hierarchy, and so there is no longer a need for a cs.luc.edu NS record. There happens still to be one, for legacy reasons, but if that were not the case, then an NS query sent to the luc.edu nameserver cs.luc.edu might return with the literally correct NODATA, or, worse, NXDOMAIN (the latter is not supposed to happen, but sometimes does). TheRFC 7816 solution to this, when a negative answer is received, is to include one more DNS-name level and repeat the query. That is, if a lookup for cs.luc.edu failed, try the full name. That said, this and other issues mean that query name minimization has not quite seen widespread adoption; see this APNIC blog post for some actual measurements. It is worth noting that the only privacy protection achieved here is from non-leaf DNS nameservers. Also, ones local DNS resolver still has full information about each query it is sent. In the presence of active caching, a local resolver would generally not need to query the root or the .edu nameservers at all. 10.1.2.2 Naked Domains If we look up both www.cs.luc.edu andcs.luc.edu, we see they resolve to the same address. The use ofwww as a hostname for a domain‚Äôs webserver is sometimes considered unnecessary and old-fashioned; many users and website administrators prefer the shorter, ‚Äúnaked‚Äù domain name, egcs.luc.edu. It might be tempting to create a CNAME record for the naked domain, cs.luc.edu, pointing to the full hostnamewww.cs.luc.edu. However, RFC 1034 does not allow this: 238 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 If a CNAME RR is present at a node, no other data should be present; this ensures that the data for a canonical name and its aliases cannot be different. There are, however, several other DNS data records for cs.luc.edu: an NS record (above), a SOA, or Start of Authority, record containing various administrative data such as the expiration time, and an MX record, discussed in the following section. All this makes www.cs.luc.edu andcs.luc.edu ineluctably quite different. RFC 1034 adds, ‚Äúthis rule also insures that a cached CNAME can be used without checking with an authoritative server for other RR types.‚Äù A better way to create a naked-domain record, at least from the perspective of DNS, is to give it its own A record. This does mean that, if the webserver address changes, there are now twoDNS records that need to be updated, but this is manageable. Recently ANAME records have been proposed to handle this issue; an ANAME is like a limited CNAME not subject to the RFC 1034 restriction above. An ANAME record for a naked domain, pointing to another hostname rather than to address, is legal. See the Internet draft draft-hunt-dnsop-aname. Some large CDNs (1.12.2 Content-Distribution Networks ) already implement similar DNS tweaks internally. This does not require end-user awareness; the user requests an A record and the ANAME is resolved at the CDN side. Finally, there is also an argument, at least when HTTP (web) trafÔ¨Åc is involved, that the www notbe deprecated, and that the naked domain should instead be redirected, at the HTTP layer, to the full hostname. This simpliÔ¨Åes some issues; for example, you now have only one website, rather than two (though it does add an extra RTT). You no longer have to be concerned with the fact that HTTP cookies with and without the ‚Äúwww‚Äù are different. And some CDNs may not be able to handle website failover to another server if the naked domain is reached via an A record. But none of these are DNS issues. 10.1.3 Other DNS Records Besides address lookups, DNS also supports a few other kinds of searches. The best known is probably reverse DNS, which takes an IP address and returns a name. This is slightly complicated by the fact that one IP address may be associated with multiple DNS names. What DNS does in this case is to return the canonical name, or CNAME; a given address can have only one CNAME. Given an IPv4 address, say 147.126.1.230, the idea is to reverse it and append to it the sufÔ¨Åx in-addr. arpa. 230.1.126.147. in-addr.arpa There is a DNS name hierarchy for names of this form, with zones and authoritative servers. If all this has been conÔ¨Ågured ‚Äì which it often is not, especially for user workstations ‚Äì a request for the PTR record corresponding to the above should return a DNS hostname. In the case above, the name luc.edu is returned (at least as of 2018). PTR records are the only DNS records to have an entirely separate hierarchy; other DNS types Ô¨Åt into the ‚Äústandard‚Äù hierarchy. For example, DNS also supports MX, or Mail eXchange, records, meant to map a domain name (which might not correspond to any hostname, and, if it does, is more likely to correspond to the name of a web server) to the hostname of a server that accepts email on behalf of the domain. In effect this allows an organization‚Äôs domain name, egluc.edu, to represent both a web server and, at a different IP address, an email server. MX records can even represent a setof IP addresses that accept email. 10.1 DNS 239
An Introduction to Computer Networks, Release 2.0.11 DNS has from the beginning supported TXT records, for arbitrary text strings. The email Sender Policy Framework ( RFC 7208 ) was developed to make it harder for email senders to pretend to be a domain they are not; this involves inserting so-called SPF records as DNS TXT records. For example, a DNS query for TXT records of google.com (not gmail.com!) might yield (2018) google.com text = "docusign=05958488-4752-4ef2-95eb-aa7ba8a3bd0e" google.com text = "v=spf1 include:_spf.google.com ~all" The SPF system is interested in only the second record; the ‚Äúv=spf1‚Äù speciÔ¨Åes the SPF version. This second record tells us to look up _spf.google.com. That lookup returns text ="v=spf1 include:_netblocks.google.com include:_netblocks2.google.com √£√ëinclude:_netblocks3.google.com ~all" Lookup of_netblocks.google.com then returns text ="v=spf1 ip4:64.233.160.0/19 ip4:66.102.0.0/20 ip4:66.249.80.0/20 √£√ëip4:72.14.192.0/18 ip4:74.125.0.0/16 ip4:108.177.8.0/21 ip4:173.194.0.0/16 √£√ëip4:209.85.128.0/17 ip4:216.58.192.0/19 ip4:216.239.32.0/19 ~all" If a host connects to an email server, and declares that it is delivering mail from someone at google.com, then the host‚Äôs email list should occur in the list above, or in one of the other included lists. If it does not, there is a good chance the email represents spam. Each DNS record (or ‚Äúresource record‚Äù) has a name ( egcs.luc.edu ) and a type ( egAorAAAA orNS orMX). Given a name and type, the set of matching resource records is known as the RRset for that name and type (technically there is also a ‚Äúclass‚Äù, but the class of all the DNS records we are interested in is IN, for Internet). When a nameserver responds to a DNS query, what is returned (in the ANSWER section) is always an entire RRset: the RRset of all resource records matching the name and type contained in the original query. In many cases, RRsets have a single member, because many hosts have a single IPv4 address. However, this is not universal. We saw above the example of a single DNS name having multiple A records when round-robin DNS is used. A single DNS name might also have separate A records for the host‚Äôs public and private IPv4 addresses. TXT records, too, often contain multiple entries in a single RRset. In the TXT example above we saw that SPF data was stored in DNS TXT records, but there are other protocols besides SPF that also use TXT records; examples include DMARC and google-site-veriÔ¨Åcation <https://support.google.com/webmasters/answer/9008080?hl=en&visit_id=6373641543224860311726883571&rd=1>_. Finally, perhaps most MX-record (Mail eXchange) RRsets have multiple entries, as organizations often prefer, for redundancy, to have more than one server that can receive email. 10.1.4 DNS Cache Poisoning The classic DNS security failure, known as cache poisoning, occurs when an attacker has been able to convince a DNS resolver that the address of, say, www.example.com is something other than what it really is. A successful attack means the attacker can direct trafÔ¨Åc meant for www.example.com to the attacker‚Äôs own, malicious site. The most basic cache-poisoning strategy is to send a stream of DNS reply packets to the resolver which declare that the IP address of www.example.com is the attacker‚Äôs chosen IP address. The source IP address 240 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 of these packets should be spoofed to be that of the example.com authoritative nameserver; such spooÔ¨Ång is relatively easy using UDP. Most of these reply packets will be ignored, but the hope is that one will arrive shortly after the resolver has sent a DNS request to the example.com authoritative nameserver, and interprets the spoofed reply packet as a legitimate reply. To prevent this, DNS requests contain a 16-bit ID Ô¨Åeld; the DNS response must echo this back. The response must also come from the correct port. This leaves the attacker to guess 32 bits in all, but often the ID Ô¨Åeld (and even more often the port) can be guessed based on past history. Another approach requires the attacker to wait for the target resolver to issue a legitimate request to the attacker‚Äôs site, attacker.com. The attacker then piggybacks in the ADDITIONAL section of the reply message an A record for example.com pointing to the attacker‚Äôs chosen bad IP address for this site. The hope is that the receiving resolver will place these A records from the ADDITIONAL section into its cache without verifying them further and without noticing they are completely unrelated. Once upon a time, such DNS resolver behavior was common. Most newer DNS resolvers carefully validate the replies: the ID Ô¨Åeld must match, the source port must match, and any received DNS records in the ADDITIONAL section must match, at a minimum, the DNS zone of the request. Additionally, the request ID Ô¨Åeld and source port should be chosen pseudorandomly in a secure fashion. For additional vulnerabilities, see RFC 3833. The central risk in cache poisoning is that a resolver can be tricked into supplying users with invalid DNS records. A closely related risk is that an attacker can achieve the same result by spooÔ¨Ång an authoritative nameserver. Both of these risks can be mitigated through the use of the DNS security extensions, known as DNSSEC. Because DNSSEC makes use of public-key signatures, we defer coverage to 29.7 DNSSEC. 10.1.5 DNS and CDNs DNS is often pressed into service by CDNs ( 1.12.2 Content-Distribution Networks ) to identify their closest ‚Äúedge‚Äù server to a given user. Typically this involves the use of geoDNS, a slightly nonstandard variation of DNS. When a DNS query comes in to one of the CDN‚Äôs authoritative nameservers, that server 1. looks up the approximate location of the client ( 14.4.4 IP Geolocation ) 2. determines the closest edge server to that location 3. replies with the IP address of that closest edge server This works reasonably well most of the time. However, the requesting client is essentially never the end user; rather, it is the DNS resolver being used by the user. Typically such resolvers are the site resolvers provided by the user‚Äôs ISP or organization, and are physically quite close to the user; in this case, the edge server identiÔ¨Åed above will be close to the user as well. However, when a user has chosen a (likely remote) public DNS resolver, as above, the IP address returned for the CDN edge server will be close to the DNS resolver but likely far from optimal for the end user. One solution to this last problem is addressed by RFC 7871, which allows DNS resolvers to include the IP address of the client in the request sent to the authoritative nameserver. For privacy reasons, usually only a preÔ¨Åx of the user‚Äôs IP address is included, perhaps /24. Even so, user‚Äôs privacy is at least partly compromised. For this reason, RFC 7871 recommends that the feature be disabled by default, and only enabled after careful analysis of the tradeoffs. 10.1 DNS 241
An Introduction to Computer Networks, Release 2.0.11 A user who is concerned about the privacy issue can ‚Äì in theory ‚Äì conÔ¨Ågure their own DNS software to include this RFC 7871 option with a zero-length preÔ¨Åx of the user‚Äôs IP address, which conveys no address information. The user‚Äôs resolver will then not change this to a longer preÔ¨Åx. Use of this option also means that the DNS resolver receiving a user query about a given hostname can no longer simply return a cached answer from a previous lookup of the hostname. Instead, the resolver needs to cache separately each xhostname,preÔ¨Åx ypair it handles, where the preÔ¨Åx is the preÔ¨Åx of the user‚Äôs IP address forwarded to the authoritative nameserver. This has the potential to increase the cache size by several orders of magnitude, which may thereby enable some cache-overÔ¨Çow attacks. 10.2 Address Resolution Protocol: ARP If a host or router A Ô¨Ånds that the destination IP address D = D IPmatches the network address of one of its interfaces, it is to deliver the packet via the LAN (probably Ethernet). This means looking up the LAN address (MAC address) D LANcorresponding to D IP. How does it do this? One approach would be via a special server, but the spirit of early IPv4 development was to avoid such servers, for both cost and reliability issues. Instead, the Address Resolution Protocol (ARP) is used. This is our Ô¨Årst protocol that takes advantage of the existence of LAN-level broadcast; on LANs without physical broadcast (such as ATM), some other mechanism (usually involving a server) must be used. The basic idea of ARP is that the host A sends out a broadcast ARP query or ‚Äúwho-has D IP?‚Äù request, which includes A‚Äôs own IPv4 and LAN addresses. All hosts on the LAN receive this message. The host for whom the message is intended, D, will recognize that it should reply, and will return an ARP reply or ‚Äúis-at‚Äù message containing D LAN. Because the original request contained A LAN, D‚Äôs response can be sent directly to A, that is, unicast. A B C D A broadcasts ‚Äúwho-has D‚Äù D replies to A, unicast, and includes its LAN address Additionally, all hosts maintain an ARP cache, consisting of xIPv4,LANyaddress pairs for other hosts on the network. After the exchange above, A has xDIP,DLANyin its table; anticipating that A will soon send it a packet to which it needs to respond, D also puts xAIP,ALANyinto its cache. ARP-cache entries eventually expire. The timeout interval used to be on the order of 10 minutes, but Linux systems now use a much smaller timeout (~30 seconds observed in 2012). Somewhere along the line, and probably related to this shortened timeout interval, repeat ARP queries about a timed-out entry are Ô¨Årst sent unicast, not broadcast, to the previous Ethernet address on record. This cuts down on the total amount of broadcast trafÔ¨Åc; LAN broadcasts are, of course, still needed for new hosts. The ARP cache on a Linux system can be examined with the command ip -s neigh; the corresponding windows command is arp -a. The above protocol is sufÔ¨Åcient, but there is one further point. When A sends its broadcast ‚Äúwho-has D?‚Äù ARP query, all other hosts C check their own cache for an entry for A. If there issuch an entry (that is, if AIPis found there), then the value for A LANis updated with the value taken from the ARP message; if there 242 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 is no pre-existing entry then no action is taken. This update process serves to avoid stale ARP-cache entries, which can arise is if a host has had its Ethernet interface replaced. (USB Ethernet interfaces, in particular, can be replaced very quickly.) ARP is quite an efÔ¨Åcient mechanism for bridging the gap between IPv4 and LAN addresses. Nodes generally Ô¨Ånd out neighboring IPv4 addresses through higher-level protocols, and ARP then quickly Ô¨Ålls in the missing LAN address. However, in some Software-DeÔ¨Åned Networking ( 3.4 Software-DeÔ¨Åned Networking ) environments, the LAN switches and/or the LAN controller may have knowledge about IPv4/LAN address correspondences, potentially making ARP superÔ¨Çuous. The LAN (Ethernet) switching network might in principle even know exactly how to route via the LAN to a given IPv4 address, potentially even making LAN addresses unnecessary. At such a point, ARP may become an inconvenience. For an example of a situation in which it is necessary to work around ARP, see 30.9.5 loadbalance31.py. 10.2.1 ARP Finer Points Most hosts today implement self-ARP, orgratuitous ARP, on startup (or wakeup): when station A starts up it sends out an ARP query for itself: ‚Äúwho-has A?‚Äù. Two things are gained from this: Ô¨Årst, all stations that had A in their cache are now updated with A‚Äôs most current A LANaddress, in case there was a change, and second, if an answer is received, then presumably some other host on the network has the same IPv4 address as A. Self-ARP is thus the traditional IPv4 mechanism for duplicate address detection. Unfortunately, it does not always work as well as might be hoped; often only a single self-ARP query is sent, and if a reply is received then frequently the only response is to log an error message; the host may even continue using the duplicate address! If the duplicate address was received via DHCP, below, then the host is supposed to notify its DHCP server of the error and request a different IPv4 address. RFC 5227 has deÔ¨Åned an improved mechanism known as Address ConÔ¨Çict Detection, or ACD. A host using ACD sends out three ARP queries for its new IPv4 address, spaced over a few seconds and leaving the ARP Ô¨Åeld for the sender‚Äôs IPv4 address Ô¨Ålled with zeroes. This last step means that any other host with that IPv4 address in its cache will ignore the packet, rather than update its cache. If the original host receives no replies, it then sends out two more ARP queries for its new address, this time with the ARP Ô¨Åeld for the sender‚Äôs IPv4 address Ô¨Ålled in with the new address; this is the stage at which other hosts on the network will make any necessary cache updates. Finally, ACD requires that hosts that do detect a duplicate address must discontinue using it. It is also possible for other stations to answer an ARP query on behalf of the actual destination D; this is called proxy ARP. An early common scenario for this was when host C on a LAN had a modem connected to a serial port. In theory a host D dialing in to this modem should be on a different subnet, but that requires allocation of a new subnet. Instead, many sites chose a simpler arrangement. A host that dialed in to C‚Äôs serial port might be assigned IP address D IP, from the same subnet as C. C would be conÔ¨Ågured to route packets to D; that is, packets arriving from the serial line would be forwarded to the LAN interface, and packets sent to C LANaddressed to D IPwould be forwarded to D. But we also have to handle ARP, and as D is not actually on the LAN it will not receive broadcast ARP queries. Instead, C would be conÔ¨Ågured to answer on behalf of D, replying with xDIP,CLANy. This generally worked quite well. Proxy ARP is also used in Mobile IP, for the so-called ‚Äúhome agent‚Äù to intercept trafÔ¨Åc addressed to the ‚Äúhome address‚Äù of a mobile device and then forward it ( egvia tunneling) to that device. See 9.9 Mobile IP. One delicate aspect of the ARP protocol is that stations are required to respond to a broadcast query. In 10.2 Address Resolution Protocol: ARP 243
An Introduction to Computer Networks, Release 2.0.11 the absence of proxies this theoretically should not create problems: there should be only one respondent. However, there were anecdotes from the Elder Days of networking when a broadcast ARP query would trigger an avalanche of responses. The protocol-design moral here is that determining who is to respond to a broadcast message should be done with great care. ( RFC 1122 section 3.2.2 addresses this same point in the context of responding to broadcast ICMP messages.) ARP-query implementations also need to include a timeout and some queues, so that queries can be resent if lost and so that a burst of packets does not lead to a burst of queries. A naive ARP algorithm without these might be: To send a packet to destination D IP, see if D IPis in the ARP cache. If it is, address the packet to D LAN; if not, send an ARP query for D To see the problem with this approach, imagine that a 32 kB packet arrives at the IP layer, to be sent over Ethernet. It will be fragmented into 22 fragments (assuming an Ethernet MTU of 1500 bytes), all sent at once. The naive algorithm above will likely send an ARP query for each of these. What we need instead is something like the following: To send a packet to destination D IP: If D IPis in the ARP cache, send to D LANand return If not, see if an ARP query for D IPis pending. If it is, put the current packet in a queue for D. If there is no pending ARP query for D IP, start one, again putting the current packet in the (new) queue for D We also need: If an ARP query for some C IPtimes out, resend it (up to a point) If an ARP query for C IPis answered, send off any packets in C‚Äôs queue 10.2.2 ARP Security Suppose A wants to log in to secure server S, using a password. How can B (for Bad) impersonate S? Here is an ARP-based strategy, sometimes known as ARP SpooÔ¨Ång. First, B makes sure the real S is down, either by waiting until scheduled downtime or by launching a denial-of-service attack against S. When A tries to connect, it will begin with an ARP ‚Äúwho-has S?‚Äù. All B has to do is answer, ‚ÄúS is-at B‚Äù. There is a trivial way to do this: B simply needs to set its own IP address to that of S. A will connect, and may be convinced to give its password to B. B now simply responds with something plausible like ‚Äúbackup in progress; try later‚Äù, and meanwhile use A‚Äôs credentials against the real S. This works even if the communications channel A uses is encrypted! If A is using the SSH protocol (29.5.1 SSH ), then A will get a message that the other side‚Äôs key has changed (B will present its own SSH key, not S‚Äôs). Unfortunately, many users (and even some IT departments) do not recognize this as a serious problem. Some organizations ‚Äì especially schools and universities ‚Äì use personal workstations with ‚Äúfrozen‚Äù conÔ¨Åguration, so that the Ô¨Ålesystem is reset to its original state on every reboot. Such systems may be resistant to viruses, but in these environments the user at A will always get a message to the effect that S‚Äôs credentials are not known. 244 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 10.2.3 ARP Failover Suppose you have two front-line servers, A and B (B for Backup), and you want B to be able to step in if A freezes. There are a number of ways of achieving this, but one of the simplest is known as ARP Failover. First, we set A IP= B IP, but for the time being B does not use the network so this duplication is not a problem. Then, once B gets the message that A is down, it sends out an ARP query for A IP, including B LANas the source LAN address. The gateway router, which previously would have had xAIP,ALANyin its ARP cache, updates this to xAIP,BLANy, and packets that had formerly been sent to A will now go to B. As long as B is trafÔ¨Åcking in stateless operations ( eghtml), B can pick up right where A left off. 10.2.4 Detecting Sniffers Finally, there is an interesting use of ARP to detect Ethernet password sniffers (generally not quite the issue it once was, due to encryption and switching). To Ô¨Ånd out if a particular host A is in promiscuous mode, send an ARP ‚Äúwho-has A?‚Äù query. Address it not to the broadcast Ethernet address, though, but to some nonexistent Ethernet address. If promiscuous mode is off, A‚Äôs network interface will ignore the packet. But if promiscuous mode is on, A‚Äôs network interface will pass the ARP request to A itself, which is likely then to answer it. Alas, Linux kernels reject at the ARP-software level ARP queries to physical Ethernet addresses other than their own. However, they do respond to faked Ethernet multicast addresses, such as ff:ff:ff:00:00:00 or ff:ff:ff:ff:ff:fe. 10.2.5 ARP and multihomed hosts If host A has two interfaces iface1 andiface2 on the same LAN, with respective IP addresses A 1and A2, then it is common for the two to be used interchangeably. TrafÔ¨Åc addressed to A 1may be received via iface2 and vice-versa, and trafÔ¨Åc from A1may be sent via iface2. In9.2.1 Multihomed hosts this is described as the weak end-system model; the idea is that we should think of the IP addresses A 1and A 2as bound to A rather than to their respective interfaces. In support of this model, ARP can usually be conÔ¨Ågured (in fact this is often the default) so that ARP requests for either IP address and received by either interface may be answered with either physical address. Usually all requests are answered with the physical address of the preferred ( iefaster) interface. As an example, suppose A has an Ethernet interface eth0 with IP address 10.0.0.2 and a faster Wi-Fi interfacewlan0 with IP address 10.0.0.3 (although Wi-Fi interfaces are notalways faster). In this setting, an ARP request ‚Äúwho-has 10.0.0.2‚Äù would be answered with wlan0 ‚Äôs physical address, and so all trafÔ¨Åc to A, to either IP address, would arrive via wlan0. Theeth0 interface would go essentially unused. Similarly, though not due to ARP, trafÔ¨Åc sent by A with source address 10.0.0.2 might depart via wlan0. This situation is on Linux systems adjusted by changing arp_ignore andarp_announce in/proc/ sys/net/ipv4/conf/all. 10.2 Address Resolution Protocol: ARP 245
An Introduction to Computer Networks, Release 2.0.11 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) DHCP is the most common mechanism by which hosts are assigned their IPv4 addresses. DHCP started as a protocol known as Reverse ARP (RARP), which evolved into BOOTP and then into its present form. It is documented in RFC 2131. Recall that ARP is based on the idea of someone broadcasting an ARP query for a host, containing the host‚Äôs IPv4 address, and the host answering it with its LAN address. DHCP involves a host, at startup, broadcasting a query containing its own LAN address, and having a server reply telling the host what IPv4 address is assigned to it, hence the ‚ÄúReverse ARP‚Äù name. The DHCP response message is also likely to carry, piggybacked onto it, several other essential startup options. Unlike the IPv4 address, these additional network parameters usually do not depend on the speciÔ¨Åc host that has sent the DHCP query; they are likely constant for the subnet or even the site. In all, a typical DHCP message includes the following: 
- IPv4 address 
- subnet mask 
- default router 
- DNS Server These four items are a standard minimal network conÔ¨Åguration; in practical terms, hosts cannot function properly without them. Most DHCP implementations support the piggybacking of the latter three above, and a wide variety of other conÔ¨Åguration values, onto the server responses. Default Routers and DHCP If you lose your default router, you cannot communicate. Here is something that used to happen to me, courtesy of DHCP: 1. I am connected to the Internet via Ethernet, and my default router is via my Ethernet interface 2. I connect to my institution‚Äôs wireless network. 3. Their DHCP server sends me a new default router on the wireless network. However, this default router will only allow access to a tiny private network, because I have neglected to complete the ‚ÄúWi-Fi network registration‚Äù process. 4. I therefore disconnect from the wireless network, and my wireless-interface default router goes away. However, my system does not automatically revert to my Ethernet default-router entry; DHCP does not work that way. As a result, I will have no router at all until the next scheduled DHCP lease renegotiation, and must Ô¨Åx things manually. The DHCP server has a range of IPv4 addresses to hand out, and maintains a database of which IPv4 address has been assigned to which LAN address. Reservations can either be permanent or dynamic; if the latter, hosts typically renew their DHCP reservation periodically (typically one to several times a day). 246 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 10.3.1 NAT, DHCP and the Small OfÔ¨Åce If you have a large network, with multiple subnets, a certain amount of manual conÔ¨Åguration is inevitable. What about, however, a home or small ofÔ¨Åce, with a single line from an ISP? A combination of NAT (9.7 Network Address Translation ) and DHCP has made autoconÔ¨Åguration close to a reality. The typical home/small-ofÔ¨Åce ‚Äúrouter‚Äù is in fact a NAT router ( 9.7 Network Address Translation ) coupled with an Ethernet switch, and usually also coupled with a Wi-Fi access point and a DHCP server. In this section, we will use the term ‚ÄúNAT router‚Äù to refer to this whole package. One specially designated port, theexternal port, connects to the ISP‚Äôs line, and uses DHCP as a client to obtain an IPv4 address for that port. The other, internal, ports are connected together by an Ethernet switch; these ports as a group are connected to the external port using NAT translation. If wireless is supported, the wireless side is connected directly to the internal ports. Isolated from the Internet, the internal ports can thus be assigned an arbitrary non-public IPv4 address block, eg192.168.0.0/24. The NAT router typically contains a DCHP server, usually enabled by default, that will hand out IPv4 addresses to everything connecting from the internal side. Generally this works seamlessly. However, if a second NAT router is also connected to the network (sometimes attempted to extend Wi-Fi range, in lieu of a commercial Wi-Fi repeater), one then has two operating DHCP servers on the same subnet. This often results in chaos, though is easily Ô¨Åxed by disabling one of the DHCP servers. While omnipresent DHCP servers have made IPv4 autoconÔ¨Åguration work ‚Äúout of the box‚Äù in many cases, in the era in which IPv4 was designed the need for such servers would have been seen as a signiÔ¨Åcant drawback in terms of expense and reliability. IPv6 has an autoconÔ¨Åguration strategy ( 11.7.2 Stateless AutoconÔ¨Åguration (SLAAC) ) that does not require DHCP, though DHCPv6 may well end up displacing it. 10.3.2 DHCP and Routers It is often desired, for larger sites, to have only one or two DHCP servers, but to have them support multiple subnets. Classical DHCP relies on broadcast, which isn‚Äôt forwarded by routers, and even if it were, the DHCP server would have no way of knowing on what subnet the host in question was actually located. This is generally addressed by DHCP Relay (sometimes still known by the older name BOOTP Relay). The router (or, sometimes, some other node on the subnet) receives the DHCP broadcast message from a host, and notes the subnet address of the arrival interface. The router then relays the DHCP request, together with this subnet address, to the designated DHCP Server; this relayed message is sent directly (unicast), not broadcast. Because the subnet address is included, the DHCP server can Ô¨Ågure out the correct IPv4 address to assign. This feature has to be specially enabled on the router. 10.4 Internet Control Message Protocol The Internet Control Message Protocol, or ICMP, is a protocol for sending IP-layer error and status messages; it is deÔ¨Åned in RFC 792. ICMP is, like IP, host-to-host, and so they are never delivered to a speciÔ¨Åc port, even if they are sent in response to an error related to something sent from that port. In other words, 10.4 Internet Control Message Protocol 247
An Introduction to Computer Networks, Release 2.0.11 individual UDP and TCP connections do not receive ICMP messages, even when it would be helpful to get them. ICMP messages are identiÔ¨Åed by an 8-bit type Ô¨Åeld, followed by an 8-bit subtype, or code. Here are the more common ICMP types, with subtypes listed in the description. Type Description Echo Request ping queries Echo Reply ping responses Destination Unreachable Destination network unreachable Destination host unreachable Destination port unreachable Fragmentation required but DF Ô¨Çag set Network administratively prohibited Source Quench Congestion control Redirect Message Redirect datagram for the network Redirect datagram for the host Redirect for TOS and network Redirect for TOS and host Router Solicitation Router discovery/selection/solicitation Time Exceeded TTL expired in transit Fragment reassembly time exceeded Bad IP Header or Parameter Pointer indicates the error Missing a required option Bad length Timestamp Timestamp Reply Likeping, but requesting a timestamp from the destination The Echo and Timestamp formats are queries, sent by one host to another. Most of the others are all error messages, sent by a router to the sender of the offending packet. Error-message formats contain the IP header and next 8 bytes of the packet in question; the 8 bytes will contain the TCP or UDP port numbers. Redirect and Router Solicitation messages are informational, but follow the error-message format. Query formats contain a 16-bit Query IdentiÔ¨Åer, assigned by the query sender and echoed back by the query responder. ping Packet Size The author once had to diagnose a problem where pings were almost 100% successful, and yet Ô¨Åle transfers failed immediately; this could have been the result of either a network fault or a Ô¨Åle-transfer application fault. The problem turned out to be a failed network device with a very high bit-error rate: 1500-byte Ô¨Åle-transfer packets were frequently corrupted, but ping packets, with a default size of 32-64 bytes, were mostly unaffected. If the bit-error rate is such that 1500-byte packets have a 50% success rate, 50-byte packets can be expected to have a 98% ( 0.51/30) success rate. Setting the ping packet size to a larger value made it immediately clear that the network, and not the Ô¨Åle-transfer application, was at fault. ICMP is perhaps best known for Echo Request/Reply, on which the ping tool ( 1.14 Some Useful Utilities ) is based. Ping remains very useful for network troubleshooting: if you can ping a host, then the network is reachable, and any problems are higher up the protocol chain. Unfortunately, ping replies are often blocked by many Ô¨Årewalls, on the theory that revealing even the existence of computers is a security risk. While this 248 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 may sometimes be an appropriate decision, it does signiÔ¨Åcantly impair the utility of ping. Ping can be asked to include IP timestamps ( 9.1 The IPv4 Header ) on Linux systems with the -Toption, and on Windows with -s. Source Quench was used to signal that congestion has been encountered. A router that drops a packet due to congestion experience was encouraged to send ICMP Source Quench to the originating host. Generally the TCP layer would handle these appropriately (by reducing the overall sending rate), but UDP applications never receive them. ICMP Source Quench did not quite work out as intended, and was formally deprecated byRFC 6633. (Routers can inform TCP connections of impending congestion by using the ECN bits.) The Destination Unreachable type has a large number of subtypes: 
- Network unreachable: some router had no entry for forwarding the packet, and no default route 
- Host unreachable: the packet reached a router that was on the same LAN as the host, but the host failed to respond to ARP queries 
- Port unreachable: the packet was sent to a UDP port on a given host, but that port was not open. TCP, on the other hand, deals with this situation by replying to the connecting endpoint with a reset packet. Unfortunately, the UDP Port Unreachable message is sent to the host, not to the application on that host that sent the undeliverable packet, and so is close to useless as a practical way for applications to be informed when packets cannot be delivered. 
- Fragmentation required but DF Ô¨Çag set: a packet arrived at a router and was too big to be forwarded without fragmentation. However, the Don‚Äôt Fragment bit in the IPv4 header was set, forbidding fragmentation. 
- Administratively Prohibited: this is sent by a router that knows it can reach the network in question, but has conÔ¨Ågureintro to drop the packet and send back Administratively Prohibited messages. A router can also be conÔ¨Ågured to blackhole messages: to drop the packet and send back nothing. In18.6 Path MTU Discovery we will see how TCP uses the ICMP message Fragmentation required but DF Ô¨Çag set as part of Path MTU Discovery, the process of Ô¨Ånding the largest packet that can be sent to a speciÔ¨Åc destination without fragmentation. The basic idea is that we set the DF bit on some of the packets we send; if we get back this message, that packet was too big. Some sites and Ô¨Årewalls block ICMP packets in addition to Echo Request/Reply, and for some messages one can get away with this with relatively few consequences. However, blocking Fragmentation required but DF Ô¨Çag set has the potential to severely affect TCP connections, depending on how Path MTU Discovery is implemented, and thus is not recommended. If ICMP Ô¨Åltering is contemplated, it is best to base block/allow decisions on the ICMP type, or even on the type and code. For example, most Ô¨Årewalls support rule sets of the form ‚Äúallow ICMP destination-unreachable; block all other ICMP‚Äù. TheTimestamp option works something like Echo Request/Reply, but the receiver includes its own local timestamp for the arrival time, with millisecond accuracy. See also the IP Timestamp option, 9.1 The IPv4 Header, which appears to be more frequently used. The type/code message format makes it easy to add new ICMP types. Over the years, a signiÔ¨Åcant number of additional such types have been deÔ¨Åned; a complete list is maintained by the IANA. Several of these later ICMP types were seldom used and eventually deprecated, many by RFC 6918. ICMP packets are usually forwarded correctly through NAT routers, though due to the absence of port numbers the router must do a little more work. RFC 3022 andRFC 5508 address this. For ICMP queries, 10.4 Internet Control Message Protocol 249
An Introduction to Computer Networks, Release 2.0.11 like ping, the ICMP Query IdentiÔ¨Åer Ô¨Åeld can be used to recognize the returning response. ICMP error messages are a little trickier, because there is no direct connection between the inbound error message and any of the previous outbound non-ICMP packets that triggered the response. However, the headers of the packet that triggered the ICMP error message are embedded in the body of the ICMP message. The NAT router can look at those embedded headers to determine how to forward the ICMP message (the NAT router must also rewrite the addresses of those embedded headers). 10.4.1 Traceroute and Time Exceeded The traceroute program uses ICMP Time Exceeded messages. A packet is sent to the destination (often UDP to an unused port), with the TTL set to 1. The Ô¨Årst router the packet reaches decrements the TTL to 0, drops it, and returns an ICMP Time Exceeded message. The sender now knows the Ô¨Årst router on the chain. The second packet is sent with TTL set to 2, and the second router on the path will be the one to return ICMP Time Exceeded. This continues until Ô¨Ånally the remote host returns something, likely ICMP Port Unreachable. For an example of traceroute output, see 1.14 Some Useful Utilities. In that example, the three traceroute probes for the Nth router are sometimes answered by two or even three different routers; this suggests routers conÔ¨Ågured to work in parallel rather than route changes. Many routers no longer respond with ICMP Time Exceeded messages when they drop packets. For the distance value corresponding to such a router, traceroute reports ***. Traceroute assumes the path does not change. This is not always the case, although in practice it is seldom an issue. Route EfÔ¨Åciency Once upon a time (~2001), traceroute showed that trafÔ¨Åc from my home to the ofÔ¨Åce, both in the Chicago area, went through the MAE-EAST Internet exchange point, outside of Washington DC. That inefÔ¨Åcient route was later Ô¨Åxed. A situation like this is typically caused by two higher-level providers who did not negotiate sufÔ¨Åcient Internet exchange points. Traceroute to a nonexistent site works up to the point when the packet reaches the Internet ‚Äúbackbone‚Äù: the Ô¨Årst router which does not have a default route. At that point the packet is not routed further (and an ICMP Destination Network Unreachable should be returned). Traceroute also interacts somewhat oddly with routers using MPLS (see 25.12 Multi-Protocol Label Switching (MPLS) ). Such routers ‚Äì most likely large-ISP internal routers ‚Äì may continue to forward the ICMP Time Exceeded message on further towards its destination before returning it to the sender. As a result, the roundtrip time measurements reported may be quite a bit larger than they should be. 10.4.2 Redirects Most non-router hosts start up with an IPv4 forwarding table consisting of a single (default) router, discovered along with their IPv4 address through DHCP. ICMP Redirect messages help hosts learn of other useful routers. Here is a classic example: 250 10 IPv4 Companion Protocols
An Introduction to Computer Networks, Release 2.0.11 R1 B R2 A A is conÔ¨Ågured so that its default router is R1. It addresses a packet to B, and sends it to R1. R1 receives the packet, and forwards it to R2. However, R1 also notices that R2 and A are on the same network, and so A could have sent the packet to R2 directly. So R1 sends an appropriate ICMP redirect message to A (‚ÄúRedirect Datagram for the Network‚Äù), and A adds a route to B via R2 to its own forwarding table. 10.4.3 Router Solicitation These ICMP messages are used by some router protocols to identify immediate neighbors. When we look at routing-update algorithms, 13 Routing-Update Algorithms, these are where the process starts. 10.5 Epilog At this point we have concluded the basic mechanics of IPv4. Still to come is a discussion of how IP routers build their forwarding tables. This turns out to be a complex topic, divided into routing within single organizations and ISPs ‚Äì 13 Routing-Update Algorithms ‚Äì and routing between organizations ‚Äì 14 Large-Scale IP Routing. But before that, in the next chapter, we compare IPv4 with IPv6, now twenty years old but still seeing limited adoption. The biggest issue Ô¨Åxed by IPv6 is IPv4‚Äôs lack of address space, but there are also several other less dramatic improvements. 10.6 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0. In 10.2 Address Resolution Protocol: ARP it was stated that, in newer implementations, ‚Äúrepeat ARP queries about a timed out entry are Ô¨Årst sent unicast‚Äù, in order to reduce broadcast trafÔ¨Åc. Suppose host A uses ARP to retrieve B‚Äôs LAN address (MAC address). A short time later, B changes its LAN address, either through a hardware swap or through software reconÔ¨Åguration. (a). What will happen if A now sends a unicast repeat ARP query for B? (b). What will happen if A now sends a broadcast repeat ARP query for B? 2.0. Suppose A broadcasts an ARP query ‚Äúwho-has B?‚Äù, receives B‚Äôs response, and proceeds to send B a regular IPv4 packet. If B now wishes to reply, why is it likely that A will already be present in B‚Äôs ARP cache? Identify a circumstance under which this can fail. 10.5 Epilog 251
An Introduction to Computer Networks, Release 2.0.11 3.0. Suppose A broadcasts an ARP request ‚Äúwho-has B‚Äù, but inadvertently lists the physical address of another machine C instead of its own (that is, A‚Äôs ARP query has IP src= A, but LAN src= C). What will happen? Will A receive a reply? Will any other hosts on the LAN be able to send to A? What entries will be made in the ARP caches on A, B and C? 4.0. Suppose host A connects to the Internet via Wi-Fi. The default router is R W. Host A now begins exchanging packets with a remote host B: A sends to B, B replies, etc. The exact form of the connection does not matter, except that TCP may not work. (a). You now plug in A‚Äôs Ethernet cable. The Ethernet port is assumed to be on a different subnet from the Wi-Fi (so that the strong and weak end-system models of 10.2.5 ARP and multihomed hosts do not play a role here). Assume A automatically selects the new Ethernet connection as its default route, with router R E. What happens to the original connection to A? Can packets still travel back and forth? Does the return address used for either direction change? (b). You now disconnect A‚Äôs Wi-Fi interface, leaving the Ethernet interface connected. What happens now to the connection to B? Hint: to what IP address are the packets from B being sent? See also 13 Routing-Update Algorithms exercise 16.0, and 18 TCP Issues and Alternatives exercise 5.0. 252 10 IPv4 Companion Protocols
11 IPV6 What has been learned from experience with IPv4? First and foremost, more than 32 bits are needed for addresses; the primary motive in developing the new version of IP known as IPv6 was the specter of running out of IPv4 addresses (something which, at the highest level, has already happened; see the discussion at the end of 1.10 IP - Internet Protocol ). Another important issue is that IPv4 requires (or used to require) a modest amount of effort at conÔ¨Åguration; IPv6 was supposed to improve this. In this chapter we outline the basic format of IPv6 packets, including address format and address assignment. The following chapter continues with additional features of IPv6. By 1990 the IPv4 address-space issue was well understood, and the IETF was actively interested in proposals to replace IPv4. A working group for the so-called ‚ÄúIP next generation‚Äù, or IPng, was created in 1993 to select the new version; RFC 1550 was this group‚Äôs formal solicitation of proposals. In July 1994 the IPng directors voted to accept a modiÔ¨Åed version of the ‚ÄúSimple Internet Protocol Plus‚Äù, or SIPP ( RFC 1710 ), as the basis for IPv6. The Ô¨Årst IPv6 speciÔ¨Åcations, released in 1995, were RFC 1883 (now RFC 2460, with updates) for the basic protocol, and RFC 1884 (now RFC 4291, again with updates) for the addressing architecture. SIPP addresses were originally 64 bits in length, but in the month leading up to adoption as the basis for IPv6 this was increased to 128. 64 bits would probably have been enough, but the problem is less the actual number than the simplicity with which addresses can be allocated; the more bits, the easier this becomes, as sites can be given relatively large address blocks without fear of waste. A secondary consideration in the 64-to-128 leap was the potential to accommodate now-obsolete CLNP addresses ( 1.15 IETF and OSI ), which were up to 160 bits in length, but compressible. IPv6 has to some extent returned to the idea of a Ô¨Åxed division between network and host portions; for most IPv6 addresses, the Ô¨Årst 64 bits is the network preÔ¨Åx (including any subnet portion) and the remaining 64 bits represents the host portion. The rule as spelled out in RFC 2460, in 1998, was that the 64/64 split would apply to all addresses except those beginning with the bits 000; those addresses were then held in reserve in the unlikely event that the 64/64 split ran into problems in the future. This was a change from 1995, when RFC 1884 envisioned 48-bit host portions and 80-bit preÔ¨Åxes. While the IETF occasionally revisits the issue, at the present time the 64/64 split seems here to stay; for discussion and justiÔ¨Åcation, see 12.3.1 Subnets and /64 andRFC 7421. The 64/64 split is not automatic, however; there is no default preÔ¨Åx length as there was in the Class A/B/C IPv4 scheme. Thus, it is misleading to think of IPv6 as a return to something like IPv4‚Äôs classful addressing scheme. Router advertisements must always include the preÔ¨Åx length, and, when assigning IPv6 addresses manually, the /64 preÔ¨Åx length must be speciÔ¨Åed explicitly; see 12.5.3 Manual address conÔ¨Åguration. High-level routing, however, can, as in IPv4, be done on preÔ¨Åxes of any length (usually that means lengths shorter than /64). Routing can also be done on different preÔ¨Åx lengths at different points of the network. IPv6 is now twenty years old, and yet usage as of 2015 remains quite modest. However, the shortage in IPv4 addresses has begun to loom ominously; IPv6 adoption rates may rise quickly if IPv4 addresses begin to climb in price. 253
An Introduction to Computer Networks, Release 2.0.11 11.1 The IPv6 Header The IPv6 Ô¨Åxed header is pictured below; at 40 bytes, it is twice the size of the IPv4 header. The Ô¨Åxed header is intended to support only what every packet needs: there is no support for fragmentation, no header checksum, and no option Ô¨Åelds. However, the concept of extension headers has been introduced to support some of these as options; some IPv6 extension headers are described in 11.5 IPv6 Extension Headers. Whatever header comes next is identiÔ¨Åed by the Next Header Ô¨Åeld, much like the IPv4 Protocol Ô¨Åeld. Some other Ô¨Åxed-header Ô¨Åelds have also been renamed from their IPv4 analogues: the IPv4 TTL is now the IPv6 Hop_Limit (still decremented by each router with the packet discarded when it reaches 0), and the IPv4 DS Ô¨Åeld has become the IPv6 TrafÔ¨Åc Class. Destination AddressSource AddressHop Limit Next Header Payload LengthFlow Label Traffic Class Version0 16 32 The Flow Label is new. RFC 2460 states that it may be used by a source to label sequences of packets for which it requests special handling by the IPv6 routers, such as non-default quality of service or ‚Äúreal-time‚Äù service. Senders not actually taking advantage of any quality-of-service options are supposed to set the Flow Label to zero. When used, the Flow Label represents a sender-computed hash of the source and destination addresses, and perhaps the trafÔ¨Åc class. Routers can use this Ô¨Åeld as a way to look up quickly any priority or reservation state for the packet. All packets belonging to the same Ô¨Çow should have the same Routing Extension header, 11.5.3 Routing Header. The Flow Label will in general notinclude any information about the source and destination port numbers, except that only some of the connections between a pair of hosts may make use of this Ô¨Åeld. AÔ¨Çow, as the term is used here, is one-way; the return trafÔ¨Åc belongs to a different Ô¨Çow. Historically, the term ‚ÄúÔ¨Çow‚Äù has also been used at various other scales: a single bidirectional TCP connection, multiple related TCP connections, or even all trafÔ¨Åc from a particular subnet ( egthe ‚Äúcomputer-lab Ô¨Çow‚Äù). 254 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 11.2 IPv6 Addresses IPv6 addresses are written in eight groups of four hex digits, with a-f preferred over A-F ( RFC 5952 ). The groups are separated by colons, and have leading 0‚Äôs removed, eg fedc:13:1654:310:fedc:bc37:61:3210 If an address contains a long run of 0‚Äôs ‚Äì for example, if the IPv6 address had an embedded IPv4 address ‚Äì then when writing the address the string ‚Äú::‚Äù should be used to represent however many blocks of 0000 as are needed to create an address of the correct length; to avoid ambiguity this can be used only once. Also, embedded IPv4 addresses may continue to use the ‚Äú.‚Äù separator: ::ffff:147.126.65.141 The above is an example of one standard IPv6 format for representing IPv4 addresses (see 12.4 Using IPv6 and IPv4 Together ). 48 bits are explicitly displayed; the :: means these are preÔ¨Åxed by 80 0-bits. The IPv6 loopback address is ::1 (that is, 127 0-bits followed by a 1-bit). Network address preÔ¨Åxes may be written with the ‚Äú/‚Äù notation, as in IPv4: 12ab:0:0:cd30::/60 RFC 3513 suggested that initial IPv6 unicast-address allocation be initially limited to addresses beginning with the bits 001, that is, the 2000::/3 block (20 in binary is 0010 0000). Generally speaking, IPv6 addresses consist of a 64-bit network preÔ¨Åx (perhaps including subnet bits) followed by a 64-bit ‚Äúinterface identiÔ¨Åer‚Äù. See 11.3 Network PreÔ¨Åxes and11.2.1 Interface identiÔ¨Åers. IPv6 addresses all have an associated scope, deÔ¨Åned in RFC 4007. The scope of a unicast address is either global, meaning it is intended to be globally routable, or link-local, meaning that it will only work with directly connected neighbors ( 11.2.2 Link-local addresses ). The loopback address is considered to have link-local scope. A few more scope levels are available for multicast addresses, eg‚Äúsite-local‚Äù ( RFC 4291 ). The scope of an IPv6 address is implicitly coded within the Ô¨Årst 64 bits; addresses in the 2000::/3 block above, for example, have global scope. Packets with local-scope addresses ( eglink-local addresses) for either the destination or the source cannot be routed (the latter because a reply would be impossible). Although addresses in the ‚Äúunique local address‚Äù category of 11.3 Network PreÔ¨Åxes ofÔ¨Åcially have global scope, in a practical sense they still behave as if they had the now-ofÔ¨Åcially-deprecated ‚Äúsite-local scope‚Äù. 11.2.1 Interface identiÔ¨Åers As mentioned earlier, most IPv6 addresses can be divided into a 64-bit network preÔ¨Åx and a 64-bit ‚Äúhost‚Äù portion, the latter corresponding to the ‚Äúhost‚Äù bits of an IPv4 address. These host-portion bits are known ofÔ¨Åcially as the interface identiÔ¨Åer; the change in terminology reÔ¨Çects the understanding that all IP addresses attach to interfaces rather than to hosts. The original plan for the interface identiÔ¨Åer was to derive it in most cases from the LAN address, though the interface identiÔ¨Åer can also be set administratively. Given a 48-bit Ethernet address, the interface identiÔ¨Åer based on it was to be formed by inserting 0xfffe between the Ô¨Årst three bytes and the last three bytes, to get 64 bits in all. The seventh bit of the Ô¨Årst byte (the Ethernet ‚Äúuniversal/local‚Äù Ô¨Çag) was then set to 1. 11.2 IPv6 Addresses 255
An Introduction to Computer Networks, Release 2.0.11 The result of this process is ofÔ¨Åcially known as the ModiÔ¨Åed EUI-64 IdentiÔ¨Åer, where EUI stands for Extended Unique IdentiÔ¨Åer; details can be found in RFC 4291. As an example, for a host with Ethernet address 00:a0:cc:24:b0:e4, the EUI-64 identiÔ¨Åer would be 0 2a0:ccff:fe24:b0e4 (the leading 00 becomes 02 when the seventh bit is turned on). At the time the EUI-64 format was proposed, it was widely expected that Ethernet MAC addresses would eventually become 64 bits in length. EUI-64 interface identiÔ¨Åers have long been recognized as a major privacy concern: no matter where a (portable) host connects to the Internet ‚Äì home or work or airport or Internet cafe ‚Äì such an interface identiÔ¨Åer always remains the same, and thus serves as a permanent host Ô¨Ångerprint. As a result, EUI-64 identiÔ¨Åers are now discouraged for personal workstations and mobile devices. (Some Ô¨Åxed-location hosts continue to use EUI-64 interface identiÔ¨Åers, or, alternatively, administratively assigned interface identiÔ¨Åers.) While these general issues are alone enough to warrant abandoning EUI-64 identiÔ¨Åers, there are, in fact, much more serious risks, such as the IPvSeeYou vulnerability of [RB22]. Consider a budget home router combined with Wi-Fi access point that uses EUI-64 addresses; many such devices remain on the market, and many more are currently in use and offer no upgrade path. There is an excellent chance that all MAC addresses for this router ‚Äì both Ethernet and Wi-Fi ‚Äì are assigned sequentially (or they are related in a way determined by the OUI). Even if a user is using a privacy-protecting interface identiÔ¨Åer when connecting the internet, the router‚Äôs MAC address can be exposed: an IPv6 traceroute to the user‚Äôs IPv6 address will reveal the router‚Äôs public-facing IPv6 address, as the last hop before the user‚Äôs own address. From this ‚Äì via EUI-64 ‚Äì the public-facing Ethernet MAC address is easily found. This router MAC address, in turn, determines the router‚Äôs Wi-Fi MAC address to within a handful of values. Finally, maps exist of the physical GPS location of almost all Wi-Fi MAC addresses, obtained via so-called ‚Äúwar-driving‚Äù (driving around scanning for Wi-Fi-access-point MAC addresses and recording the GPS coordinates of each); see, for example, wigle.net. Thus, from the user‚Äôs non-EUI privacy-implementing IPv6 address, the user‚Äôs real home location is straightforward to determine. RFC 7217 proposes a privacy-improving alternative to EUI-64 identiÔ¨Åers: the interface identiÔ¨Åer is a secure hash ( 28.6 Secure Hashes ) of a so-called ‚ÄúNet_Iface‚Äù parameter, the 64-bit IPv6 address preÔ¨Åx, and a host-speciÔ¨Åc secret key (a couple other parameters are also thrown into the mix, but they need not concern us here). The ‚ÄúNet_Iface‚Äù parameter can be the interface‚Äôs MAC address, but can also be the interface‚Äôs ‚Äúname‚Äù, egeth0. Interface identiÔ¨Åers created this way change from connection point to connection point (because the preÔ¨Åx changes), do not reveal the Ethernet address, and are randomly scattered (because of the key, if nothing else) through the 264-sized interface-identiÔ¨Åer space. The last feature makes probing for IPv6 addresses effectively impossible; see exercise 4.0. Interface identiÔ¨Åers as in the previous paragraph do not change unless the preÔ¨Åx changes, which normally happens only if the host is moved to a new network. In 11.7.2.1 SLAAC privacy we will see that interface identiÔ¨Åers are often changed at regular intervals, for privacy reasons. Finally, interface identiÔ¨Åers are often centrally assigned, using DHCPv6 ( 11.7.3 DHCPv6 ). Remote probing for IPv6 addresses based on EUI-64 identiÔ¨Åers is much easier than for those based on RFC7217 identiÔ¨Åers, as the former are not very random. If an attacker can guess the hardware vendor, and thus the Ô¨Årst three bytes of the Ethernet address ( 2.1.3 Ethernet Address Internal Structure ), there are only 224 possibilities, down from 264. As the last three bytes are often assigned in serial order, considerable further narrowing of the search space may be possible. While it may amount to security through obscurity, keeping internal global IPv6 addresses hidden is often of practical importance. Additional discussion of host-scanning in IPv6 networks can be found in RFC 7707 and draft-ietf-opsecipv6-host-scanning-06. 256 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 11.2.2 Link-local addresses IPv6 deÔ¨Ånes link-local addresses, with so-called link-local scope, intended to be used only on a single LAN and never routed. These begin with the 64-bit link-local preÔ¨Åx consisting of the ten bits 1111 1110 10 followed by 54 more zero bits; that is, fe80::/64. The remaining 64 bits are the interface identiÔ¨Åer for the link interface in question, above. The EUI-64 link-local address of the machine in the previous section with Ethernet address 00:a0:cc:24:b0:e4 is thus fe80::2a0:ccff:fe24:b0e4. The main applications of link-local addresses are as a ‚Äúbootstrap‚Äù address for global-address autoconÔ¨Åguration ( 11.7.2 Stateless AutoconÔ¨Åguration (SLAAC) ), and as an optional permanent address for routers. IPv6 routers often communicate with neighboring routers via their link-local addresses, with the understanding that these do not change when global addresses (or subnet conÔ¨Ågurations) change ( RFC 4861 ¬ß6.2.8). If EUI-64 interface identiÔ¨Åers are used then the link-local address does change whenever the Ethernet hardware is replaced. However, if RFC 7217 interface identiÔ¨Åers are used and that mechanism‚Äôs ‚ÄúNet_Iface‚Äù parameter represents the interface name rather than its physical address, the link-local address can be constant for the life of the host. (When RFC 7217 is used to generate link-local addresses, the ‚ÄúpreÔ¨Åx‚Äù hash parameter is the link-local preÔ¨Åx fe80::/64.) A consequence of identifying routers to their neighbors by their link-local addresses is that it is often possible to conÔ¨Ågure routers so they do not even have global-scope addresses; for forwarding trafÔ¨Åc and for exchanging routing-update messages, link-local addresses are sufÔ¨Åcient. Similarly, many ordinary hosts forward packets to their default router using the latter‚Äôs link-local address. We will return to router addressing in12.6.2 Setting up a router and12.6.2.1 A second router. For non-Ethernet-like interfaces, egtunnel interfaces, there may be no natural candidate for the interface identiÔ¨Åer, in which case a link-local address may be assigned manually, with the low-order 64 bits chosen to be unique for the link in question. When sending to a link-local address, one must separately supply somewhere the link‚Äôs ‚Äúzone identiÔ¨Åer‚Äù, often by appending a string containing the interface name to the IPv6 address, egfe80::f00d:cafe %eth0. See12.5.1 ping6 and12.5.2 TCP connections using link-local addresses for examples of such use of link-local addresses. IPv4 also has true link-local addresses, deÔ¨Åned in RFC 3927, though they are rarely used; such addresses are in the 169.254.0.0/16 block (not to be confused with the 192.168.0.0/16 private-address block). Other than these, IPv4 addresses always implicitly identify the link subnet by virtue of the network preÔ¨Åx. Once the link-local address is created, it must pass the duplicate-address detection test before being used; see11.7.1 Duplicate Address Detection. 11.2.3 Anycast addresses IPv6 also introduced anycast addresses. An anycast address might be assigned to each of a set of routers (in addition to each router‚Äôs own unicast addresses); a packet addressed to this anycast address would be delivered to only one member of this set. Note that this is quite different from multicast addresses; a packet addressed to the latter is delivered to every member of the set. It is up to the local routing infrastructure to decide which member of the anycast group would receive the packet; normally it would be sent to the ‚Äúclosest‚Äù member. This allows hosts to send to any of a set of routers, rather than to their designated individual default router. 11.2 IPv6 Addresses 257
An Introduction to Computer Networks, Release 2.0.11 Anycast addresses are not marked as such, and a node sending to such an address need not be aware of its anycast status. Addresses are anycast simply because the routers involved have been conÔ¨Ågured to recognize them as such. IPv4 anycast exists also, but in a more limited form ( 15.8 BGP and Anycast ); generally routers are conÔ¨Ågured much more indirectly ( egthrough BGP). 11.3 Network PreÔ¨Åxes We have been assuming that an IPv6 address, at least as seen by a host, is composed of a 64-bit network preÔ¨Åx and a 64-bit interface identiÔ¨Åer. As of 2015 this remains a requirement; RFC 4291 (IPv6 Addressing Architecture) states: For all unicast addresses, except those that start with the binary value 000, Interface IDs are required to be 64 bits long.. .. This /64 requirement is occasionally revisited by the IETF, but is unlikely to change for mainstream IPv6 trafÔ¨Åc. This Ô¨Årm 64/64 split is a departure from IPv4, where the host/subnet division point has depended, since the development of subnets, on local conÔ¨Åguration. Note that while the net/interface (net/host) division point is Ô¨Åxed, routers may still use CIDR ( 14.1 Classless Internet Domain Routing: CIDR ) and may still base forwarding decisions on preÔ¨Åxes shorter than /64. As of 2015, all allocations for globally routable IPv6 preÔ¨Åxes are part of the 2000::/3 block. IPv6 also deÔ¨Ånes a variety of specialized network preÔ¨Åxes, including the link-local preÔ¨Åx and preÔ¨Åxes for anycast and multicast addresses. For example, as we saw earlier, the preÔ¨Åx ::ffff:0:0/96 identiÔ¨Åes IPv6 addresses with embedded IPv4 addresses. The most important class of 64-bit network preÔ¨Åxes, however, are those supplied by a provider or other address-numbering entity, and which represent the Ô¨Årst half of globally routable IPv6 addresses. These are the preÔ¨Åxes that will be visible to the outside world. IPv6 customers will typically be assigned a relatively large block of addresses, eg/48 or /56. The former allows 64‚Äì48 = 16 bits for local ‚Äúsubnet‚Äù speciÔ¨Åcation within a 64-bit network preÔ¨Åx; the latter allows 8 subnet bits. These subnet bits are ‚Äì as in IPv4 ‚Äì supplied through router conÔ¨Åguration; see 12.3 IPv6 Subnets. The closest IPv6 analogue to the IPv4 subnet mask is that all network preÔ¨Åxes are supplied to hosts with an associated length, although that length will almost always be 64 bits. Many sites will have only a single externally visible address block. However, some sites may be multihomed and thus have multiple independent address blocks. Sites may also have private unique local address preÔ¨Åxes, corresponding to IPv4 private address blocks like 192.168.0.0/16 and 10.0.0.0/8. They are ofÔ¨Åcially called Unique Local Unicast Addresses and are deÔ¨Åned in RFC 4193. These replace an earlier site-local address plan (and ofÔ¨Åcial site-local scope) formally deprecated in RFC 3879 (though unique-local addresses are sometimes still informally referred to as sitelocal). The Ô¨Årst 8 bits of a unique-local preÔ¨Åx are 1111 1101 (fd00::/8). The related preÔ¨Åx 1111 1100 (fc00::/8) is reserved for future use; the two together may be consolidated as fc00::/7. The last 16 bits of a 64-bit uniquelocal preÔ¨Åx represent the subnet ID, and are assigned either administratively or via autoconÔ¨Åguration. The 258 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 40 bits in between, from bit 8 up to bit 48, represent the Global ID. A site is to set the Global ID to a pseudorandom value. The resultant unique-local preÔ¨Åx is ‚Äúalmost certainly‚Äù globally unique (and is considered to have global scope in the sense of 11.2 IPv6 Addresses ), although it is not supposed to be routed off a site. Furthermore, a site would generally not admit any packets from the outside world addressed to a destination with the Global ID as preÔ¨Åx. One rationale for choosing unique Global IDs for each site is to accommodate potential later mergers of organizations without the need for renumbering; this has been a chronic problem for sites using private IPv4 address blocks. Another justiÔ¨Åcation is to accommodate VPN connections from other sites. For example, if I use IPv4 block 10.0.0.0/8 at home, and connect using VPN to a site also using 10.0.0.0/8, it is possible that my printer will have the same IPv4 address as their application server. 11.4 IPv6 Multicast IPv6 has moved away from LAN-layer broad cast, instead providing a wide range of LAN-layer multi cast groups. (Note that LAN-layer multicast is often straightforward; it is general IP-layer multicast (25.5 Global IP Multicast ) that is problematic. See 2.1.2 Ethernet Multicast for the Ethernet implementation.) This switch to multicast is intended to limit broadcast trafÔ¨Åc in general, though many switches still propagate LAN multicast trafÔ¨Åc everywhere, like broadcast. An IPv6 multicast address is one beginning with the eight bits 1111 1111 (ff00::/8); numerous speciÔ¨Åc such addresses, and even classes of addresses, have been deÔ¨Åned. For actual delivery, IPv6 multicast addresses correspond to LAN-layer ( egEthernet) multicast addresses through a well-deÔ¨Åned static correspondence; speciÔ¨Åcally, if x, y, z and w are the last four bytes of the IPv6 multicast address, in hex, then the corresponding Ethernet multicast address is 33:33:x:y:z:w ( RFC 2464 ). A typical IPv6 host will need to join (that is, subscribe to) several Ethernet multicast groups. The IPv6 multicast address with the broadest scope is all-nodes, with address ff02::1; the corresponding Ethernet multicast address is 33:33:00:00:00:01. This essentially corresponds to IPv4‚Äôs LAN broadcast, though the use of LAN multicast here means that non-IPv6 hosts should not see packets sent to this address. Another important IPv6 multicast address is ff02::2, the all-routers address. This is meant to be used to reach all routers, and routers only; ordinary hosts do not subscribe. Generally speaking, IPv6 nodes on Ethernets send LAN-layer Multicast Listener Discovery (MLD) messages to multicast groups they wish to start using; these messages allow multicast-aware Ethernet switches to optimize forwarding so that only those hosts that have subscribed to the multicast group in question will receive the messages. Otherwise switches are supposed to treat multicast like broadcast; worse, some switches may simply fail to forward multicast packets to destinations that have not explicitly opted to join the group. 11.5 IPv6 Extension Headers In IPv4, the IP header contained a Protocol Ô¨Åeld to identify the next header; usually UDP or TCP. All IPv4 options were contained in the IP header itself. IPv6 has replaced this with a scheme for allowing an arbitrary chain of supplemental IPv6 headers. The IPv6 Next Header Ô¨Åeld canindicate that the following header is UDP or TCP, but can also indicate one of several IPv6 options. These optional, or extension, headers include: 11.4 IPv6 Multicast 259
An Introduction to Computer Networks, Release 2.0.11 
- Hop-by-Hop options header 
- Destination options header 
- Routing header 
- Fragment header 
- Authentication header 
- Mobility header 
- Encapsulated Security Payload header These extension headers must be processed in order; the recommended order for inclusion is as above. Most of them are intended for processing only at the destination host; the hop-by-hop and routing headers are exceptions. 11.5.1 Hop-by-Hop Options Header This consists of a set of xtype,valueypairs which are intended to be processed by each router on the path. A tag in the type Ô¨Åeld indicates what a router should do if it does not understand the option: drop the packet, or continue processing the rest of the options. The only Hop-by-Hop options provided by RFC 2460 were for padding, so as to set the alignment of later headers. RFC 2675 later deÔ¨Åned a Hop-by-Hop option to support IPv6 jumbograms: datagrams larger than 65,535 bytes. The need for such large packets remains unclear, in light of 7.3 Packet Size. IPv6 jumbograms are not meant to be used if the underlying LAN does not have an MTU larger than 65,535 bytes; the LAN world is not currently moving in this direction. Because Hop-by-Hop Options headers must be processed by each router encountered, they have the potential to overburden the Internet routing system. As a result, RFC 6564 strongly discourages new Hop-by-Hop Option headers, unless examination at every hop is essential. 11.5.2 Destination Options Header This is very similar to the Hop-by-Hop Options header. It again consists of a set of xtype,valueypairs, and the original RFC 2460 speciÔ¨Åcation only deÔ¨Åned options for padding. The Destination header is intended to be processed at the destination, before turning over the packet to the transport layer. Since RFC 2460, a few more Destination Options header types have been deÔ¨Åned, though none is in common use. RFC 2473 deÔ¨Åned a Destination Options header to limit the nesting of tunnels, called the Tunnel Encapsulation Limit. RFC 6275 deÔ¨Ånes a Destination Options header for use in Mobile IPv6. RFC 6553, on the Routing Protocol for Low-Power and Lossy Networks, or RPL, has deÔ¨Åned a Destination (and Hopby-Hop) Options type for carrying RPL data. A complete list of Option Types for Hop-by-Hop Option and Destination Option headers can be found at www.iana.org/assignments/ipv6-parameters; in accordance with RFC 2780. 260 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 11.5.3 Routing Header The original, or Type 0, Routing header contained a list of IPv6 addresses through which the packet should be routed. These did not have to be contiguous. If the list to be visited en route to destination D was xR1,R2,.. . ,Rny, then this option header contained xR2,R3,.. . ,Rn,D ywith R1 as the initial destination address; R1 then would update this header to xR1,R3,.. . ,Rn,D y(that is, the old destination R1 and the current next-router R2 were swapped), and would send the packet on to R2. This was to continue on until Rn addressed the packet to the Ô¨Ånal destination D. The header contained a Segments Left pointer indicating the next address to be processed, incremented at each Ri. When the packet arrived at D the Routing Header would contain the routing list xR1,R3,.. . ,Rny. This is, in general principle, very much like IPv4 Loose Source routing. Note, however, that routers between the listed routers R1.. . Rn did not need to examine this header; they processed the packet based only on its current destination address. This form of routing header was deprecated by RFC 5095, due to concerns about a trafÔ¨Åc-ampliÔ¨Åcation attack. An attacker could send off a packet with a routing header containing an alternating list of just two routersxR1,R2,R1,R2,.. . ,R1,R2,D y; this would generate substantial trafÔ¨Åc on the R1‚ÄìR2 link. RFC 6275 andRFC 6554 deÔ¨Åne more limited routing headers. RFC 6275 deÔ¨Ånes a quite limited routing header to be used for IPv6 mobility (and also deÔ¨Ånes the IPv6 Mobility header). The RFC 6554 routing header used for RPL, mentioned above, has the same basic form as the Type 0 header described above, but its use is limited to speciÔ¨Åc low-power routing domains. 11.5.4 IPv6 Fragment Header IPv6 supports limited IPv4-style fragmentation via the Fragment Header. This header contains a 13-bit Fragment Offset Ô¨Åeld, which contains ‚Äì as in IPv4 ‚Äì the 13 high-order bits of the actual 16-bit offset of the fragment. This header also contains a 32-bit IdentiÔ¨Åcation Ô¨Åeld; all fragments of the same packet must carry the same value in this Ô¨Åeld. IPv6 fragmentation is done only by the original sender; routers along the way are not allowed to fragment or re-fragment a packet. Sender fragmentation would occur if, for example, the sender had an 8 kB IPv6 packet to send via UDP, and needed to fragment it to accommodate the 1500-byte Ethernet MTU. If a packet needs to be fragmented, the sender Ô¨Årst identiÔ¨Åes the unfragmentable part, consisting of the IPv6 Ô¨Åxed header and any extension headers that must accompany each fragment (these would include Hop-by-Hop and Routing headers). These unfragmentable headers are then attached to each fragment. IPv6 also requires that every link on the Internet have an MTU of at least 1280 bytes beyond the LAN header; link-layer fragmentation and reassembly can be used to meet this MTU requirement (which is what ATM links ( 5.5 Asynchronous Transfer Mode: ATM ) carrying IP trafÔ¨Åc do). Generally speaking, fragmentation should be avoided at the application layer when possible. UDP-based applications that attempt to transmit Ô¨Ålesystem-sized (usually 8 kB) blocks of data remain persistent users of fragmentation. 11.5.5 General Extension-Header Issues In the IPv4 world, many middleboxes ( 9.7.2 Middleboxes ) examine not just the destination address but also the TCP port numbers; Ô¨Årewalls, for example, do this routinely to block all trafÔ¨Åc except to a designated list of ports. In the IPv6 world, a middlebox may have difÔ¨Åculty Ô¨Ånding the TCP header, as it must traverse 11.5 IPv6 Extension Headers 261
An Introduction to Computer Networks, Release 2.0.11 a possibly lengthy list of extension headers. Worse, some of these extension headers may be newer than the middlebox, and thus unrecognized. Some middleboxes would simply drop packets with unrecognized extension headers, making the introduction of new such headers problematic. RFC 6564 addresses this by requiring that all future extension headers use a common ‚Äútype-length-value‚Äù format: the Ô¨Årst byte indicates the extension-header‚Äôs type and the second byte indicates its length. This facilitiates rapid traversal of the extension-header chain. A few older extension headers ‚Äì for example the Encapsulating Security Payload header of RFC 4303 ‚Äì do not follow this rule; middleboxes must treat these as special cases. RFC 2460 states With one exception [that is, Hop-by-Hop headers], extension headers are not examined or processed by any node along a packet‚Äôs delivery path, until the packet reaches the node (or each of the set of nodes, in the case of multicast) identiÔ¨Åed in the Destination Address Ô¨Åeld of the IPv6 header. Nonetheless, sometimes intermediate nodes do attempt to add extension headers. This can break Path MTU Discovery ( 18.6 Path MTU Discovery ), as the sender no longer controls the total packet size. RFC 7045 attempts to promulgate some general rules for the real-world handling of extension headers. For example, it states that, while routers are allowed to drop packets with certain extension headers, they may not do this simply because those headers are unrecognized. Also, routers may ignore Hop-by-Hop Option headers, or else process packets with such headers via a slower queue. 11.6 Neighbor Discovery IPv6 Neighbor Discovery, or ND, is a set of related protocols that replaces several IPv4 tools, most notably ARP, ICMP redirects and most non-address-assignment parts of DHCP. The messages exchanged in ND are part of the ICMPv6 framework, 12.2 ICMPv6. The original speciÔ¨Åcation for ND is in RFC 2461, later updated by RFC 4861. ND provides the following services: 
- Finding the local router(s) [ 11.6.1 Router Discovery ] 
- Finding the set of network address preÔ¨Åxes that can be reached via local delivery (IPv6 allows there to be more than one) [ 11.6.2 PreÔ¨Åx Discovery ] 
- Finding a local host‚Äôs LAN address, given its IPv6 address [ 11.6.3 Neighbor Solicitation ] 
- Detecting duplicate IPv6 addresses [ 11.7.1 Duplicate Address Detection ] 
- Determining that some neighbors are now unreachable 11.6.1 Router Discovery IPv6 routers periodically send Router Advertisement (RA) packets to the all-nodes multicast group. Ordinary hosts wanting to know what router to use can wait for one of these periodic multicasts, or can request an RA packet immediately by sending a Router Solicitation request to the all-routers multicast group. Router Advertisement packets serve to identify the routers; this process is sometimes called Router Discovery. In IPv4, by comparison, the address of the default router is usually piggybacked onto the DHCP response message ( 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ). 262 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 These RA packets, in addition to identifying the routers, also contain a list of all network address preÔ¨Åxes in use on the LAN. This is ‚ÄúpreÔ¨Åx discovery‚Äù, described in the following section. To a Ô¨Årst approximation on a simple network, preÔ¨Åx discovery supplies the network portion of the IPv6 address; on IPv4 networks, DHCP usually supplies the entire IPv4 address. RA packets may contain other important information about the LAN as well, such as an agreed-on MTU. These IPv6 router messages represent a change from IPv4, in which routers need not send anything besides forwarded packets. To become an IPv4 router, a node need only have IPv4 forwarding enabled in its kernel; it is then up to DHCP (or the equivalent) to inform neighboring nodes of the router. IPv6 puts the responsibility for this notiÔ¨Åcation on the router itself: for a node to become an IPv6 router, in addition to forwarding packets, it ‚ÄúMUST‚Äù ( RFC 4294 ) also run software to support Router Advertisement. Despite this mandate, however, the RA mechanism does not play a role in the forwarding process itself; an IPv6 network can run without Router Advertisements if every node is, for example, manually conÔ¨Ågured to know where the routers are and to know which neighbors are on-link. (We emphasize that manual conÔ¨Åguration like this scales very poorly.) On Linux systems, the Router Advertisement agent is most often the radvd daemon. See 12.6 IPv6 Connectivity via Tunneling below. 11.6.2 PreÔ¨Åx Discovery Closely related to Router Discovery is the PreÔ¨Åx Discovery process by which hosts learn what IPv6 network-address preÔ¨Åxes, above, are valid on the network. It is also where hosts learn which preÔ¨Åxes are considered to be local to the host‚Äôs LAN, and thus reachable at the LAN layer instead of requiring router assistance for delivery. IPv6, in other words, does notlimit determination of whether delivery is local to the IPv4 mechanism of having a node check a destination address against each of the network-address preÔ¨Åxes assigned to the node‚Äôs interfaces. Even IPv4 allows two IPv4 network preÔ¨Åxes to share the same LAN ( ega private one 10.1.2.0/24 and a public one 147.126.65.0/24), but a consequence of IPv4 routing is that two such LAN-sharing subnets can only reach one another via a router on the LAN, even though they should in principle be able to communicate directly. IPv6 drops this restriction. The Router Advertisement packets sent by the router should contain a complete list of valid network-address preÔ¨Åxes, as the PreÔ¨Åx Information option. In simple cases this list may contain a single globally routable 64-bit preÔ¨Åx corresponding to the LAN subnet. If a particular LAN is part of multiple (overlapping) physical subnets, the preÔ¨Åx list will contain an entry for each subnet; these 64-bit preÔ¨Åxes will themselves likely share a common site-wide preÔ¨Åx of length N<64. For multihomed sites the preÔ¨Åx list may contain multiple unrelated preÔ¨Åxes corresponding to the different address blocks. Finally, site-speciÔ¨Åc ‚Äúunique local‚Äù IPv6 address preÔ¨Åxes may also be included. Each preÔ¨Åx will have an associated lifetime; nodes receiving a preÔ¨Åx from an RA packet are to use it only for the duration of this lifetime. On expiration (and likely much sooner) a node must obtain a newer RA packet with a newer preÔ¨Åx list. The rationale for inclusion of the preÔ¨Åx lifetime is ultimately to allow sites to easily renumber; that is, to change providers and switch to a new network-address preÔ¨Åx provided by a new router. Each preÔ¨Åx is also tagged with a bit indicating whether it can be used for autoconÔ¨Åguration, as in11.7.2 Stateless AutoconÔ¨Åguration (SLAAC) below. Each preÔ¨Åx also comes with a Ô¨Çag indicating whether the preÔ¨Åx is on-link. If set, then every node receiving 11.6 Neighbor Discovery 263
An Introduction to Computer Networks, Release 2.0.11 that preÔ¨Åx is supposed to be on the same LAN. Nodes assume that to reach a neighbor sharing the same on-link address preÔ¨Åx, Neighbor Solicitation is to be used to Ô¨Ånd the neighbor‚Äôs LAN address. If a neighbor shares an off-link preÔ¨Åx, a router must be used. The IPv4 equivalent of two nodes sharing the same on-link preÔ¨Åx is sharing the same subnet preÔ¨Åx. For an example of subnets with preÔ¨Åx-discovery information, see 12.3 IPv6 Subnets. Routers advertise off-link preÔ¨Åxes only in special cases; this would mean that a node is part of a subnet but cannot reach other members of the subnet directly. This may apply in some wireless settings, egMANETs (4.2.8 MANETs ) where some nodes on the same subnet are out of range of one another. It may also apply when using IPv6 Mobility ( 9.9 Mobile IP ,RFC 3775 ). 11.6.3 Neighbor Solicitation Neighbor Solicitation messages are the IPv6 analogues of IPv4 ARP requests. These are essentially queries of the form ‚Äúwho has IPv6 address X?‚Äù While ARP requests were broadcast, IPv6 Neighbor Solicitation messages are sent to the solicited-node multicast address, which at the LAN layer usually represents a rather small multicast group. This address is ff02::0001:255.y.z.w, where y, z and w are the low-order three bytes of the IPv6 address the sender is trying to look up (note that we are using the notation here for an embedded IPv4 address, even though y, z and w are from an IPv6 address). Each IPv6 host on the LAN will need to subscribe to all the solicited-node multicast addresses corresponding to its own IPv6 addresses (normally this is not too many). Neighbor Solicitation messages are repeated regularly, but followup veriÔ¨Åcations are initially sent to the unicast LAN address on Ô¨Åle (this is common practice with ARP implementations, but is optional). Unlike with ARP, other hosts on the LAN are not expected to eavesdrop on the initial Neighbor Solicitation message. The target host‚Äôs response to a Neighbor Solicitation message is called Neighbor Advertisement; a host may also send these unsolicited if it believes its LAN address may have changed. The analogue of Proxy ARP is still permitted, in that a node may send Neighbor Advertisements on behalf of another. The most likely reason for this is that the node receiving proxy services is a ‚Äúmobile‚Äù host temporarily remote from the home LAN. Neighbor Advertisements sent as proxies have a Ô¨Çag to indicate that, if the real target does speak up, the proxy advertisement should be ignored. Once a node (host or router) has discovered a neighbor‚Äôs LAN address through Neighbor Solicitation, it continues to monitor the neighbor‚Äôs continued reachability. Neighbor Solicitation also includes Neighbor Unreachability Detection. Each node (host or router) continues to monitor its known neighbors; reachability can be inferred either from ongoing IPv6 trafÔ¨Åc exchanges or from Neighbor Advertisement responses. If a node detects that a neighboring host has become unreachable, the original node may retry the multicast Neighbor Solicitation process, in case the neighbor‚Äôs LAN address has simply changed. If a node detects that a neighboring router has become unreachable, it attempts to Ô¨Ånd an alternative path. Finally, IPv4 ICMP Redirect messages have also been moved in IPv6 to the Neighbor Discovery protocol. These allow a router to tell a host that another router is better positioned to handle trafÔ¨Åc to a given destination. 264 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 11.6.4 Security and Neighbor Discovery In the protocols outlined above, received ND messages are trusted; this can lead to problems with nodes pretending to be things they are not. Here are two examples: 
- A host can pretend to be a router simply by sending out Router Advertisements; such a host can thus capture trafÔ¨Åc from its neighbors, and even send it on ‚Äì perhaps selectively ‚Äì to the real router. 
- A host can pretend to be another host, in the IPv6 analog of ARP spooÔ¨Ång ( 10.2.2 ARP Security ). If host A sends out a Neighbor Solicitation for host B, nothing prevents host C from sending out a Neighbor Advertisement claiming to be B (after previously joining the appropriate multicast group). These two attacks can have the goal either of eavesdropping or of denial of service; there are also purely denial-of-service attacks. For example, host C can answer host B‚Äôs DAD queries (below at 11.7.1 Duplicate Address Detection ) by claiming that the IPv6 address in question is indeed in use, preventing B from ever acquiring an IPv6 address. A good summary of these and other attacks can be found in RFC 3756. These attacks, it is worth noting, can only be launched by nodes on the same LAN; they cannot be launched remotely. While this reduces the risk, though, it does not eliminate it. Sites that allow anyone to connect, such as Internet caf√©s, run the highest risk, but even in a setting in which all workstations are ‚Äúlocked down‚Äù, a node compromised by a virus may be able to disrupt the network. RFC 4861 suggested that, at sites concerned about these kinds of attacks, hosts might use the IPv6 Authentication Header or the Encapsulated Security Payload Header to supply digital signatures for ND packets (see 29.6 IPsec ). If a node is conÔ¨Ågured to require such checks, then most ND-based attacks can be prevented. Unfortunately, RFC 4861 offered no suggestions beyond static conÔ¨Åguration, which scales poorly and also rather completely undermines the goal of autoconÔ¨Åguration. A more Ô¨Çexible alternative is Secure Neighbor Discovery, or SEND, speciÔ¨Åed in RFC 3971. This uses public-key encryption ( 29 Public-Key Encryption ) to validate ND messages; for the remainder of this section, some familiarity with the material at 29 Public-Key Encryption may be necessary. Each message is digitally signed by the sender, using the sender‚Äôs private key; the recipient can validate the message using the sender‚Äôs corresponding public key. In principle this makes it impossible for one message sender to pretend to be another sender. In practice, the problem is that public keys by themselves guarantee (if not compromised) only that the sender of a message is the same entity that previously sent messages using that key. In the second bulleted example above, in which C sends an ND message falsely claiming to be B, straightforward applications of public keys would prevent this ifthe original host A had previously heard from B, and trusted that sender to be the real B. But in general A would not know which of B or C was the real B. A cannot trust whichever host it heard from Ô¨Årst, as it is indeed possible that C started its deception with A‚Äôs very Ô¨Årst query for B, beating B to the punch. A common solution to this identity-guarantee problem is to create some form of ‚Äúpublic-key infrastructure‚Äù such as certiÔ¨Åcate authorities, as in 29.5.2.1 CertiÔ¨Åcate Authorities. In this setting, every node is conÔ¨Ågured to trust messages signed by the certiÔ¨Åcate authority; that authority is then conÔ¨Ågured to vouch for the identities of other nodes whenever this is necessary for secure operation. SEND implements its own version of certiÔ¨Åcate authorities; these are known as trust anchors. These would be conÔ¨Ågured to guarantee the identities of all routers, and perhaps hosts. The details are somewhat simpler than the mechanism outlined in29.5.2.1 CertiÔ¨Åcate Authorities, as the anchors and routers are under common authority. When trust anchors are used, each host needs to be conÔ¨Ågured with a list of their addresses. 11.6 Neighbor Discovery 265
An Introduction to Computer Networks, Release 2.0.11 SEND also supports a simpler public-key validation mechanism known as cryptographically generated addresses, or CGAs ( RFC 3972 ). These are IPv6 interface identiÔ¨Åers that are secure hashes ( 28.6 Secure Hashes ) of the host‚Äôs public key (and a few other non-secret parameters). CGAs are an alternative to the interface-identiÔ¨Åer mechanisms discussed in 11.2.1 Interface identiÔ¨Åers. DNS names in the .onion domain used by TOR also use CGAs. The use of CGAs makes it impossible for host C to successfully claim to be host B: only B will have the public key that hashes to B‚Äôs address andthe matching private key. If C attempts to send to A a neighbor advertisement claiming to be B, then C can sign the message with its own private key, but the hash of the corresponding public key will not match the interface-identiÔ¨Åer portion of B‚Äôs address. Similarly, in the DAD scenario, if C attempts to tell B that B‚Äôs newly selected CGA address is already in use, then again C won‚Äôt have a key matching that address, and B will ignore the report. In general, CGI addresses allow recipients of a message to verify that the source address is the ‚Äúowner‚Äù of the associated public key, without any need for a public-key infrastructure ( 29.3 Trust and the Man in the Middle ). C canstill pretend to be a router, using its own CGA address, because router addresses are not known by the requester beforehand. However, it is easier to protect routers using trust anchors as there are fewer of them. SEND relies on the fact that Ô¨Ånding two inputs hashing to the same 64-bit CGA is infeasible, as in general this would take about 264tries. An IPv4 analog would be impossible as the address host portion won‚Äôt have enough bits to prevent Ô¨Ånding hash collisions via brute force. For example, if the host portion of the address has ten bits, it would take C about 210tries (by tweaking the supplemental hash parameters) until it found a match for B‚Äôs CGA. SEND has seen very little use in the IPv6 world, partly because IPv6 itself has seen such slow adoption, but also because of the perception that the vulnerabilities SEND protects against are difÔ¨Åcult to exploit. RA-guard is a simpler mechanism to achieve ND security, but one that requires considerable support from the LAN layer. Outlined in RFC 6105, it requires that each host connects directly to a switch; that is, there must be no shared-media Ethernet. The switches must also be fairly smart; it must be possible to conÔ¨Ågure them to know which ports connect to routers rather than hosts, and, in addition, it must be possible to conÔ¨Ågure them to block Router Advertisements from host ports that are notrouter ports. This is quite effective at preventing a host from pretending to be a router, and, while it assumes that the switches can do a signiÔ¨Åcant amount of packet inspection, that is in fact a fairly common Ethernet switch feature. If Wi-Fi is involved, it does require that access points (which are a kind of switch) be able to block Router Advertisements; this isn‚Äôt quite as commonly available. In determining which switch ports are connected to routers, RFC 6105 suggests that there might be a brief initial learning period, during which all switch ports connecting to a device that claims to be a router are considered, permanently, to be router ports. 11.7 IPv6 Host Address Assignment IPv6 provides two competing ways for hosts to obtain their full IP addresses. One is DHCPv6, based on IPv4‚Äôs DHCP ( 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP) ), in which the entire address is handed out by a DHCPv6 server. The other is StateLess Address AutoConÔ¨Åguration, or SLAAC, in which the interface-identiÔ¨Åer part of the address is generated locally, and the network preÔ¨Åx is obtained via preÔ¨Åx discovery. The original idea behind SLAAC was to support complete plug-and-play network setup: hosts on an isolated LAN could talk to one another out of the box, and if a router was introduced connecting the LAN 266 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 to the Internet, then hosts would be able to determine unique, routable addresses from information available from the router. In the early days of IPv6 development, in fact, DHCPv6 may have been intended only for address assignments to routers and servers, with SLAAC meant for ‚Äúordinary‚Äù hosts. In that era, it was still common for IPv4 addresses to be assigned ‚Äústatically‚Äù, via per-host conÔ¨Åguration Ô¨Åles. RFC 4862 states that SLAAC is to be used when ‚Äúa site is not particularly concerned with the exact addresses hosts use, so long as they are unique and properly routable.‚Äù SLAAC and DHCPv6 evolved to some degree in parallel. While SLAAC solves the autoconÔ¨Åguration problem quite neatly, at this point DHCPv6 solves it just as effectively, and provides for greater administrative control. For this reason, SLAAC may end up less widely deployed. On the other hand, SLAAC gives hosts greater control over their IPv6 addresses, and so may end up offering hosts a greater degree of privacy by allowing endpoint management of the use of private and temporary addresses (below). When a host Ô¨Årst begins the Neighbor Discovery process, it receives a Router Advertisement packet. In this packet are two special bits: the M (managed) bit and the O (other conÔ¨Åguration) bit. The M bit is set to indicate that DHCPv6 is available on the network for address assignment. The O bit is set to indicate that DHCPv6 is able to provide additional conÔ¨Åguration information ( egthe name of the DNS server) to hosts that are using SLAAC to obtain their addresses. In addition, each individual preÔ¨Åx in the RA packet has an A bit, which when set indicates that the associated preÔ¨Åx may be used with SLAAC. 11.7.1 Duplicate Address Detection Whenever an IPv6 host obtains a unicast address ‚Äì a link-local address, an address created via SLAAC, an address received via DHCPv6 or a manually conÔ¨Ågured address ‚Äì it goes through a duplicate-address detection (DAD) process. The host sends one or more Neighbor Solicitation messages (that is, like an ARP query), as in 11.6 Neighbor Discovery, asking if any other host has this address. If anyone answers, then the address is a duplicate. As with IPv4 ACD ( 10.2.1 ARP Finer Points ), but notas with the original IPv4 self-ARP, the source-IP-address Ô¨Åeld of this NS message is set to a special ‚ÄúunspeciÔ¨Åed‚Äù value; this allows other hosts to recognize it as a DAD query. Because this NS process may take some time, and because addresses are in fact almost always unique, RFC 4429 deÔ¨Ånes an optimistic DAD mechanism. This allows limited use of an address before the DAD process completes; in the meantime, the address is marked as ‚Äúoptimistic‚Äù. Outside the optimistic-DAD interval, a host is not allowed to use an IPv6 address if the DAD process has failed. RFC 4862 in fact goes further: if a host with an established address receives a DAD query for that address, indicating that some other host wants to use that address, then the original host should discontinue use of the address. If the DAD process fails for an address based on an EUI-64 identiÔ¨Åer, then some other node has the same Ethernet address and you have bigger problems than just Ô¨Ånding a working IPv6 address. If the DAD process fails for an address constructed with the RFC 7217 mechanism, 11.2.1 Interface identiÔ¨Åers, the host is able to generate a new interface identiÔ¨Åer and try again. A counter for the number of DAD attempts is included in the hash that calculates the interface identiÔ¨Åer; incrementing this counter results in an entirely new identiÔ¨Åer. While DAD works quite well on Ethernet-like networks with true LAN-layer multicast, it may be inefÔ¨Åcient on, say, MANETs ( 4.2.8 MANETs ), as distant hosts may receive the DAD Neighbor Solicitation message 11.7 IPv6 Host Address Assignment 267
An Introduction to Computer Networks, Release 2.0.11 only after some delay, or even not at all. Work continues on the development of improvements to DAD for such networks. 11.7.2 Stateless AutoconÔ¨Åguration (SLAAC) To obtain an address via SLAAC, deÔ¨Åned in RFC 4862, the Ô¨Årst step for a host is to generate its link-local address (above, 11.2.2 Link-local addresses ), appending the standard 64-bit link-local preÔ¨Åx fe80::/64 to its interface identiÔ¨Åer ( 11.2.1 Interface identiÔ¨Åers ). The latter is likely derived from the host‚Äôs LAN address using either EUI-64 or the RFC 7217 mechanism; the important point is that it is available without network involvement. The host must then ensure that its newly conÔ¨Ågured link-local address is in fact unique; it uses DAD (above) to verify this. Assuming no duplicate is found, then at this point the host can talk to any other hosts on the same LAN, egto Ô¨Ågure out where the printers are. The next step is to see if there is a router available. The host may send a Router Solicitation (RS) message to the all-routers multicast address. A router ‚Äì if present ‚Äì should answer with a Router Advertisement (RA) message that also contains a PreÔ¨Åx Information option; that is, a list of IPv6 network-address preÔ¨Åxes (11.6.2 PreÔ¨Åx Discovery ). As mentioned earlier, the RA message will mark with a Ô¨Çag those preÔ¨Åxes eligible for use with SLAAC; if no preÔ¨Åxes are so marked, then SLAAC should not be used. All preÔ¨Åxes will also be marked with a lifetime, indicating how long the host may continue to use the preÔ¨Åx. Once the preÔ¨Åx expires, the host must obtain a new one via a new RA message. The host chooses an appropriate preÔ¨Åx, stores the preÔ¨Åx-lifetime information, and appends the preÔ¨Åx to the front of its interface identiÔ¨Åer to create what should now be a routable address. The address so formed must now be veriÔ¨Åed through the DAD mechanism above. In the era of EUI-64 interface identiÔ¨Åers, it would in principle have been possible for the receiver of a packet to extract the sender‚Äôs LAN address from the interface-identiÔ¨Åer portion of the sender‚Äôs SLAAC-generated IPv6 address. This in turn would allow bypassing the Neighbor Solicitation process to look up the sender‚Äôs LAN address. This was never actually permitted, however, even before the privacy options below, as there is no way to be certain that a received address was in fact generated via SLAAC. With RFC 7217 -based interface identiÔ¨Åers, LAN-address extraction is no longer even potentially an option. A host using SLAAC may receive multiple network preÔ¨Åxes, and thus generate for itself at least one address per preÔ¨Åx. RFC 6724 deÔ¨Ånes a process for a host to determine, when it wishes to connect to destination address D, which of its own multiple addresses to use. For example, if D is a unique-local address, not globally visible, then the host will likely want to choose a source address that is also unique-local. RFC 6724 also includes mechanisms to allow a host with a permanent public address (possibly corresponding to a DNS entry, but just as possibly formed directly from an interface identiÔ¨Åer) to prefer alternative ‚Äútemporary‚Äù or ‚Äúprivacy‚Äù addresses for outbound connections; see, for example, 11.7.2.1 SLAAC privacy. Finally, RFC 6724 also deÔ¨Ånes the sorting order for multiple addresses representing the same destination; see 12.4 Using IPv6 and IPv4 Together. At the end of the SLAAC process, the host knows its IPv6 address (or set of addresses) and its default router. In IPv4, these would have been learned through DHCP along with the identity of the host‚Äôs DNS server; one concern with SLAAC is that it originally did not provide a way for a host to Ô¨Ånd its DNS server. One strategy is to fall back on DHCPv6 for this. However, RFC 6106 now deÔ¨Ånes a process by which IPv6 routers can 268 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 include DNS-server information in the RA packets they send to hosts as part of the SLAAC process; this completes the Ô¨Ånal step of autoconÔ¨Åguration. How to get DNS names for SLAAC-conÔ¨Ågured IPv6 hosts into the DNS servers is an entirely separate issue. One approach is simply not to give DNS names to such hosts. In the NAT-router model for IPv4 autoconÔ¨Åguration, hosts on the inward side of the NAT router similarly do not have DNS names (although they are also not reachable directly, while SLAAC IPv6 hosts would be reachable). If DNS names are needed for hosts, then a site might choose DHCPv6 for address assignment instead of SLAAC. It is also possible to Ô¨Ågure out the addresses SLAAC would use (by identifying the host-identiÔ¨Åer bits) and then creating DNS entries for these hosts. Finally, hosts can also use Dynamic DNS (RFC 2136 ) to update their own DNS records. 11.7.2.1 SLAAC privacy A portable host that always uses SLAAC as it moves from network to network and always bases its SLAAC addresses on the EUI-64 interface identiÔ¨Åer (or on any other static interface identiÔ¨Åer) will be easy to track: its interface identiÔ¨Åer will never change. This is one reason why the obfuscation mechanism of RFC 7217 interface identiÔ¨Åers ( 11.2.1 Interface identiÔ¨Åers ) includes the network preÔ¨Åx in the hash: connecting to a new network will then result in a new interface identiÔ¨Åer. Well before RFC 7217, however, RFC 4941 introduced a set of privacy extensions to SLAAC: optional mechanisms for the generation of alternative interface identiÔ¨Åers, based as with RFC 7217 on pseudorandom generation using the original LAN-address-based interface identiÔ¨Åer as a ‚Äúseed‚Äù value. RFC 4941 goes further, however, in that it supports regular changes to the interface identiÔ¨Åer, to increase the difÔ¨Åculty of tracking a host over time even if it does not change its network preÔ¨Åx. One Ô¨Årst selects a 128-bit secure-hash function F(), egMD5 ( 28.6 Secure Hashes ). New temporary interface IDs (IIDs) can then be calculated as follows (IID new,seed new) = F(seed old, IID old) where the left-hand pair represents the two 64-bit halves of the 128-bit return value of F() and the arguments to F() are concatenated together. (The seventh bit of IID newmust also be set to 0; cf 11.2.1 Interface identiÔ¨Åers where this bit is set to 1.) This process is privacy-safe even if the initial IID is based on EUI-64. The probability of two hosts accidentally choosing the same interface identiÔ¨Åer in this manner is vanishingly small; the Neighbor Solicitation mechanism with DAD must, however, still be used to verify that the address is in fact unique within the host‚Äôs LAN. The privacy addresses above are to be used only for connections initiated by the client; to the extent that the host accepts incoming connections and so needs a ‚ÄúÔ¨Åxed‚Äù IPv6 address, the address based on the original EUI-64/RFC-7217 interface identiÔ¨Åer should still be available. As a result, the RFC 7217 mechanism is still important for privacy even if the RFC 4941 mechanism is fully operational. RFC 4941 stated that privacy addresses were to be disabled by default, largely because of concerns about frequently changing IP addresses. These concerns have abated with experience and so privacy addresses are often now automatically enabled. Typical address lifetimes range from a few hours to 24 hours. Once an address has ‚Äúexpired‚Äù it generally remains available but deprecated for a few temporary-address cycles longer. A consequence of privacy addresses (for either SLAAC or DHCPv6) is that one host will typically have 11.7 IPv6 Host Address Assignment 269
An Introduction to Computer Networks, Release 2.0.11 multiple active addresses for any one network preÔ¨Åx, at any given time. RFC 7934 suggests that a host might change its address, for privacy reasons, once per day, and that each address would have a lifetime of seven days. Add to that the use of separate addresses for virtual machines, and perhaps also for containerized applications, and RFC 7934 suggests that up to 20 addresses might be needed. The number might be quite a bit higher; some proposals for privacy addresses suggest changing them much more often than once a day (though the address lifetimes might also be reduced). It would not be entirely unreasonable, in fact, for a browser to use a separate IPv6 address for each separate website accessed. The use of too many addresses does add to the memory and trafÔ¨Åc requirements of router Neighbor Discovery ( 11.6 Neighbor Discovery ), however. DHCPv6 also provides an option for temporary address assignments, again to improve privacy, but one of the potential advantages of SLAAC is that this process is entirely under the control of the end system. Regularly ( egevery few hours, or less) changing the host portion of an IPv6 address should make external tracking of a host more difÔ¨Åcult, at least if tracking via web-browser cookies is also somehow prevented. However, for a residential ‚Äúsite‚Äù with only a handful of hosts, a considerable degree of tracking may be obtained simply by observing the common 64-bit preÔ¨Åx. For a general discussion of privacy issues related to IPv6 addressing, see RFC 7721. 11.7.3 DHCPv6 The job of a DHCPv6 server is to tell an inquiring host its network preÔ¨Åx(es) and also supply a 64-bit hostidentiÔ¨Åer, very similar to an IPv4 DHCPv4 server. Hosts begin the process by sending a DHCPv6 request to the All_DHCP_Relay_Agents_and_Servers multicast IPv6 address ff02::1:2 (versus the broadcast address for IPv4). As with DHCPv4, the job of a relay agent is to tag a DHCPv6 request with the correct current subnet, and then to forward it to the actual DCHPv6 server. This allows the DHCPv6 server to be on a different subnet from the requester. Note that the use of multicast does nothing to diminish the need for relay agents. In fact, the All_DHCP_Relay_Agents_and_Servers multicast address scope is limited to the current LAN; relay agents then forward to the actual DHCPv6 server using the site-scoped address All_DHCP_Servers. Hosts using SLAAC to obtain their address can still use a special Information-Request form of DHCPv6 to obtain their DNS server and any other ‚Äústatic‚Äù DHCPv6 information. Clients may ask for temporary addresses. These are identiÔ¨Åed as such in the ‚ÄúIdentity Association‚Äù Ô¨Åeld of the DHCPv6 request. They are handled much like ‚Äúpermanent‚Äù address requests, except that the client may ask for a new temporary address only a short time later. When the client does so, a different temporary address will be returned; a repeated request for a permanent address, on the other hand, would usually return the same address as before. Temporary addresses are typically used to improve privacy, by making it more difÔ¨Åcult to track users by IPv6 address. When the DHCPv6 server returns a temporary address, it may of course keep a log of this address. When SLAAC is used, a log is still possible, as each new address must run through the Neighbor Discovery (11.6 Neighbor Discovery ) process. However, SLAAC does place control of the cryptographic mechanisms for temporary-address creation in the hands of the end user, rather than in a centralized service. For example, the DHCPv6 temporary-address mechanism might have a Ô¨Çaw that would allow a remote observer to infer a relationship between different temporary addresses, though the secure-hash mechanism described below appears to be secure as long as the secret_key portion is not compromised. 270 11 IPv6
An Introduction to Computer Networks, Release 2.0.11 A DHCPv6 response contains a list (perhaps of length 1) of IPv6 addresses. Each separate address has an expiration date. The client must send a new request before the expiration of any address it is actually using. In DHCPv4, the host portion of addresses typically comes from ‚Äúaddress pools‚Äù representing small ranges of integers such as 64-254; these values are generally allocated consecutively. A DHCPv6 server, on the other hand, should take advantage of the enormous range (264) of possible host portions by allocating values more sparsely, through the use of pseudorandomness. This is in part to make it very difÔ¨Åcult for an outsider who knows one of a site‚Äôs host addresses to guess the addresses of other hosts, cf 11.2.1 Interface identiÔ¨Åers. The Internet Draft draft-ietf-dhc-stable-privacy-addresses proposes the following mechanism by which a DHCPv6 server may generate the interface-identiÔ¨Åer bits for the addresses it hands out; F() is a secure-hash function and its arguments are concatenated together: F(preÔ¨Åx, client_DUID, IAID, DAD_counter, secret_key) The preÔ¨Åx, DAD_counter and secret_key arguments are as in 11.7.2.1 SLAAC privacy. The client_DUID is the string by which the client identiÔ¨Åes itself to the DHCPv6 server; it may be based on the Ethernet address though other options are possible. The IAID, or Identity Association identiÔ¨Åer, is a client-provided name for this request; different names are used when requesting temporary versus permanent addresses. Some older DHCPv6 servers may still allocate interface identiÔ¨Åers in serial order; such obsolete servers might make the SLAAC approach more attractive. 11.8 Epilog IPv4 has run out of large address blocks, as of 2011. IPv6 has reached a mature level of development. Most common operating systems provide excellent IPv6 support. Yet conversion has been slow. Many ISPs still provide limited (to nonexistent) support, and inexpensive IPv6 Ô¨Årewalls to replace the ubiquitous consumer-grade NAT routers are just beginning to appear. Time will tell how all this evolves. However, while IPv6 has now been around for twenty years, top-level IPv4 address blocks disappeared much more recently. It is quite possible that this will prove to be just the catalyst IPv6 needs. 11.9 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0. Each IPv6 address is associated with a speciÔ¨Åc solicited-node multicast address. (a). Explain why, on a typical Ethernet, if the original IPv6 host address was obtained via SLAAC then the LAN multicast group corresponding to the host‚Äôs solicited-node multicast addresses is likely to be small, in many cases consisting of one host only. (Packet delivery to small LAN multicast groups can be much more efÔ¨Åcient than delivery to large multicast groups.) (b). What steps might a DHCPv6 server take to ensure that, for the IPv6 addresses it hands out, the LAN multicast groups corresponding to the host addresses‚Äô solicited-node multicast addresses will be small? 2.0. If an attacker sends a large number of probe packets via IPv4, you can block them by blocking the attacker‚Äôs IP address. Now suppose the attacker uses IPv6 to launch the probes; for each probe, the attacker 11.8 Epilog 271
An Introduction to Computer Networks, Release 2.0.11 changes the low-order 64 bits of the address. Can these probes be blocked efÔ¨Åciently? If so, what do you have to block? Might you also be blocking other users? 3.0. Which of the following IPv6 addresses are part of the 2401:3c00::/22 block? (a). 2401:3cfe:2357:f00d::1 (b). 2401:4c02:1248:beef::1 (c). 2401:3b0e:0139:cafe::1 (d). 2410:3d04:1247:face::1 4.0. An IPv6 Ô¨Åxed-header is 40 bytes. Taking this as the minimum packet size, how long will it take to send 1015hosts (one quadrillion) probe packets to a target site ( egping, 12.5.1 ping6, but not necessarily that), if the bandwidth is 1 Gbps? 272 11 IPv6
12 IPV6 ADDITIONAL FEATURES In this chapter we continue describing IPv6, with special attention paid to comparisons between IPv4 and IPv6, interoperability between IPv4 and IPv6, and operating IPv6 in an environment where it is not natively supported. 12.1 Globally Exposed Addresses Perhaps the most striking difference between a contemporary IPv4 network and an IPv6 network is that on the former, most end-user workstation addresses are likely to be ‚Äúhidden‚Äù behind a NAT router ( 9.7 Network Address Translation ). On an IPv6 network, on the other hand, every host address is globally visible, though inbound access may be limited by Ô¨Årewalls. While IPv4 NAT is sometimes claimed to provide better security, its real beneÔ¨Åt is that it enables sites to cope with the limited number of IPv4 addresses available. (IPv6-only networks do often use a form of NAT to allow connectivity to IPv4-only servers.) In addition to limiting the number of IPv4 addresses needed, legacy IPv4 NAT routers provide a measure of each of privacy, security and nuisance. Privacy in IPv6 can be handled, as in the previous chapter, through private or temporary addresses. Recall that in the IPv6 world such addresses are still globally visible; privacy here means that the address is changed regularly. The degree of security provided via NAT is mostly due to the fact that all connections must be initiated from the inside; no packet from the outside is allowed through the NAT Ô¨Årewall unless it is a response to a packet sent from the inside. This feature, however, can also be implemented via a conventional Ô¨Årewall (IPv4 or IPv6), without address translation. Furthermore, given such a conventional Ô¨Årewall, it is then straightforward to modify it so as to support limited and regulated connections from the outside world as desired; an analogous modiÔ¨Åcation of a NAT router is more difÔ¨Åcult. (That said, a blanket ban on IPv6 connections initiated from the outside can prove as frustrating as IPv4 NAT.) A second security beneÔ¨Åt for hiding IPv4 addresses is that with IPv4 it is easy to map a /24 subnet by pinging or otherwise probing each of the 254 possible hosts; such mapping may reveal internal structure. In IPv6 such mapping is meant to be impractical as a /64 subnet has 26418 quintillion hosts (though see the randomness note in 11.2.1 Interface identiÔ¨Åers ). If the low-order 64 bits of a host‚Äôs IPv6 address are chosen with sufÔ¨Åcient randomness, Ô¨Ånding the host by probing is virtually impossible; see exercise 4.0 of the previous chapter. As for nuisance, NAT has always broken protocols that involve negotiation of new connections ( egTFTP, FTP, or SIP, used by V oIP); IPv6 has the potential to make these much easier to manage. That said, a strict Ô¨Årewall rule blocking all inbound connections would eliminate this potential beneÔ¨Åt. 12.2 ICMPv6 RFC 4443 deÔ¨Ånes an updated version of the ICMP protocol for IPv6. As with the IPv4 version, messages are identiÔ¨Åed by 8-bit type and code (subtype) Ô¨Åelds, making it reasonably easy to add new message formats. We have already seen the ICMP messages that make up Neighbor Discovery ( 11.6 Neighbor Discovery ). 273
An Introduction to Computer Networks, Release 2.0.11 Unlike ICMPv4, ICMPv6 distinguishes between informational and error messages by the Ô¨Årst bit of the type Ô¨Åeld. Unknown informational messages are simply dropped, while unknown error messages must be handed off, if possible, to the appropriate upper-layer process. For example, ‚Äú[UDP] port unreachable‚Äù messages are to be delivered to the UDP sender of the undeliverable packet. ICMPv6 includes an IPv6 version of Echo Request / Echo Reply, upon which the ‚Äúping6‚Äù command (12.5.1 ping6 ) is based; unlike with IPv4, arriving IPv6 echo-reply messages are delivered to the process that generated the corresponding echo request. The base ICMPv6 speciÔ¨Åcation also includes formats for the error conditions below; this list is somewhat cleaner than the corresponding ICMPv4 list: Destination Unreachable In this case, one of the following numeric codes is returned: 0.No route to destination, returned when a router has no next_hop entry. 1.Communication with destination administratively prohibited, returned when a router has a next_hop entry, but declines to use it for policy reasons. Codes 5 and 6, below, are special cases of this situation; these more-speciÔ¨Åc codes are returned when appropriate. 2.Beyond scope of source address, returned when a router is, for example, asked to route a packet to a global address, but the return address is not, egis unique-local. In IPv4, when a host with a private address attempts to connect to a global address, NAT is almost always involved. 3.Address unreachable, a catchall category for routing failure not covered by any other message. An example is if the packet was successfully routed to the last_hop router, but Neighbor Discovery failed to Ô¨Ånd a LAN address corresponding to the IPv6 address. 4.Port unreachable, returned when, as in ICMPv4, the destination host does not have the requested UDP port open. 5.Source address failed ingress/egress policy, see code 1. 6.Reject route to destination, see code 1. Packet Too Big This is like ICMPv4‚Äôs ‚ÄúFragmentation Required but DontFragment Ô¨Çag set‚Äù; IPv6 however has no routerbased fragmentation. Time Exceeded This is used for cases where the Hop Limit was exceeded, and also where source -based fragmentation was used and the fragment-reassembly timer expired. Parameter Problem This is used when there is a malformed entry in the IPv6 header, an unrecognized Next Header type, or an unrecognized IPv6 option. _node information: 274 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 12.2.1 Node Information Messages ICMPv6 also includes Node Information (NI) Messages, deÔ¨Åned in RFC 4620. One form of NI query allows a host to be asked directly for its name; this is accomplished in IPv4 via reverse-DNS lookups (10.1.3 Other DNS Records ). Other NI queries allow a host to be asked for its other IPv6 addresses, or for its IPv4 addresses. Recipients of NI queries may be conÔ¨Ågured to refuse to answer. 12.3 IPv6 Subnets In the IPv4 world, network managers sometimes struggle to divide up a limited address space into a pool of appropriately sized subnets. In IPv6, this is much simpler: all subnets are of size /64, following the guidelines set out in 11.3 Network PreÔ¨Åxes. There is one common exception: RFC 6164 permits the use of 127-bit preÔ¨Åxes at each end of a point-topoint link. The 128th bit is then 0 at one end and 1 at the other. A site receiving from its provider an address preÔ¨Åx of size /56 can assign up to 256 /64 subnets. As with IPv4, the reasons for IPv6 subnetting are to join incompatible LANs, to press intervening routers into service as inter-subnet Ô¨Årewalls, or otherwise to separate trafÔ¨Åc. The diagram below shows a site with an external preÔ¨Åx of 2001::/62, two routers R1 and R2 with interfaces numbered as shown, and three internal LANS corresponding to three subnets 2001:0:0: 1::/64, 2001:0:0: 2::/64 and 2001:0:0: 3::/64. The subnet 2001:0:0: 0::/64 (2001::/64) is used to connect to the provider. Provider R12001:0:0:0::/64R22001:0:0:2::/64 2001:0:0:3::/642001:0:0:1::/64012 31 Interface 0 of R1 would be assigned an address from the /64 block 2001:0:0: 0/64, perhaps 2001::2. R1 will announce over its interface 1 ‚Äì via router advertisements ‚Äì that it has a route to ::/0, that is, it has the default route. It will also advertise via interface 1 the on-link preÔ¨Åx 2001:0:0:1::/64. R2 will announce via interface 1 its routes to 2001:0:0:2::/64 and 2001:0:0:3::/64. It will also announce the default route on interfaces 2 and 3. On interface 2 it will advertise the on-link preÔ¨Åx 2001:0:0:2::/64, and on interface 3 the preÔ¨Åx 2001:0:0:3::/64. It could also, as a backup, advertise preÔ¨Åx 2001:0:0:1::/64 on its interface 1. On each subnet, only the subnet‚Äôs on-link preÔ¨Åx is advertised. 12.3.1 Subnets and /64 Fixing the IPv6 division of preÔ¨Åx and host (interface) lengths at 64 bits for each is a compromise. While it does reduce the maximum number of subnets from 2128to 264, in practice this is not a realistic concern, as 12.3 IPv6 Subnets 275
An Introduction to Computer Networks, Release 2.0.11 264is still an enormous number. By leaving 64 bits for host identiÔ¨Åers, this 64/64 split leaves enough room for the privacy mechanisms of 11.7.2.1 SLAAC privacy and11.7.3 DHCPv6 to provide reasonable protection. Much of the recent motivation for considering divisions other than 64/64 is grounded in concerns about ISP address-allocation policies. By declaring that users should each receive a /64 allocation, one hope is that users will in fact get enough for several subnets. Even a residential customer with only, say, two hosts and a router needs more than a single /64 address block, because the link from ISP to customer needs to be on its own subnet (it could use a 127-bit preÔ¨Åx, as above, but many customers would in fact have a need for multiple /64 subnets). By requiring /64 for a subnet, the hope is that users will all be allocated, for example, preÔ¨Åxes of at least /60 (16 subnets) or even /56 (256 subnets). Even if that hope does not pan out, the 64/64 rule means that every user should at least get a /64 allocation. On the other hand, if users aregiven only /64 blocks, and they want to use subnets, then they have to break the 64/64 rule locally. Perhaps they can create four subnets each with a preÔ¨Åx of length 66 bits, and each with only 62 bits for the host identiÔ¨Åer. Wanting to do that in a standard way would dictate more Ô¨Çexibility in the preÔ¨Åx/host division. But if the preÔ¨Åx/host division becomes completely arbitrary, there is nothing to stop ISPs from handing out preÔ¨Åxes with lengths of /80 (leaving 48 host bits) or even /120. The general hope is that ISPs will not be so stingy with preÔ¨Åx lengths. But with IPv6 adoption still relatively modest, how this will all work out is not yet clear. In the IPv4 world, users use NAT ( 9.7 Network Address Translation ) to create as many subnets as they desire. In the IPv6 world, NAT is generally considered to be a bad idea. Finally, in theory it is possible to squeeze a site with two subnets onto a single /64 by converting the site‚Äôs main router to a switch; all the customer‚Äôs hosts now connect on an equal footing to the ISP. But this means making it much harder to use the router as a Ô¨Årewall, as described in 12.1 Globally Exposed Addresses. For most users, this is too risky. 12.4 Using IPv6 and IPv4 Together In this section we will assume that IPv6 connectivity exists at a site; if it does not, see 12.6 IPv6 Connectivity via Tunneling. If IPv6 coexists on a client machine with IPv4, in a so-called dual-stack conÔ¨Åguration, which is used? If the client wants to connect using TCP to an IPv4-only website (or to some other network service), there is no choice. But what if the remote site also supports both IPv4 and IPv6? The Ô¨Årst step is the DNS lookup, triggered by the application‚Äôs call to the appropriate addresslookup library procedure; in the Java stalk example of 16.1.3.3 The Client we useInetAddress. getByName(). In the C language, address lookup is done with getaddrinfo() or (the nowdeprecated) gethostbyname(). The DNS system on the client then contacts its DNS resolver and asks for the appropriate address record corresponding to the server name. For IPv4 addresses, DNS maintains so-called ‚ÄúA‚Äù records, for ‚ÄúAddress‚Äù. The IPv6 equivalent is the ‚ÄúAAAA‚Äù record, for ‚ÄúAddress four times longer‚Äù. A dual-stack machine usually requests both. The Internet Draft draft-vavrusa-dnsop-aaaa-for-free proposes that, whenever a DNS server delivers an IPv4 A 276 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 record, it also includes the corresponding AAAA record, much as IPv4 CNAME records are sent with piggybacked corresponding A records ( 10.1.2 nslookup and dig ). The DNS requests are sent to the client‚Äôs pre-conÔ¨Ågured DNS-resolver address (probably set via DHCP). IPv6 and this book This book is, as of April 2015, available via IPv6. Within the cs.luc.edu DNS zone are deÔ¨Åned the following: 
- intronetworks: both A and AAAA records 
- intronetworks6: AAAA records only 
- intronetworks4: A records only DNS itself can run over either IPv4 or IPv6. A DNS server (authoritative nameserver or just resolver) using only IPv4 can answer IPv6 AAAA-record queries, and a DNS server using only IPv6 can answer IPv4 Arecord queries. Ideally each nameserver would eventually support both IPv4 and IPv6 for all queries, though it is common for hosts with newly enabled IPv6 connectivity to continue to use IPv4-only resolvers. See RFC 4472 for a discussion of some operational issues. Here is an example of DNS requests for A and AAAA records made with the nslookup utility from the command line. (In this example, the DNS resolver was contacted using IPv4.) nslookup -query=A facebook.com Name: facebook.com Address: 173.252.120.6 nslookup -query=AAAA facebook.com facebook.com has AAAA address 2a03:2880:2130:cf05: face:b00c :0:1 A few sites have IPv6-only DNS names. If the DNS query returns only an AAAA record, IPv6 must be used. One example in 2015 is ipv6.google.com. In general, however, IPv6-only names such as this are recommended only for diagnostics and testing. The primary DNS names for IPv4/IPv6 sites should have both types of DNS records, as in the Facebook example above (and as for google.com). Java getByName() The JavagetByName() call may notabide by system-wide RFC 6742 -style preferences; the Java Networking Properties documentation (2015) states that ‚Äúthe default behavior is to prefer using IPv4 addresses over IPv6 ones‚Äù. This can be changed by setting the system property java.net. preferIPv6Addresses totrue, usingSystem.setProperty(). If the client application uses a library call like Java‚Äôs InetAddress.getByName(), which returns a single IP address, the client will then attempt to connect to the address returned. If an IPv4 address is returned, the connection will use IPv4, and similarly with IPv6. If an IPv6 address is returned and IPv6 connectivity is not working, then the connection will fail. For such an application, the DNS resolver library thus effectively makes the IPv4-or-IPv6 decision. RFC 6724, which we encountered above in 11.7.2 Stateless AutoconÔ¨Åguration (SLAAC), provides a conÔ¨Åguration 12.4 Using IPv6 and IPv4 Together 277
An Introduction to Computer Networks, Release 2.0.11 mechanism, through a small table of IPv6 preÔ¨Åxes and precedence values such as the following. preÔ¨Åx precedence ::1/128 50 IPv6 loopback ::/0 40 ‚Äúdefault‚Äù match 2002::/16 30 6to4 address; see sidebar in 12.6 IPv6 Connectivity via Tunneling ::ffff:0:0/96 10 Matches embedded IPv4 addresses; see 11.3 Network PreÔ¨Åxes fc00::/7 3 unique-local plus reserved; see 11.3 Network PreÔ¨Åxes An address is assigned a precedence by looking it up in the table, using the longest-match rule ( 14.1 Classless Internet Domain Routing: CIDR ); a list of addresses is then sorted in decreasing order of precedence. There is no entry above for link-local addresses, but by default they are ranked below global addresses. This can be changed by including the link-local preÔ¨Åx fe80::/64 in the above table and ranking it higher than, say, ::/0. The default conÔ¨Åguration is generally to prefer IPv6 if IPv6 is available; that is, if an interface has an IPv6 address that is (or should be) globally routable. Given the availability of both IPv6 and IPv4, a preference for IPv6 is implemented by assigning the preÔ¨Åx ::/0 ‚Äì matching all IPv6 addresses ‚Äì a higher precedence than that assigned to the IPv4-speciÔ¨Åc preÔ¨Åx ::ffff:0:0/96. This is done in the table above. Preferring IPv6 does not always work out well, however; many hosts have IPv6 connectivity through tunneling that may be slow, limited or outright down. The precedence table can be changed to prefer IPv4 over IPv6 by raising the precedence for the preÔ¨Åx ::ffff:0.0.0.0/96 to a value higher than that for ::/0. Such system-wide conÔ¨Åguration is usually done on Linux hosts by editing /etc/gai.conf and on Windows via thenetsh command; for example, netsh interface ipv6 show prefixpolicies. We can see this systemwide IPv4/IPv6 preference in action using OpenSSH (see 29.5.1 SSH ), between two systems that each support both IPv4 and IPv6 (the remote system here is intronetworks.cs.luc.edu). With the IPv4-matching preÔ¨Åx precedence set high, connection is automatically via IPv4: /etc/gai.conf: precedence ::ffff:0:0/96 100 ssh:Connecting to intronetworks.cs.luc.edu [162.216.18.28] ... With the IPv4-preÔ¨Åx precedence set low, new connections use IPv6: /etc/gai.conf: precedence ::ffff:0:0/96 10 ssh:Connecting to intronetworks.cs.luc.edu [2600:3c03::f03c:91ff:fe69:f438] ... Applications can also use a DNS-resolver call that returns a listof all addresses matching a given hostname. (Often this list will have just two entries, for the IPv4 and IPv6 addresses, though round-robin DNS (10.1 DNS ) can make the list much longer.) The C language getaddrinfo() call returns such a list, as does the Java InetAddress.getAllByName(). The RFC 6724 preferences then determine the relative order of IPv4 and IPv6 entries in this list. If an application requests such a list of all addresses, probably the most common strategy is to try each address in turn, according to the system-provided order. In the example of the previous paragraph, OpenSSH does in fact request a list of addresses, using getaddrinfo(), but, according to its source code, tries them in order and so usually connects to the Ô¨Årst address on the list, that is, to the one preferred by the RFC 6724 rules. Alternatively, an application might implement user-speciÔ¨Åed conÔ¨Åguration preferences to 278 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 decide between IPv4 and IPv6, though user interest in this tends to be limited (except, perhaps, by readers of this book). 12.4.1 Happy Eyeballs The ‚Äú Happy Eyeballs ‚Äù algorithm, RFC 8305, offers a more nuanced strategy for deciding whether an application should connect using IPv4 or IPv6. Initially, the client might try the IPv6 address (that is, will send TCP SYN to the IPv6 address, 17.3 TCP Connection Establishment ). If that connection does not succeed within, say, 250 ms, the client would try the IPv4 address. 250 ms is barely enough time for the TCP handshake to succeed; it does not allow ‚Äì and is not meant to allow ‚Äì sufÔ¨Åcient time for a retransmission. The client falls back to IPv4 well before the failure of IPv6 is certain. IPv6 servers As of 2015, the list of websites supporting IPv6 was modest, though the number has crept up since then. Some sites, such as apple.com and microsoft.com, require the ‚Äúwww‚Äù preÔ¨Åx for IPv6 availability. Networking providers are more likely to be IPv6-available. Sprint.com gets an honorable mention for having the shortest IPv6 address I found: 2600::aaaa. A Happy-Eyeballs client is also encouraged to cache the winning protocol, so for the next connection the client will attempt to use only the protocol that was successful before. The cache timeout is to be on the order of 10 minutes, so that if IPv6 connectivity failed and was restored then the client can resume using it with only moderate delay. Unfortunately, if the Happy Eyeballs mechanism is implemented at the application layer, which is often the case, then the scope of this cache may be limited to the particular application. As IPv6 becomes more mainstream, Happy Eyeballs implementations are likely to evolve towards placing greater conÔ¨Ådence in the IPv6 option. One simple change is to increase the time interval during which the client waits for an IPv6 response before giving up and trying IPv4. We can test for the Happy Eyeballs mechanism by observing trafÔ¨Åc with WireShark. As a Ô¨Årst example, we imagine giving our client host a unique-local IPv6 address (in addition to its automatic link-local address); recall that unique-local addresses are not globally routable. If we now were to connect to, say, google.com, and monitor the trafÔ¨Åc using WireShark, we would see a DNS AAAA query (IPv6) for ‚Äúgoogle.com‚Äù followed immediately by a DNS A query (IPv4). The subsequent TCP SYN, however, would be sent only to the IPv4 address: the client host would know that its IPv6 unique-local address is not routable, and it is not even tried. Next let us change the IPv6 address for the client host to 2000:dead:beef:cafe::2, through manual conÔ¨Åguration ( 12.5.3 Manual address conÔ¨Åguration ), and without providing an actual IPv6 connection. (We also manually specify a fake default router.) This address is part of the 2000::/3 block, and is supposed to be globally routable. We now try two connections to google.com, TCP port 80. The Ô¨Årst is via the Firefox browser. 12.4 Using IPv6 and IPv4 Together 279
An Introduction to Computer Networks, Release 2.0.11 We see two DNS queries, AAAA and A, in packets 1-4, followed by the Ô¨Årst attempt (highlighted in orange) at T=0.071 to negotiate a TCP connection via IPv6 by sending a TCP SYN packet ( 17.3 TCP Connection Establishment ) to thegoogle.com IPv6 address 2607:f8b0:4009:80b::200e. Only 250 ms later, at T=0.321, we see a second DNS A-query (IPv4), followed by an ultimately successful connection attempt using IPv4 starting at T=0.350. This particular version of Firefox, in other words, has implemented the Happy Eyeballs dual-stack mechanism. Now we try the connection using the previously mentioned OpenSSH application, using -p 80 to connect to port 80. (This example was generated somewhat later; DNS now returns 2607:f8b0:4009:807::1004 as google.com‚Äôs IPv6 address.) We see two DNS queries, AAAA and A, in packets numbered 4 and 6 (pale blue); these are made by the client from its IPv4 address 10.2.5.19. Half a millisecond after the A query returns (packet 7), the client sends a TCP SYN packet to google.com‚Äôs IPv6 address; this packet is highlighted in orange. This SYN packet is retransmitted 3 seconds and then 9 seconds later (in black), to no avail. After 21 seconds, the client gives up on IPv6 and attempts to connect to google.com at its IPv4 address, 173.194.46.105; this connection (in green) is successful. The long delay shows that Happy Eyeballs was not implemented by OpenSSH, which its source code conÔ¨Årms. (The host initiating the connections here was running Ubuntu 10.04 LTS, from 2010. The ultimately failing TCP connection gives up after three tries over only 21 seconds; newer systems make more tries and take much longer before they abandon a connection attempt.) 12.5 IPv6 Examples Without a Router In this section we present a few IPv6 experiments that can be done without an IPv6 connection and without even an IPv6 router. Without a router, we cannot use SLAAC or DHCPv6. We will instead use link-local 280 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 addresses, which require the speciÔ¨Åcation of the interface along with the address, and manually conÔ¨Ågured unique-local ( 11.3 Network PreÔ¨Åxes ) addresses. One practical problem with link-local addresses is that application documentation describing how to include a speciÔ¨Åcation of the interface is sometimes sparse. 12.5.1 ping6 The IPv6 analogue of the familiar ping command, used to send ICMPv6 Echo Requests, is ping6 on Linux and Mac systems and ping -6 on Windows. The ping6 command supports an option to specify the interface; eg-I eth0; as noted above, this is mandatory when sending to link-local addresses. Here are a few ping6 examples: ping6 ::1: This pings the host‚Äôs loopback address; it should always work. ping6 -I eth0 ff02::1: This pings the all-nodes multicast group on interface eth0. Here are two of the answers received: 
- 64 bytes from fe80::3e97:eff:fe2c:2beb (this is the host I am pinging from) 
- 64 bytes from fe80::2a0:ccff:fe24:b0e4 (a second Linux host) Answers were also received from a Windows machine and an Android phone. A V oIP phone ‚Äì on the same subnet but supporting IPv4 only ‚Äì remained mute, despite V oIP‚Äôs difÔ¨Åculties with IPv4 NAT that would be avoided with IPv6. In lieu of the interface option -I eth0, the ‚Äúzone-identiÔ¨Åer‚Äù syntax ping6 ff02::1%eth0 also usually works; see the following section. ping6 -I eth0 fe80::2a0:ccff:fe24:b0e4: This pings the link-local address of the second Linux host answering the previous query; again, the %eth0 syntax should also work. The destination interface identiÔ¨Åer here uses the now-deprecated EUI-64 format; note the ‚Äúff:fe‚Äù in the middle. Also note the Ô¨Çipped seventh bit of the two bytes 02a0; the destination has Ethernet address 00:a0:cc:24:b0:e4. 12.5.2 TCP connections using link-local addresses The next experiment is to create a TCP connection. Some commands, like ping6 above, may provide for a way of specifying the interface as a command-line option. Failing that, RFC 4007 deÔ¨Ånes the concept of azone identiÔ¨Åer that is appended to the IPv6 address, separated from it by a ‚Äú%‚Äù character, to specify the link involved. On Linux systems the zone identiÔ¨Åer is most often the interface name, egeth0 orppp1. Numeric zone identiÔ¨Åers are also used, in which case it represents the number of the particular interface in some designated list and can be called the zone index. On Windows systems the zone index for an interface can often be inferred from the output of the ipconfig command, which should include it with each linklocal address. The use of zone identiÔ¨Åers is often restricted to literal (numeric) IPv6 addresses, perhaps because there is little demand for symbolic link-local addresses. The following link-local address with zone identiÔ¨Åer creates an ssh connection to the second Linux host in the example of the preceding section: ssh fe80::2a0:ccff:fe24:b0e4 %eth0 That the ssh service is listening for IPv6 connections can be veriÔ¨Åed on that host by netstat -a | grep -i tcp6. That the ssh connection actually used IPv6 can be veriÔ¨Åed by, say, use of a network sniffer like WireShark (for which the Ô¨Ålter expression ipv6 orip.version == 6 is useful). If the 12.5 IPv6 Examples Without a Router 281
An Introduction to Computer Networks, Release 2.0.11 connection fails, but ssh works for IPv4 connections and shows as listening in the tcp6 list from the netstat command, a Ô¨Årewall-blocked port is a likely suspect. 12.5.3 Manual address conÔ¨Åguration The use of manually conÔ¨Ågured addresses is also possible, for either global or unique-local ( ienot connected to the Internet) addresses. However, without a router there can be no PreÔ¨Åx Discovery, 11.6.2 PreÔ¨Åx Discovery, and this may create subtle differences. The Ô¨Årst step is to pick a suitable preÔ¨Åx; in the example below we use the unique-local preÔ¨Åx fd37:beef:cafe::/64 (though this particular preÔ¨Åx does notmeet the randomness rules for unique-local preÔ¨Åxes). We could also use a globally routable preÔ¨Åx, but here we do not want to mislead any hosts about reachability. Without a router as a source of Router Advertisements, we need some way to specify both the preÔ¨Åx and the preÔ¨Åx length; the latter can be thought of as corresponding to the IPv4 subnet mask. One might be forgiven for imagining that the default preÔ¨Åx length would be /64, given that this is the only preÔ¨Åx length generally allowed ( 11.3 Network PreÔ¨Åxes ), but this is often not the case. In the commands below, the preÔ¨Åx length is included at the end as the /64. This usage is just slightly peculiar, in that in the IPv4 world the slash notation is most often used only with true preÔ¨Åxes, with all bits zero beyond the slash length. (The Linux ipcommand also uses the slash notation in the sense here, to specify an IPv4 subnet mask, eg10.2.5.37/24. Theifconfig and Windows netsh commands specify the IPv4 subnet mask the traditional way, eg 255.255.255.0.) Hosts will usually assume that a preÔ¨Åx conÔ¨Ågured this way with a length represents an on-link preÔ¨Åx, meaning that neighbors sharing the preÔ¨Åx are reachable directly via the LAN. We can now assign the low-order 64 bits manually. On Linux this is done with: 
- host1:ip -6 address add fd37:beef:cafe::1/64 dev eth0 
- host2:ip -6 address add fd37:beef:cafe::2/64 dev eth0 Macintosh systems can be conÔ¨Ågured similarly except the name of the interface is probably en0 rather than eth0. On Windows systems, a typical IPv6-address-conÔ¨Åguration command is netsh interface ipv6 add address "Local Area Connection" fd37:beef:cafe::1/64 Now on host1 the command ssh fd37:beef:cafe::2 should create an ssh connection to host2, again assuming ssh on host2 is listening for IPv6 connections. Because the addresses here are not link-local, /etc/host entries may be created for them to simplify entry. Assigning IPv6 addresses manually like this is notrecommended, except for experiments. On a LAN not connected to the Internet and therefore with no actual routing, it is nonetheless possible to start up a Router Advertisement agent ( 11.6.1 Router Discovery ), such as radvd, with a manually conÔ¨Ågured /64 preÔ¨Åx. The RA agent will include this preÔ¨Åx in its advertisements, and reasonably modern hosts will then construct full addresses for themselves from this preÔ¨Åx using SLAAC. IPv6 can then be used within the LAN. If this is done, the RA agent should also be conÔ¨Ågured to announce only a meaningless route, such as ::/128, or else nodes may falsely believe the RA agent is providing full Internet connectivity. 282 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 12.6 IPv6 Connectivity via Tunneling The best option for IPv6 connectivity is native support by one‚Äôs ISP. In such a situation one‚Äôs router should be sending out Router Advertisement messages, and from these all the hosts should discover how to reach the IPv6 Internet. If native IPv6 support is not forthcoming, however, a short-term option is to connect to the IPv6 world using packet tunneling (less often, some other VPN mechanism is used). RFC 4213 outlines the common 6in4 strategy of simply attaching an IPv4 header to the front of the IPv6 packet; it is very similar to the IPv4-in-IPv4 encapsulation of 9.9.1 IP-in-IP Encapsulation. There are several available providers for this service; they can be found by searching for ‚ÄúIPv6 tunnel broker‚Äù. Some tunnel brokers provide this service at no charge. 6in4, 6to4 6in4 tunneling should not be confused with 6 to4 tunneling, which uses the same encapsulation as 6in4 but which constructs a site‚Äôs IPv6 preÔ¨Åx by embedding its IPv4 address: a site with IPv4 address 129.3.5.7 gets IPv6 preÔ¨Åx 2002: 8103:0507::/48 (129 decimal = 0x81). See RFC 3056. There is also a 6 over4, RFC 2529. The basic idea behind 6in4 tunneling is that the tunnel broker allocates you a /64 preÔ¨Åx out of its own address block, and agrees to create an IPv4 tunnel to you using 6in4 encapsulation. All your IPv6 trafÔ¨Åc from the Internet is routed by the tunnel broker to you via this tunnel; similarly, IPv6 packets from your site reach the outside world using this same tunnel. The tunnel, in other words, is your link to an IPv6 router. Generally speaking, the MTU of the tunnel must be at least 20 bytes less than the MTU of the physical interface, to allow space for the header. At the near end this requires a local conÔ¨Åguration change; tunnel brokers often provide a way for users to set the MTU at the far end. Practical MTU values vary from a mandatory IPv6 minimum of 1280 to the Ethernet maximum of 1500‚Äì20 = 1480. Setting up the tunnel does not involve creating a stateful connection. All that happens is that the tunnel client (ieyour endpoint) and the broker record each other‚Äôs IPv4 addresses, and agree to accept encapsulated IPv6 packets from one another provided these two endpoint addresses are used as source and destination. The tunnel at the client end is represented by an appropriate ‚Äúvirtual network interface‚Äù, egsit0 orgif0 or IP6Tunnel. Tunnel providers generally supply the basic commands necessary to get the tunnel interface conÔ¨Ågured and the MTU set. Once the tunnel is created, the tunnel interface at the client end must be assigned an IPv6 address and then a (default) route. We will assume that the /64 preÔ¨Åx for the broker-to-client link is 2001:470:0:10::/64, with the broker at 2001:470:0:10:: 1and with the client to be assigned the address 2001:470:0:10:: 2. The address and route are set up on the client with the following commands (Linux/Mac/Windows respectively; interface names may vary, and some commands assume the interface represents a point-to-point link): ip addr add 2001:470:0:10::2/64 dev sit1 ip route add ::/0 dev sit1 ifconfig gif0 inet6 2001:470:0:10::2 2001:470:0:10::1 prefixlen 128 route -n add -inet6 default 2001:470:0:10::1 (continues on next page) 12.6 IPv6 Connectivity via Tunneling 283
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) netsh interface ipv6 add address IP6Tunnel 2001:470:0:10::2 netsh interface ipv6 add route ::/0 IP6Tunnel 2001:470:0:10::1 At this point the tunnel client should have full IPv6 connectivity! To verify this, one can use ping6, or visit IPv6-only versions of websites ( egintronetworks6.cs.luc.edu), or visit IPv6-identifying sites such as IsMyIPv6Working.com. Alternatively, one can often install a browser plugin to at least make visible whether IPv6 is used. Finally, one can use netcat with the-6option to force IPv6 use, following the HTTP example in 17.7.1 netcat again. There is one more potential issue. If the tunnel client is behind an IPv4 NAT router, that router must deliver arriving encapsulated 6in4 packets correctly. This can sometimes be a problem; encapsulated 6in4 packets are at some remove from the TCP and UDP trafÔ¨Åc that the usual consumer-grade NAT router is primarily designed to handle. Careful study of the router forwarding settings may help, but sometimes the only Ô¨Åx is a newer router. A problem is particularly likely if two different inside clients attempt to set up tunnels simultaneously; see 9.9.1 IP-in-IP Encapsulation. 12.6.1 IPv6 Ô¨Årewalls It is strongly recommended that an IPv6 host block new inbound connections over its IPv6 interface ( eg the tunnel interface), much as an IPv4 NAT router would do. Exceptions may be added as necessary for essential services (such as ICMPv6). Using the linux ip6tables Ô¨Årewall command, with IPv6-tunneled interfacesit1, this might be done with the following: ip6tables --append INPUT -- in-interface sit1 --protocol icmpv6 --jump ACCEPT ip6tables --append INPUT -- in-interface sit1 --match conntrack --ctstate √£√ëESTABLISHED,RELATED --jump ACCEPT ip6tables --append INPUT -- in-interface sit1 --jump DROP At this point the Ô¨Årewall should be tested by attempting to access inside hosts from the outside. At a minimum, ping6 from the outside to any global IPv6 address of any inside host should fail if the ICMPv6 exception above is removed (and should succeed if the ICMPv6 exception is restored). This can be checked by using any of several websites that send pings on request; such sites can be found by searching for ‚Äúonline ipv6 ping‚Äù. There are also a few sites that will run a remote IPv6 TCP port scan; try searching for ‚Äúonline ipv6 port scan‚Äù. See also exercise 4.0. 12.6.2 Setting up a router The next step, if desired, is to set up the tunnel endpoint as a router, so other hosts at the client site can also enjoy IPv6 connectivity. For this we need a second /64 preÔ¨Åx; we will assume this is 2001:470:0: 20::/64 (note this is not an ‚Äúadjacent‚Äù /64; the two /64 preÔ¨Åxes cannot be merged into a /63). Let R be the tunnel endpoint, with eth0 its LAN interface, and let A be another host on the LAN. We will use the linux radvd package as our Router Advertisement agent ( 11.6.1 Router Discovery ). In the radvd.conf Ô¨Åle, we need to say that we want the LAN preÔ¨Åx 2001:470:0:20::/64 advertised as on-link over interface eth0: 284 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 interface eth0 { ... prefix 2001:470:0:20::/64 { AdvOnLink on; # advertise this prefix as on-link AdvAutonomous on; # allows SLAAC with this prefix }; }; Ifradvd is now started, other LAN hosts ( egA) will automatically get the preÔ¨Åx (and thus a full SLAAC address).Radvd will automatically share R‚Äôs default route (::/0), taking it not from the conÔ¨Åguration Ô¨Åle but from R‚Äôs routing table. (It may still be necessary to manually conÔ¨Ågure the IPv6 address of R‚Äôs eth0 interface, egas 2001:470:0:20:: 1.) On the author‚Äôs version of host A, the IPv6 route is now (with some irrelevant attributes not shown) default via fe80::2a0:ccff:fe24:b0e4 dev eth0 That is, host A routes to R via the latter‚Äôs link-local address, always guaranteed on-link, rather than via the subnet address. Ifradvd or its equivalent is not available, the manual approach is to assign R and A each a /64 address: On host R: ip -6 address add 2001:470:0:20::1/64 dev eth0 On host A: ip -6 address add 2001:470:0:20::2/64 dev eth0 Because of the ‚Äú/64‚Äù here ( 12.5.3 Manual address conÔ¨Åguration ), R and A understand that they can reach each other via the LAN, and do so. Host A also needs to be told of the default route via R: On host A: ip -6 route add ::/0 via 2001:470:0:10::1 dev eth0 Here we use the subnet address of R, but we could have used R‚Äôs link-local address as well. It is likely that A‚Äôs eth0 will also need its MTU conÔ¨Ågured, so that it matches that of R‚Äôs virtual tunnel interface (which, recall, should be at least 20 bytes less than the MTU of R‚Äôs physical outbound interface). 12.6.2.1 A second router Now let us add a second router R2, as in the diagram below. The R R2 link is via a separate Ethernet LAN, not a point-to-point link. The LAN with A is, as above, subnet 2001:470:0:20::/64. R R2 Aeth0 eth1 eth2 2001:470:0:20::/64 subnet fe80::ba5e:ba11 fe80::dead:beefTunnel provider In this case, it is R2 that needs to run the Router Advertisement agent ( egradvd). If this were an IPv4 network, the interfaces eth0 and eth1 on the R R2 link would need IPv4 addresses from some new subnet (though the use of private addresses is an option). We can‚Äôt use unnumbered interfaces ( 9.8 Unnumbered Interfaces ), because the R R2 connection is not a point-to-point link. 12.6 IPv6 Connectivity via Tunneling 285
An Introduction to Computer Networks, Release 2.0.11 But with IPv6, we can conÔ¨Ågure the R R2 routing to use only link-local addresses. Let us assume for mnemonic convenience these are as follows: R‚Äôs eth0:fe80::ba5e:ba11 R2‚Äôs eth1:fe80::dead:beef R2‚Äôs forwarding table will have a default route with next_hop fe80::ba5e:ba11 (R). Similarly, R‚Äôs forwarding table will have an entry for destination subnet 2001:470:0:20::/64 with next_hop fe80::dead:beef (R2). Neither eth0 nor eth1 needs any other IPv6 address. R2‚Äôs eth2 interface will likely need a global IPv6 address, eg2001:470:0:20::1 again. Otherwise R2 may not be able to determine that its eth2 interface is in fact connected to the 2001:470:0:20::/64 subnet. One advantage of not giving eth0 or eth1 global addresses is that it is then impossible for an outside attacker to reach these interfaces directly. It also saves on subnets, although one hopes with IPv6 those are not in short supply. All routers at a site are likely to need, for management purposes, an IP address reachable throughout the site, but this does not have to be globally visible. 12.7 IPv6-to-IPv4 Connectivity What happens if you switch to IPv6 completely, perhaps because your ISP (or phone provider) has run out of IPv4 addresses? Some of the time ‚Äì hopefully more and more of the time ‚Äì you will only need to talk to IPv6 servers. For example, the DNS names facebook.com andgoogle.com each correspond to an IPv4 address, but also to an IPv6 address (above). But what do you do if you want to reach an IPv4-only server? Such servers are expected to continue operating for a long time to come. It is necesary to have some sort of centralized IPv6-to-IPv4 translator. An early strategy was NAT-PT ( RFC 2766 ). The translator was assigned a /96 preÔ¨Åx. The IPv6 host would append to this preÔ¨Åx the 32-bit IPv4 address of the destination, and use the resulting address to contact the IPv4 destination. Packets sent to this address would be delivered via IPv6 to the translator, which would translate the IPv6 header into IPv4 and then send the translated packet on to the IPv4 destination. As in IPv4 NAT ( 9.7 Network Address Translation ), the reverse translation will typically involve TCP port numbers to resolve ambiguities. This approach requires the IPv6 host to be aware of the translator, and is limited to TCP and UDP (because of the use of port numbers). Due to these and several other limitations, NAT-PT was formally deprecated in RFC 4966. Do you still have IPv4 service? As of 2017, several phone providers have switched many of their customers to IPv6 while on their mobiledata networks. The change can be surprisingly inconspicuous. Connections to IPv4-only services still work just Ô¨Åne, courtesy of NAT64. About the only way to tell is to look up the phone‚Äôs IP address. The replacement protocol is NAT64, documented in RFC 6146. This is also based on address translation, and, as such, cannot allow connections initiated from IPv4 hosts to IPv6 hosts. It is, however, transparent to both the IPv6 and IPv4 hosts involved, and is not restricted to TCP (though only TCP, UDP and ICMP are supported by RFC 6146 ). It uses a special DNS variant, DNS64 ( RFC 6147 ), as a companion protocol. To use NAT64, an IPv6 client sends out its ordinary DNS query to Ô¨Ånd the addresses of the destination server. 286 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 The DNS resolver ( 10.1 DNS ) receiving the request must use DNS64. If the destination has only an IPv4 address, then the DNS resolver will return to the IPv6 client (as an AAAA record) a synthetic IPv6 address consisting of a preÔ¨Åx and the embedded IPv4 address of the server, much as in NAT-PT above (though multiple preÔ¨Åx-length options exist; see RFC 6052 ). The preÔ¨Åx belongs to the actual NAT64 translator; any packet addressed to an IPv6 address starting with the preÔ¨Åx will be delivered to the translator. There is no relationship between the NAT64 translator and the DNS64 resolver beyond the fact that the former‚Äôs preÔ¨Åx is conÔ¨Ågured into the latter. The IPv6 client now uses this synthetic IPv6 address to contact the IPv4 server. Its packets will be routed to the NAT64 translator itself, by virtue of the preÔ¨Åx, much as in NAT-PT. Upon receiving the Ô¨Årst packet from the IPv6 client, the NAT64 translator will assign one of its IPv4 addresses to the new connection. As IPv4 addresses are in short supply, this pool of available IPv4 addresses may be small, so NAT64 allows one IPv4 address to be used by many IPv6 clients. To this end, the NAT64 translator will also (for TCP and UDP) establish a port mapping between the incoming IPv6 source port and a port number allocated by the NAT64 to ensure that trafÔ¨Åc is uniquely reversable. As with IPv4 NAT, if two IPv6 clients try to contact the same IPv4 server using the same source ports, and are assigned the same NAT64 IPv4 address, then one of the clients will have its port number changed. If an ICMP query is being sent, the Query IdentiÔ¨Åer is used in lieu of port numbers. To extend NAT64 to new protocols, an appropriate analog of port numbers must be identiÔ¨Åed, to allow demultiplexing of multiple connections sharing a single IPv4 address. After the translation is set up, by creating appropriate table entries, the translated packet is sent on to the IPv4 server address that was embedded in the synthetic IPv6 address. The source address will be the assigned IPv4 address of the translator, and the source port will have been rewritten in accordance with the new port mapping. At this point packets can Ô¨Çow freely between the original IPv6 client and its IPv4 destination, with neither endpoint being aware of the translation (unless the IPv6 client carefully inspects the synthetic address it receives via DNS64). A timer within the NAT64 translator will delete the association between the IPv6 and IPv4 addresses if the connection is not used for a while. As an example, suppose the IPv6 client has address 2000:1234::abba, and is trying to reach intronetworks4.cs.luc.edu at TCP port 80. It contacts its DNS server, which Ô¨Ånds no AAAA record but IPv 4address 162.216.18.28 (in hex, a2d8:121c). It takes the preÔ¨Åx for its NAT64 translator, which we will assume is 2000:cafe::, and returns the synthetic address 2000:cafe::a2d8:121c. 12.7 IPv6-to-IPv4 Connectivity 287
An Introduction to Computer Networks, Release 2.0.11 abba intro networks4 NAT64DNS64 cs.luc.edu intronetworks4.cs.luc.edu 162.216.18.28 (a2d8:121c)DNS query for intronetworks4 cs.luc.edu nameserverAAAA query none A query 162.216.18.28 2000:cafe::/642000:cafe::a2d8:121c TCP connect to 2000:cafe:a2d8:121c from port 4000 To 162.216.18.28 from port 40022000:1234::abba IPv6 addr src IPv6 port src IPv4 addr src IPv4 addr dest IPv4 port src 2000:1234::abba 4000 200.0.0.1 162.216.18.28 4002200.0.0.1 not all columns shown The IPv6 client now tries to connect to 2000:cafe::a2d8:121c, using source port 4000. The Ô¨Årst packet arrives at the NAT64 translator, which assigns the connection the outbound IPv4 address of 200.0.0.1, and reassigns the source port on the IPv4 side to 4002. The new IPv4 packet is sent on to 162.216.18.28. The reply from intronetworks4.cs.luc.edu comes back, to x200.0.0.1,4002 y. The NAT64 translator looks this up and Ô¨Ånds that this corresponds to x2000:1234::abba,4000 y, and forwards it back to the original IPv6 client. 12.7.1 IPv6-to-IPv6 Connectivity While we are on the subject of connectivity, there is a signiÔ¨Åcant lack of connectivity within the IPv6 world: two major ISPs do not connect to one another, neither directly nor indirectly. As of 2022, Cogent Communications and Hurricane Electric have no connectivity via IPv6, and apparently have not connected for some years. This has happened occasionally in the IPv4 world, but usually the ISPs involved come to an agreement quickly. Each company maintains a looking-glass site (reachable via IPv4), from which one can launch IPv6 pings and traceroutes; these are cogentco.com/en/looking-glass and lg.he.net. From Cogent, one can choose either an IPv4 or an IPv6 ping; the IPv6 ping to he.net fails. The same happens from Hurricane Electric to cogentco.com, though the selection of IPv4 or IPv6 is made after entering the destination. Ultimately, this situation is due to a disagreement as to who should payfor the interconnection, or who should pay what share. In the language of 15.10 BGP Relationships, both ISPs are top-level backbone providers, or ‚Äúpeers‚Äù. Both are generally considered ‚Äútier-1‚Äù, although a common deÔ¨Åning rule for tier-1 providers is that they directly connect to every other Tier 1 provider, and these two do not. This situation, while deÔ¨Ånitely a problem, is not necessarily as calamitous as it may sound; IPv6 customers of each may be able to reach all major IPv6 services; they just cannot reach each other. While IPv6, with its general absence of NAT, supports direct connections between individual end-users, most connections are between end-users and servers, and most of these connections still work. Customers of other ISPs typically have full connectivity, including to Cogent and to HE. 288 12 IPv6 Additional Features
An Introduction to Computer Networks, Release 2.0.11 12.8 Epilog IPv4 and IPv6 are, functionally, rather similar. However, the widespread use of NAT in the IPv4 world makes IPv4 in practice appear rather different. IPv4 and IPv6 can, of course, coexist side-by-side, as two parallel and independent IP layers. But the demand for IPv4-to-IPv6 connectivity has led to multiple solutions. 12.9 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0. Suppose someone tried to implement ping6 so that, if the address was a link-local address and no interface was speciÔ¨Åed, the ICMPv6 Echo Request was sent out all non-loopback interfaces. Could the end result be different than conventional ping6 with the correct interface supplied? If so, how likely is this? 2.0. Create an IPv6 ssh connection as in 12.5 IPv6 Examples Without a Router. Examine the connection‚Äôs packets using WireShark or the equivalent. Does the TCP handshake ( 17.3 TCP Connection Establishment ) look any different over IPv6? 3.0. Create an IPv6 ssh connection using manually conÔ¨Ågured addresses as in 12.5.3 Manual address conÔ¨Åguration. Again use WireShark or the equivalent to monitor the connection. Is DAD ( 11.7.1 Duplicate Address Detection ) used? 4.0. Suppose host A gets its IPv6 trafÔ¨Åc through tunnel provider H, as in 12.6 IPv6 Connectivity via Tunneling. To improve security, A blocks all packets that are not part of connections it has initiated (which is common), and makes no exception for ICMPv6 trafÔ¨Åc (which is not a good idea). H is correctly conÔ¨Ågured to know the MTU of the A‚ÄìH link. For (a) and (b), this MTU is 1280, the minimum allowed for IPv6. Much of the Internet, however, allows larger MTU values. AHInternet B (a). If A attempts to send a larger-than-1280-byte IPv6 packet to remote host B, will A be informed of the resultant failure? Why or why not? (b). Suppose B attempts to send a larger-than-1280-byte IPv6 packet to A. Will B receive an ICMPv6 Packet Too Big message? Why or why not? (c). Now suppose the MTU of the A‚ÄìH link is raised to 1400 bytes. Outline a scenario in which A sends a packet of size greater than 1280 bytes to remote host B, the packet is too big to make it all the way to B, and yet A receives no notiÔ¨Åcation of this. 12.8 Epilog 289
An Introduction to Computer Networks, Release 2.0.11 290 12 IPv6 Additional Features
13 ROUTING-UPDATE ALGORITHMS How do IP routers build and maintain their forwarding tables? Ethernet bridges always have the option of fallback-to-Ô¨Çooding for unknown destinations, so they can afford to build their forwarding tables ‚Äúincrementally‚Äù, putting a host into the forwarding table only when that host is Ô¨Årst seen as a sender. For IP, there is no fallback delivery mechanism: forwarding tables must be built before delivery can succeed. While manual table construction is possible, it is not practical. In the literature it is common to refer to router-table construction as ‚Äúrouting algorithms‚Äù. We will avoid that term, however, to avoid confusion with the fundamental datagram-forwarding algorithm; instead, we will call these ‚Äúrouting-update algorithms‚Äù. The two classes of algorithms we will consider here are distance-vector andlink-state. In the distancevector approach, often used at smaller sites, routers exchange information with their immediately neighboring routers; tables are built up this way through a sequence of such periodic exchanges. In the link-state approach, routers rapidly propagate information about the state of each link; all routers in the organization receive this link-state information and each one uses it to build and maintain a map of the entire network. The forwarding table is then constructed (sometimes on demand) from this map. Both approaches assume that consistent information is available as to the cost of each link ( egthat the two routers at opposite ends of each link know this cost, and agree on how the cost is determined). This requirement classiÔ¨Åes these algorithms as interior routing-update algorithms: the routers involved are internal to a larger organization or other common administrative regime that has an established policy on how to assign link weights. The set of routers following a common policy is known as a routing domain or (from the BGP protocol) an autonomous system. The simplest link-weight strategy is to give each link a cost of 1; link costs can also be based on bandwidth, propagation delay, Ô¨Ånancial cost, or administrative preference value. Careful assignment of link costs often plays a major role in herding trafÔ¨Åc onto the faster or ‚Äúbetter‚Äù links. In the following chapter we will look at the Border Gateway Protocol, or BGP, in which no link-cost calculations are made. BGP is used to select routes that traverse other organizations, and Ô¨Ånancial rather than technical factors may therefore play the dominant role in making routing choices. Generally, all these algorithms apply to IPv6 as well as IPv4, though speciÔ¨Åc protocols of course may need modiÔ¨Åcation. Finally, we should point out that from the early days of the Internet, routing was allowed to depend not just on the destination, but also on the ‚Äúquality of service‚Äù (QoS) requested; thus, forwarding table entries are strictly speaking not xdestination, next_hop ybut ratherxdestination, QoS, next_hop y. Originally, the Type of Service Ô¨Åeld in the IPv4 header ( 9.1 The IPv4 Header ) could be used to specify QoS (often then called ToS). Packets could request low delay, high throughput or high reliability, and could be routed accordingly. In practice, the Type of Service Ô¨Åeld was rarely used, and was eventually taken over by the DS Ô¨Åeld and ECN bits. The Ô¨Årst three bits of the Type of Service Ô¨Åeld, known as the precedence bits, remain available, however, and can still be used for QoS routing purposes (see the Class Selector PHB of 25.7 Differentiated Services for examples of these bits). See also RFC 2386. In much of the following, we are going to ignore QoS information, and assume that routing decisions are based only on the destination. See, however, the Ô¨Årst paragraph of 13.5 Link-State Routing-Update 291
An Introduction to Computer Networks, Release 2.0.11 Algorithm, and also 13.6 Routing on Other Attributes. 13.1 Distance-Vector Routing-Update Algorithm Distance-vector is the simplest routing-update algorithm, used by the Routing Information Protocol, or RIP. Version 2 of the protocol is speciÔ¨Åed in RFC 2453. Routers identify their router neighbors (through some sort of neighbor-discovery mechanism), and add a third column to their forwarding tables representing the total cost for delivery to the corresponding destination. These costs are the ‚Äúdistance‚Äù of the algorithm name. Forwarding-table entries are now of the form xdestination,next_hop,cost y. Costs are administratively assigned to each link, and the algorithm then calculates the total cost to a destination as the sum of the link costs along the path. The simplest case is to assign a cost of 1 to each link, in which case the total cost to a destination will be the number of links to that destination. This is known as the ‚Äúhopcount‚Äù metric; it is also possible to assign link costs that reÔ¨Çect each link‚Äôs bandwidth, or delay, or whatever else the network administrators wish. Thoughtful cost assignments are a form of trafÔ¨Åc engineering and sometimes play a large role in network performance. At this point, each router then reports thexdestination,cost yportion of its table to its neighboring routers at regular intervals; these table portions are the ‚Äúvectors‚Äù of the algorithm name. It does not matter if neighbors exchange reports at the same time, or even at the same rate. Each router also monitors its continued connectivity to each neighbor; if neighbor N becomes unreachable then its reachability cost is set to inÔ¨Ånity. In a real IP network, actual destinations would be subnets attached to routers; one router might be directly connected to several such destinations. In the following, however, we will identify all a router‚Äôs directly connected subnets with the router itself. That is, we will build forwarding tables to reach every router. While it is possible that one destination subnet might be reachable by two or more routers, thus breaking our identiÔ¨Åcation of a router with its set of attached subnets, in practice this is of little concern. See exercise 6.0 for an example in which subnets are notidentiÔ¨Åed with adjacent routers. In30.5 IP Routers With Simple Distance-Vector Implementation we present a simpliÔ¨Åed working implementation of RIP using the Mininet network emulator. 13.1.1 Distance-Vector Update Rules Let A be a router receiving a report xD,c Dyfrom neighbor N at cost c N. Note that this means A can reach D via N with cost c = c D+ cN. A updates its own table according to the following three rules: 1.New destination: D is a previously unknown destination. A adds xD,N,cyto its forwarding table. 2.Lower cost: D is a known destination with entry xD,M,c oldy, but the new total cost c is less than c old. A switches to the cheaper route, updating its entry for D to xD,N,cy. It is possible that M=N, meaning that N is now reporting a cost decrease to D. (If c = c old, A ignores the new report; see exercise 8.0.) 3.Next_hop increase: A has an existing entry xD,N,coldy, and the new total cost c is greater than c old. Because this is a cost increase from the neighbor N that A is currently using to reach D, A must incorporate the increase in its table. A updates its entry for D to xD,N,cy. 292 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 The Ô¨Årst two rules are for new destinations and a shorter path to existing destinations. In these cases, the cost to each destination monotonically decreases (at least if we consider all unreachable destinations as being at cost8). Convergence is automatic, as the costs cannot decrease forever. The third rule, however, introduces the possibility of instability, as a cost may also go up. It represents the bad-news case, in that neighbor N has learned that some link failure has driven up its own cost to reach D, and is now passing that ‚Äúbad news‚Äù on to A, which routes to D viaN. The next_hop-increase case only passes bad news along; the very Ô¨Årst cost increase must always come from a router discovering that a neighbor N is unreachable, and thus updating its cost to N to 8. Similarly, if router A learns of a next_hop increase to destination D from neighbor B, then we can follow the next_hops back until we reach a router C which is either the originator of the cost= 8report, or which has learned of an alternative route through one of the Ô¨Årst two rules. 13.1.2 Example 1 For our Ô¨Årst example, no links will break and thus only the Ô¨Årst two rules above will be used. We will start out with the network below with empty forwarding tables; all link costs are 1. A DC EB 1 11 11 1 After initial neighbor discovery, here are the forwarding tables. Each node has entries only for its directly connected neighbors: A:xB,B,1yxC,C,1yxD,D,1y B:xA,A,1yxC,C,1y C:xA,A,1yxB,B,1yxE,E,1y D:xA,A,1yxE,E,1y E:xC,C,1yxD,D,1y Now let D report to A; it sends records xA,1yandxE,1y. A ignores D‚Äôs xA,1yrecord, butxE,1yrepresents a new destination; A therefore adds xE,D,2yto its table. Similarly, let A now report to D, sending xB,1yxC,1y xD,1yxE,2y(the last is the record we just added). D ignores A‚Äôs records xD,1yandxE,2ybut A‚Äôs records xB,1yandxC,1ycause D to create entries xB,A,2yandxC,A,2y. A and D‚Äôs tables are now, in fact, complete. Now suppose C reports to B; this gives B an entry xE,C,2y. If C also reports to E, then E‚Äôs table will have xA,C,2yandxB,C,2y. The tables are now: A:xB,B,1yxC,C,1yxD,D,1yxE,D,2y B:xA,A,1yxC,C,1yxE,C,2y C:xA,A,1yxB,B,1yxE,E,1y D:xA,A,1yxE,E,1yxB,A,2yxC,A,2y 13.1 Distance-Vector Routing-Update Algorithm 293
An Introduction to Computer Networks, Release 2.0.11 E:xC,C,1yxD,D,1yxA,C,2yxB,C,2y We have two missing entries: B and C do not know how to reach D. If A reports to B and C, the tables will be complete; B and C will each reach D via A at cost 2. However, the following sequence of reports might also have occurred: 
- E reports to C, causing C to add xD,E,2y 
- C reports to B, causing B to add xD,C, 3y In this case we have 100% reachability but B routes to D via the longer-than-necessary path B‚ÄìC‚ÄìE‚ÄìD. However, one more report will Ô¨Åx this: suppose A reports to B. B will received xD,1yfrom A, and will update its entry xD,C,3ytoxD,A,2y. Note that A routes to E via D while E routes to A via C; this asymmetry was due to indeterminateness in the order of initial table exchanges. If all link weights are 1, and if each pair of neighbors exchange tables once before any pair starts a second exchange, then the above process will discover the routes in order of length, iethe shortest paths will be the Ô¨Årst to be discovered. This is not, however, a particularly important consideration. 13.1.3 Example 2 The next example illustrates link weights other than 1. The Ô¨Årst route discovered between A and B is the direct route with cost 8; eventually we discover the longer A‚ÄìC‚ÄìD‚ÄìB route with cost 2+1+3=6. A B DC 8 12 3 The initial tables are these: A:xC,C,2yxB,B,8y B:xA,A,8yxD,D,3y C:xA,A,2yxD,D,1y D:xB,B,3yxC,C,1y After A and C exchange, A has xD,C,3yand C hasxB,A,10y. After C and D exchange, C updates its xB,A,10yentry toxB,D,4yand D addsxA,C,3y; D receives C‚Äôs report of xB,10ybut ignores it. Now Ô¨Ånally suppose B and D exchange. D ignores B‚Äôs route to A, as it has a better one. B, however, gets D‚Äôs reportxA,3yand updates its entry for A to xA,D,6y. D also reports xC,1yand so B creates an entry xC,D,4y for C. At this point the tables are as follows: A:xC,C,2yxB,B,8yxD,C,3y B:xA,D,6yxD,D,3yxC,D,4y C:xA,A,2yxD,D,1yxB,D,4y D:xB,B,3yxC,C,1yxA,C,3y 294 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 We have one more thing to Ô¨Åx before we are done: A has an inefÔ¨Åcient route to B. This will be Ô¨Åxed when C reportsxB,4yto A; A will replace its route to B with xB,C,6y. If we look only at the A‚ÄìB route, B discovers the lower-cost route to A when, Ô¨Årst, C reports to D and, second, after that, D reports to B; a similar sequence leads to A‚Äôs discovering the lower-cost route. 13.1.4 Example 3 Our third example will illustrate how the algorithm proceeds when a link breaks. We return to the Ô¨Årst diagram, with all tables completed, and then suppose the D‚ÄìE link breaks. This is the ‚Äúbad-news‚Äù case: a link has broken, and is no longer available; this will bring the third rule into play. A DC EB 1 11 11 1 We shall assume, as above, that A reaches E via D, but we will here assume ‚Äì contrary to Example 1 ‚Äì that C reaches D via A (see exercise 5.0 for the original case). Initially, upon discovering the break, D and E update their tables to xE,-,8yandxD,-,8yrespectively (whether or not they actually enter 8into their tables is implementation-dependent; we may consider this as equivalent to removing their entries for one another; the ‚Äú-‚Äù as next_hop indicates there is no next_hop). Eventually D and E will report the break to their respective neighbors A and C. A will apply the ‚Äúbad-news‚Äù rule above and update its entry for E to xE,-,8y. We have assumed that C, however, routes to D via A, and so it will ignore E‚Äôs report. We will suppose that the next steps are for C to report to E and to A. When C reports its route xD,2yto E, E will add the entry xD,C,3y, and will again be able to reach D. When C reports to A, A will add the route xE,C,2y. The Ô¨Ånal step will be when A next reports to D, and D will have xE,A,3y. Connectivity is restored. 13.1.5 Example 4 The previous examples have had a ‚Äúglobal‚Äù perspective in that we looked at the entire network. In the next example, we look at how one speciÔ¨Åc router, R, responds when it receives a distance-vector report from its neighbor S. Neither R nor S nor we have any idea of what the entire network looks like. Suppose R‚Äôs table is initially as follows, and the S‚ÄìR link has cost 1: destination next_hop cost A S 3 B T 4 C S 5 D U 6 13.1 Distance-Vector Routing-Update Algorithm 295
An Introduction to Computer Networks, Release 2.0.11 S now sends R the following report, containing only destinations and its costs: destination cost A 2 B 3 C 5 D 4 E 2 R then updates its table as follows: destination next_hop cost reason A S 3 No change; S probably sent this report before B T 4 No change; R‚Äôs cost via S is tied with R‚Äôs cost via T C S 6 Next_hop increase D S 5 Lower-cost route via S E S 3 New destination Whatever S‚Äôs cost to a destination, R‚Äôs cost to that destination via S is one greater. 13.2 Distance-Vector Slow-Convergence Problem There is a signiÔ¨Åcant problem with distance-vector table updates in the presence of broken links. Not only can routing loops form, but the loops can persist indeÔ¨Ånitely! As an example, suppose we have the following arrangement, with all links having cost 1: D A B ‚ü®D,D,1‚ü© ‚ü®D,A,2‚ü©1 1 Now suppose the D‚ÄìA link breaks: D A B ‚ü®D,-,‚ü© ‚ü®D,A,2‚ü© If A immediately reports to B that D is no longer reachable (cost = 8), then all is well. However, it is possible that B reports to A Ô¨Årst, telling A that ithas a route to D, with cost 2, which B still believes it has. This means A now installs the entry xD,B,3y. At this point we have what we called in 1.6 Routing Loops a linear routing loop: if a packet is addressed to D, A will forward it to B and B will forward it back to A. Worse, this loop will be with us a while. At some point A will report xD,3yto B, at which point B will update its entry to xD,A,4y. Then B will report xD,4yto A, and A‚Äôs entry will be xD,B,5y,etc. This process is known as slow convergence to inÔ¨Ånity. If A and B each report to the other once a minute, it will take 2,000 years for the costs to overÔ¨Çow an ordinary 32-bit integer. 296 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 13.2.1 Slow-Convergence Fixes The simplest Ô¨Åx to this problem is to use a small value for inÔ¨Ånity. Most Ô¨Çavors of the RIP protocol use inÔ¨Ånity=16, with updates every 30 seconds. The drawback to so small an inÔ¨Ånity is that no path through the network can be longer than this; this makes paths with weighted link costs difÔ¨Åcult. Cisco IGRP uses a variable value for inÔ¨Ånity up to a maximum of 256; the default inÔ¨Ånity is 100. There are several well-known other Ô¨Åxes: 13.2.1.1 Split Horizon Under split horizon, if A uses N as its next_hop for destination D, then A simply does not report to N that it can reach D; that is, in preparing its report to N it Ô¨Årst deletes all entries that have N as next_hop. In the example above, split horizon would mean B would never report to A about the reachability of D because A is B‚Äôs next_hop to D. Split horizon prevents all linear routing loops. However, there are other topologies where it cannot prevent loops. One is the following: D A CB‚ü®D,A,2‚ü© ‚ü®D,A,2‚ü©1 111 Suppose the A-D link breaks, and A updates to xD,-,8y. A then reports xD,8yto B, which updates its table toxD,-,8y. But then, before A can also report xD,8yto C, C reports xD,2yto B. B then updates to xD,C,3y, and reportsxD,3yback to A; neither this nor the previous report violates split-horizon. Now A‚Äôs entry isxD,B,4y. Eventually A will report to C, at which point C‚Äôs entry becomes xD,A,5y, and the numbers keep increasing as the reports circulate counterclockwise. The actual routing proceeds in the other direction, clockwise. Split horizon often also includes poison reverse: if A uses N as its next_hop to D, then A in fact reports xD,8yto N, which is a more deÔ¨Ånitive statement that A cannot reach D by itself. However, coming up with a scenario where poison reverse actually affects the outcome is not trivial. 13.2.1.2 Triggered Updates In the original example, if A was Ô¨Årst to report to B then the loop resolved immediately; the loop occurred if B was Ô¨Årst to report to A. Nominally each outcome has probability 50%. Triggered updates means that any router should report immediately to its neighbors whenever it detects any change for the worse. If A reports Ô¨Årst to B in the Ô¨Årst example, the problem goes away. Similarly, in the second example, if A reports to both B and C before B or C report to one another, the problem goes away. There remains, however, a small window where B could send its report to A just as A has discovered the problem, before A can report to B. 13.2 Distance-Vector Slow-Convergence Problem 297
An Introduction to Computer Networks, Release 2.0.11 13.2.1.3 Hold Down Hold down is sort of a receiver-side version of triggered updates: the receiver does not use new alternative routes for a period of time (perhaps two router-update cycles) following discovery of unreachability. This gives time for bad news to arrive. In the Ô¨Årst example, it would mean that when A received B‚Äôs report xD,2y, it would set this aside. It would then report xD,8yto B as usual, at which point B would now report xD,8y back to A, at which point B‚Äôs earlier report xD,2ywould be discarded. A signiÔ¨Åcant drawback of hold down is that legitimate new routes are also delayed by the hold-down period. These mechanisms for preventing slow convergence are, in the real world, quite effective. The Routing Information Protocol (RIP, RFC 2453 ) implements all but hold-down, and has been widely adopted at smaller installations. However, the potential for routing loops and the limited value for inÔ¨Ånity led to the development of alternatives. One alternative is the link-state strategy, 13.5 Link-State Routing-Update Algorithm. Another alternative is Cisco‚Äôs Enhanced Interior Gateway Routing Protocol, or EIGRP, 13.4.4 EIGRP. While part of the distance-vector family, EIGRP is provably loop-free, though to achieve this it must sometimes suspend forwarding to some destinations while tables are in Ô¨Çux. 13.3 Observations on Minimizing Route Cost Does distance-vector routing actually achieve minimum costs? For that matter, does each packet incur the cost its sender expects? Suppose node A has a forwarding entry xD,B,cy, meaning that A forwards packets to destination D via next_hop B, and expects the total cost to be c. If A sends a packet to D, and we follow it on the actual path it takes, must the total link cost be c? If so, we will say that the network has accurate costs. The answer to the accurate-costs question, as it turns out, is yesfor the distance-vector algorithm, if we follow the rules carefully, and the network is stable (meaning that no routing reports are changing, or, more concretely, that every update report now circulating is based on the current network state); a proof is below. However, if there is a routing loop, the answer is of course no: the actual cost is now inÔ¨Ånite. The answer would also be no if A‚Äôs neighbor B has just switched to using a longer route to D than it last reported to A. It turns out, however, that we seek the shortest route not because we are particularly trying to save money on transit costs; a route 50% longer would generally work just Ô¨Åne. (AT&T, back when they were the Phone Company, once ran a series of print advertisements claiming longer routes as a feature: if the direct path was congested, they could still complete your call by routing you the long way ‚Äòround.) However, we areguaranteed that if all routers seek the shortest route ‚Äì and if the network is stable ‚Äì then all paths are loop-free, because in this case the network will have accurate costs. Here is a simple example illustrating the importance of global cost-minimization in preventing loops. Suppose we have a network like this one: A BC Dcost=20 cost=20 cost=1 cost=1 298 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 Now suppose that A and B use distance-vector but are allowed to choose the shortest route to within 10%. A would get a report from C that D could be reached with cost 1, for a total cost of 21. The forwarding entry via C would be xD,C,21y. Similarly, A would get a report from B that D could be reached with cost 21, for a total cost of 22: xD,B,22y. Similarly, B has choices xD,C,21yandxD,A,22y. If A and B both choose the minimal route, no loop forms. But if A and B both use the 10%-overage rule, they would be allowed to choose the other route: A could choose xD,B,22yand B could choose xD,A,22y. If this happened, we would have a routing loop: A would forward packets for D to B, and B would forward them right back to A. As we apply distance-vector routing, each router independently builds its tables. A router might have some notion of the path its packets would take to their destination; for example, in the case above A might believe that with forwarding entry xD,B,22yits packets would take the path A‚ÄìB‚ÄìC‚ÄìD (though in distance-vector routing, routers do not particularly worry about the big picture). Consider again the accurate-cost question above. This fails in the 10%-overage example, because the actual path is now inÔ¨Ånite. We now prove that, in distance-vector routing, the network will have accurate costs, provided 
- each router selects what it believes to be the shortest path to the Ô¨Ånal destination, and 
- the network is stable, meaning that further dissemination of any reports would not result in changes To see this, suppose the actual route taken by some packet from source to destination, as determined by application of the distributed distance-vector algorithm, is longer than the cost calculated by the source. Choose an example of such a path with the fewest number of links, among all such paths in the network. Let S be the source, D the destination, and k the number of links in the actual path P. Let S‚Äôs forwarding entry for D be xD,N,cy, where N is S‚Äôs next_hop neighbor. To have obtained this route through the distance-vector algorithm, S must have received report xD,c Dyfrom N, where we also have the cost of the S‚ÄìN link as c Nand c = c D+ cN. If we follow a packet from N to D, it must take the same path P with the Ô¨Årst link deleted; this sub-path has length k-1 and so, by our hypothesis that k was the length of the shortest path with non-accurate costs, the cost from N to D is c D. But this means that the cost along path P, from S to D via N, must be c D+ cN= c, contradicting our selection of P as a path longer than its advertised cost. There is one Ô¨Ånal observation to make about route costs: any cost-minimization can occur only within a single routing domain, where full information about all links is available. If a path traverses multiple routing domains, each separate routing domain may calculate the optimum path traversing that domain. But these ‚Äúlocal minimums‚Äù do not necessarily add up to a globally minimal path length, particularly when one domain calculates the minimum cost from one of its routers only to the other domain rather than to a router within that other domain. Here is a simple example. Routers BR1 and BR2 are the border routers connecting the domain LD to the left of the vertical dotted line with domain RD to the right. From A to B, LD will choose the shortest path to RD (not to B, because LD is not likely to have information about links within RD). This is the path of length 3 through BR2. But this leads to a total path length of 3+8=11 from A to B; the global minimum path length, however, is 4+1=5, through BR1. 13.3 Observations on Minimizing Route Cost 299
An Introduction to Computer Networks, Release 2.0.11 BR1 BR2B A Domain LD Domain RDcost=4 cost=1 cost=8 cost=3 In this example, domains LD and RD join at two points. For a route across two domains joined at only a single point, the domain-local shortest paths do add up to the globally shortest path. 13.4 Loop-Free Distance Vector Algorithms It is possible for routing-update algorithms based on the distance-vector idea to eliminate routing loops ‚Äì and thus the slow-convergence problem ‚Äì entirely. We present brief descriptions of two such algorithms. 13.4.1 DSDV DSDV, or Destination-Sequenced Distance Vector, was proposed in [PB94]. It avoids routing loops by the introduction of sequence numbers: each router will always prefer routes with the most recent sequence number, and bad-news information will always have a lower sequence number then the next cycle of corrected information. DSDV was originally proposed for MANETs ( 4.2.8 MANETs ) and has some additional features for trafÔ¨Åc minimization that, for simplicity, we ignore here. It is perhaps best suited for wired networks and for small, relatively stable MANETs. DSDV forwarding tables contain entries for every other reachable node in the system. One successor of DSDV, Ad Hoc On-Demand Distance Vector routing or AODV, 13.4.2 AODV, allows forwarding tables to contain only those destinations in active use; a mechanism is provided for discovery of routes to newly active destinations. Under DSDV, each forwarding table entry contains, in addition to the destination, cost and next_hop, the current sequence number for that destination. When neighboring nodes exchange their distance-vector reachability reports, the reports include these per-destination sequence numbers. When a router R receives a report from neighbor N for destination D, and the report contains a sequence number larger than the sequence number for D currently in R‚Äôs forwarding table, then R always updates to use the new information. The three cost-minimization rules of 13.1.1 Distance-Vector Update Rules above are used only when the incoming and existing sequence numbers are equal. Each time a router R sends a report to its neighbors, it includes a new value for its own sequence number, which it always increments by 2. This number is then entered into each neighbor‚Äôs forwarding-table entry for R, and is then propagated throughout the network via continuing report exchanges. Any sequence number 300 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 originating this way will be even, and whenever another node‚Äôs forwarding-table sequence number for R is even, then its cost for R will be Ô¨Ånite. InÔ¨Ånite-cost reports are generated in the usual way when former neighbors discover they can no longer reach one another; however, in this case each node increments the sequence number for its former neighbor by 1, thus generating an odd value. Any forwarding-table entry with inÔ¨Ånite cost will thus always have an odd sequence number. If A and B are neighbors, and A‚Äôs current sequence number is s, and the A‚ÄìB link breaks, then B will start reporting A at cost 8with sequence number s+1 while A will start reporting its own new sequence number s+2. Any other node now receiving a report originating with B (with sequence number s+1) will mark A as having cost 8, but will obtain a valid route to A upon receiving a report originating from A with new (and larger) sequence number s+2. The triggered-update mechanism is used: if a node receives a report with some destinations newly marked with inÔ¨Ånite cost, it will in turn forward this information immediately to its other neighbors, and so on. This is, however, not essential; ‚Äúbad‚Äù and ‚Äúgood‚Äù reports are distinguished by sequence number, not by relative arrival time. It is now straightforward to verify that the slow-convergence problem is solved. After a link break, if there is some alternative path from router R to destination D, then R will eventually receive D‚Äôs latest even sequence number, which will be greater than any sequence number associated with any report listing D as unreachable. If, on the other hand, the break partitioned the network and there is no longer any path to D from R, then the highest sequence number circulating in R‚Äôs half of the original network will be odd and the associated table entries will all list D at cost 8. One way or another, the network will quickly settle down to a state where every destination‚Äôs reachability is accurately described. In fact, a stronger statement is true: not even transient routing loops are created. We outline a proof. First, whenever router R has next_hop N for a destination D, then N‚Äôs sequence number for D must be greater than or equal to R‚Äôs, as R must have obtained its current route to D from one of N‚Äôs reports. A consequence is that all routers participating in a loop for destination D must have the same (even) sequence number s for D throughout. This means that the loop would have been created if only the reports with sequence number s were circulating. As we noted in 13.1.1 Distance-Vector Update Rules, any application of the next_hopincrease rule must trace back to a broken link, and thus must involve an odd sequence number. Thus, the loop must have formed from the sequence-number-s reports by the application of the Ô¨Årst two rules only. But this violates the claim in Exercise 10.0. There is one drawback to DSDV: nodes may sometimes brieÔ¨Çy switch to routes that are longer than optimum (though still correct). This is because a router is required to use the route with the newest sequence number, even if that route is longer than the existing route. If A and B are two neighbors of router R, and B is closer to destination D but slower to report, then every time D‚Äôs sequence number is incremented R will receive A‚Äôs longer route Ô¨Årst, and switch to using it, and B‚Äôs shorter route shortly thereafter. DSDV implementations usually address this by having each router R keep track of the time interval between theÔ¨Årstarrival at R of a new route to a destination D with a given sequence number, and the arrival of the bestroute with that sequence number. During this interval following the arrival of the Ô¨Årst report with a new sequence number, R will use the new route, but will refrain from including the route in the reports it sends to its neighbors, anticipating that a better route will soon arrive. This works best when the hopcount cost metric is being used, because in this case the best route is likely to arrive Ô¨Årst (as the news had to travel the fewest hops), and at the very least will arrive soon after the Ô¨Årst route. However, if the network‚Äôs cost metric is unrelated to the hop count, then the time interval between Ô¨Årst-route and best-route arrivals can involve multiple update cycles, and can be substantial. 13.4 Loop-Free Distance Vector Algorithms 301
An Introduction to Computer Networks, Release 2.0.11 13.4.2 AODV AODV, or Ad-hoc On-demand Distance Vector routing, is another routing mechanism often proposed for MANETs, though it is suitable for some wired networks as well. Unlike DSDV, above, AODV messages circulate only if a link breaks, or when a node is looking for a route to some other node; this second case is the rationale for the ‚Äúon-demand‚Äù in the name. For larger MANETs, this may result in a signiÔ¨Åcant reduction in routing-management trafÔ¨Åc. AODV is described in [PR99] and RFC 3561. The ‚Äúad hoc‚Äù in the name was intended to suggest that the protocol is well-suited for mobile nodes forming an ad hoc network ( 4.2.4 Access Points ). It is, but the protocol is also works well with infrastructure (those with access points) Wi-Fi networks. AODV has three kinds of messages: RouteRequest or RREQ, for nodes that are looking for a path to a destination, RouteReply or RREP, as the response, and RouteError or RERR for the reporting of broken links. AODV performs reasonably well for MANETs in which the nodes are highly mobile, though it does assume all routing nodes are trustworthy. AODV is loop-free, due to the way it uses sequence numbers. However, it does not always Ô¨Ånd the shortest route right away, and may in fact not Ô¨Ånd the shortest route for an arbitrarily long interval. Each AODV node maintains a node sequence number and also a broadcast counter. Every routing message contains a sequence number for the destination, and every routing record kept by a node includes a Ô¨Åeld for the destination‚Äôs sequence number. Copies of a node‚Äôs sequence number held by other nodes may not be the most current; however, nodes always discard routes with an older (smaller) sequence number as soon as they hear about a route with a newer sequence number. AODV nodes also keep track of other nodes that are directly reachable; in the diagram below we will assume these are the nodes connected by a line. If node A wishes to Ô¨Ånd a route to node F, as in the diagram below, the Ô¨Årst step is for A to increment its sequence number and send out a RouteRequest. This message contains the addresses of A and F, A‚Äôs just-incremented sequence number, the highest sequence number of any previous route to F that is known to A (if any), a hopcount Ô¨Åeld set initially to 1, and A‚Äôs broadcast counter. The end result should be a route from A to F, entered at each node along the path, and also a return route from F back to A. AB C EDG FRREQ ‚Üí RREQ ‚Üí TheRouteRequest is sent initially to A‚Äôs direct neighbors, B and C in the diagram above, using UDP. We will assume for the moment that the RouteRequest reaches all the way to F before a RouteReply is generated. This is always the case if the ‚Äúdestination only‚Äù Ô¨Çag is set, though if not then it is possible for an intermediate node to generate the RouteReply. 302 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 A node that receives a RouteRequest must Ô¨Çood it (‚Äúbroadcast‚Äù it) out all its interfaces to all its directly reachable neighbors, after incrementing the hopcount Ô¨Åeld. B therefore sends A‚Äôs message to C and D, and C sends it to B and E. For this example, we will assume that C is a bit slow sending the message to E. Each node receiving a RouteRequest must hang on to it for a short interval (typically 3 seconds). During this period, if it sees a duplicate of the RouteRequest, identiÔ¨Åed by having the same source and the same broadcast counter, it discards it. This discard rule ensures that RouteRequest messages do not circulate endlessly around loops; it may be compared to the reliable-Ô¨Çooding algorithm in 13.5 Link-State RoutingUpdate Algorithm. A node receiving a new RouteRequest also records (or updates) a routing-table entry for reaching the source of the RouteRequest. Unless there was a pre-existing newer route (that is, with larger sequence number), the entry is marked with the sequence number contained in the message, and with next_hop the neighbor from which the RouteRequest was received. This process ensures that, as part of each node‚Äôs processing of aRouteRequest message, it installs a return route back to the originator. We will suppose that the following happen in the order indicated: 
- B forwards the RouteRequest to D* 
- D forwards the RouteRequest to E and G 
- C forwards the RouteRequest to E 
- E forwards the RouteRequest to F Because E receives D‚Äôs copy of the RouteRequest Ô¨Årst, it ignores C‚Äôs copy. This will mean that, at least initially, the return path will be longer than necessary. Variants of AODV (such as HWMP below) sometimes allow E to accept C‚Äôs message on the grounds that C has a shorter path back to A. This does mean that initial RouteRequest messages farther on in the network now have incorrect hopcount values, though these will be corrected by later RouteRequest messages. After the above messages have been received, each node has a path back to A as indicated by the blue arrows below: AB C EDG F F now increments its own sequence number and creates a RouteReply message; F then sends it to A by following the highlighted (unicast) arrows above, F √ëE√ëD√ëB√ëA. As each node on the path processes the message, it creates (or updates) its route to the Ô¨Ånal destination, F; the return route to A had been created earlier when the node processed the corresponding RouteRequest. At this point, A and F can communicate bidirectionally. (Each RouteRequest is acknowledged to ensure bidirectionality of each individual link.) 13.4 Loop-Free Distance Vector Algorithms 303
An Introduction to Computer Networks, Release 2.0.11 This F√ëE√ëD√ëB√ëA is longer than necessary; a shorter path is F √ëE√ëC√ëA. The shorter path will be adopted if, at some future point, E learns that E √ëC√ëA is a better path, though there is no mechanism to seek out this route. If the ‚Äúdestination only‚Äù Ô¨Çag were not set, any intermediate node reached by the RouteRequest Ô¨Çooding could have answered with a route to F, if it had one. Such a node would generate the RouteReply on its own, without involving F. The sequence number of the intermediate node‚Äôs route to F must be greater than the sequence number in the RouteRequest message. If two neighboring nodes can no longer reach one another, each sends out a RouteError message, to invalidate the route. Nodes keep track of what routes pass through them, for just this purpose. One node‚Äôs message will reach the source and the other‚Äôs the destination, at which point the route is invalidated. In larger networks, it is standard for the originator of a RouteRequest to set the IPv4 header TTL value (or the IPv6 Hop_Limit) to a smallish value ( RFC 3561 recommends an intial value of 1) to limit the scope of theRequestRoute messages. If no answer is received, the originator tries again, with a slightly larger TTL value. In a large network, this reduces the volume of RouteRequest messages that have gone too far and therefore cannot be of use in Ô¨Ånding a route. AODV cannot form even short-term loops. To show this, we start with the observation that whenever a xdestination,next_hop yforwarding entry installed at a node, due either to a RouteRequest or to a RouteReply, the next_hop is always the node from which the RouteRequest orRouteReply was received, and therefore the destination sequence number cannot get smaller as we move from the original node to its next_hop. That is, as we follow any route to a destination, the destination sequence numbers are nondecreasing. It immediately follows that, for a routing loop, the destination sequence number is constant along the loop. This means that each node on the route must have heard of the route via the same RouteRequest orRouteReply message, as forwarded. The second observation, completing the argument, is that the hopcount Ô¨Åeld must strictly decrease as we travel along the route to the destination; the processing rules for RouteRequests andRouteReplies mean that each node installs a hopcount of one more than that of the neighboring node from which the route was received. This is impossible for a route that returns to the same node. 13.4.3 HWMP The Hybrid Wireless Mesh Protocol is based on AODV, and has been chosen for the IEEE 802.11s WiFi mesh networking standard ( 4.2.4.4 Mesh Networks ). In the discussion here, we will assume HWMP is being used in a Wi-Fi network, though the protocol applies to any type of network. A set of nodes is designated as the routing (or forwarding) nodes; ordinary Wi-Fi stations may or may not be included here. HWMP replaces the hopcount metric used in AODV with an ‚Äúairtime link metric‚Äù which decreases as the link throughput increases and as the link error rate decreases. This encourages the use of higher-quality wireless links. HWMP has two route-generating modes: an on-demand mode very similar to AODV, and a proactive mode used when there is at least one identiÔ¨Åed ‚Äúroot‚Äù node that connects to the Internet. In this case, the route-generating protocol determines a loop-free subset of the relevant routing links (that is, a spanning tree) by which each routing node can reach the root (or one of the roots). This tree-building process does not attempt to Ô¨Ånd best paths between pairs of non-root nodes, though such nodes can use the on-demand mode as necessary. 304 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 In the Ô¨Årst, on-demand, mode, HWMP implements a change to classic AODV in that if a node receives aRouteRequest message and then later receives a second RouteRequest message with the same sequence number but a lower-cost route, then the second route replaces the Ô¨Årst. In the proactive mode, the designated root node ‚Äì typically the node with wired Internet access ‚Äì periodically sends out specially marked RouteRequest messages. These are sent to the broadcast address, rather than to any speciÔ¨Åc destination, but otherwise propagate in the usual way. Routing nodes receiving two copies from two different neighbors pick the one with the shortest path. Once this process stabilizes, each routing node knows the best path to the root (or to aroot); the fact that each routing node chooses the best path from among all RouteRequest messages received ensures eventual route optimality. Routing nodes that have trafÔ¨Åc to send can at any time generate a RouteReply, which will immediately set up a reverse route from the root to the node in question. Finally, reversing each link to the root allows the root to send broadcast messages. HWMP has yet another mode: the root nodes can send out RootAnnounce (RANN) messages. These let other routing nodes know what the root is, but are not meant to result in the creation of routes to the root. 13.4.4 EIGRP EIGRP, or the Enhanced Interior Gateway Routing Protocol, is a once-proprietary Cisco distance-vector protocol that was released as an Internet Draft in February 2013. As with DSDV, it eliminates the risk of routing loops, even ephemeral ones. It is based on the ‚Äúdistributed update algorithm‚Äù (DUAL) of [JG93]. EIGRP is an actual protocol; we present here only the general algorithm. Our discussion follows [CH99]. Each router R keeps a list of neighbor routers N R, as with any distance-vector algorithm. Each R also maintains a data structure known (somewhat misleadingly) as its topology table. It contains, for each destination D and each N in N R, an indication of whether N has reported the ability to reach D and, if so, the reported cost c(D,N). The router also keeps, for each N in N R, the cost c Nof the link from R to N. Finally, the forwarding-table entry for any destination can be marked ‚Äúpassive‚Äù, meaning safe to use, or ‚Äúactive‚Äù, meaning updates are in process and the route is temporarily unavailable. Initially, we expect that for each router R and each destination D, R‚Äôs next_hop to D in its forwarding table is the neighbor N for which the following total cost is a minimum: c(D,N) + c N Now suppose R receives a distance-vector report from neighbor N 1that it can reach D with cost c(D,N 1). This is processed in the usual distance-vector way, unless it represents an increased cost and N 1is R‚Äôs next_hop to D; this is the third case in 13.1.1 Distance-Vector Update Rules. In this case, let C be R‚Äôs current cost to D, and let us say that neighbor N of R is a feasible next_hop (feasible successor in Cisco‚Äôs terminology) if N‚Äôs cost to D (that is, c(D,N)) is strictly less than C. R then updates its route to D to use the feasible neighbor N for which c(D,N) + c Nis a minimum. Note that this may not in fact be the shortest path; it is possible that there is another neighbor M for which c(D,M)+c Mis smaller, but c(D,M) ¬•C. However, because N‚Äôs path to D is loop-free, and because c(D,N) < C, this new path through N must also be loop-free; this is sometimes summarized by the statement ‚Äúone cannot create a loop by adopting a shorter route‚Äù. If no neighbor N of R is feasible ‚Äì which would be the case in the D‚ÄîA‚ÄîB example of 13.2 DistanceVector Slow-Convergence Problem, then R invokes the ‚ÄúDUAL‚Äù algorithm. This is sometimes called a ‚Äúdiffusion‚Äù algorithm as it invokes a diffusion-like spread of table changes proceeding away from R. 13.4 Loop-Free Distance Vector Algorithms 305
An Introduction to Computer Networks, Release 2.0.11 Let C in this case denote the new cost from R to D as based on N 1‚Äôs report. R marks destination D as ‚Äúactive‚Äù (which suppresses forwarding to D) and sends a special query to each of its neighbors, in the form of a distance-vector report indicating that its cost to D has now increased to C. The algorithm terminates when all R‚Äôs neighbors reply back with their own distance-vector reports; at that point R marks its entry for D as ‚Äúpassive‚Äù again. Some neighbors may be able to process R‚Äôs report without further diffusion to other nodes, remain ‚Äúpassive‚Äù, and reply back to R immediately. However, other neighbors may, like R, now become ‚Äúactive‚Äù and continue the DUAL algorithm. In the process, R may receive other queries that elicit its distance-vector report; as long as R is ‚Äúactive‚Äù it will report its cost to D as C. We omit the argument that this process ‚Äì and thus the network ‚Äì must eventually converge. 13.5 Link-State Routing-Update Algorithm Link-state routing is an alternative to distance-vector. It is often ‚Äì though certainly not always ‚Äì considered to be the routing-update algorithm class of choice for networks that are ‚ÄúsufÔ¨Åciently large‚Äù, such as those of ISPs. There are two speciÔ¨Åc link-state protocols: the IETF‚Äôs Open Shortest Path First ( OSPF ,RFC 2328 ), and OSI‚Äôs Intermediate Systems to Intermediate Systems ( IS-IS, documented unofÔ¨Åcially in RFC 1142 ). In distance-vector routing, each node knows a bare minimum of network topology: it knows nothing about links beyond those to its immediate neighbors. In the link-state approach, each node keeps a maximum amount of network information: a full map of all nodes and all links. Routes are then computed locally from this map, using the shortest-path-Ô¨Årst algorithm. The existence of this map allows, in theory, the calculation of different routes for different quality-of-service requirements. The map also allows calculation of a new route as soon as news of the failure of the existing route arrives; distance-vector protocols on the other hand must wait for news of a new route after an existing route fails. Link-state protocols distribute network map information through a modiÔ¨Åed form of broadcast of the status of each individual link. Whenever either side of a link notices the link has died (or if a node notices that a new link has become available), it sends out link-state packets (LSPs) that ‚ÄúÔ¨Çood‚Äù the network. This broadcast process is called reliable Ô¨Çooding. In general, broadcast mechanisms are not compatible with networks that have topological looping (that is, redundant paths); broadcast packets may circulate around the loop endlessly. Link-state protocols must be carefully designed to ensure that both every router sees every LSP, and also that no LSPs circulate repeatedly. (The acronym LSP is used by IS-IS; the preferred acronym used by OSPF is LSA, where A is for advertisement.) LSPs are sent immediately upon link-state changes, like triggered updates in distance-vector protocols except there is no ‚Äúrace‚Äù between ‚Äúbad news‚Äù and ‚Äúgood news‚Äù. It is possible for ephemeral routing loops to exist; for example, if one router has received a LSP but another has not, they may have an inconsistent view of the network and thus route to one another. However, as soon as the LSP has reached all routers involved, the loop should vanish. There are no ‚Äúrace conditions‚Äù, as with distance-vector routing, that can lead to persistent routing loops. The link-state Ô¨Çooding algorithm avoids the usual problems of broadcast in the presence of loops by having each node keep a database of all LSP messages. The originator of each LSP includes its identity, information about the link that has changed status, and also a sequence number. Other routers need only keep in their databases the LSP packet with the largest sequence number; older LSPs can be discarded. When a router receives a LSP, it Ô¨Årst checks its database to see if that LSP is old, or is current but has been received before; 306 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 in these cases, no further action is taken. If, however, an LSP arrives with a sequence number not seen before, then in typical broadcast fashion the LSP is retransmitted over all links except the arrival interface. As an example, consider the following arrangement of routers: EA B C D Suppose the A‚ÄìE link status changes. A sends LSPs to C and B. Both these will forward the LSPs to D; suppose B‚Äôs arrives Ô¨Årst. Then D will forward the LSP to C; the LSP traveling C √ëD and the LSP traveling D√ëC might even cross on the wire. D will ignore the second LSP copy that it receives from C and C will ignore the second copy it receives from D. It is important that LSP sequence numbers not wrap around. (Protocols that doallow a numeric Ô¨Åeld to wrap around usually have a clear-cut idea of the ‚Äúactive range‚Äù that can be used to conclude that the numbering has wrapped rather than restarted; this is harder to do in the link-state context.) OSPF uses lollipop sequencenumbering here: sequence numbers begin at -231and increment to 231-1. At this point they wrap around back to 0. Thus, as long as a sequence number is less than zero, it is guaranteed unique; at the same time, routing will not cease if more than 231updates are needed. Other link-state implementations use 64-bit sequence numbers. Actual link-state implementations often give link-state records a maximum lifetime; entries must be periodically renewed. 13.5.1 Shortest-Path-First Algorithm The next step is to compute routes from the network map, using the shortest-path-Ô¨Årst (SPF) algorithm. This algorithm computes shortest paths from a given node, A in the example here, to all other nodes. Below is our example network; we are interested in the shortest paths from A to B, C and D. A B C D3 9 10 211 4 Before starting the algorithm, we note the shortest path from A to D is A-B-C-D, which has cost 3+4+2=9. The algorithm builds the set Rof all shortest-path routes iteratively. Initially, Rcontains only the 0-length route to the start node; one new destination and route is added to Rat each stage of the iteration. At each stage we have a current node, representing the node most recently added to R. The initial current node is our starting node, in this case, A. 13.5 Link-State Routing-Update Algorithm 307
An Introduction to Computer Networks, Release 2.0.11 We will also maintain a set T, for tentative, of routes to other destinations. This is also initialized to empty. At each stage, we Ô¨Ånd all nodes which are immediate neighbors of the current node and which do not already have routes in the set R. For each such node N, we calculate the cost of the route from the start node to N that goes through the current node. We see if this is our Ô¨Årst route to N, or if the route improves on any route to N already in T; if so, we add or update the route in Taccordingly. Doing this, the routes will be discovered in order of increasing (or nondecreasing) cost. At the end of this process, we choose the shortest path in T, and move the route and destination node to R. The destination node of this shortest path becomes the next current node. Ties can be resolved arbitrarily, but note that, as with distance-vector routing, we must choose the minimum or else the accurate-costs property will fail. We repeat this process until all nodes have routes in the set R. For the example above, we start with current = A and R= {xA,A,0y}. The set Twill be {xB,B,3y,xC,C,10y, xD,D,11y}. The lowest-cost entry is xB,B,3y, so we move that to Rand continue with current = B. No path through C or D can possibly have lower cost. For the next stage, the neighbors of B without routes in Rare C and D; the routes from A to these through B arexC,B,7yandxD,B,12y. The former is an improvement on the existing TentryxC,C,10yand so replaces it; the latter is not an improvement over xD,D,11y.Tis now {xC,B,7y,xD,D,11y}. The lowest-cost route in Tis that to C, so we move this node and route to Rand set C to be current. Again,xC,B,7ymust be the shortest path to C. If any lower-cost path to C existed, then we would be selecting that shorter path ‚Äì or a preÔ¨Åx of it ‚Äì at this point, instead of the xC,B,7ypath; see the proof below. For the next stage, D is the only nonRneighbor; the path from A to D via C has entry xD,B,9y, an improvement over the existing xD,D,11yinT. The only entry in Tis nowxD,B,9y; this has the lowest cost and thus we move it to R. We now have routes in Rto all nodes, and are done. Here is another example, again with links labeled with costs: A B D C3 12 8 We start with current = A. At the end of the Ô¨Årst stage, xB,B,3yis moved into R,Tis {xD,D,8y}, and current is B. The second stage adds xC,B,5ytoT, and then moves this to R;current then becomes C. The third stage introduces the route (from A) xD,B,6y; this is an improvement over xD,D,8yand so replaces it in T; at the end of the stage this route to D is moved to R. In both the examples above, the current nodes progressed along a path, A √ëB√ëC√ëD. This is not generally the case; here is a similar example but with different lengths in which current jumps from B to D: 308 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 A B D C3 13 4 As in the previous example, at the end of the Ô¨Årst stage xB,B,3yis moved into R, with T= {xD,D,4y}, and B becomes current. The second stage adds xC,B,6ytoT. However, the shortest path in Tis nowxD,D,4y, and so it is D that becomes the next current. The Ô¨Ånal stage replaces xC,B,6yin T withxC,D,5y. At that point this route is added to Rand the algorithm is completed. Proof that SPF paths are shortest: suppose, by contradiction, that, for some node, a shorter path exists than the one generated by SPF. Let A be the start node, and let U be the Ô¨Årstnode generated for which the SPF path is not shortest. Let Tbe the Tentative set and let Rbe the set of completed routes at the point when we choose U as current, and let d be the cost of the new route to U. Let P=xA,.. . ,X,Y ,.. . ,U ybe the shorter path to U, with cost c<d, where Y is the Ô¨Årst node along the path notto have a route in R(it is possible Y=U). AU Y X nodes with routes in Rlower-cost path Ppath to U chosen by SPF At some strictly earlier stage in the algorithm, we must have added a route to node X, as the route to X is in R. In the following stage, we would have included the preÔ¨Åx xA,.. . ,X,YyofP in Tentative. This path to Y has cost ¬§c. This route must still be in Tat the point we chose U ascurrent, as there is no route to Y in R, but this means we should instead have chosen Y as current, contradicting the choice of U. A link-state source node S computes the entire path to a destination D (in fact it computes the path to every destination). But as far as the actual path that a packet sent by S will take to D, S has direct control only as far as the Ô¨Årst hop N. While the accurate-cost rule we considered in distance-vector routing will still hold, the actual path taken by the packet may differ from the path computed at the source, in the presence of alternative paths of the same length. For example, S may calculate a path S‚ÄìN‚ÄìA‚ÄìD, and yet a packet may take path S‚ÄìN‚ÄìB‚ÄìD, so long as the N‚ÄìA‚ÄìD and N‚ÄìB‚ÄìD paths have the same length. Link-state routing allows calculation of routes on demand (results are then cached), or larger-scale calculation. Link-state also allows routes calculated with quality-of-service taken into account, via straightforward extension of the algorithm above. Because the starting node is Ô¨Åxed, the shortest-path-Ô¨Årst algorithm can be classiÔ¨Åed as a single-source approach. If the goal is to compute the shortest paths between all pairs of nodes in a network, the FloydWarshall algorithm is an alternative, with slightly better performance in networks with large numbers of 13.5 Link-State Routing-Update Algorithm 309
An Introduction to Computer Networks, Release 2.0.11 links. 13.6 Routing on Other Attributes There is sometimes a desire to route on packet attributes other than the destination, or the destination and QoS bits. For example, we might want to route packets based in part on the packet source, or on the TCP port number. This kind of routing is decidedly nonstandard, though it is often available, and often an important component of trafÔ¨Åc engineering. This option is often known as policy-based routing, because packets are routed according to attributes speciÔ¨Åed by local administrative policy. (This term should not be confused with BGP routing policy (15 Border Gateway Protocol (BGP) ), which means something quite different.) Policy-based routing is not used frequently, but one routing decision of this type can have far-reaching effects. If an ISP wishes to route customer voice trafÔ¨Åc differently from customer data trafÔ¨Åc, for example, it need only apply policy-based routing to classify trafÔ¨Åc at the point of entry, and send the voice trafÔ¨Åc to its own router. After that, ordinary routers on the voice path and on the separate data path can continue the forwarding without using policy-based methods. Sometimes policy-based routing is used to mark packets for special processing; this might mean different routing further downstream or it might mean being sent along the same path as the other trafÔ¨Åc but with preferential treatment. For two packet-marking strategies, see 25.7 Differentiated Services and25.12 MultiProtocol Label Switching (MPLS). On Linux systems policy-based routing is part of the Linux Advanced Routing facility, often grouped with some advanced queuing features known as TrafÔ¨Åc Control; the combination is referred to as LARTC. As a simple example of what can be done, suppose a site has two links L1 and L2 to the Internet, with L1 the default route to the Internet. Perhaps L1 is faster and L2 serves more as a backup; perhaps L2 has been added to increase outbound capacity. A site may wish to route some outbound trafÔ¨Åc via L2 for any of the following reasons: 
- the trafÔ¨Åc may involve protocols deemed lower in priority ( egemail) 
- the trafÔ¨Åc may be real-time trafÔ¨Åc that can beneÔ¨Åt from reduced competition on L2 
- the trafÔ¨Åc may come from lower-priority senders; egsome customers within the site may be relegated to using L2 because they are paying less 
- a few large-volume elephant Ô¨Çows may be ofÔ¨Çoaded from L1 to L2 In the Ô¨Årst two cases, routing might be based on the destination port numbers; in the third, it might be based on the source IP address. In the fourth case, a site‚Äôs classiÔ¨Åcation of its elephant Ô¨Çows may have accumulated over time. Note that nothing can be done in the inbound direction unless L1 and L2 lead to the same ISP, and even there any special routing would be at the discretion of that ISP. The trick with LARTC is to be compatible with existing routing-update protocols; this would be a problem if the kernel forwarding table simply added columns for other packet attributes that neighboring non-LARTC routers knew nothing about. Instead, the forwarding table is split up into multiple xdest, next_hopy(orxdest, 310 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 QoS, next_hop y) tables. One of these tables is the main table, and is the table that is updated by routingupdate protocols interacting with neighbors. Before a packet is forwarded, administratively supplied rules are consulted to determine which table to apply; these rules areallowed to consult other packet attributes. The collection of tables and rules is known as the routing policy database. As a simple example, in the situation above the main table would have an entry xdefault, L1y(more precisely, it would have the IP address of the far end of the L1 link instead of L1 itself). There would also be another table, perhaps named slow, with a single entry xdefault, L2y. If a rule is created to have a packet routed using the ‚Äúslow‚Äù table, then that packet will be forwarded via L2. Here is one such Linux rule, which causes all trafÔ¨Åc from host 10.0.0.17 to be routed using table ‚Äúslow‚Äù: ip rule add from10.0.0.17 table slow Now suppose we want to route trafÔ¨Åc to port 25 (the SMTP port) via L2. This is harder; Linux provides no support here for routing based on port numbers. However, we can instead use the iptables mechanism to ‚Äúmark‚Äù all packets destined for port 25, and then create a routing-policy rule to have such marked trafÔ¨Åc use the slow table. The mark is known as the forwarding mark, or fwmark; its value is 0 by default. The fwmark is not actually part of the packet; it is associated with the packet only while the latter remains within the kernel. iptables --table mangle --append PREROUTING \\ --protocol tcp --dport 25 --jump MARK --set-mark 1 ip rule add fwmark 1 table slow Consult the applicable man pages for further details. The iptables mechanism can also be used to set the appropriate QoS bits ‚Äì the IPv4 DS bits ( 9.1 The IPv4 Header ) or the IPv6 TrafÔ¨Åc Class bits ( 11.1 The IPv6 Header ) ‚Äì so that a single standard IP forwarding table can be used, though support for the IPv4 QoS bits is limited. Linux actually maintains a numbered set of up to 255 tables; the main table is number 254. These associations between numbers and table names are stored in /etc/iproute2/rt_tables. If we want a table named ‚Äúslow‚Äù then we have to give it a number in this Ô¨Åle, perhaps with an entry 100 slow (Alternatively, it is possible to replace the name ‚Äúslow‚Äù in the earlier commands with the number.) 13.6.1 Reverse-Path Filtering There is one other issue we must deal with to build a working example from the fragments above. When a packet arrives at a router, a feature called reverse-path Ô¨Åltering will prevent forwarding the packet if the router does not have a forwarding entry that would forward the source address back out the interface by which the packet just arrived; that is, if the router knows it won‚Äôt be able to route reply packets properly, it won‚Äôt forward the original packet. This is very commonly implemented, and is meant to prevent spooÔ¨Ång. If an ISP, for example, has IP address block 200.1.0.0/16, then reverse-path Ô¨Åltering will prevent miscreant ISP customers from sending out packets with ‚Äúspoofed‚Äù source addresses not in this block. If every ISP implemented this feature, certain malicious attacks such as SYN Ô¨Çooding ( 17.3 TCP Connection Establishment ) would be much harder. 13.6 Routing on Other Attributes 311
An Introduction to Computer Networks, Release 2.0.11 But for the router above with the two paths L1 and L2, reverse-path Ô¨Åltering must be disabled. Suppose this router is R, and let D be the destinaton that the ‚Äúslow‚Äù path is being set up to reach. R will have a ‚Äúnormal‚Äù (perhaps default) route to reach D via L1. The point of policy-based routing is so packets to D will be routed using the other, ‚Äúslow‚Äù path, over L2. But with reverse-path Ô¨Åltering in place the router will detect that return packets from D, arriving via L2, have a source address that does not get forwarded back over L2, because the return packets don‚Äôt match the routing-policy rule. So, unless reverse-path-Ô¨Åltering is disabled, R will drop these return packets. Packets toD will be delivered, but all D‚Äôs replies will be lost. On Linux systems, the Ô¨Åles in the directory /proc/sys/net/ipv4/conf determine the status of reverse-path Ô¨Åltering. This directory contains a subdirectory for each interface, plus default. Each subdirectory contains, among other things, a Ô¨Åle rp_filter; if this Ô¨Åle contains a 0, reverse-path Ô¨Åltering is off, while a value of 1 enables it. A value of 2 enables ‚Äúloose reverse-path Ô¨Åltering‚Äù, in that a packet is forwarded if there is some interface ‚Äì not necessarily the arrival interface ‚Äì by which the router can reach the source address. 13.7 ECMP Equal-Cost MultiPath routing, or ECMP, is a technique for combining two (or more) routes to a destination into a single unit, so that trafÔ¨Åc to that destination is distributed (not necessarily equally) among the routes. ECMP is supported by EIGRP ( 13.4.4 EIGRP ) and the link-state implementations OSPF and IS-IS (13.5 Link-State Routing-Update Algorithm ). At the Ethernet level, ECMP is supported in spirit (if not in name) by TRILL and SPB ( 3.3 TRILL and SPB ). It is also supported by BGP ( 15 Border Gateway Protocol (BGP) ) for inter-AS routing. A simpler alternative to ECMP is channel bonding, also known as link aggregation, and often based on the IEEE 802.3ad standard. In channel bonding, two parallel Ethernet links are treated as a single unit. In many cases it is simpler and cheaper to bond two or three 1 Gbps Ethernet links than to upgrade everything to support 10 Gbps. Channel bonding applies, however, in limited circumstances; for example, the two channels must both be Ethernet, and must represent a single link. In the absence of channel bonding, equal-cost does not necessarily mean equal-propagation-delay. Even for two short parallel links, queuing delays on one link may mean that packet delivery order is not preserved. As TCP usually interprets out-of-order packet delivery as evidence of packet loss ( 19.3 TCP Tahoe and Fast Retransmit ), this can lead to large numbers of spurious retransmissions. For this reason, ECMP is almost always conÔ¨Ågured to send all the packets of any one TCP connection over just one of the links (as determined by a hash function); some channel-bonding implementations do the same, in fact. A consequence is that ECMP conÔ¨Ågured this way must see a large number of parallel TCP connections in order to utilize all participating paths reasonably equally. In special cases, however, it may be practical to conÔ¨Ågure ECMP to alternate between the paths on a per-packet basis, using round-robin transmission; this approach has the potential to achieve much better load-balancing between the paths. In terms of routing-update protocols, ECMP can be viewed as allowing two (or more) next_hop values, each with the same cost, to be associated with the same destination. See30.9.4 multitrunk.py for an example of the use of software-deÔ¨Åned networking to have multiple TCP connections take different paths to the same destination, in a way similar to the ECMP approach. 312 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 13.8 Epilog At this point we have concluded the basics of IP routing, involving routing within large (relatively) homogeneous organizations such as multi-site corporations or Internet Service Providers. Every router involved must agree to run the same protocol, and must agree to a uniform assignment of link costs. At the very largest scales, these requirements are impractical. The next chapter is devoted to this issue of very-large-scale IP routing, egon the global Internet. 13.9 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.10 Solutions for Routing-Update Algorithms. 1.0. Suppose the network is as follows, where distance-vector routing update is used. Each link has cost 1, and each router initially has entries in its forwarding table only for its immediate neighbors (so A‚Äôs table containsxB,B,1y,xD,D,1yand B‚Äôs table contains xA,A,1y,xC,C,1y). A D E FC B 1 11 1 1 1 (a). Suppose each router creates a report from its initial conÔ¨Åguration and sends that to each of its neighbors. What will each router‚Äôs forwarding table be after this set of exchanges? The exchanges, in other words, are all conducted simultaneously; each router Ô¨Årst sends out its own report and then processes the reports arriving from its two neighbors. (b). What destinations, with corresponding cost values, will be added to each router‚Äôs table after the simultaneous-and-parallel exchange process of part (a) is repeated a second time? Ignore the next_hop values. Hint: for each router, this step will add one additional destination entry. This new destination entry can be determined without going through the table exchanges in detail. 2.0. Now suppose the conÔ¨Åguration of routers has the link weights shown below. Each router again initially has entries in its forwarding table only for its immediate neighbors. A D E FC B3 4 1 112 2 13.8 Epilog 313
An Introduction to Computer Networks, Release 2.0.11 (a). As in the previous exercise, give each router‚Äôs forwarding table after each router exchanges with its immediate neighbors simultaneously and in parallel. (b). How many iterations of such parallel exchanges will it take before C learns to reach F via B; that is, before it creates the entry xF,B,11y? Count the answer to part (a) as the Ô¨Årst iteration. 3.0.‚ô¢A router R has the following distance-vector table: destination cost next hop A 2 R1 B 3 R2 C 4 R1 D 5 R3 R now receives the following report from R1; the cost of the R‚ÄìR1 link is 1. destination cost A 1 B 2 C 4 D 3 Give R‚Äôs updated table after it processes R1‚Äôs report. For each entry that changes, give a brief explanation 4.0. A router R has the following distance-vector table: destination cost next hop A 5 R1 B 6 R1 C 7 R2 D 8 R2 E 9 R3 R now receives the following report from R1; the cost of the R‚ÄìR1 link is 1. destination cost A 4 B 7 C 7 D 6 E 8 F 8 Give R‚Äôs updated table after it processes R1‚Äôs report. For each entry that changes, give a brief explanation, in the style of 13.1.5 Example 4. 314 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 5.0. At the start of Example 3 ( 13.1.4 Example 3 ), we changed C‚Äôs routing table so that it reached D via A instead of via E: C‚Äôs entry xD,E,2ywas changed to xD,A,2y. This meant that C had a valid route to D at the start. How might the scenario of Example 3 play out if C‚Äôs table had not been altered? Give a sequence of reports that leads to correct routing between D and E. 6.0. In the following exercise, A-D are routers and the attached subnets N1-N6, which are the ultimate destinations, are shown explicitly. In the case of N1 through N4, the links arethe subnets. Routers still exchange distance-vector reports with neighboring routers, as usual. In the tables requested below, if a router has a direct connection to a subnet, you may report the next_hop as ‚Äúdirect‚Äù, eg, from A‚Äôs table, xN1,direct,0y A D CBN5 N6N1 N2N3 N4 (a). Give the initial tables for A through D, before any distance-vector exchanges. (b). Give the tables after each router A-D exchanges with its immediate neighbors simultaneously and in parallel. (c). At the end of (b), what subnets are notknown by what routers? 7.0. Suppose A, B, C, D and E are connected as follows. Each link has cost 1, and so each forwarding table is uniquely determined; B‚Äôs table is xA,A,1y,xC,C,1y,xD,A,2y,xE,C,2y. Distance-vector routing update is used. B A DCB E1 1 1 1 1 Now suppose the D‚ÄìE link fails, and so D updates its entry for E to xE,-,8y. (a). Give A‚Äôs table after D reports xE,8yto A (b). Give B‚Äôs table after A reports to B (c). Give A‚Äôs table after B reports to A; note that B has an entry xE,C,2y (d). Give D‚Äôs table after A reports to D. 13.9 Exercises 315
An Introduction to Computer Networks, Release 2.0.11 8.0. In the network below, A receives alternating reports about destination D from neighbors B and C. Suppose A uses a modiÔ¨Åed form of Rule 2 of 13.1.1 Distance-Vector Update Rules, in which it updates its forwarding table whenever new cost c is less than or equal to cold. A DB C1 1 11 Explain why A‚Äôs forwarding entry for destination D never stabilizes. 9.0. Consider the network in 13.2.1.1 Split Horizon :, using distance-vector routing updates. B and C‚Äôs table entries for destination D are shown. All link costs are 1. D A CB‚ü®D,A,2‚ü© ‚ü®D,A,2‚ü©1 111 Suppose the D‚ÄìA link breaks and then these update reports occur: 
- A reportsxD,8yto B (as before) 
- C reportsxD,2yto B (as before) 
- A now reports xD,8yto C (instead of B reporting xD,3yto A) (a). Give A, B and C‚Äôs forwarding-table records for destination D, including the cost, after these three reports. (b). What additional reports (a pair should sufÔ¨Åce) will lead to the formation of the routing loop? (c). What (single) additional report will eliminate the possibility of the routing loop? 10.0. Suppose the network of 13.2 Distance-Vector Slow-Convergence Problem is changed to the following. Distance-vector update is used; again, the D‚ÄìA link breaks. D A B‚ü®D,D,1‚ü© ‚ü®D,E,2‚ü© E11 1 1 (a). Explain why B‚Äôs report back to A, after A reports xD,-,8y, is now valid. (b). Explain why hold down ( 13.2.1.3 Hold Down ) will delay the use of the new route A‚ÄìB‚ÄìE‚ÄìD. 316 13 Routing-Update Algorithms
An Introduction to Computer Networks, Release 2.0.11 11.0. Suppose the routers are A, B, C, D, E and F, and all link costs are 1. The distance-vector forwarding tables for A and F are below. Give the network with the fewest links that is consistent with these tables. Hint: any destination reached at cost 1 is directly connected; if X reaches Y via Z at cost 2, then Z and Y must be directly connected. A‚Äôs table dest cost next_hop B 1 B C 1 C D 2 C E 2 C F 3 B F‚Äôs table dest cost next_hop A 3 E B 2 D C 2 D D 1 D E 1 E 12.0. (a) Suppose routers A and B somehow end up with respective forwarding-table entries xD,B,nyand xD,A,my, thus creating a routing loop. Explain why the loop may be removed more quickly if A and B both usepoison reverse with split horizon ( 13.2.1.1 Split Horizon ), versus if A and B use split horizon only. (b). Suppose the network looks like the following. The A‚ÄìB link is extremely slow. CA BD slow Suppose A and B send reports to each other advertising their routes to D, and immediately afterwards the C‚ÄìD link breaks and C reports to A and B that D is unreachable. After those unreachability reports from C are processed, A and B‚Äôs reports sent to each other before the break Ô¨Ånally arrive. Explain why the network is now in the state described in part (a). 13.0. Suppose the distance-vector algorithm is run on a network and no links break (so by the last paragraph of13.1.1 Distance-Vector Update Rules the next_hop-increase rule is never applied). (a). Prove that whenever A is B‚Äôs next_hop to destination D, then A‚Äôs cost to D is strictly less than B‚Äôs. Hint: assume that if this claim is true, then it remains true after any application of the rules in 13.1.1 Distance-Vector Update Rules. If the lower-cost rule is applied to B after receiving a report from 13.9 Exercises 317
An Introduction to Computer Networks, Release 2.0.11 A, resulting in a change to B‚Äôs cost to D, then one needs to show A‚Äôs cost is less than B‚Äôs, and also B‚Äôs new cost is less than that of any neighbor C that uses B as its next_hop to D. (b). Use (a) to prove that no routing loops ever form. 14.0. It was mentioned in 13.5 Link-State Routing-Update Algorithm that link-state routing might give rise to an ephemeral routing loop. Give a concrete scenario illustrating creation (and then dissolution) of such a loop. Hint: consider the following network. AB C C1B1 C‚Äôs next_hop to A is B, due to the relative link weights (not shown) of the A‚ÄìB1 and A‚ÄìC1 links. Then B gets some news. (There are also several other possible scenarios.) 15.0. Use the Shortest-Path-First algorithm to Ô¨Ånd the shortest path from A to E in the network below. Show the sets RandT, and the node current, after each step. A C ED B5 2131 4 5 16.0. Suppose you take a laptop, plug it into an Ethernet LAN, andconnect to the same LAN via Wi-Fi. From laptop to LAN there are now two routes. Which route will be preferred? How can you tell which way trafÔ¨Åc is Ô¨Çowing? How can you conÔ¨Ågure your OS to prefer one path or another? (See also 10.2.5 ARP and multihomed hosts ,9 IP version 4 exercise 4.0, and 17 TCP Transport Basics exercise 5.0.) 318 13 Routing-Update Algorithms
14 LARGE-SCALE IP ROUTING In the previous chapter we considered two classes of routing-update algorithms: distance-vector and linkstate. Each of these approaches requires that participating routers have agreed Ô¨Årst to a common protocol, and then to a common understanding of how link costs are to be assigned. We will address this in the following chapter on BGP, 15 Border Gateway Protocol (BGP); a basic problem is that if one site prefers the hop-count approach, assigning every link a cost of 1, while another site prefers to assign link costs in proportion to their bandwidth, then meaningful path cost comparisons between the two sites simply cannot be done. Before we can get to BGP, however, we need to revisit the basic IP forwarding idea. From the beginning we envisioned an IP address as being divisible into network and host portions; the job of most routers was to examine only the network preÔ¨Åx, and make the next_hop forwarding decision based on that. The net/host division for IPv4 addresses was originally based on the Class A/B/C mechanism; the net/host division for most IPv6 addresses was 64 bits for each. In 9.5 The Classless IP Delivery Algorithm we saw how to route on IPv4 network preÔ¨Åxes of arbitrary length; the same idea works for IPv6. In this chapter we are going to expand on this by allowing different routers at different positions in the routing universe to make their forwarding decisions based on different network-preÔ¨Åx lengths, all for the same destination address. That is, a backbone router might forward to a given IPv4 address D based only on the Ô¨Årst 12 bits of D; a more regional router might base its decision on the Ô¨Årst 18 bits, and a site router might forward to the Ô¨Ånal subnet based on the Ô¨Årst 24 bits. This allows the creation of routing hierarchies with multiple levels, which has the potential to greatly increase routing scalability and reduce the size of the forwarding tables. The actual mechanics of forwarding by any one router will still be as in 9.5 The Classless IP Delivery Algorithm. The term routing domain is used to refer to a set of routers under common administration, using a common link-cost assignment; another term for this is autonomous system. In the previous chapter, all routing took place within one routing domain; now we will envision the Internet as a whole consisting of a patchwork of independent routing domains. While use of a common routing-update protocol within the routing domain is not an absolute requirement ‚Äì for example, some subnets may internally use distance-vector while the site‚Äôs ‚Äúbackbone‚Äù routers use link-state ‚Äì we can assume that all routers have a uniform view of the site‚Äôs topology and cost metrics. ‚ÄúLarge-scale‚Äù IP routing is fundamentally about the coordination of routing between multiple independent routing domains. Even in the earliest Internet there were multiple routing domains, if for no other reason than that how to measure link costs was (and still is) too unsettled to set in stone. However, another component of large-scale routing is support for hierarchical routing, above the level of subnets; we turn to this next. 14.1 Classless Internet Domain Routing: CIDR CIDR is the mechanism for supporting hierarchical routing in the Internet backbone. Subnetting moves the network/host division line further rightwards; CIDR allows moving it to the left as well. With subnetting, the revised division line is visible only within the organization that owns the IP network-address block; subnetting is not visible outside. CIDR allows aggregation of IP address blocks in a way that isvisible to the Internet backbone. 319
An Introduction to Computer Networks, Release 2.0.11 When CIDR was introduced to IPv4 in 1993, the following were some of the justiÔ¨Åcations for it, all relating to the increasing size of the backbone IP forwarding tables, and expressed in terms of the then-current Class A/B/C mechanism: 
- The Internet is running out of Class B addresses (this happened in the mid-1990‚Äôs) 
- There are too many Class C‚Äôs (the most numerous) for backbone forwarding tables to be efÔ¨Åcient 
- Eventually IANA (the Internet Assigned Numbers Authority) will run out of IP addresses (this happened in 2011) Assigning non-CIDRed multiple Class C‚Äôs in lieu of a single Class B would have helped with the Ô¨Årst point in the list above, but made the second point worse. Ironically, the current (2013) very tight market for IPv4 address blocks is likely to lead to larger and larger backbone IPv4 forwarding tables, as sites are forced to use multiple small address blocks instead of one large block. By the year 2000, CIDR had essentially eliminated the Class A/B/C mechanism from the backbone Internet, and had more-or-less completely changed how backbone routing worked. You purchased an address block from a provider or some other IP address allocator, and it could be whatever size you needed, from /32 to /15. What CIDR enabled is IP routing based on an address preÔ¨Åx of any length; the Class A/B/C mechanism of course used Ô¨Åxed preÔ¨Åx lengths of 8, 16 and 24 bits. Furthermore, CIDR allows different routers, at different levels of the backbone, to route on preÔ¨Åxes of different lengths. If organization P were allocated a /10 block, for example, then P could suballocate into /20 blocks. At the top level, routing to P would likely be based on the Ô¨Årst 10 bits, while routing within P would be based on the Ô¨Årst 20 bits. IPv6 never had address classes, and so arguably CIDR was supported natively from the beginning. Routing to an address 2400:1234:5678:abcd:: could be done based on the /32 preÔ¨Åx 2400:1234::, or the /48 preÔ¨Åx 2400:1234:5678::, or the /56 preÔ¨Åx 2400:1234:5678:ab::, or any other length. CIDR was formally introduced to IPv4 by RFC 1518 andRFC 1519. For a while there were strategies in place to support compatibility with non-CIDR-aware routers; these are now obsolete. In particular, it is no longer appropriate for large-scale IPv4 routers to fall back on the Class A/B/C mechanism in the absence of CIDR information; if the latter is missing, the routing should fail. One way to look at the basic strategy of CIDR is as a mechanism to consolidate multiple network blocks going to the same destination into a single entry. Suppose a router has four IPv4 class C‚Äôs all to the same destination: 200.7.0.0/24√ù√ëfoo 200.7.1.0/24√ù√ëfoo 200.7.2.0/24√ù√ëfoo 200.7.3.0/24√ù√ëfoo The router can replace all these with the single entry 200.7.0.0/ 22√ù√ëfoo It does not matter here if foo represents a single ultimate destination or if it represents four sites that just happen to be routed to the same next_hop. 320 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 It is worth looking closely at the arithmetic to see why the single entry uses /22. This means that the Ô¨Årst 22 bits must match 200.7.0.0; this is all of the Ô¨Årst and second bytes and the Ô¨Årst six bits of the third byte. Let us look at the third byte of the network addresses above in binary: 200.7.000000 00.0/24 √ù√ëfoo 200.7.000000 01.0/24 √ù√ëfoo 200.7.000000 10.0/24 √ù√ëfoo 200.7.000000 11.0/24 √ù√ëfoo The /24 means that the network addresses stop at the end of the third byte. The four entries above cover every possible combination of the last two bits of the third byte; for an address to match one of the entries above it sufÔ¨Åces to begin 200.7 and then to have 0-bits as the Ô¨Årst six bits of the third byte. This is another way of saying the address must match 200.7.0.0/22. Most implementations actually use a bitmask, eg255.255.252.0, rather than the number 22. Note 252 is, in binary, 1111 1100, with 6 leading 1-bits, so 255.255.252.0 has 8+8+6=22 1-bits followed by 10 0-bits. The IP delivery algorithm of 9.5 The Classless IP Delivery Algorithm still works with CIDR, with the understanding that the router‚Äôs forwarding table can now have a network-preÔ¨Åx length associated with any entry. Given a destination D, we search the forwarding table for network-preÔ¨Åx destinations B/k until we Ô¨Ånd a match; that is, equality of the Ô¨Årst k bits. In terms of masks, given a destination D and a list of table entriesxpreÔ¨Åx,masky=xB[i],M[i]y, we search for i such that (D & M[i]) = B[i]. But what about the possibility of multiple matches? For subnets, avoiding this was the responsibility of the subnetting site, but responsibility for avoiding this with CIDR is much too distributed to be declared illegal by IETF mandate. Instead, CIDR introduced the longest-match rule: if destination D matches both B 1/k1 and B 2/k2, with k 1< k 2, then the longer match B 2/k2match is to be used. (Note that if D matches two distinct entries B 1/k1and B 2/k2then either k 1< k2or k 2< k1). 14.2 Hierarchical Routing Strictly speaking, CIDR is simply a mechanism for routing to IP address blocks of any preÔ¨Åx length; that is, for setting the network/host division point to an arbitrary place within the 32-bit IP address. However, by making this network/host division point variable, CIDR introduced support for routing on different preÔ¨Åx lengths at different places in the backbone routing infrastructure. For example, top-level routers might route on /8 or /9 preÔ¨Åxes, while intermediate routers might route based on preÔ¨Åxes of length 14. This feature of routing on fewer bits at one point in the Internet and more bits at another point is exactly what is meant by hierarchical routing. We earlier saw hierarchical routing in the context of subnets: trafÔ¨Åc might Ô¨Årst be routed to a class-B site 147.126.0.0/16, and then, within that site, to subnets such as 147.126.1.0/24, 147.126.2.0/24, etc. But with CIDR the hierarchy can be much more Ô¨Çexible: the top level of the hierarchy can be much larger than the ‚Äúcustomer‚Äù level, lower levels need not be administratively controlled by the higher levels (as is the case with subnets), and more than two levels can be used. CIDR is an address-block-allocation mechanism; it does not directly speak to the kinds of policy we might wish to implement with it. Here are four possible applications; the latter two involve hierarchical routing: 14.2 Hierarchical Routing 321
An Introduction to Computer Networks, Release 2.0.11 
- Application 1 (legacy): CIDR allows the allocation of multiple blocks of Class C, or fragments of a Class A, to a single customer, so as to require only a single forwarding-table entry for that customer 
- Application 2 (legacy): CIDR allows opportunistic aggregation of routes: a router that sees the four 200.7.x.0/24 routes above in its table may consolidate them into a single entry. 
- Application 3 (current): CIDR allows huge provider blocks, with suballocation by the provider. This is known as provider-based routing. 
- Application 4 (hypothetical): CIDR allows huge regional blocks, with suballocation within the region, somewhat like the original scheme for US phone numbers with area codes. This is known as geographical routing. Each of these has the potential to achieve a considerable reduction in the size of the backbone forwarding tables, which is arguably the most important goal here. Each involves using CIDR to support the creation of arbitrary-sized address blocks and then routing to them as a single unit. For example, the Internet backbone might be much happier if all its routers simply had to maintain a single entry x200.0.0.0/8, R1 y, versus 256 entriesx200.x.0.0/16, R1 yfor every value of x. (As we will see below, this is still useful even if a few of the x‚Äôs have a different next_hop.) Secondary CIDR goals include bringing some order to IP address allocation and (for the last two items in the list above) enabling a routing hierarchy that mirrors the actual Ô¨Çow of most trafÔ¨Åc. Hierarchical routing does introduce one new wrinkle: the routes chosen may no longer be globally optimal, at least if we also apply the routing-update algorithms hierarchically. Suppose, for example, at the top level forwarding is based on the Ô¨Årst eight bits of the address, and all trafÔ¨Åc to 200.0.0.0/8 is routed to router R1. At the second level, R1 then routes trafÔ¨Åc (hierarchically) to 200.20.0.0/16 via R2. A packet sent to 200.20.1.2 by an independent router R3 might therefore pass through R1, even if there were a lowercost path R3√ëR4√ëR2 that bypassed R1. The top-level forwarding entry x200.0.0.0/8,R1 y, in other words, may represent a simpliÔ¨Åcation of the real situation. Prohibiting ‚Äúback-door‚Äù routes like R3 √ëR4√ëR2 is impractical (and would not be helpful either); customers are independent entities. This non-optimal routing issue cannot happen if all routers agree upon one of the shortest-path mechanisms of13 Routing-Update Algorithms; in that case R3 would learn of the lower-cost R3 √ëR4√ëR2 path. But then the potential hierarchical beneÔ¨Åts of decreasing the size of forwarding tables would be lost. More seriously, complete global agreement of all routers on one common update protocol is simply not practical; in fact, one of the goals of hierarchical routing is to provide a workable alternative. We will return to this below in 14.4.3 Hierarchical Routing via Providers. 14.3 Legacy Routing Back in the days of NSFNet, the Internet backbone was a single routing domain. While most customers did not connect directly to the backbone, the intervening providers tended to be relatively compact, geographically ‚Äì that is, regional ‚Äì and often had a single primary routing-exchange point with the backbone. IP addresses were allocated to subscribers directly by the IANA, and the backbone forwarding tables contained entries for every site, even the Class C‚Äôs. Because the NSFNet backbone and the regional providers did not necessarily share link-cost information, routes were even at this early point not necessarily globally optimal; compromises and approximations were made. However, in the NSFNet model routers generally did Ô¨Ånd a reasonable approximation to the shortest path to each site referenced by the backbone tables. While the legacy backbone routing domain was not 322 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 all-encompassing, if there were differences between two routes, at least the backbone portions ‚Äì the longest components ‚Äì would be identical. 14.4 Provider-Based Routing In provider-based routing, large CIDR blocks are allocated to large-scale providers. The different providers each know how to route to one another. While some subscribers will still have legacy IP-address blocks assigned to them directly, many others will obtain their IP addresses from within their providers‚Äô blocks. For the latter group, trafÔ¨Åc from the outside is routed Ô¨Årst to the provider via the provider‚Äôs address block, and then, within the provider‚Äôs routing domain, to the subscriber. We may even have a hierarchy of providers, so packets would be routed Ô¨Årst to the large-scale provider, and eventually to the local provider. There may no longer be a central backbone; instead, multiple providers may each build parallel transcontinental networks. Here is a simpler example, in which providers have unique paths to one another. Suppose we have providers P0, P1 and P2, with customers as follows: 
- P0: customers A,B,C 
- P1: customers D,E 
- P2: customers F,G We will also assume that each provider has an IP address block as follows: 
- P0: 200.0.0.0/8 
- P1: 201.0.0.0/8 
- P2: 202.0.0.0/8 Let us now allocate addresses to the customers: A: 200.0.0.0/16 B: 200.1.0.0/16 C: 200.2.16.0/20 (16 = 0001 0000) D: 201.0.0.0/16 E: 201.1.0.0/16 F: 202.0.0.0/16 G: 202.1.0.0/16 The routing model is that packets are Ô¨Årst routed to the appropriate provider, and then to the customer. While this model may not in general guarantee the shortest end-to-end path, it does in this case because each provider has a single point of interconnection to the others. Here is the network diagram: 14.4 Provider-Based Routing 323
An Introduction to Computer Networks, Release 2.0.11 P0 P1 P2 ABC DE FG With this diagram, P0‚Äôs forwarding table looks something like this: P0 destination next_hop 200.0.0.0/16 A 200.1.0.0/16 B 200.2.16.0/20 C 201.0.0.0/8 P1 202.0.0.0/8 P2 That is, P0‚Äôs table consists of 
- one entry for each of P0‚Äôs own customers 
- one entry for each other provider If we had 1,000,000 customers divided equally among 100 providers, then each provider‚Äôs table would have only 10,099 entries: 10,000 for its own customers and 99 for the other providers. Without CIDR, each provider‚Äôs forwarding table would have 1,000,000 entries. CIDR enables hierarchical routing by allowing the routing decision to be made on different preÔ¨Åx lengths in different contexts. For example, when a packet is sent from D to A, P1 looks at the Ô¨Årst 8 bits while P0 looks at the Ô¨Årst 16 bits. Within customer A, routing might be made based on the Ô¨Årst 24 bits. Even if we have some additional ‚Äúsecondary‚Äù links, that is, additional links that do not create alternative paths between providers, the routing remains relatively straightforward. Shown here are the private customer-to-customer links B‚ÄìD and E‚ÄìF; these are likely used only by the customers they connect. Two customers, A and E, are multihomed; that is, they have connections to alternative providers: A‚ÄìP1 and E‚ÄìP2. (The term ‚Äúmultihomed‚Äù is often applied to any host with multiple network interfaces on different LANs, which includes any router; here we mean more speciÔ¨Åcally that there are multiple network interfaces connecting to different providers.) Typically, though, while A and E may use their alternative-provider links all they want for outbound trafÔ¨Åc, their respective inbound trafÔ¨Åc would still go through their primary providers P0 and P1 respectively. 324 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 P0 P1 P2 ABC DE FG 14.4.1 Internet Exchange Points The long links joining providers in these diagrams are somewhat misleading; providers do not always like sharing long links and the attendant problems of sharing responsibility for failures. Instead, providers often connect to one another at Internet eXchange Points or IXPs; the link P0 P1 might actually be P0 IXP P1, where P0 owns the left-hand link and P1 the right-hand. IXPs can either be third-party sites open to all providers, or private exchange points. The term ‚ÄúMetropolitan Area Exchange‚Äù, or MAE, appears in the names of the IXPs MAE-East, originally near Washington DC, and MAE-West, originally in San Jose, California; each of these is now actually a setof IXPs. MAE in this context is now a trademark. 14.4.2 CIDR and Staying Out of Jail Suppose we want to change providers. One way we can do this is to accept a new IP-address block from the new provider, and change all our IP addresses. The paper Renumbering: Threat or Menace [LKCT96] was frequently cited ‚Äì at least in the early days of CIDR ‚Äì as an intimation that such renumbering was inevitably a Bad Thing. In principle, therefore, we would like to allow at least the option of keeping our IP address allocation while changing providers. An address-allocation standard that did not allow changing of providers might even be a violation of the US Sherman Antitrust Act; see American Society of Mechanical Engineers v Hydrolevel Corporation, 456 US 556 (1982). The IETF thus had the added incentive of wanting to stay out of jail, when writing the CIDR standard so as to allow portability between providers (actually, antitrust violations usually involve civil penalties). The CIDR longest-match rule turns out to be exactly what we (and the IETF) need. Suppose, in the diagrams above, that customer C wants to move from P0 to P1, and does not want to renumber. What routing changes need to be made? One solution is for P0 to add a route x200.2.16.0/20, P1 ythat routes all of C‚Äôs trafÔ¨Åc to P1; P1 will then forward that trafÔ¨Åc on to C. P1‚Äôs table will be as follows, and P1 will use the longest-match rule to distinguish trafÔ¨Åc for its new customer C from trafÔ¨Åc bound for P0. 14.4 Provider-Based Routing 325
An Introduction to Computer Networks, Release 2.0.11 P1 destination next_hop 200.0.0.0/8 P0 202.0.0.0/8 P2 201.0.0.0/16 D 201.1.0.0/16 E 200.2.16.0/20 C This does work, but all C‚Äôs inbound trafÔ¨Åc except for that originating in P1 will now be routed through C‚Äôs ex-provider P0, which as an ex-provider may not be on the best of terms with C. Also, the routing is inefÔ¨Åcient: C‚Äôs trafÔ¨Åc from P2 is routed P2 √ëP0√ëP1 instead of the more direct P2 √ëP1. A better solution is for allproviders other than P1 to add the route x200.2.16.0/20, P1 y. While trafÔ¨Åc to 200.0.0.0/8 otherwise goes to P0, this particular sub-block is instead routed by each provider to P1. The important case here is P2, as a stand-in for all other providers and their routers: P2 routes 200.0.0.0/8 trafÔ¨Åc to P0 except for the block 200.2.16.0/20, which goes to P1. Having every other provider in the world need to add an entry for C has the potential to cost some money, and, one way or another, C will be the one to pay. But at least there is a choice: C can consent to renumbering (which is not difÔ¨Åcult if they have been diligent in using DHCP and perhaps NAT too), or they can pay to keep their old address block. As for the second diagram above, with the various private links (shown as dashed lines), it is likely that the longest-match rule is notneeded for these links to work. A‚Äôs ‚Äúprivate‚Äù link to P1 might only mean that 
- A can send outbound trafÔ¨Åc via P1 
- P1 forwards A‚Äôs trafÔ¨Åc to A via the private link P2, in other words, is still free to route to A via P0. P1 may not advertise its route to A to anyone else. 14.4.3 Hierarchical Routing via Providers With provider-based routing, the route taken may no longer be end-to-end optimal; we have replaced the problem of Ô¨Ånding an optimal route from A to B with the two problems of Ô¨Ånding an optimal route from A to B‚Äôs provider P, and then from P‚Äôs entry point to B. This strategy mirrors the two-stage hierarchical routing process of Ô¨Årst routing on the address bits that identify the provider, and then routing on the address bits including the subscriber portion. This two-stage strategy may not yield the same result as Ô¨Ånding the globally optimal route. The result will be the same if B‚Äôs customers can only be reached through P‚Äôs single entry-point router RP, which models the situation that P and its customers look like a single site. However, either or both of the following can disrupt this model: 
- There may be multiple entry-point routers into provider P‚Äôs network, egRP1, RP 2and RP 3, with different costs from A. 
- P‚Äôs customer B may have an alternative connection to the outside world via a different provider, as in the second diagram in 14.4 Provider-Based Routing. 326 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 Consider the following example representing the Ô¨Årst situation (the more important one in practice), in which providers P1 and P2 have three interconnection points IX1, IX2, IX3 (from Internet eXchange, 14.4.1 Internet Exchange Points ). Links are labeled with costs; we assume that P1‚Äôs costs happen to be comparable with P2‚Äôs costs. R1 R2 R3 S1 S2 S3R1 R2 R3R S BA 1 13 5 7 4 3 8 Provider P2Provider P1 IX1 IX2IX3 1 000 00 The globally shortest path between A and B is via the R2‚ÄìIX2‚ÄìS2 crossover, with total length 5+1+0+4=10. However, trafÔ¨Åc from A to B will be routed by P1 to its closest crossover to P2, namely the R3‚ÄìIX3‚ÄìS3 link. The total path is 3+0+1+8+4=16. TrafÔ¨Åc from B to A will be routed by P2 via the R1‚ÄìIX1‚ÄìS1 crossover, for a length of 3+0+1+7+5=16. This routing strategy is sometimes called hot-potato routing; each provider tries to get rid of any trafÔ¨Åc (the potatoes) as quickly as possible, by routing to the closest exit point. Not only are the paths taken inefÔ¨Åcient, but the A√ù√ëB and B√ù√ëA paths are now asymmetric. This can be a problem if forward and reverse timings are critical, or if one of P1 or P2 has signiÔ¨Åcantly more bandwidth or less congestion than the other. In practice, however, route asymmetry is seldom important. As for the route inefÔ¨Åciency itself, this also is not necessarily a signiÔ¨Åcant problem; the primary reason routing-update algorithms focus on the shortest path is to guarantee that all computed paths are loop-free. As long as each half of a path is loop-free, and the halves do not intersect except at their common midpoint, these paths too will be loop-free. The BGP ‚ÄúMED‚Äù value ( 15.6.3 MULTI_EXIT_DISC ) offers an optional mechanism for P1 to agree that A√ù√ëB trafÔ¨Åc should take the r1‚Äìs1 crossover. This might be desired if P1‚Äôs network were ‚Äúbetter‚Äù and customer A was willing to pay extra to keep its trafÔ¨Åc within P1‚Äôs network as long as possible. 14.4.4 IP Geolocation In principle, provider-based addressing may mean that consecutive IP addresses are scattered all over a continent. In practice, providers (even many mobile providers) do not do this; any given small address block ‚Äì perhaps /24 ‚Äì is used in a limited geographical area. Different blocks are used in different areas. A consequence of this is that it is possible in principle to determine, from a given IP address, the corresponding approximate geographical location; this is known as IP geolocation. Even satellite-Internet users can be geolocated, although sometimes only to within a couple hundred miles. Several companies have created detailed geolocation maps, identifying many locations roughly down to the zip code, and typically available as an online service. 14.4 Provider-Based Routing 327
An Introduction to Computer Networks, Release 2.0.11 IP geolocation was originally developed so that advertisers could serve up regionally appropriate advertisements. It is, however, now used for a variety of purposes including identiÔ¨Åcation of the closest CDN edge server ( 1.12.2 Content-Distribution Networks ), network security, compliance with national regulations, higher-level user tracking, and restricting the streaming of copyrighted content. 14.5 Geographical Routing The classical alternative to provider-based routing is geographical routing; the archetypal model for this is the telephone area code system. A call from anywhere in the US to Loyola University‚Äôs main switchboard, 773-274-3000, would traditionally be routed Ô¨Årst to the 773 area code in Chicago. From there the call would be routed to the north-side 274 exchange, and from there to subscriber 3000. A similar strategy canbe used for IP routing. Geographical addressing has some advantages. Figuring out a good route to a destination is usually straightforward, and close to optimal in terms of the path physical distance. Changing providers never involves renumbering (though moving may). And approximate IP address geolocation (determining a host‚Äôs location from its IP address) is automatic. Geographical routing has some minor technical problems. First, routing may be inefÔ¨Åcient between immediate neighbors A and B that happen to be split by a boundary for larger geographical areas; the path might go from A to the center of A‚Äôs region to the center of B‚Äôs region and then to B. Another problem is that some larger sites ( eglarge corporations) are themselves geographically distributed; if efÔ¨Åciency is the goal, each ofÔ¨Åce of such a site would need a separate IP address block appropriate for its physical location. But the real issue with geographical routing is apparently the business question of who carries the trafÔ¨Åc. The provider-based model has a very natural answer to this: every link is owned by a speciÔ¨Åc provider. For geographical IP routing, my local provider might know at once from the preÔ¨Åx that a packet of mine is to be delivered from Chicago to San Francisco, but who will carry it there? My provider might have to enter into different trafÔ¨Åc contracts for multiple different regions. If different local providers make different arrangements for long-haul packet delivery, the routing efÔ¨Åciency (at least in terms of table size) of geographical routing is likely lost. Finally, there is no natural answer for who should own those long inter-region links. It may be useful to recall that the present area-code system was created when the US telephone system was an AT&T monopoly, and the question of who carried trafÔ¨Åc did not exist. That said, the top Ô¨Åve Regional Internet Registries represent geographical regions (usually continents), and provider-based addressing is below that level. That is, the IANA handed out address blocks to the geographical RIRs, and the RIRs then allocated address blocks to providers. At the intercontinental level, geography does matter: some physical link paths are genuinely more expensive than other (shorter) paths. It is much easier to string terrestrial cable than undersea cable. However, within a continent physical distance does not always matter as much as might be supposed. Furthermore, a large geographically spread-out provider can always divide up its address blocks by region, allowing internal geographical routing to the correct region. Here is a diagram of IP address allocation as of 2006: http://xkcd.com/195. 328 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 14.6 Epilog CIDR was a deceptively simple idea. At Ô¨Årst glance it is a straightforward extension of the subnet concept, moving the net/host division point to the left as well as to the right. But it has ushered in true hierarchical routing, most often provider-based. While CIDR was originally offered as a solution to some early crises in IPv4 address-space allocation, it has been adopted into the core of IPv6 routing as well. 14.7 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.11 Solutions for Large-Scale IP Routing. 1.0.‚ô¢Consider the following IPv4 forwarding table that uses CIDR. destination next_hop 200.0.0.0/8 A 200.64.0.0/10 B 200.64.0.0/12 C 200.64.0.0/16 D For each of the following IP addresses, indicate to what destination it is forwarded. 64 is 0x40, or 0100 0000 in binary. (i) 200.63.1.1 (ii) 200.80.1.1 (iii) 200.72.1.1 (iv) 200.64.1.1 2.0. Consider the following IPv4 forwarding table that uses CIDR. IP address bytes are in hexadecimal here, so each hex digit corresponds to four address bits. This makes preÔ¨Åxes such as /12 and /20 align with hex-digit boundaries. As a reminder of the hexadecimal numbering, ‚Äú:‚Äù is used as the separator rather than ‚Äú.‚Äù destination next_hop 81:30:0:0/12 A 81:3c:0:0/16 B 81:3c:50:0/20 C 81:40:0:0/12 D 81:44:0:0/14 E real hex IPv4 addresses The hexadecimal format used here for IPv4 addresses is not allowed in the real world, but it turns out there isa valid hex format with reasonably common support, namely 0xac.0xd9.0x04.0xce. This was 14.6 Epilog 329
An Introduction to Computer Networks, Release 2.0.11 never standardized beyond being something BSD Unix did, but as of 2021 it is recognized (with http:/ /in front of it) by the Chrome and Firefox browsers. For more examples of archaic IPv4 address formats, see this blog post. For each of the following IP addresses, give the next_hop for each entry in the table above that it matches. If there are multiple matches, use the longest-match rule to identify where the packet would be forwarded. (i) 81:3b:15:49 (ii) 81:3c:56:14 (iii) 81:3c:85:2e (iv) 81:4a:35:29 (v) 81:47:21:97 (vi) 81:43:01:c0 3.0. Consider the following IPv4 forwarding table, using CIDR. As in exercise 1, IP address bytes are in hexadecimal, and ‚Äú:‚Äù is used as the separator as a reminder. destination next_hop 00:0:0:0/2 A 40:0:0:0/2 B 80:0:0:0/2 C c0:0:0:0/2 D (a). To what next_hop would each of the following be routed? 63:b1:82:15, 9e:00:15:01, de:ad:be:ef (b). Explain why every IP address is routed somewhere, even though there is no default entry. Hint: convert the Ô¨Årst bytes to binary. 4.0. Give an IPv4 forwarding table ‚Äì using CIDR ‚Äì that will route all Class A addresses (Ô¨Årst bit 0) to next_hop A, all Class B addresses (Ô¨Årst two bits 10) to next_hop B, and all Class C addresses (Ô¨Årst three bits 110) to next_hop C. 5.0. Suppose an IPv4 router using CIDR has the following entries. Address bytes are in decimal except for the third byte, which is in binary. destination next_hop 37.149. 0000 0000.0/18 A 37.149. 0100 0000.0/18 A 37.149. 1000 0000.0/18 A 37.149. 1100 0000.0/18 B If the next_hop for the last entry were also A, we could consolidate these four into a single entry 37.149.0.0/ 16√ëA. But with the Ô¨Ånal next_hop as B, how could these four be consolidated into twoentries? You will need to assume the longest-match rule. 330 14 Large-Scale IP Routing
An Introduction to Computer Networks, Release 2.0.11 6.0. Suppose P, Q and R are ISPs with respective CIDR address blocks (with bytes in decimal) 51.0.0.0/8, 52.0.0.0/8 and 53.0.0.0/8. P then has customers A and B, to which it assigns address blocks as follows: A: 51.10.0.0/16 B: 51.23.0.0/16 Q has customers C and D and assigns them address blocks as follows: C: 52.14.0.0/16 D: 52.15.0.0/16 (a).‚ô¢Give forwarding tables for P, Q and R assuming they connect to each other and to each of their own customers. (b). Now suppose A switches from provider P to provider Q, and takes its address block with it. Give the changes to the forwarding tables for P, Q and R. Do not assume P is willing to forward trafÔ¨Åc from R destined for its ex-customer A. the longest-match rule may be needed to resolve conÔ¨Çicts. 7.0. Let P, Q and R be the ISPs of exercise 6.0. This time, suppose customer C switches from provider Q to provider R. R will now have a new entry 52.14.0.0/16 √ëC. Give the changes to the forwarding tables of P and Q. Again, do not assume provider Q will forward trafÔ¨Åc from P destined for C. 8.0. Suppose P, Q and R are ISPs as in exercise 6.0. This time, P and R do not connect directly; they route trafÔ¨Åc to one another via Q. In addition, customer B is multihomed and has a secondary connection to provider R; customer D is also multihomed and has a secondary connection to provider P. R and P use these secondary connections to send to B and D respectively; however, these secondary connections are used only within R and P respectively. Give forwarding tables for P, Q and R. 9.0. Suppose that Internet routing in the US used geographical routing, and the Ô¨Årst 12 bits of every IP address represent a geographical area similar in size to a telephone area code. Megacorp gets the preÔ¨Åx 12.34.0.0/16, based geographically in Chicago, and allocates subnets from this preÔ¨Åx to its ofÔ¨Åces in all 50 states. Megacorp routes all its internal trafÔ¨Åc over its own network. (a). Assuming all Megacorp trafÔ¨Åc must enter and exit in Chicago, what is the route of trafÔ¨Åc to and from the San Diego ofÔ¨Åce to a client also in San Diego? (b). Now suppose each ofÔ¨Åce has its own link to a local ISP for outbound trafÔ¨Åc, but still uses its 12.34.0.0/16 IP addresses. Now what is the route of trafÔ¨Åc between the San Diego ofÔ¨Åce and its neighbor? (c). Suppose Megacorp gives up and gets a separate geographical preÔ¨Åx for each ofÔ¨Åce, eg12.35.1.0/24 for San Diego and 12.37.3.0/24 for Boston. TrafÔ¨Åc into and out of Megacorp will now take geographically reasonable paths. However, Megacorp now wants to be sure that interofÔ¨Åce trafÔ¨Åc stays on its internal network. How must Megacorp conÔ¨Ågure its internal IP forwarding tables to ensure this? 14.7 Exercises 331
An Introduction to Computer Networks, Release 2.0.11 332 14 Large-Scale IP Routing
15 BORDER GATEWAY PROTOCOL (BGP) The Border Gateway Protocol, BGP, is themechanism by which neighboring (that is, directly connected) ISPs share routing information. In13 Routing-Update Algorithms, we considered interior routing-update protocols: those in which all the routers involved are under common management. That management can then dictate the routing-update protocol to be used, and also the rules for assigning per-link costs. For both Distance-Vector and Link State methods, the per-link cost played an essential role: by trying to minimize the cost, we were assured that no routing loops would be present in a stable network ( 13.3 Observations on Minimizing Route Cost ). But now consider the problem of exterior routing; that is, of choosing among routes that have been offered by independent neighboring organizations. The routing-update protocol has to be universal, so any pair of neighboring organizations will be able to communicate. And link costs are difÔ¨Åcult to use, because there are multiple incompatible ways of measuring link cost. Exterior routing is the role for BGP. As a Ô¨Årst BGP example, in the diagram below suppose that A, B, C and D are each managed independently; it may be useful to think of A, B and C as three ISPs and D as some destination. AB CD Organization (or ISP) A has two routes to destination D ‚Äì one via B and one via C ‚Äì and must choose between them. If A wanted to use one of the interior routing-update protocols to choose its path to D, it would face several purely technical problems. First, what if B uses distance-vector while C speaks only in link-state LSP messages? Second, what if B measures its path costs using the hopcount metric, while C assigns costs based on bandwidth, or congestion, or pecuniary considerations? The mixing of unrelated metrics isn‚Äôt necessarily useless: all that is required for the shortest-path-is-loopfree result mentioned above is that the two ends of each link agree on the cost assigned to that link. But apples-and-oranges comparison of different metrics would completely undermine the intended use of those metrics to inÔ¨Çuence the selection of which links should carry the most trafÔ¨Åc. Sharing link-cost information without a common administrative policy to set those costs does not, in practical terms, make sense. But A also faces a larger issue: to reach D it must rely on having its trafÔ¨Åc carried by an outsider ‚Äì either B or C. Outsiders are likely not inclined to offer this service without some form of compensation, either monetary or through reciprocal exchange. If A reaches an understanding with B on this matter of trafÔ¨Åc carriage, then A does not want its trafÔ¨Åc routed via C even if that latter route is of lower technical cost. If A is paying B, it is going to expect to use B. If A is not paying C, C is going to expect that A not use C. 333
An Introduction to Computer Networks, Release 2.0.11 TheBorder Gateway Protocol, or BGP, is assigned the job of handling exterior routing; that is, of handling exchange of routing information between neighboring independent organizations. The current version is BGP-4, documented in RFC 4271. BGP‚Äôs primary goal is to provide support for what are sometimes called routing policies; that is, for choosing routes based on managerial oradministrative input. We address this below in 15.4 BGP Filtering and Routing Policies. (Routing policies have nothing to do with the policy-based routing described in 13.6 Routing on Other Attributes, in which different packets with the same destination address may be routed differently because a site has a ‚Äúpolicy‚Äù to take packet attributes other than destination into account. With BGP, once a site‚Äôs policies to choose a route to a given destination are applied, all trafÔ¨Åc to that destination takes that single route.) Ultimately, the administrative input used by BGP very likely relates to who is paying what for the trafÔ¨Åc carried. It is also possible, though less common, to use BGP to implement other preferences, such as for domestic trafÔ¨Åc to remain within national boundaries. The BGP term for a routing domain under coordinated administration, and using one consistent interior protocol and link-cost metric throughout, is Autonomous System, or AS. That said, all that is strictly required is that all BGP routers within an AS have the same consistent view of routing, and in fact some Autonomous Systems do run multiple routing protocols and may even use different metrics at different points. As indicated above, BGP does notsupport the exchange of link-cost information between Autonomous Systems. Autonomous Systems are identiÔ¨Åed by a globally unique AS number, originally 16 bits but now extended to 32 bits. A site needs to run BGP (and so needs to have an AS number) if it connects to (or might connect to) more than one other AS; sites that connect only to a single ISP do not need BGP. Every site running BGP will have one or more BGP speakers: the devices that run BGP. If there is more than one, they must remain coordinated with one another so as to present a consistent view of the site‚Äôs connections and advertisements; this coordination process is sometimes called internal BGP to distinguish it from the communication with neighboring Autonomous Systems. The latter process is then known as external BGP. The BGP speakers of a site are often not the busy border routers that connect directly to the neighboring AS, though they are usually located near them and are often on the same subnet. Each interconnection point with a neighboring AS generally needs its own BGP speaker. Connections between BGP speakers of neighboring Autonomous Systems ‚Äì sometimes called BGP peers ‚Äì are generally conÔ¨Ågured administratively; they are not subject to a ‚Äúneighbor discovery‚Äù process like that used by most interior routers. The BGP speakers must maintain a database of all routes received, not just of the routes actually used. However, the speakers forward to their neighbors only routes they (and thus their AS) actually use themselves; this is a Ô¨Årm BGP rule. Many BGP implementations support Equal-Cost Multi-Path routing ( 13.7 ECMP ), by which two (or more) links to the same neighbor may be treated as one. The Internet Draft draft-lapukhov-bgp-ecmpconsiderations-01 addresses this further. 15.1 AS-paths At its most basic level, BGP involves the exchange of lists of reachable destinations, like distance-vector routing without the distance information. But that strategy, alone, cannot detect routing loops. BGP solves the loop problem by having routers exchange, not just destination information, but also the entire path used 334 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 to reach each destination. Paths including each router would be cumbersome; instead, BGP abbreviates the path to the list of ASes traversed. This is called the AS-path. This allows routers to make sure their routes do not traverse any AS more than once, and thus do not have loops. As an example of this, consider the network below, in which we consider Autonomous Systems also to be destinations. Initially, we will assume that each AS discovers its immediate neighbors. AS3 and AS5 will then each advertise to AS4 their routes to AS2, but AS4 will have no reason at this level to prefer one route to the other (BGP does use the shortest AS-path as part of its tie-breaking rule, but, before falling back on that rule, AS4 is likely to have a commercial preference for which of AS3 and AS5 it uses to reach AS2). AS1 AS2 AS3 AS5 AS4 Also, AS2 will advertise to AS3 its route to reach AS1; that advertisement will contain the AS-path xAS2,AS1y. Similarly, AS3 will advertise this route to AS4 and then AS4 will advertise it to AS5. When AS5 in turn advertises this AS1-route to AS2, it has the potential to create a loop. It does not, however, because it will include the entire AS-path xAS5,AS4,AS3,AS2,AS1 yin the advertisement it sends to AS2. AS2 will know not to use this route because it will see that it is a member of the AS-path. Thus, BGP is spared the kind of slow-convergence problem that traditional distance-vector approaches were subject to. It is theoretically possible that the shortest path (in the sense, say, of the hopcount metric) from one host to another traverses some AS twice. If so, BGP will not allow this route. AS-paths potentially add considerably to the size of the AS database. The number of paths a site must keep track of is proportional to the number of ASes, because there will be one AS-path to each destination AS. (Actually, an AS may have to record many times that many AS-paths, as an AS may hear of AS-paths that it elects not to use.) As of 2019 there were about 80 thousand ASes in the world. Let A be the number of ASes. Typically the average length of an AS-path is about log(A), although this depends on connectivity; in 2019 this average length was about six. The amount of memory required by BGP is CAlog(A) + KN, where C and K are constants. The other major goal of BGP is to allow administrative input to what, for interior routing, is largely a technical calculation (though an interior-routing administrator can set link costs). BGP is the interface between ISPs (and between ISPs and their larger customers), and can be used to implement contractual agreements made regarding which ISPs will carry other ISPs‚Äô trafÔ¨Åc. If ISP2 tells ISP1 it has a route to destination D, but ISP1 chooses not to send trafÔ¨Åc to ISP2, BGP can be used to implement this. Perhaps more likely, if ISP2 has a route to D but does not want ISP1 to use it until they pay for the privilege, BGP can be used to implement this as well. Despite the exchange of AS-path information, temporary routing loops may still exist. This is because BGP may Ô¨Årst decide to use a route and only then export the new AS-path; the AS on the other side may realize there is a problem as soon as the AS-path is received but by then the loop will have at least brieÔ¨Çy been in existence. See the Ô¨Årst example below in 15.11 Examples of BGP Instability. 15.1 AS-paths 335
An Introduction to Computer Networks, Release 2.0.11 BGP‚Äôs predecessor was EGP, which guaranteed loop-free routes by allowing only a single route to any AS, thus forcing the Internet into a tree topology, at least at the level of Autonomous Systems. The AS graph could contain no cycles or alternative routes, and hence there could be no redundancy provided by alternative paths. EGP also thus avoided having to make decisions as to the preferred path; there was never more than one choice. EGP was sometimes described as a reachability protocol; its only concern was whether a given network was reachable. 15.2 AS-Paths and Route Aggregation There is some conÔ¨Çict between the goal of reporting precise AS-paths to each destination, and of consolidating as many address preÔ¨Åxes as possible into a single preÔ¨Åx (single CIDR block). Consider the following network: AS1 AS2 AS3 AS4 Suppose AS2 has paths path=xAS2y, destination 200.0.0/23 path=xAS2,AS3y, destination 200.0.2/24 path=xAS2,AS4y, destination 200.0.3/24 If AS2 wants to optimize address-block aggregation using CIDR, it may prefer to aggregate the three destinations into the single block 200.0.0/22. In this case there would be two options for how AS2 reports its routes to AS1: 
- Option 1: report 200.0.0/22 with path xAS2y. But this ignores the ASes AS3 and AS4! These are legitimately part of the AS-paths to some of the destinations within the block 200.0.0/22; loop detection could conceivably now fail. 
- Option 2: report 200.0.0/22 with path xAS2,AS3,AS4 y, which is not a real path but which does include all the ASes involved. This ensures that the loop-detection algorithm works, but artiÔ¨Åcially inÔ¨Çates the length of the AS-path, which is used for certain tie-breaking decisions. As neither of these options is ideal, the concept of the AS-set was introduced. A list of Autonomous Systems traversed in order now becomes an AS-sequence. In the example above, AS2 can thus report net 200.0.0/22 with 
- AS-sequence= xAS2y 
- AS-set={AS3,AS4} AS2 thus both achieves the desired aggregation and also accurately reports the AS-path length. The AS-path can in general be an arbitrary list of AS-sequence and AS-set parts, but in cases of simple aggregation such as the example here, there will be one AS-sequence followed by one AS-set. 336 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 RFC 6472 now recommends against using AS-sets entirely, and recommends that aggregation as above be avoided. One consequence of this recommendation is that every IP-address preÔ¨Åx announced by any public Autonomous System will result in a corresponding entry in the forwarding tables of the backbone routers. 15.3 Transit TrafÔ¨Åc It is helpful to distinguish between two kinds of trafÔ¨Åc, as seen from a given AS. Local trafÔ¨Åc is trafÔ¨Åc that either originates or terminates at that AS; this is trafÔ¨Åc that ‚Äúbelongs‚Äù to that AS. At leaf sites (that is, sites that connect only to their ISP and not to other sites), all trafÔ¨Åc is local. The other kind of trafÔ¨Åc is transit trafÔ¨Åc; the AS is forwarding it along on behalf of some nonlocal party. For ISPs, most trafÔ¨Åc is transit trafÔ¨Åc. A large almost-leaf site might also carry a small amount of transit trafÔ¨Åc for one particular related (but autonomous!) organization. The decision as to whether to carry transit trafÔ¨Åc is a classic example of an administrative choice, implemented by BGP‚Äôs support for routing policies. Most real-world BGP conÔ¨Åguration issues relate to the carriage (or non-carriage) of transit trafÔ¨Åc. 15.4 BGP Filtering and Routing Policies As stated above, one of the goals of BGP is to support routing policies; that is, routing based on managerial or administrative concerns in addition to technical ones. A BGP speaker may be aware of multiple routes to a destination. To choose the one route that we will use, it may combine a mixture of optimization rules and policy rules. Some examples of policy rules might be: 
- do not use AS13 as we have an adversarial relationship with them 
- do not allow transit trafÔ¨Åc BGP implements policy through Ô¨Åltering rules ‚Äì that is, rules that allow rejection of certain routes ‚Äì at three different stages: 1.Import Ô¨Åltering is applied to the lists of routes a BGP speaker receives from its neighbors. 2.Best-path selection is then applied as that BGP speaker chooses which of the routes accepted by the Ô¨Årst step it will actually use. 3.Export Ô¨Åltering is done to decide what routes from the previous step a BGP speaker will actually advertise. A BGP speaker can only advertise paths it uses, but does not have to advertise every such path. While there are standard default rules for all these (accept everything imported, use simple tie-breakers, export everything), a site will usually implement at least some policy rules through this Ô¨Åltering process ( eg ‚Äúprefer routes through the ISP we have a contract with‚Äù). As an example of import Ô¨Åltering, a site might elect to ignore all routes from a particular neighbor, or to ignore all routes whose AS-path contains a particular AS, or to ignore temporarily all routes from a neighbor that has demonstrated too much recent ‚Äúroute instability‚Äù (that is, rapidly changing routes). Import Ô¨Åltering canalso be done in the best-path-selection stage, by having the best-path-selection process ignore routes from selected neighbors. 15.3 Transit TrafÔ¨Åc 337
An Introduction to Computer Networks, Release 2.0.11 BGP Breakdowns In the real world, it sometimes happens that a small regional ISP is misconÔ¨Ågured to attempt to report to some high-level provider that it can reach, say, every site in the world. Export Ô¨Åltering on the part of the small ISP and best-path selection and import Ô¨Åltering on the part of the large ISP often ‚Äì though not always ‚Äì catches this. Occasionally, such incidents represent malicious BGP hijacking. See 15.12 BGP Security and Route Registries for one approach to addressing the problems of BGP security. The next stage is best-path selection, to pick the preferred routes from among all those just imported. The Ô¨Årst step is to eliminate AS-paths with loops. Even if the neighbors have been diligent in not advertising paths with loops, an AS will still need to reject routes that contain itself in the associated AS-path. The next step in the best-path-selection stage, generally the most important in BGP conÔ¨Åguration, is to assign a local_preference, or weight, to each route received. An AS may have policies that add a certain amount to the local_preference for routes that use a certain AS, etc. Very commonly, larger sites will have preferences based on contractual arrangements with particular neighbors. Provider ASes, for example, will in general prefer routes learned from their customers, as these are ‚Äúcheaper‚Äù. A smaller ISP that connects to two larger ones might be paying to route the majority of its outbound trafÔ¨Åc through a particular one of the two; its local_preference values will then implement this choice. After BGP calculates the local_preference value for every route, the routes with the best local_preference are then selected. Domains are free to choose their local_preference rules however they wish. In principle this can involve rather strange criteria; for example, in 15.11 Examples of BGP Instability we will consider an example where AS1 prefers routes with AS-path xAS3,AS2yto the strictly shorter path xAS2y. That example, however, demonstrates instability; domains are encouraged to set their rules in accordance with some standard principles, below, to avoid this. Local_preference values are communicated internally via the LOCAL_PREF path attribute, below. They are not shared with other Autonomous Systems. In the event of ties ‚Äì two routes to the same destination with the same local_preference ‚Äì a Ô¨Årst tie-breaker rule is to prefer the route with the shorter AS-path. While this superÔ¨Åcially resembles a shortest-path algorithm, the real work should have been done in administratively assigning local_preference values. The shorter-AS-path tie-breaker is perhaps best thought of as similar in spirit to the smaller-AS-number tie-breaker (although the sometimes-signiÔ¨Åcant Multi-Exit-Discriminator tie-breaker, next, comes between them). The Ô¨Ånal signiÔ¨Åcant step of the route-selection phase is to apply the Multi-Exit-Discriminator value, 15.6.3 MULTI_EXIT_DISC. A site may very well choose to ignore this value entirely. Finally we get to the trivial tie-breaker rules, though if a tie-breaker rule assigns signiÔ¨Åcant trafÔ¨Åc to one AS over another then it may have economic consequences and shouldn‚Äôt be considered ‚Äútrivial‚Äù. If this situation is detected, it would probably be addressed in the local-preferences phase. The trivial tie-breakers take into account the internal routing cost, the numeric value of the AS number, and the numeric value of the neighbor‚Äôs IP address. After the best-path-selection stage is complete, the BGP speaker has now selected the routes its own Autonomous System will use. These routes are then communicated to the actual routers, which are often different devices. 338 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 The Ô¨Ånal stage is to decide what rules will be exported to which neighbors. Only routes the BGP speaker has decided it will use ‚Äì that is, routes that have made it to this point ‚Äì can be exported; a site cannot route to destination D through AS1 but export a route claiming D can be reached through AS2. It is at the export-Ô¨Åltering stage that an AS can enforce no-transit rules. If it does not wish to carry transit trafÔ¨Åc to destination D, it will not advertise D to any of its AS-neighbors. The export stage can lead to anomalies. Suppose, for example, that AS1 reaches D and AS5 via AS2, and announces this to AS4. AS4 AS1 AS5DAS2 AS3 Later, we imagine, AS1 switches to reaching D via AS3, but is forbidden by policy to announce to AS4 any routes with AS-path containing AS3; such a policy is straightforward to implement via export Ô¨Åltering. Then AS1 must simply withdraw the announcement to AS4 that it could reach D at all, even though the route to D via AS2 is still there. 15.5 BGP Table Size In principle, there is a one-to-one correspondence between IP address preÔ¨Åxes announced by public Autonomous Systems and entries in the backbone IP forwarding table. (The now-obsolete technique of route aggregation, 15.2 AS-Paths and Route Aggregation, used to create a modest discrepancy here.) The set of all routes received by a BGP speaker, after import Ô¨Åltering, is sometimes called the Routing Information Base, or RIB. The resultant forwarding table created after best-path selection is then the Forwarding Information Base, or FIB, although the full FIB may also contain routes learned via non-BGP protocols. Each FIB entry will also contain the actual next-hop router, versus the next-AS information actually received via BGP. For simplicity, we will refer to the forwarding table generated from BGP records only as the BGP FIB. The size of the IPv4 BGP FIB ‚Äì that is, the number of distinct preÔ¨Åxes in a backbone IPv4 forwarding table ‚Äì is plotted in the chart below, based on data courtesy of bgp.potaroo.net, with some modest smoothing applied. 15.5 BGP Table Size 339
An Introduction to Computer Networks, Release 2.0.11 0100000200000300000400000500000600000700000800000 1988 1991 1994 1997 2000 2003 2006 2009 2012 2015 2018 2021 The time range is from 1994 to July 2019; at the end, there are 788 thousand IP preÔ¨Åxes from (not shown in the graph) around 65 thousand Autonomous Systems. The graph is Ô¨Çat from 2001 to 2002, reÔ¨Çecting the aftereffects of the so-called dot-com bubble. Overall the increase with time is roughly quadratic, but in the last decade has been closer to linear. The graph does not entirely represent growth of the Internet; it also represents fragmentation. In recent years, only smaller address blocks have been available, and so many sites and providers have cobbled together their Internet presence from multiple such blocks, where they might have preferred a single block. 15.6 BGP Path attributes BGP supports the inclusion of various path attributes when exchanging routing information. Attributes exchanged with neighbors can be transitive ornon-transitive; the difference is that if a neighbor AS does not recognize a received path attribute then it should pass it along anyway if it is marked transitive, but not otherwise. Some path attributes are entirely local, that is, internal to the AS of origin. Other Ô¨Çags are used 340 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 to indicate whether recognition of a path attribute is required or optional, and whether recognition can be partial or must be complete. The AS-path itself is perhaps the most fundamental path attribute. Here are a few other common attributes: 15.6.1 NEXT_HOP This mandatory external attribute allows BGP speaker B1 of AS1 to inform its BGP peer B2 of AS2 what actual router to use to reach a given destination. If B1, B2 and AS1‚Äôs actual border router R1 are all on the same subnet, B1 will include R1‚Äôs IP address as its NEXT_HOP attribute. If B1 is noton the same subnet as B2, it may not know R1‚Äôs IP address; in this case it may include its own IP address as the NEXT_HOP attribute. Routers on AS2‚Äôs side will then look up the ‚Äúimmediate next hop‚Äù they would use as the Ô¨Årst step to reach B1, and forward trafÔ¨Åc there. This should either be R1 or should lead to R1, which will then route the trafÔ¨Åc properly ( notnecessarily on to B1). B1 R1 B2Rest of AS1 AS1 AS2 AS border 15.6.2 LOCAL_PREF If one BGP speaker in an AS has been conÔ¨Ågured with local_preference values, used in the best-pathselection phase above, it uses the LOCAL_PREF path attribute to share those preferences with all other BGP speakers at a site. In other words, once one BGP speaker has determined the local_preference value of a given route, the LOCAL_PREF attribute is used to distribute that value uniformly throughout the AS. 15.6.3 MULTI_EXIT_DISC The Multi-Exit Discriminator, or MED, attribute allows one AS to learn something of the internal structure of another AS, should it elect to do so. Using the MED information provided by a neighbor has the potential to cause an AS to incur higher costs, as it may end up carrying trafÔ¨Åc for longer distances internally; MED values received from a neighboring AS are therefore only recognized when there is an explicit administrative decision to do so. SpeciÔ¨Åcally, if an autonomous system AS1 has multiple links to neighbor AS2, then AS1 can, when advertising an internal destination D to AS2, have each of its BGP speakers provide associated MED values so that AS2 can know which link AS1 would prefer that AS2 use to reach D. This allows AS2 to route trafÔ¨Åc 15.6 BGP Path attributes 341
An Introduction to Computer Networks, Release 2.0.11 to D so that it is carried primarily by AS2 rather than by AS1. The alternative is for AS2 to use only the closest gateway to AS1, which means trafÔ¨Åc is likely carried primarily by AS1. MED values are considered late in the best-path-selection process; in this sense the use of MED values is a tie-breaker when two routes have the same local_preference. As an example, consider the following network (from 14.4.3 Hierarchical Routing via Providers, with providers now replaced by Autonomous Systems); the numeric values on links are their relative costs. We will assume that each site has three BGP speakers co-located at the exchange points IX1, IX2 and IX3. R1 R2 R3 S1 S2 S3R1 R2 R3R S BA 1 13 5 7 4 3 8 AS 2AS1 IX1 IX2IX3 1 000 00 In the absence of the MED, AS1 will send trafÔ¨Åc from A to B via the R3‚ÄìIX3‚ÄìS3 link, and AS2 will return the trafÔ¨Åc via S1‚ÄìIX1‚ÄìR1. These are the links that are closest to R and S, respectively, representing AS1 and AS2‚Äôs desire to hand off the outbound trafÔ¨Åc as quickly as possible. However, AS1‚Äôs BGP speakers at IX1, IX2 and IX3 can provide MED values to AS2 when advertising destination A, indicating a preference for AS2 √ëAS1 trafÔ¨Åc to use the rightmost link: 
- IX1: destination A has MED 200 
- IX2: destination A has MED 150 
- IX3: destination A has MED 100 If this is done, and AS2 abides by this information, then AS2 will route trafÔ¨Åc from B to A via IX3; that is, via the exchange point with the lowest MED value. Note the importance of fact that AS2 is allowed to ignore the MED; use of it may shift costs from AS1 to AS2! The relative order of the MED values for R1 and R2 is irrelevant, unless the IX3 exchange becomes disabled, in which case the numeric MED values above would mean that AS2 should then prefer IX2 for reaching A. We cannot use MED values to cause A‚ÄìB trafÔ¨Åc to take the path through IX2; that path has minimal cost only in the global sense, and the only way to achieve global cost minimization is for the two ASes to agree to use a common distance metric and a common metric-based routing algorithm, in effect becoming one AS. While AS1 does provide different numeric MED values for the three exchange points, they are used only in ranking precedence, not as numeric measures of cost (though they are sometimes derived from that). In the example above, importing and using MED values raises AS2‚Äôs costs, by causing it to route AS2to-AS1 trafÔ¨Åc so that it stays for a longer path within AS2‚Äôs network. This is, in fact, almost always the 342 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 case when using MED values. Why, then, would AS2 agree to this? One simple reason might be that AS2 and AS1 have, together, negotiated this arrangement; perhaps AS1 gives AS2 a break on interconnection (‚Äúpeering‚Äù) fees in exchange for AS2‚Äôs accepting and using AS1‚Äôs MED data. It is also possible that AS2‚Äôs use of AS1‚Äôs MED data may improve the quality of service AS2 can offer to its customers; we will return to an example of this in 15.7.1 MED values and trafÔ¨Åc engineering. Also in the example above, the MED values are used to decide between multiple routes to the same destination that all pass through the same AS, namely AS1. Some BGP implementations allow the use of MED values to decide between different routes through different neighbor ASes. The different neighbors must all have the same local_preference values. For example, AS2 might connect to AS3 and AS4 and receive the following BGP information: 
- AS3: destination A has MED 200 
- AS4: destination A has MED 100 Assuming AS2 assigns the same local_preference to AS3 and AS4, it might be conÔ¨Ågured to use these MED values as the tie-breaker, and thus routing trafÔ¨Åc to A via AS3. On Cisco routers, the always-compare-med command is used to create this behavior. MED values are not intended to be used to communicate routing preferences to non-neighboring ASes. Additional information on the use of MED values can be found in RFC 4451. 15.6.4 COMMUNITY This is simply a tag to attach to routes. Routes can have multiple tags corresponding to membership in multiple communities. Some communities are deÔ¨Åned globally; for example, NO_EXPORT and NO_ADVERTISE. A route marked with one of these two communities will not be shared further. Other communities may be relevant only to a particular AS. The importance of communities is that they allow one AS to place some of its routes into speciÔ¨Åc categories when advertising them to another AS; the categories must have been created and recognized by the receiving AS. The receiving AS is not obligated to honor the community memberships, of course, but doing so has the effect of allowing the original AS to ‚ÄúconÔ¨Ågure itself‚Äù without involving the receiving AS in the process. Communities are often used, for example, by (large) customers of an ISP to request speciÔ¨Åc routing treatment. A customer would have to Ô¨Ånd out from the provider what communities the provider deÔ¨Ånes, and what their numeric codes are. At that point the customer can place itself into the provider‚Äôs community at will. Here are some of the community values once supported by a no-longer-extant ISP that we shall call AS1. The full community value would have included AS1‚Äôs AS-number. value action 90 set local_preference used by AS1 to 90 100 set local_preference used by AS1 to 100, the default 105 set local_preference used by AS1 to 105 110 set local_preference used by AS1 to 110 990 the route will not leave AS1‚Äôs domain; equivalent to NO_EXPORT 991 route will only be exported to AS1‚Äôs other customers 15.6 BGP Path attributes 343
An Introduction to Computer Networks, Release 2.0.11 15.7 BGP and TrafÔ¨Åc Engineering BGP is themechanism for inter-autonomous-system trafÔ¨Åc engineering. The Ô¨Årst-line tools are import and export Ô¨Åltering and best-path selection. For autonomous systems with multiple interconnection points, the Multi-Exit Discriminator above also may play a large role. After establishing basic connectivity, perhaps the most important decision a site makes via its BGP conÔ¨Åguration is whether or not it will accept transit trafÔ¨Åc. As a Ô¨Årst example of this, let us consider the case of conÔ¨Åguring a private link, such as the dashed link1 below between ‚Äúfriendly‚Äù but unafÔ¨Åliated sites A and B (link1 can be either a shared ‚Äúreal‚Äù link or a short ‚Äújumper‚Äù link within an Internet exchange point): The InternetISP1 ISP2 BA link1 Suppose A exports its link1 route to B to its provider ISP1. Then ISP1 may in turn announce this route to the Internet at large, and so some or all of B‚Äôs inbound trafÔ¨Åc may be routed through ISP1 (paid by A) and through A itself. Similarly, B may end up paying to carry A‚Äôs trafÔ¨Åc if B exports its link1 route to A to ISP2. Economically, carrying someone else‚Äôs transit trafÔ¨Åc not desirable unless you are compensated for it. The primary issue here is the use of the ISP1‚ÄìA link by B and the ISP2‚ÄìB link by A; use of the shared link1 might be a secondary issue depending on the relative bandwidths and A and B‚Äôs understandings of appropriate uses for link1. Two common options A and B might agree to regarding link1 are no-transit andbackup. For the no-transit option, A and B simply do not export the route to their respective ISPs at all. This is done via export Ô¨Åltering. If ISP1 does not know A can reach B, it will not send any of B‚Äôs trafÔ¨Åc to A. For the backup option, the intent is that trafÔ¨Åc to A will normally arrive via ISP1, but if the ISP1 link is down then A‚Äôs trafÔ¨Åc will be allowed to travel through ISP2 and B. To achieve this, A and B can export their link1route to each other, but arrange for ISP1 and ISP2 respectively to assign this route a low local_preference value. As long as ISP1 hears of a route to B from itsupstream provider, it will reach B that way, and will not advertise the existence of the link1 route to B; ditto ISP2. However, if the ISP2 route to B fails, then A‚Äôs upstream provider will stop advertising any route to B, and so ISP1 will begin to use the link1 route to B and begin advertising it to the Internet. The link1 route will be the primary route to B until ISP2‚Äôs service is restored. A and B must convince their respective ISPs to assign the link1 route a low local_preference; they cannot mandate this directly. However, if their ISPs recognize community attributes that, as above, allow customers to inÔ¨Çuence their local_preference value, then A and B can use this to create the desired local_preference. To use the shared link for backup outbound trafÔ¨Åc, A and B will need a way to send through one another if their own ISP link is down. If A detects that its ISP link is down, it can simply change its default route to point to B. One way to automate this is for A and B to view their default-route path ( egto 0.0.0.0/0) to be a concrete destination within BGP. ISP1 advertises this to A, using BGP, but so does B, and A has conÔ¨Ågured 344 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 its import rules so B‚Äôs route to 0.0.0.0/0 has a higher cost. Then A will route to 0.0.0.0/0 through ISP1 ‚Äì that is, will use ISP1 as its default route ‚Äì as long as it is available, and will switch to B when it is not. A and B might also wish to use their shared private link for load balancing, but for this BGP offers limited help. If ISP1 and ISP2 both export routes to A, then A has lost all control over how other sites will prefer one to the other. A may be able to make one path artiÔ¨Åcially appear more expensive, perhaps by duplicating one of the ISPs in the AS-path. A might then be able to keep tweaking this cost until the inbound loads are comparable, but there is no guarantee (or even likelihood) this will be stable. Outbound load-balancing is up to A and B‚Äôs respective internal routers. Providers in the business of carrying transit trafÔ¨Åc must also make decisions about exactly whose trafÔ¨Åc they will carry; these decisions are again implemented with BGP. In the diagram below, two transit-providing Autonomous Systems B and C connect to individual sites (or regional ISPs) A and D. IXP1 IXP2Transit B Transit CA D In the diagram above, the left and right interconnections are shown taking place at Internet exchange points IXP1 and IXP2 ( 14.4.1 Internet Exchange Points ). IXPs are typically where such interconnections take place but are not required; the essential topology is simply this: A DTransit B Transit C B would like to make sure C does not attempt to save on its long-haul transit costs by forwarding A √ù√ëD trafÔ¨Åc over to B at IXP1, and D √ù√ëA trafÔ¨Åc over to B at IXP2. B avoids this problem by not advertising to C that it can reach A and D, and similarly with C. Transit providers are quite careful about not advertising reachability to any other AS for whom they do not intend to provide transit service, because to do so is likely to mean getting stuck with that trafÔ¨Åc. If B advertises to A that it can reach D, then A may accept that route, and send all its D-bound trafÔ¨Åc via B, with C not involved at all. B is not likely to do this unless A pays for the privilege. If B and C both advertise to A that they can reach D, then A has a choice, which it will make via its best-path-selection rules. But in such a case A will want to be sure that it does not end up paying full price to both B and C to carry its trafÔ¨Åc while using only one of them. Site A might, for example, agree to payment based on the actual volume of carried trafÔ¨Åc, meaning that if it prefers B‚Äôs route then it will pay only B. It is quite possible that B advertises to A that it can reach D, but does not advertise to D that it can reach A. As we have seen, B advertises to A that it can reach D only if A has paid for this privilege; perhaps D prefers to do business with C rather than with B. In that case, A-to-D trafÔ¨Åc would travel via B, while D-to-A trafÔ¨Åc would travel via C. In the unlikely event that B and C both advertise to one another at IXP1 their route to D, a routing loop may 15.7 BGP and TrafÔ¨Åc Engineering 345
An Introduction to Computer Networks, Release 2.0.11 even be created. B might forward D-bound trafÔ¨Åc to C while C forwards it back to B. But in that case B would state, in its next BGP advertisement to C at IXP1, that it reaches D via an AS-path that begins with C, and C would do similarly. B and C would then see themselves in the AS-paths they receive and would stop using these routes. 15.7.1 MED values and trafÔ¨Åc engineering Let us now address why an AS would bother with importing and using MED values, given that doing so will almost always increase the site‚Äôs cost. Consider the following diagram of autonomous systems AS1 and AS2, with link costs shown: R1 R2 S1 S2R1 R S ADC 1 12 12 3 3 AS 2AS1 IX1IX200 00 S3 S44 4 Site DC in the diagram above is a datacenter that wants its user ‚Äì at site A ‚Äì to experience high-performance downloads. Perhaps DC delivers high-performance streaming video, and needs to minimize both congestion and packet losses. In order to achieve this superior quality, it builds a particularly robust network R1‚ÄìR‚ÄìR2, shown above as AS1. A Ô¨Årst step is to have AS1 connect (or peer) directly to customer networks such as AS2, rather than relying on the Internet backbone. Two such interconnection points are shown above, IX1 and IX2. At this point, trafÔ¨Åc from A to DC will take IX1 (on the shortest path from A to AS1), and so will travel most of the way in AS1. This is good, but trafÔ¨Åc from A to DC is probably mostly acknowledgments; these are unlikely to beneÔ¨Åt from the special network. The actual data, sent from DC to A, will take IX2, because that is AS1‚Äôs shortest path to reach AS2. The data will thus travel most of the way in AS2, bypassing AS1‚Äôs high-performance network. This is not what DC wants. However, the picture changes if AS1 agrees to accept MED information from AS2 (and other providers). If AS2 tells AS1 that AS2‚Äôs preferred link for reaching A is via IX1, then trafÔ¨Åc from DC to A will travel through R1 to IX1, and from there onto A. This keeps DC‚Äôs outbound trafÔ¨Åc in the AS1 network as long as possible, instead of handing it off to the other network of lower quality. This iswhat DC wants; this is why DC built the high-performance network. Rather than building its own high-performance network, DC might simply contract with an existing highperformance network. That would make AS1‚Äôs business model the following: 
- peering with as many potential customer networks as possible 346 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 
- importing and using the MED information from those networks 
- advertising to potential customers like DC that their network will give DC‚Äôs users a better experience 15.8 BGP and Anycast In10.1.5 DNS and CDNs we discussed how some CDNs use DNS tricks to arrange for user trafÔ¨Åc to be delivered to the closest edge server. Another CDN option is anycast: using the same IP address for all the edge servers, and arranging for routers to deliver to the closest server. IPv6 routers can be conÔ¨Ågured to have some awareness of anycast delivery, but in IPv4 this must be done more passively, using BGP. To implement the anycast approach, the CDN uses the same IP address block at each of its datacenter locations. Each customer has a server at each CDN datacenter, and each of these servers is assigned the same IP address. It is up to the CDN to make sure that the content made available at each server is identical. At each of its locations, the CDN then announces this address block to its local BGP neighbors. Reachability information for the address block then propagates, via BGP, throughout the Internet. An AS connected to a single CDN datacenter will route the CDN‚Äôs address block to that datacenter. If AS1 hears about the CDN from neighbors AS2 and AS3, then AS1 will apply its usual best-path-selection process to determine whether to route the CDN‚Äôs block via AS2 or AS3. Ultimately, every AS on the Internet will deploy exactly one route to the CDN. Each such route will lead to one of the CDN‚Äôs datacenters, but different ASes may deploy routes to different datacenters. One advantage to the anycast approach, over the DNS approach, is that users who use a geographically distant DNS resolver will not pay a penalty. Another is that the BGP best-path-selection process is likely to produce better routes in general than a process based solely on geographical distance; for example, ASes may choose best paths based on available bandwidth rather than distance. In IP routing, geography is not destiny. It may at Ô¨Årst seem odd to have multiple servers with the same public IP address, given that such conÔ¨Åguration within an organization usually represents a dire error. However, none of the CDN‚Äôs datacenters will use these addresses to talk to one another; the CDN will arrange for the use of other IP addresses for inter-datacenter trafÔ¨Åc. 15.9 BGP for Interior Routing While BGP was designed for connecting autonomous systems under separate management, it is sometimes used as an interior routing protocol, replacing distance-vector ( 13.1 Distance-Vector Routing-Update Algorithm ) and link-state ( 13.5 Link-State Routing-Update Algorithm ) approaches. This use of BGP is typically conÔ¨Åned to large-scale datacenters; see RFC 7938 and [AKSLYZ21]. The typical beneÔ¨Åts of BGP, versus traditional interior routing protocols, lie in its conÔ¨Ågurability, its administrative control of best-path selection and route advertisement, and the ability to attach community attributes to routes. As an example ‚Äì taken from [AKSLYZ21] and here ‚Äì we return to the example of 3.4.3.1 Interconnection Fabric. The bottom row here consists of so-called top-of-rack switches, and the upper row consists of an intermediate switch layer. We are interested here in routing trafÔ¨Åc between the top (S1 and S2) and bottom (S3‚ÄìS5) rows. The example here corresponds to what Facebook calls a ‚Äúserver pod‚Äù, though the discussion applies to their similar ‚Äúspine planes‚Äù as well, which form the next layer up. 15.8 BGP and Anycast 347
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S5 S4 S3 One goal here is to ensure that, if the S1‚ÄìS3 link fails, data is routed over the two paths S1‚ÄìS4-S2‚ÄìS3 and S1‚ÄìS5‚ÄìS2‚ÄìS3. To this end, we begin by making each of S1 through S5 its own AS. Later, the set of S1 through S5 will be treated as a single AS, using AS confederation (RFC 5065 ). Because S1 through S5 do not export their individual AS numbers outside the confederation, the same AS numbers can be reused over and over. S3 starts out by announcing to both S1 and S2 the address preÔ¨Åx used by its server rack. S3 marks this announcement with a BGP community ‚Äúrack_preÔ¨Åx‚Äù. S1 and S2 aggregate the rack preÔ¨Åxes received as they advertise upwards, but they also advertise the individual rack preÔ¨Åxes back downwards, now tagged with a different BGP community ‚Äúbackup_path‚Äù. S4 and S5 receive these latter advertisements, and advertise them back upwards to S1 (and S2). At this point, S1 has two routes to S3 tagged with ‚Äúbackup_path‚Äù: S1‚ÄìS4‚ÄìS2‚ÄìS3 and S1‚ÄìS5‚ÄìS2‚ÄìS3. Longer paths ( egS1‚ÄìS4‚ÄìS2‚ÄìS5‚ÄìS1‚ÄìS3) don‚Äôt get this tag. Now, when the S1‚ÄìS3 link fails, S1 (and S3) can look for alternative routes tagged with ‚Äúbackup_path‚Äù. These are easily located, and can then be bonded together using ECMP ( 13.7 ECMP ). This allows a quick transition to the backup until the S1‚ÄìS3 link is up again. All this could also be achieved by adapting a more conventional interior routing protocol, but BGP worked directly out of the box. 15.10 BGP Relationships Arbitrarily complex policies may be created through BGP, and, as we shall see in the following section, convergence to a stable set of routes is not guaranteed. Nonconvergence does not mean distance-vector‚Äôs ‚Äúslow convergence to inÔ¨Ånity‚Äù, but rather a regular oscillation of routes among competing alternatives. It turns out, however, that if some constraints are applied to the different AS-to-AS relationships, then better behavior is obtained. The paper [LG01] analyzed BGP networks in which each AS-to-AS relationship Ô¨Åt one of the following three business patterns, discussed further below: 1. Customer to provider (the most common pattern) 2. Peer to peer ( egtwo top-level providers mutually exchanging trafÔ¨Åc) 3. Sibling to sibling (for very close AS-to-AS relationships) A major consequence these relationships is the extent to which the autonomous systems involved accept one 348 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 another‚Äôs ‚Äúnon-customer‚Äù routes (below), and hence the extent to which they provide each other with transit services. We start with the most basic case, that of customer and provider. If autonomous systems C and P have a customer-to-provider relationship, with C as the customer and P as the provider, then C is paying P to carry some or all of its trafÔ¨Åc to the ‚Äúoutside world‚Äù. P may not carry all such trafÔ¨Åc, because C may also be a customer of another provider P1. C may also have its own sub-customers, such as C1: C CP P In offering itself as a provider, P will export all the routes it has, from all sources, to C, in effect telling C ‚Äúthis is what I can reach‚Äù. If C has no other providers it might accept these routes in the form of a single default-route entry pointing to P; if C has another provider P1then it might accept some routes from P and some from P1. Similarly, C will always export its own routes to P. If C has customers of its own, such as C1, then it will also export those routes to P. Collectively, we will say that C‚Äôs own routes and the routes of its own customers and sub-customers are its customer routes. But what about non-customer routes, egroutes learned from other providers? These C generally does not export. If C were to export to P a route to destination D that it learned from second provider P1, then C might end up providing transport service to P, carrying P‚Äôs D-bound trafÔ¨Åc to P1. As a customer, this is probably not what C intends. To summarize, a provider does export its non-customer routes to its customer, but a customer generally does not export its non-customer routes to its providers. This rule is not, in the world of real business relationships, absolute; ASes may negotiate all sorts of special arrangements. A nominal customer might, for example, agree to provide transit service for some set of destinations, in exchange for a lower-priced rate for the handling of its other trafÔ¨Åc. Nonetheless, the rule is largely accurate, and provides a helpful starting point to understanding customer-provider relationships. Below, in 15.10.1 BGP No-Valley Theorem, we will in effect use this rule as a deÔ¨Ånition of customer-provider relationships. Now let us consider a peer-to-peer relationship, which is a connection between two top-level transit providers that have agreed to exchange all their customer trafÔ¨Åc with each other; thus carrying transit trafÔ¨Åc for one another. Often the idea is for the interconnection to be seen as equally valuable by both parties ( eg because the parties exchange comparable volumes of trafÔ¨Åc); in such a case the relationship would likely besettlement-free, that is, involving no monetary exchange. If, however, the volume Ô¨Çow is signiÔ¨Åcantly asymmetric then compensation can certainly be negotiated, making the relationship more like customer-toprovider. As with customers and providers, two peers P1 and P2 each export all their customer routes to the other; that way, P2 knows it how to reach P1‚Äôs customers and vice-versa. By doing this, P1 and P2 each carry transit trafÔ¨Åc for their own customers. Peers do not, however, generally export their non-customer routes, in either direction. If P1 learns of a route 15.10 BGP Relationships 349
An Introduction to Computer Networks, Release 2.0.11 to destination D from another peer (or provider) P3, it does not export this to P2. If it were to do so, then P1 would carry non-customer transit trafÔ¨Åc from P2 to P3. Instead, P2 is expected also to peer with P3, and learn of P3‚Äôs route to D that way. Alternatively, P3 can become a customer of P1, and thus pay for P1‚Äôs transit carriage of P3‚Äôs trafÔ¨Åc. Peers are often known as tier-1 providers; these represent the top-level ‚Äúbackbone‚Äù providers. Each tier-1 AS must, as a rule, peer with every other tier-1 AS, though ASes are free to negotiate exceptions. The term ‚Äútier-1‚Äù is supposed to suggest strongly that connection agreements with the other tier-1 providers are settlement-free, though such agreements are typically private and so it is hard to say. See also 12.7.1 IPv6to-IPv6 Connectivity. Finally, some autonomous system relationships that do not Ô¨Åt the customer-to-provider or peer-to-peer patterns can be characterized as sibling-to-sibling. Siblings are ISPs that have a close relationship; often siblings are ASes that, due to mergers, are now part of the same organization. Siblings may also be nominal competitors who intend to use their mutual link as a cooperative backup, as in 15.7 BGP and TrafÔ¨Åc Engineering. Two siblings may or may not have the same upstream ISP as provider. Siblings typically export everything to one another ‚Äì both customer and non-customer routes ‚Äì and thus do potentially use their connection for transit trafÔ¨Åc in both directions (although they may rank routes through one another at low preference, so as to use the shared link only when nothing else is available). We can summarize the three kinds of relationships in terms of how they export non-customer routes: 
- in peer-to-peer relationships, non-customer routes are not exported in either direction. 
- in customer-to-provider relationships, non-customer routes are exported only from the provider to the customer. 
- in sibling-to-sibling relationships, non-customer routes are exported in both directions. It is possible to make at least some inferences about BGP relationships from sites‚Äô actual export information, though accuracy is imperfect because sites may negotiate non-standard arrangements; see [LG01]. In the real world, BGP sibling relationships are relatively rare, probably because they do not really Ô¨Åt the model of trafÔ¨Åc carriage as a service. This may be fortunate, as sibling relationships, with universal and bidirectional route export, tend to introduce the greatest complexity. The non-convergence examples of 15.11 Examples of BGP Instability all require sibling relationships. One problematic sibling case is the following, in which P1 and P2 are providers for C1 and C2, respectively, and C1 and C2 are siblings: C1C2P2 P1 C1siblings Suppose P1 exports to C1 a route to destination D. C1 then exports it to sibling C2. If C2 treats this as a customer route, it will export it to P2, in which case C1 and C2 are now providing transit service to trafÔ¨Åc from P2 bound for D. Sibling relationships can be tamed considerably, however, if we adopt a requirement that collections of linked siblings act as a unit, keeping track of the original non-sibling source (that is, customer, provider or 350 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 peer) of each route. Let us say that autonomous systems S and S1are in the same sibling family if there is a chain of autonomous systems S 0.. . S nso that S=S 0, Sn=S1, and each consecutive S i-1and S i, i¬§n, are siblings. We can then deÔ¨Åne the following property: Selective Export Property: A sibling family satisÔ¨Åes this property if, whenever one member of the family learns of a route from a provider (respectively peer or customer) then all other members of the family treat the route as a provider (respectively peer or customer) route when deciding whether to export. In other words, in the situation diagrammed above, in which C1 has learned of a route to D from its provider P1, C2 will also treat this route as a non-customer route and will not export it to P2. In the real world, BGP relationships may not Ô¨Åt any of the above three categories, or else there may be many sibling relationships for which the selective-export property fails. However, quite often these relationships do hold to a useful degree. We can also specialize the relationships to a particular set of destinations, or even to an individual destination; for example, autonomous systems C and P might be said to have a customer-to-provider relationship for destination D if C learned its route to D from a non-customer, does not export this route to P, and P does export to C its own route to D. BGP certainly allows for complicated variations: if a regional provider is a customer of a large transit backbone, then the backbone might only announce routes listed in transit agreement (rather than all routes, as above). There is a supposition here that the regional provider has multiple connections, and has contracted with that particular transit backbone only for certain routes. But we can Ô¨Åt this into the classiÔ¨Åcation above either by restricting attention to the set of routes listed in the agreement, or by declaring that in principle the transit provider exports all routes, but the regional customer doesn‚Äôt import the ones it hasn‚Äôt paid for. 15.10.1 BGP No-Valley Theorem A consequence of adherence to the above classiÔ¨Åcation and attendant export rules is the no-valley theorem of [LG01]: Suppose every pair of adjacent ASes has a relationship described by the customer-provider, peerto-peer or sibling rules above (now taken to be deÔ¨Ånitions of these three relationships). In addition, every sibling family abides by the selective-export property. Let A=A 0be an autonomous system that has received a route to destination D with AS-path xA1,A2,.. . ,A ny.Then: in this AS-path, there is at most one peer-topeer link. Links to the left of the peer-to-peer link (that is, closer to A) are either customer √ëprovider links or sibling√ësibling links; that is, they are non-downwards. To the right of the peer-to-peer link, there are only provider√ëcustomer or sibling √ësibling links; that is, these are non-upwards. If there is no peer-to-peer link, then we can still divide the AS-path into a non-downwards Ô¨Årst part and a non-upwards second part. Intuitively, autonomous systems on the right (non-upwards) part of the path export the route to D as a customer route. Autonomous systems on the left (non-downwards) part of the path export the route from provider to customer. The no-valley theorem can be seen as an illustration of the power of the restrictions built into the customerto-provider and peer-to-peer export rules. We give an informal argument for the case in which the AS-path has no peer-to-peer link. First, note that BGP rules mean that each autonomous system AS iin the path has received the route to D from neighbor ASi+1with AS-pathxAi+1,.. . ,A ny. 15.10 BGP Relationships 351
An Introduction to Computer Networks, Release 2.0.11 If the no-valley theorem were to fail, then somewhere along the AS-path in order of increasing i we would have a downward link followed by, eventually, an upward link. Choose the largest i for which this arrangement appears, and let k be the position of the Ô¨Årst subsequent upward link, so that 
- A ito A i+1is provider-to-customer 
- A jto A j+1is sibling-to-sibling for i<j<k-1 
- A k-1to A kis customer-to-provider. Then the route to D was acquired by A k-1from its provider A k, and so is a provider route. The set {Ai+1,.. . ,A k-1} is a sibling family, and so by the selective-export rule A i+1also treats this route to D as a provider route. It therefore cannot export this non-customer route to different provider A i, a contradiction. For the case with a peer-to-peer edge, see exercise 5.0. If the hypotheses of the no-valley theorem hold only for routes involving a particular destination or set of destinations, then the theorem is still true for those routes. The hypotheses of the no-valley theorem are not quite sufÔ¨Åcient to guarantee convergence of the BGP system to a stable set of routes. To ensure convergence in the case without sibling relationships, it is shown in [GR01] that the following simple local_preference rule sufÔ¨Åces: If AS1 gets two routes r1 and r2 to a destination D, and the Ô¨Årst AS of the r1 route is a customer of AS1, and the Ô¨Årst AS of r2 is not, then r1 will be assigned a higher local_preference value than r2. More complex rules exist that allow for cases when the local_preference values can be equal; one such rule states that strict inequality is only required when r2 is a provider route. Other straightforward rules handle the case of sibling relationships, egby requiring that siblings have local_preference rules consistent with the use of their shared connection only for backup. As a practical matter, whether or not actual BGP relationships are consistent with the rules above, arrangements resulting in actual BGP instability appear rare on the Internet. 15.11 Examples of BGP Instability What if the ‚Äúnormal‚Äù rules regarding BGP preferences are notfollowed? It turns out that BGP allows genuinely unstable situations to occur; this is a consequence of allowing each AS a completely independent hand in selecting preference functions. Here are two simple examples, from [GR01]. Example 1: A stable state exists, but convergence to it is not guaranteed. Consider the following network arrangement: AS0 D AS2AS1 352 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 We assume AS1 prefers AS-paths to destination D in the following order: xAS2,AS0y,xAS0y That is,xAS2,AS0yis preferred to the direct path xAS0y(one way to express this preference might be ‚Äúprefer routes for which the AS-PATH begins with AS2‚Äù; perhaps the AS1‚ÄìAS0 link is more expensive). Similarly, we assume AS2 prefers paths to D in the order xAS1,AS0y,xAS0y. Both AS1 and AS2 start out using pathxAS0y; they advertise this to each other. As each receives the other‚Äôs advertisement, they apply their preference order and therefore each switches to routing D‚Äôs trafÔ¨Åc to the other; that is, AS1 switches to the route with AS-path xAS2,AS0yand AS2 switches to xAS1,AS0y. This, of course, causes a routing loop! However, as soon as they export these paths to one another, they will detect the loop in the AS-path and reject the new route, and so both will switch back to xAS0yas soon as they announce to each other the change in what they use. This oscillation may continue indeÔ¨Ånitely, as long as both AS1 and AS2 switch away from xAS0yat the same moment. If, however, AS1 switches to xAS2,AS0ywhile AS2 continues to use xAS0y, then AS2 is ‚Äústuck‚Äù and the situation is stable. In practice, therefore, eventual convergence to a stable state is likely. AS1 and AS2 might choose not to export their D-route to each other to avoid this instability. Because they do export this route to one another, they are siblings in the sense of the previous section. Example 2: No stable state exists. This example is from [VGE00]. Assume that the destination D is attached to AS0, and that AS0 in turn connects to AS1, AS2 and AS3 as in the following diagram: DAS1 AS0 AS3 AS2 AS1-AS3 each have a direct route to AS0, but we assume each prefers the AS-path that takes their clockwise neighbor; that is, AS1 prefers xAS3,AS0ytoxAS0y; AS3 prefers xAS2,AS0ytoxAS0y, and AS2 prefers xAS1,AS0ytoxAS0y. This is a peculiar, but legal, example of input Ô¨Åltering. Suppose all initially adopt AS-path xAS0y, and advertise this, and AS1 is the Ô¨Årst to look at the incoming advertisements. AS1 switches to the route xAS3,AS0y, and announces this to AS2 and AS3. At this point, AS2 sees that AS1 uses xAS3,AS0y; if AS2 switches to AS1 then its path would be xAS1,AS3,AS0 yrather thanxAS1,AS0yand so it does not make the switch. But AS3 does switch: it prefers xAS2,AS0yand this is still available. Once it makes this switch, and advertises it, AS1 sees that the route it had been using, xAS3,AS0y, has becomexAS3,AS1,AS0 y. At this point AS1 switches back to xAS0y. Now AS2 can switch to using xAS1,AS0y, and does so. After that, AS3 Ô¨Ånds it is now using xAS2,AS1,AS0 y and it switches back to xAS0y. This allows AS1 to switch to the longer route, and then AS2 switches back to the direct route, and then AS3 gets the longer route, then AS2 again, etc, forever rotating clockwise. 15.11 Examples of BGP Instability 353
An Introduction to Computer Networks, Release 2.0.11 Because each of AS1, AS2 and AS3 export their route to D to both their neighbors, they must all be siblings of one another. 15.12 BGP Security and Route Registries As indicated in the sidebar BGP Breakdowns in15.4 BGP Filtering and Routing Policies, it is quite easy for a site to advertise a route to a destination that it in fact cannot reach; BGP neighbors who accept such a route will Ô¨Ånd that destination unreachable. BGP is traditionally based on trust, and security is difÔ¨Åcult. One strategy for preventing false BGP claims is to crosscheck destinations advertised by BGP neighbors with public databases of information about autonomous systems and about assignments of IP address blocks. Such databases are managed by multiple Internet registries, including the top-level regional registries ARIN, RIPE, APNIC, AfriNIC and LACNIC mentioned in 1.10.2 The Future of IPv4 and many more subregistries. These databases are known as Internet Routing Registries, or IRRs. It is common for some IRRs to mirror the data held by others. The term ‚ÄúIRR‚Äù, in the singular, is sometimes used to denote the collective union of all the individual IRRs. Any BGP route report indicates, by virtue of the AS-path, the origin AS for the IP destination address block. The most basic form of BGP security is origin validation: ensuring that the origin AS appearing in the AS-path is in fact entitled to be the origin AS for that IP address block; that is, the origin AS either owns the block or is the designated ISP for the owner of the block. Origin validation would prevent an autonomous system from advertising itself as the origin AS for someone else‚Äôs active block of IP addresses, thus preventing access to that block; this is known as BGP hijacking. It would also prevent an AS from Ô¨Ånding a block of IP addresses nobody is actually using, and fraudulently claiming it as its own; this is known as IP-preÔ¨Åx squatting and is surprisingly common. Origin validation would not, by itself, prevent an AS, say Q, from claiming to have a direct connection to another AS, say R, that is the valid owner of an IP address block D. If Q is not in fact connected to R, then trafÔ¨Åc routed to D via Q will be lost. While Q could thus block access to D, it would not be able to make use of those addresses. Ideally, we also want some form of path validation; one form of this might be veriÔ¨Åcation that each AS along an AS-path is properly connected to the adjacent ASes in the AS-path. IRR-based validation is intended for customer-provider AS relationships in the sense of 15.10 BGP Relationships. Let P be an AS and let Q be its customer; we outline how P can use IRRs ‚Äì and some modest coercion ‚Äì to validate routes received by Q. From the deÔ¨Ånition of the customer-provider relationship, Q will only advertise to P its ‚Äúcustomer routes‚Äù: its own routes, and the customer routes of its own customers. If Q advertises to P a route to one of its own address blocks A, P √êQ-A, then P can use the IRR data to conÔ¨Årm that A is in fact owned by Q; an example of address-block lookup appears in the next section. It is quite possible that Q will not have listed A with any IRR, so P may elect to require such listing by adopting a policy that routes from customers will not be accepted without matching IRR address-block records. In the absence of such requirements, IRR data is notoriously incomplete, though it may still be useful. Now suppose R is a customer of Q that advertises address block B to Q, and Q in turn advertises B to P: P√êQR-B. The Ô¨Årst step is origin validation: P will need to Ô¨Ånd an IRR record establishing R as the owner of B. This time, however, for complete validation P will also need evidence that R is a customer of Q. This second veriÔ¨Åcation ‚Äì path validation ‚Äì is beyond the scope of origin validation. However, if this is not established, Q might potentially be attempting to block access to B, at least partially, by claiming a route to 354 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 B via a false connection to R. One approach to path validation is for P to require that Q provides a list of its customers, in the form of an ‚ÄúAS Set‚Äù, below. There is one additional, though less malicious, common error: accidentally and mistakenly offering transit service. This happens if an autonomous system, say Q, has a valid route to a non-customer address block D (which might even be the entire Internet), and unintentionally advertises this route to a neighboring AS P. P may now route all its trafÔ¨Åc to D via Q, which the latter may be ill-prepared to handle. This is known as a route leak. IRR validation can prevent route leaks as well as hijacking and preÔ¨Åx squatting: if Q accidentally advertises to P a non-customer route, then the IRR data can be used to recognize it as such, and P can then decline to accept it. If the BGP relationship between Q and P is one of siblings, rather than customer and provider, then Q will export to P allits routes (and vice-versa). It is probably impractical for P to validate every route it receives from Q. However, sibling ASes typically have a much closer relationship than provider-customer ASes; P here might be inclined simply to trust the routes exported by Q. 15.12.1 IRR Lookups IRR records can be accessed by using the whois command, although sometimes other interfaces are also provided. While whois is perhaps most commonly used to look up information about domains ( egwhois luc.edu ), it also supports multiple other query types. Advanced uses of whois usually require specifying a particular whois server, via the -hoption, in addition to the query. In the examples here, we will use the whois.radb.net server. IRR records are created using the Routing Policy SpeciÔ¨Åcation Language, RPSL, deÔ¨Åned in RFC 2622 and with companion usage document RFC 2650; the created records are then uploaded to the appropriate IRR. Loyola University owns the 147.126.0.0/16 address block, and advertises this block to its BGP peers. To usewhois to query about this block, we can use the command whois -h whois.radb.net 147.126.0.0/16 We get back a list of information, including the AS number and the owner (the output here has been abbreviated): route: 147.126.0.0/16 origin: AS7968 descr: Loyola University Chicago We can also ask about a speciÔ¨Åc IP address, eg147.126.2.45. It turns out that Loyola has records for multiple sub-blocks of 147.126.0.0/16 (to accommodate the assignment of IP addresses to different campuses), and for this query we get back records for both the top-level /16 block and for 147.126.0.0/20. (Reading closely, we also Ô¨Ånd that, while these sub-block records are managed directly by Loyola, the top-level 147.126.0.0/16 block is managed (as of 2020) by proxy by XO Communications.) We can also query the AS number itself. AS8, in the following example (from the RADB documentation), belongs to Rice University. whois -h whois.radb.net as8 The abbreviated output (as of 2020) is 15.12 BGP Security and Route Registries 355
An Introduction to Computer Networks, Release 2.0.11 aut-num: AS8 as-name: RICE descr: Rice University AS import: from AS2914 action pref=700; accept ANY AND NOT {0.0.0.0/0} import: from AS3356 4.30.109.33 at 4.30.109.34 action pref=700; med=20; accept ANY export: to AS2914 128.241.2.165 at 128.241.2.166 announce RS-RICE-PUBLIC-SUBNETS export: to AS3356 4.7.145.189 at 4.7.145.190 announce RS-RICE-PUBLIC-SUBNETS export: to AS3356 4.30.109.33 at 4.30.109.34 announce RS-RICE-PUBLIC-SUBNETS In this example we see both import andexport rules, corresponding to the BGP import and export Ô¨Ålters. The Ô¨Årst import rule says that from AS2914 (NTT Communications) we accept any route except a default route (0.0.0.0/0). The second import rule, for AS3356 (Level3/CenturyLink) includes MED values ( 15.6.3 MULTI_EXIT_DISC ). In addition to validating address blocks, the provider ASes can use the declared import and export rules to crosscheck against the advertisements received via BGP. The export rules here contain a Route Set RS-RICE-PUBLIC-SUBNETS (Route Set names must begin with ‚ÄúRS-‚Äú); we can query for more information about this route set with whois -h whois.radb. net '!iRS-RICE-PUBLIC-SUBNETS'. The use of Route Sets makes export rules easier to write (and maintain!) when multiple IP-address blocks are being exported. It is also possible to deÔ¨Åne AS Sets, which are lists of ASes. This is the preferred way for an AS to supply to its provider a list of its customers. For example, if we look up AS19754 (The Fusion Network), we get (among other things) import: from AS19754 :AS-CUSTOMERS accept PeerAS export: to AS1299 announce AS-TFNNET export: to AS19754:AS-CUSTOMERS announce ANY ThePeerAS keyword stands for the recipient‚Äôs AS number. AS-TFNNET is a single-level AS Set name; AS19754:AS-CUSTOMERS is a hierarchical AS Set name (AS Set names must be globally unique, so prepending ones AS-number is often useful). The third line above means that all routes are announced to the local AS-CUSTOMERS list; that is, transit services are provided to the AS‚Äôs customers, as expected. Any higher-level provider can retrieve this AS-CUSTOMERS list, egby using the ‚Äú!i‚Äù Ô¨Çag in the whois query as with the Route Set example above. Note that, in the context of a customer-provider relationship, a higher-level provider can reasonably infer that any AS to which AS19754 provides transit services is a customer, and thus that the AS19754:AS-CUSTOMERS AS Set is in fact a customer list for AS19754; the fact that ‚ÄúCUSTOMER‚Äù appears in the name is conventional but not essential. These and other commands can give a good description, for a given AS, of what networks and what ASes are its customers, and for whom it provides transit services, assuming the AS has taken care to create the appropriate records. With suitable automated processing, and suitable BGP support, this approach can be very effective in validating the information received via BGP. 356 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 The organization MANRS (Mutually Agreed Norms for Routing Security) was created in 2014 to improve routing security and to encourage best practices. One of those best practices is the use of IRR data for BGP validation. 15.12.2 RPKI Another, newer, approach to BGP integrity is Resource Public Key Infrastructure, or RPKI, described in RFC 6840. The idea here is to set up a mechanism by which ASes can obtain cryptographically signed assurances ‚Äì known as certiÔ¨Åcates, and very similar to the certiÔ¨Åcates of 29.5.2 TLS and29.5.2.1 CertiÔ¨Åcate Authorities ‚Äì that a given IP address block is in fact owned by the AS that claims to be the origin of routes to that address block. This achieves the origin validation described above, though other attacks may still be possible. An AS wanting to participate in RPKI bundles its AS number and IP address blocks into a ‚ÄúcertiÔ¨Åcate request‚Äù, which it then submits to an appropriate authority for digital signing. Once the request is signed, it becomes a Route Origin Authorization, or ROA, which other ASes can then use to verify that the Ô¨Årst AS is properly the origin of its claimed address blocks. The signers of ROAs ‚Äì akin to certiÔ¨Åcate authorities, as in 29.5.2.1 CertiÔ¨Åcate Authorities ‚Äì are largely the same organizations that run IRRs: the IANA, the Ô¨Åve regional registries ARIN, RIPE, APNIC, AfriNIC and LACNIC mentioned in 1.10.2 The Future of IPv4, and any local registries delegated by one of these. Because these are the organizations that hand out IP address blocks, they are in a strong position to verify address-block ownership. But IP-address ownership is a legal concern, not a technical one. If an AS sells an address block to someone else, must the transfer be approved by the appropriate route registry? Most registries have always required this on paper, but IRRs and RPKI give teeth to this policy. What if an IP-address transfer is part of a legal action valid in the country of the ASes involved, but is not recognized in the country of the registrar? Who might be liable if access to someone‚Äôs site is blocked due to an improperly signed ROA? For these and other reasons, ARIN has a somewhat daunting Relying Party Agreement that ASes must sign before being allowed to participate in ROA creation. An analysis in [YW19] suggests that legal considerations have slowed RPKI adoption. The veriÔ¨Åcation of ROAs, along with access to ROAs and IRRs generally, is notpart of the BGP protocol. If an AS wants to use an IRR or ROA to validate its routing decisions, it must obtain the appropriate information outside of BGP itself. Typically this is done by having the local BGP router communicate with a local ‚Äúvalidation server‚Äù, which handles the details of veriÔ¨Åcations for newly seen routes. 15.13 Epilog Interior routing ‚Äì using either distance-vector or link-state protocols ‚Äì is neat and mathematical. Exterior routing with BGP is messy and arbitrary. Perhaps the most surprising thing about BGP is that the Internet works as well as it does, given the complexity of provider interconnections. The business side of routing almost never has an impact on ordinary users. To an extent, BGP works well because providers voluntarily limit the complexity of their Ô¨Åltering preferences, but that in turn seems to be largely because the business relationships of real-world ISPs do not seem to require complex Ô¨Åltering. 15.13 Epilog 357
An Introduction to Computer Networks, Release 2.0.11 15.14 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.12 Solutions for Border Gateway Protocol. 1.0. Consider the following network of providers P-S, all using BGP. The providers are the horizontal lines; each provider is its own AS. Provider P Provider SProvider RProvider Q net NR net NSnet NQ (a).‚ô¢What routes to network NS will P receive, assuming each provider exports all its routes to its neighbors without Ô¨Åltering? For each route, list the AS-path. (b). What routes to network NQ will P receive? For each route, list the AS-path. (c). Suppose R now uses export Ô¨Åltering so as not to advertise any of its routes to P, though it does continue to advertise its routes to S. What routes to network NR will P receive, with AS-paths? 2.0. Consider the following network of Autonomous Systems AS1 through AS6, which double as destinations. When AS1 advertises itself via BGP to AS2, for example, the AS-path it provides is xAS1y. AS1 AS2 AS3: :: AS4 AS5 AS6 (a). If neither AS3 nor AS6 exports their AS3‚ÄìAS6 link to their neighbors AS2 and AS5 to the left, what routes will AS2 receive to reach AS5? Specify routes by AS-path. (b). Again assuming no export by AS3 nor AS6, what routes will AS5 receive to reach AS3? (c). Suppose AS3 exports to AS2 its link to AS6, but AS6 continues not to export the AS3‚ÄìAS6 link to AS5. How will AS5 now reach AS3? How will AS2 now reach AS6? Assume that there are no local preferences in use in BGP best-path selection, and that the shortest AS-path wins. 3.0. Suppose we try to use BGP‚Äôs strategy of exchanging destinations plus paths as an interior routing-update 358 15 Border Gateway Protocol (BGP)
An Introduction to Computer Networks, Release 2.0.11 strategy, perhaps replacing distance-vector routing. No costs or hop-counts are used, but routers attach to each destination a list of the routers used to reach that destination. Routers can also have route preferences, such as ‚Äúprefer my link to B whenever possible‚Äù. (a). Consider the network of 13.2 Distance-Vector Slow-Convergence Problem: D A B The D‚ÄìA link breaks, and B offers A what it thinks is its own route to D. Explain how exchanging path information prevents a routing loop here. (b). Suppose the network is as below, and initially each router knows about itself and its immediately adjacent neighbors. What sequence of router announcements can lead to A reaching F via A√ëD√ëE√ëB√ëC√ëF, and what individual router preferences would be necessary? (Initially, for example, A would reach B directly; what preference might make it prefer A √ëD√ëE√ëB?) A B C D E F (c). Explain why this method is equivalent to using the hopcount metric with either distance-vector or link-state routing, if routers are not allowed to have preferences and if the router-path length is used as a tie-breaker. 4.0. In the following AS-path from AS0 to AS4, with customers lower than providers, how far can a customer route of AS0 be exported towards AS4? How far can a customer route of AS4 be exported towards AS0? AS1 / \ / \ / AS2--peer--AS3 / \ AS0 AS4 5.0. Complete the proof of the no-valley theorem of 15.10 BGP Relationships to include peer-to-peer links. (a). Show that the existing argument also works if the A i-to-A i+1link was peer-to-peer rather than providerto-customer, establishing that an upwards link cannot appear to the right of a peer-to-peer link. (b). Show that the existing argument works if the A k-1-to-A klink was peer-to-peer rather than customer-toprovider, establishing that a downwards link cannot appear to the left of a peer-to-peer link. (c). Show that there cannot be two peer-to-peer links. 15.14 Exercises 359
An Introduction to Computer Networks, Release 2.0.11 360 15 Border Gateway Protocol (BGP)
16 UDP TRANSPORT The standard transport protocols riding above the IP layer are TCP andUDP. As we saw in , UDP provides simple datagram delivery to remote sockets, that is, to xhost,portypairs. TCP provides a much richer functionality for sending data, but requires that the remote socket Ô¨Årst be connected. In this chapter, we start with the much-simpler UDP, including the UDP-based Trivial File Transfer Protocol. We also review some fundamental issues any transport protocol must address, such as lost Ô¨Ånal packets and packets arriving late enough to be subject to misinterpretation upon arrival. These fundamental issues will be equally applicable to TCP connections. 16.1 User Datagram Protocol ‚Äì UDP RFC 1122 refers to UDP as ‚Äúalmost a null protocol‚Äù; while that is something of a harsh assessment, UDP is indeed fairly basic. The two features it adds beyond the IP layer are port numbers and a checksum. The UDP header consists of the following: Source Port Destination Port Length Data Checksum0 16 32 The port numbers are what makes UDP into a real transport protocol: with them, an application can now connect to an individual server process (that is, the process ‚Äúowning‚Äù the port number in question), rather than simply to a host. UDP is unreliable, in that there is no UDP-layer attempt at timeouts, acknowledgment and retransmission; applications written for UDP must implement these. As with TCP, a UDP xhost,portypair is known as a socket (though UDP ports are considered a separate namespace from TCP ports). UDP is also unconnected, or stateless; if an application has opened a port on a host, any other host on the Internet may deliver packets to thatxhost,portysocket without preliminary negotiation. An old bit of Internet humor about UDP‚Äôs unreliability has it that if I send you a UDP joke, you might not get it. UDP packets use the 16-bit Internet checksum (7.4 Error Detection ) on the data. While it is seldom done today, the checksum can be disabled by setting the checksum Ô¨Åeld to the all-0-bits value, which never occurs as an actual ones-complement sum. The UDP checksum covers the UDP header, the UDP data and also a ‚Äúpseudo-IP header‚Äù that includes the source and destination IP addresses (and also a duplicate copy of the UDP-header length Ô¨Åeld). If a NAT router rewrites an IP address or port, the UDP checksum must be updated. 361
An Introduction to Computer Networks, Release 2.0.11 UDP packets can be dropped due to queue overÔ¨Çows either at an intervening router or at the receiving host. When the latter happens, it means that packets are arriving faster than the receiver can process them. Higherlevel protocols that deÔ¨Åne ACK packets ( egUDP-based RPC, below) typically include some form of Ô¨Çow control to prevent this. UDP is popular for ‚Äúlocal‚Äù transport, conÔ¨Åned to one LAN. In this setting it is common to use UDP as the transport basis for a Remote Procedure Call, or RPC, protocol. The conceptual idea behind RPC is that one host invokes a procedure on another host; the parameters and the return value are transported back and forth by UDP. We will consider RPC in greater detail below, in 16.5 Remote Procedure Call (RPC); for now, the point of UDP is that on a local LAN we can fall back on rather simple mechanisms for timeout and retransmission. UDP is well-suited for ‚Äúrequest-reply‚Äù semantics beyond RPC; one can use TCP to send a message and get a reply, but there is the additional overhead of setting up and tearing down a connection. DNS uses UDP, largely for this reason. However, if there is any chance that a sequence of request-reply operations will be performed in short order then TCP may be worth the overhead. UDP is also popular for real-time transport; the issue here is head-of-line blocking. If a TCP packet is lost, then the receiving host queues any later data until the lost data is retransmitted successfully, which can take several RTTs; there is no option for the receiving application to request different behavior. UDP, on the other hand, gives the receiving application the freedom simply to ignore lost packets. This approach is very successful for voice and video, which are loss-tolerant in that small losses simply degrade the received signal slightly, but delay-intolerant in that packets arriving too late for playback might as well not have arrived at all. Similarly, in a computer game a lost position update is moot after any subsequent update. Loss tolerance is the reason the Real-time Transport Protocol, or RTP, is built on top of UDP rather than TCP. It is common for V oIP telephone calls to use RTP and UDP. See also the NoTCP Manifesto. There is a dark side to UDP: it is sometimes the protocol of choice in Ô¨Çooding attacks on the Internet, as it is easy to send UDP packets with spoofed source address. See the Internet Draft draft-byrne-opsec-udpadvisory. That said, it is not especially hard to send TCP connection-request (SYN) packets with spoofed source address. It is, however, quite difÔ¨Åcult to get TCP source-address spooÔ¨Ång to work for long enough that data is delivered to an application process; see 18.3.1 ISNs and spooÔ¨Ång. UDP also sometimes enables what are called trafÔ¨Åc ampliÔ¨Åcation attacks: the attacker sends a small message to a server, with spoofed source address, and the server then responds to the spoofed address with a much larger response message. This creates a larger volume of trafÔ¨Åc to the victim than the attacker would be able to generate directly. One approach is for the server to limit the size of its response ‚Äì ideally to the size of the client‚Äôs request ‚Äì until it has been able to verify that the client actually receives packets sent to its claimed IP address. QUIC uses this approach; see 18.15.4.4 Connection handshake and TLS encryption. 16.1.1 QUIC Sometimes UDP is used simply because it allows new or experimental protocols to run entirely as user-space applications; no kernel updates are required, as would be the case with TCP changes. Google has created a protocol named QUIC (Quick UDP Internet Connections, chromium.org/quic) in this category, rather speciÔ¨Åcally to support the HTTP protocol. QUIC can in fact be viewed as a transport protocol speciÔ¨Åcally tailored to HTTPS: HTTP plus TLS encryption ( 29.5.2 TLS ). QUIC also takes advantage of UDP‚Äôs freedom from head-of-line blocking. For example, one of QUIC‚Äôs goals includes supporting multiplexed streams in a single connection ( egfor the multiple components of a 362 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 web page). A lost packet blocks its own stream until it is retransmitted, but the other streams can continue without waiting. An early version of QUIC supported error-correcting codes ( 7.4.2 Error-Correcting Codes ); this is another feature that would be difÔ¨Åcult to add to TCP. In many cases QUIC eliminates the initial RTT needed for setting up a TCP connection, allowing data delivery with the very Ô¨Årst packet. This usually this requires a recent previous connection, however, as otherwise accepting data in the Ô¨Årst packet opens the recipient up to certain spooÔ¨Ång attacks. Also, QUIC usually eliminates the second (and maybe third) RTT needed for negotiating TLS encryption ( 29.5.2 TLS ). QUIC provides support for advanced congestion control, currently (2014) including a UDP analog of TCP CUBIC ( 22.15 TCP CUBIC ). QUIC does this at the application layer but new congestion-control mechanisms within TCP often require client operating-system changes even when the mechanism lives primarily at the server end. (QUIC may require kernel support to make use of ECN congestion feedback, 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN), as this requires setting bits in the IP header.) QUIC represents a promising approach to using UDP‚Äôs Ô¨Çexibility to support innovative or experimental transport-layer features. One downside of QUIC is its nonstandard programming interface, but note that Google can (and does) achieve widespread web utilization of QUIC simply by distributing the client side in its Chrome browser. Another downside, more insidious, is that QUIC breaks the ‚Äúsocial contract‚Äù that everyone should use TCP so that everyone is on the same footing regarding congestion. It turns out, though, that TCP users are not in fact all on the same footing, as there are now multiple TCP variants ( 22 Newer TCP Implementations ). Furthermore, QUIC is supposed to compete fairly with TCP. Still, QUIC does open an interesting can of worms. Because many of the speciÔ¨Åc features of QUIC were chosen in response to perceived difÔ¨Åculties with TCP, we will explore the protocol‚Äôs details after introducing TCP, in 18.15.4 QUIC Revisited. 16.1.2 DCCP The Datagram Congestion Control Protocol, or DCCP, is another transport protocol build atop UDP, preserving UDP‚Äôs fundamental tolerance to packet loss. It is outlined in RFC 4340. DCCP adds a number of TCP-like features to UDP; for our purposes the most signiÔ¨Åcant are connection setup and teardown (see 18.15.3 DCCP ) and TCP-like congestion management (see 21.3.3 DCCP Congestion Control ). DCCP data packets, while numbered, are delivered to the application in order of arrival rather than in order of sequence number. DCCP also adds acknowledgments to UDP, but in a specialized form primarily for congestion control. There is no assumption that unacknowledged data packets will ever be retransmitted; that decision is entirely up to the application. Acknowledgments can acknowledge single packets or, through the DCCP acknowledgment-vector format, all packets received in a range of recent sequence numbers (SACK TCP, 19.6 Selective Acknowledgments (SACK), also supports this). DCCP does support reliable delivery of control packets, used for connection setup, teardown and option negotiation. Option negotiation can occur at any point during a connection. DCCP packets include not only the usual application-speciÔ¨Åc UDP port numbers, but also a 32-bit service code. This allows Ô¨Åner-grained packet handling as it unambiguously identiÔ¨Åes the processing requested by an incoming packet. The use of service codes also resolves problems created when applications are forced to use nonstandard port numbers due to conÔ¨Çicts. DCCP is speciÔ¨Åcally intended to run in in the operating-system kernel, rather than in user space. This is because the ECN congestion-feedback mechanism ( 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN) ) requires 16.1 User Datagram Protocol ‚Äì UDP 363
An Introduction to Computer Networks, Release 2.0.11 setting Ô¨Çag bits in the IP header, and most kernels do not allow user-space applications to do this. 16.1.3 UDP Simplex-Talk One of the early standard examples for socket programming is simplex-talk. The client side reads lines of text from the user‚Äôs terminal and sends them over the network to the server; the server then displays them on its terminal. The server does not acknowledge anything sent to it, or in fact send any response to the client at all. ‚ÄúSimplex‚Äù here refers to the one-way nature of the Ô¨Çow; ‚Äúduplex talk‚Äù is the basis for Instant Messaging, or IM. Even at this simple level we have some details to attend to regarding the data protocol: we assume here that the lines are sent with a trailing end-of-line marker. In a world where different OS‚Äôs use different end-ofline marks, including them in the transmitted data can be problematic. However, when we get to the TCP version, if arriving packets are queued for any reason then the embedded end-of-line character will be the only thing to separate the arriving data into lines. As with almost every Internet protocol, the server side must select a port number, which with the server‚Äôs IP address will form the socket address to which clients connect. Clients must discover that port number or have it written into their application code. Clients too will have a port number, but it is largely invisible. On the server side, simplex-talk must do the following: 
- ask for a designated port number 
- create a socket, the sending/receiving endpoint 
- bind the socket to the socket address, if this is not done at the point of socket creation 
- receive packets sent to the socket 
- for each packet received, print its sender and its content The client side has a similar list: 
- look up the server‚Äôs IP address, using DNS 
- create an ‚Äúanonymous‚Äù socket; we don‚Äôt care what the client‚Äôs port number is 
- read a line from the terminal, and send it to the socket address xserver_IP,porty 16.1.3.1 The Server We will start with the server side, presented here in Java. The Java socket implementation is based mostly on the BSD socket library, 1.16 Berkeley Unix. We will use port 5432; this can easily be changed if, for example, on startup an error message like ‚Äúcannot create socket with port 5432‚Äù appears. The port we use here, 5432, has also been adopted by PostgreSQL for TCP connections. (The client, of course, would also need to be changed.) JavaDatagramPacket type 364 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 JavaDatagramPacket objects contain the packet data and the xIP_address,port ysource or destination. Packets themselves combine both data and address, of course, but nonetheless combining these in a single programming-language object is not an especially common design choice. The original BSD socket library implemented data and address as separate parameters, and many other languages have followed that precedent. A case can be made that the Java approach violates the single-responsibility principle, because data and address are so often handled separately. The socket-creation and port-binding operations are combined into the single operation new DatagramSocket(destport). Once created, this socket will receive packets from any host that addresses a packet to it; there is no need for preliminary connection. In the original BSD socket library, a socket is created with socket() and bound to an address with the separate operation bind(). The server application needs no parameters; it just starts. (That said, we could make the port number a parameter, to allow easy change.) The server accepts both IPv4 and IPv6 connections; we return to this below. Though it plays no role in the protocol, we will also have the server time out every 15 seconds and display a message, just to show how this is done. Implementations of real UDP protocols essentially always must arrange when attempting to receive a packet to time out after a certain interval with no response. The Ô¨Åle below is at udp_stalks.java. /*simplex-talk server, UDP version */ import java.net. *; import java.io. *; public class stalks { static public int destport = 5432; static public int bufsize = 512; static public final int timeout = 15000; // time in milliseconds static public void main(String args[]) { DatagramSocket s; // UDP uses DatagramSockets try { s = new DatagramSocket(destport); } catch (SocketException se) { System.err.println("cannot create socket with port " + destport); return; } try { s.setSoTimeout(timeout); // set timeout in milliseconds } catch (SocketException se) { System.err.println("socket exception: timeout not set!"); } // create DatagramPacket object for receiving data: DatagramPacket msg = new DatagramPacket(new byte[bufsize], bufsize); (continues on next page) 16.1 User Datagram Protocol ‚Äì UDP 365
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) while(true) { // read loop try { msg.setLength(bufsize); // max received packet size s.receive(msg); // the actual receive operation System.err.println("message from <" + msg.getAddress().getHostAddress() + "," + msg.getPort() + √£√ë">"); } catch (SocketTimeoutException ste) { // receive() timed out System.err.println("Response timed out!"); continue; } catch (IOException ioe) { // should never happen! System.err.println("Bad receive"); break; } String str = new String(msg.getData(), 0, msg.getLength()); System.out.print(str); // newline must be part of str } s.close(); } // end of main } 16.1.3.2 UDP and IP addresses The server line s = new DatagramSocket(destport) creates aDatagramSocket object bound to the given port. If a host has multiple IP addresses (that is, is multihomed), packets sent to that port to any of those IP addresses will be delivered to the socket, including localhost (and in fact all IPv4 addresses between 127.0.0.1 and 127.255.255.255) and the subnet broadcast address ( eg192.168.1.255). If a client attempts to connect to the subnet broadcast address, multiple servers may receive the packet (in this we are perhaps fortunate that the stalk server does not reply). Alternatively, we could have used s = new DatagramSocket(int port, InetAddress local_addr) in which case only packets sent to the host and port through the host‚Äôs speciÔ¨Åc IP address local_addr would be delivered. It does not matter here whether IP forwarding on the host has been enabled. In the original C socket library, this binding of a port to (usually) a server socket was done with the bind() call. To allow connections via any of the host‚Äôs IP addresses, the special IP address INADDR_ANY is passed to bind(). When a host has multiple IP addresses, the BSD socket library and its descendents do not appear to provide a way to Ô¨Ånd out to which these an arriving UDP packet was actually sent (although it is supposed to, according to RFC 1122, ¬ß4.1.3.5). Normally, however, this is not a major difÔ¨Åculty. If a host has only one interface on an actual network ( ienot counting loopback), and only one IP address for that interface, then any remote clients must send to that interface and address. Replies (if any, which there are not with stalk) will also come from that address. Multiple interfaces do not necessarily create an ambiguity either; the easiest such case to experiment with 366 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 involves use of the loopback and Ethernet interfaces (though one would need to use an application that, unlike stalk, sends replies). If these interfaces have respective IPv4 addresses 127.0.0.1 and 192.168.1.1, and the client is run on the same machine, then connections to the server application sent to 127.0.0.1 will be answered from 127.0.0.1, and connections sent to 192.168.1.1 will be answered from 192.168.1.1. The IP layer sees these as different subnets, and Ô¨Ålls in the IP source-address Ô¨Åeld according to the appropriate subnet. The same applies if multiple Ethernet interfaces are involved, or if a single Ethernet interface is assigned IP addresses for two different subnets, eg192.168. 1.1 and 192.168. 2.1. Life is slightly more complicated if a single interface is assigned multiple IP addresses on the same subnet, eg192.168.1. 1and 192.168.1. 2. Regardless of which address a client sends its request to, the server‚Äôs reply will generally always come from one designated address for that subnet, eg192.168.1.1. Thus, it is possible that a legitimate UDP reply will come from a different IP address than that to which the initial request was sent. If this behavior is not desired, one approach is to create multiple server sockets, and to bind each of the host‚Äôs network IP addresses to a different server socket. The fact that the IP layer usually chooses the source address adds a slight wrinkle to the discussion of network protocol layers at 1.15 IETF and OSI. The classic ‚Äúencapsulation‚Äù model would suggest that the UDP layer writes the UDP header and then passes the packet (and destination IP address) to the IP layer, which then writes the IP header and passes the packet in turn down to the LAN layer. But this cannot work quite as described, because, if the IP source address is seen as supplied by the IP layer, then would not be available at the time the UDP-header checksum Ô¨Åeld is Ô¨Årst Ô¨Ålled in. Checksums are messy, and real implementations simply blur the layering ‚Äúrules‚Äù: typically the UDP layer asks the IP layer for early determination of the IP source address. The situation is further complicated by the fact that nowadays the bulk of the checksum calculation is often performed at the LAN layer, by the LAN hardware; see 17.5 TCP OfÔ¨Çoading. 16.1.3.3 The Client Next is the Java client version udp_stalkc.java. The client ‚Äì any client ‚Äì must provide the name of the host to which it wishes to send; as with the port number this can be hard-coded into the application but is more commonly speciÔ¨Åed by the user. The version here uses host localhost as a default but accepts any other hostname as a command-line argument. The call to InetAddress.getByName(desthost) invokes the DNS system, which looks up name desthost and, if successful, returns an IP address. ( InetAddress. getByName() also accepts addresses in numeric form, eg‚Äú127.0.0.1‚Äù, in which case DNS is not necessary.) When we create the socket we do not designate a port in the call to new DatagramSocket(); this means any port will do for the client. When we create the DatagramPacket object, the Ô¨Årst parameter is a zero-length array as the actual data array will be provided within the loop. A certain degree of messiness is introduced by the need to create a BufferedReader object to handle terminal input. // simplex-talk CLIENT injava, UDP version import java.net. *; import java.io. *; public class stalkc { (continues on next page) 16.1 User Datagram Protocol ‚Äì UDP 367
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) static public BufferedReader bin; static public int destport = 5432; static public int bufsize = 512; static public void main(String args[]) { String desthost = "localhost"; if(args.length >= 1) desthost = args[0]; bin = new BufferedReader(new InputStreamReader(System. in)); InetAddress dest; System.err.print( "Looking up address of " + desthost + "..."); try{ dest = InetAddress.getByName(desthost); // DNS query } catch (UnknownHostException uhe) { System.err.println( "unknown host: " + desthost); return; } System.err.println( " got it!" ); DatagramSocket s; try{ s = new DatagramSocket(); } catch(IOException ioe) { System.err.println( "socket could not be created" ); return; } System.err.println( "Our own port is " + s.getLocalPort()); DatagramPacket msg = new DatagramPacket(new byte[0], 0, dest, √£√ëdestport); while(true) { String buf; int slen; try{ buf = bin.readLine(); } catch (IOException ioe) { System.err.println( "readLine() failed" ); return; } if(buf == null) break; // user typed EOF character buf = buf + "\n"; // append newline character slen = buf.length(); byte[] bbuf = buf.getBytes(); (continues on next page) 368 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) msg.setData(bbuf); msg.setLength(slen); try{ s.send(msg); } catch (IOException ioe) { System.err.println( "send() failed" ); return; } } // while s.close(); } } The default value of desthost here islocalhost; this is convenient when running the client and the server on the same machine, in separate terminal windows. All packets are sent to the xdest,destportyaddress speciÔ¨Åed in the initialization of msg. Alternatively, we could have called s.connect(dest,destport). This causes nothing to be sent over the network, as UDP is connectionless, but locally marks the socket sallowing it to send only to xdest,destporty. In Java we still have to embed the destination address in every DatagramPacket wesend(), so this offers no beneÔ¨Åt, but in other languages this can simplify subsequent sending operations. Like the server, the client works with both IPv4 and IPv6. The InetAddress objectdest in the server code above can hold either IPv4 or IPv6 addresses; InetAddress is the base class with child classes Inet4Address andInet6Address. If the client and server can communicate at all via IPv6 and if the value ofdesthost supplied to the client is an IPv6-only name, then dest will be anInet6Address object and IPv6 will be used. For example, if the client is invoked from the command line with java stalkc ip6-localhost, and the nameip6-localhost resolves to the IPv6 loopback address ::1, the client will send its packets to an stalk server on the same host using IPv6 (and the loopback interface). If greater IPv4-versus-IPv6 control is desired, one can replace the getByName() call with the following, wheredests now has type InetAddress[]: dests = InetAddress.getAllByName(desthost); This returns an array of all addresses associated with the given name. One can then Ô¨Ånd the IPv6 addresses by searching this array for addresses addr for whichaddr instanceof Inet6Address. For non-Java languages, IP-address objects often have an AddressFamily attribute that can be used to determine whether an address is IPv4 or IPv6. See also 12.4 Using IPv6 and IPv4 Together. Finally, here is a simple python version of the client, udp_stalkc.py. #!/usr/bin/python3 from socket import * (continues on next page) 16.1 User Datagram Protocol ‚Äì UDP 369
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) from sys import argv portnum = 5432 deftalk(): rhost = "localhost" iflen(argv) > 1: rhost = argv[1] print("Looking up address of " + rhost + "...", end="") try: dest = gethostbyname(rhost) except(GAIerror, herror) asmesg: # GAIerror: error in √£√ëgethostbyname() errno,errstr=mesg.args print("\n", errstr); return; print("got it: " + dest) addr=(dest, portnum) # a socket address s = socket(AF_INET, SOCK_DGRAM) s.settimeout(1.5) # we don't actually need to set √£√ëtimeout here while True: try: buf = input( "> ") except: break s.sendto(bytes(buf + "\n",'ascii'), addr) talk() Why not C? While C is arguably the most popular language for network programming, it does not support IP addresses and other network objects as Ô¨Årst-class types, and so we omit it here. But see 28.2.2 An Actual StackOverÔ¨Çow Example and29.5.3 A TLS Programming Example for TCP-based C versions of stalk-like programs. (The problem is not entirely C‚Äôs fault; a network address might be an IPv4 address or an IPv6 address (or even a ‚Äúnamed pipe‚Äù address); these objects are of different sizes and so addresses must be handled by reference, which is awkward in C.) To experiment with these on a single host, start the server in one window and one or more clients in other windows. One can then try the following: 
- have two clients simultaneously running, and sending alternating messages to the same server 
- invoke the client with the external IP address of the server in dotted-decimal, eg10.0.0.3 (note that localhost is 127.0.0.1) 
- run the java and python clients simultaneously, sending to the same server 
- run the server on a different host ( ega virtual host or a neighboring machine) 370 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 
- invoke the client with a nonexistent hostname One can also use netcat, below, as a client, though netcat as a server will not work for the multipleclient experiments. Note that, depending on the DNS server, the last one may not actually fail. When asked for the DNS name of a nonexistent host such as zxqzx.org, many ISPs will return the IP address of a host running a web server hosting an error/search/advertising page (usually their own). This makes some modicum of sense when attention is restricted to web searches, but is annoying if it is not, as it means non-web applications have no easy way to identify nonexistent hosts. Simplex-talk will work if the server is on the public side of a NAT Ô¨Årewall. No server-side packets need to be delivered to the client! But if the other direction works, something is very wrong with the Ô¨Årewall. 16.1.4 netcat The versatile netcat utility (also sometimes spelled nc) utility enables sending and receiving of individual UDP (and TCP) packets; we can use it to substitute for the stalk client, or, with a limitation, the server. (The netcat utility, unlike stalk, supports bidirectional communication.) Thenetcat utility is available for Windows, Linux and Macintosh systems, in both binary and source forms, from a variety of places and in something of a variety of versions. The classic version is available from sourceforge.net/projects/nc110; a newer implementation is ncat. The Wikipedia page has additional information. As with stalk, netcat sends the Ô¨Ånal end-of-line marker along with its data. The -uÔ¨Çag is used to request UDP. To send to port 5432 on localhost using UDP, like an stalk client, the command is netcat -u localhost 5432 One can then type multiple lines that should all be received by a running stalk server. If desired, the source port can be speciÔ¨Åed with the -poption; egnetcat -u -p 40001 localhost 5432. To act as an stalk server, we need the -loption to ask netcat to listen instead of sending: netcat -l -u 5432 One can then send lines using stalkc ornetcat in client mode. However, once netcat in server mode receives its Ô¨Årst UDP packet, it will not accept later UDP packets from different sources (some versions of netcat have a-koption to allow this for TCP, but not for UDP). (This situation arises because netcat makes use of the connect() call on the server side as well as the client, after which the server can only send to and receive from the socket address to which it has connected. This simpliÔ¨Åes bidirectional communication. Often, UDP connect() is called only by the client, if at all. See the paragraph about connect() following the Java stalkc code in 16.1.3.3 The Client .) 16.1.5 Binary Data In the stalk example above, the client sent strings to the server. However, what if we are implementing a protocol that requires us to send binary data? Or designing such a protocol? The client and server will now have to agree on how the data is to be encoded. 16.1 User Datagram Protocol ‚Äì UDP 371
An Introduction to Computer Networks, Release 2.0.11 As an example, suppose the client is to send to the server a list of 32-bit integers, organized as follows. The length of the list is to occupy the Ô¨Årst two bytes; the remainder of the packet contains the consecutive integers themselves, four bytes each, as in the diagram: 4 2003 3011 4003 10009 packet data layout The client needs to create the byte array organized as above, and the server needs to extract the values. (The inclusion of the list length as a short int is not really necessary, as the receiver will be able to infer the list length from the packet size, but we want to be able to illustrate the encoding of both int andshort int values.) The protocol also needs to deÔ¨Åne how the integers themselves are laid out. There are two common ways to represent a 32-bit integer as a sequence of four bytes. Consider the integer 0x01020304 = 1 2563+ 22562 + 3256 + 4. This can be encoded as the byte sequence [1,2,3,4], known as big-endian encoding, or as [4,3,2,1], known as little-endian encoding; the former was used by early IBM mainframes and the latter is used by most Intel processors. (We are assuming here that both architectures represent signed integers using twos-complement; this is now universal but was not always.) To send 32-bit integers over the network, it is certainly possible to tag the data as big-endian or little-endian, or for the endpoints to negotiate the encoding. However, by far the most common approach on the Internet ‚Äì at least below the application layer ‚Äì is to follow the convention of RFC 1700 and use big-endian encoding exclusively; big-endian encoding has since come to be known as ‚Äúnetwork byte order‚Äù. How one converts from ‚Äúhost byte order‚Äù to ‚Äúnetwork byte order‚Äù is language-dependent. It must always be done, even on big-endian architectures, as code may be recompiled on a different architecture later. In Java the byte-order conversion is generally combined with the process of conversion from int to byte[]. The client will use a DataOutputStream class to support the writing of the binary values to an output stream, through methods such as writeInt() andwriteShort(), together with a ByteArrayOutputStream class to support the conversion of the output stream to type byte[]. The code below assumes the list of integers is initially in an ArrayList<Integer> namedtheNums. ByteArrayOutputStream baos = new ByteArrayOutputStream(); DataOutputStream dos = new DataOutputStream(baos); try{ dos.writeShort(theNums.size()); for(int n: theNums) { dos.writeInt(n); } } catch (IOException ioe) { / *exception handling */ } byte[] bbuf = baos.toByteArray(); msg.setData(bbuf); // msg isthe DatagramPacket √£√ëobject to be sent The server then needs to to the reverse; again, msg is the arriving DatagramPacket. The code below simply calculates the sum of the 32-bit integers in msg: 372 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 ByteArrayInputStream bais = new ByteArrayInputStream(msg.getData(), 0, msg. √£√ëgetLength()); DataInputStream dis = new DataInputStream(bais); int sum = 0; try{ int count = dis.readShort(); for(int i=0; i<count; i++) { sum += dis.readInt(); } } catch (IOException ioe) { / *more exception handling */ } A version of simplex-talk for lists of integers can be found in client saddc.java and server sadds.java. The client reads from the command line a list of character-encoded integers (separated by whitespace), constructs the binary encoding as above, and sends them to the server; the server prints their sum. Port 5434 is used; this can be changed if necessary. In the C language, we can simply allocate a char[] of the appropriate size and write the network-byteorder values directly into it. Conversion to network byte order and back is done with the following library calls: 
- htonl(): host-to-network conversion for long (32-bit) integers 
- ntohl(): network-to-host conversion for long integers 
- htons(): host-to-network conversion for short (16-bit) integers 
- ntohs(): network-to-host conversion for short integers A certain amount of casting between int*andchar*is also necessary. As both casting and byte-order conversions are error-prone, it is best if all conversions are made in a block, just after a packet arrives or just before it is sent, rather than on demand throughout the program. In general, the designer of a protocol needs to select an unambiguous format for all binary data; protocoldeÔ¨Åning RFCs always include such format details. This can be a particular issue for Ô¨Çoating-point data, for which two formats can have the same endianness but still differ, egin normalization or the size of the exponent Ô¨Åeld. Formats for structured data, such as arrays, must also be spelled out; in the example above the list size was indicated by a length Ô¨Åeld but other options are possible. The example above illustrates Ô¨Åxed-Ô¨Åeld-width encoding. Another possible option, using variable-length encoding, is ASN.1 using the Basic Encoding Rules ( 26.6 ASN.1 Syntax and SNMP ); Ô¨Åxed-Ô¨Åeld encoding sometimes becomes cumbersome as data becomes more hierarchical. At the application layer, the use of non-binary encodings is common, though binary encodings continue to remain common as well. Two popular formats using human-readable unicode strings for data encoding are ASN.1 with its XML Encoding Rules and JSON. While the latter format originated with JavaScript, it is now widely supported by many other languages. 16.2 Trivial File Transport Protocol, TFTP We now introduce a real protocol based on UDP: the Trivial File Transport Protocol, or TFTP. While TFTP supports Ô¨Åle transfers in both directions, we will restrict attention to the more common case where the client 16.2 Trivial File Transport Protocol, TFTP 373
An Introduction to Computer Networks, Release 2.0.11 requests a Ô¨Åle from the server. TFTP does not support a mechanism for authentication; any requestable Ô¨Åles are available to anyone. In this TFTP does not differ from basic web browsing; as with web servers, a TFTP Ô¨Åle server must ensure that requests are disallowed if the Ô¨Åle ‚Äì for example ../../../etc/passwd ‚Äì is not within a permitted directory. Because TFTP is UDP-based, and clients can be implemented very compactly, it is well-suited to the downloading of startup Ô¨Åles to very compact systems, including diskless systems. Because it uses stop-and-wait, often uses a Ô¨Åxed timeout interval, and offers limited security, TFTP is typically conÔ¨Åned to internal use within a LAN. Although TFTP is a very simple protocol, for correct operation it must address several fundamental transport issues; these are discussed in detail in the following section. TFTP is presented here partly as a way to introduce these transport issues; we will later return to these same issues in the context of TCP ( 18.4 Anomalous TCP scenarios ). TFTP, documented Ô¨Årst in RFC 783 and updated in RFC 1350, has Ô¨Åve packet types: 
- Read ReQuest, RRQ, containing the Ô¨Ålename and a text/binary indication 
- Write ReQuest, WRQ 
- Data, containing a 16-bit block number and up to 512 bytes of data 
- ACK, containing a 16-bit block number 
- Error, for certain designated errors. All errors other than ‚ÄúUnknown Transfer ID‚Äù are cause for sender termination. Data block numbering begins at 1; we will denote the packet with the Nth block of data as Data[N]. Acknowledgments contain the block number of the block being acknowledged; thus, ACK[N] acknowledges Data[N]. All blocks of data contain 512 bytes except the Ô¨Ånal block, which is identiÔ¨Åed asthe Ô¨Ånal block by virtue of containing less than 512 bytes of data. If the Ô¨Åle size was divisible by 512, the Ô¨Ånal block will contain 0 bytes of data. TFTP block numbers are 16 bits in length, and are not allowed to wrap around. Because TFTP uses UDP (as opposed to TCP) it must take care of packetization itself, and thus must choose a block size small enough to avoid fragmentation ( 9.4 Fragmentation ). While negotiation of the block size would have been possible, as is done by TCP‚Äôs 18.6 Path MTU Discovery, it would have added considerable complexity. The TFTP server listens on UDP port 69 for arriving RRQ packets (and WRQ, though we will not consider those here). For each RRQ requesting a valid Ô¨Åle, TFTP server implementations almost always create a separate process (or thread) to handle the transfer. That child process will then obtain an entirely new UDP port, which will be used for all further interaction with the client, at least for this particular transfer. As we shall see below, this port change has signiÔ¨Åcant functional implications in preventing old-duplicate packets, though for now we can justify it as making the implementer‚Äôs life much easier. With the port change, the server child process responsible for the transfer has to interact with only one client; all arriving packets must have come from the client for which the child process was created (while it is possible for stray packets to arrive from other endpoints, the child process can ignore them). Without the port change, on the other hand, handling multiple concurrent transfers would be decidedly complicated: the server would have to sort out, for each arriving packet, which transfer it belonged to. Each transfer would have its own state information including block number, open Ô¨Åle, and the time of the last successful packet. The port-change rule does have the drawback of preventing the use of TFTP through NAT Ô¨Årewalls. 374 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 In the absence of packet loss or other errors, TFTP Ô¨Åle requests typically proceed as follows: 1. The client sends a RRQ to server port 69. 2. The server creates a child process, which obtains a new port, s_port, from the operating system. 3. The server child process sends Data[1] from s_port. 4. The client receives Data[1], and thus learns the value of s_port. The client will verify that each future Data[N] arrives from this same port. 5. The client sends ACK[1] (and all future ACKs) to the server‚Äôs s_port. 6. The server child process sends Data[2], etc, each time waiting for the client ACK[N] before sending Data[N+1]. 7. The transfer process stops when the server sends its Ô¨Ånal block, of size less than 512 bytes, and the client sends the corresponding ACK. We will refer to the client‚Äôs learning of s_port in step 3 as latching on to that port. Here is a diagram; the server child process (with new port s_port) is represented by the blue line at right. TFTP no lossesRRQ Ack 1 Data 2 Data 3, <512 B Ack 3Ack 2Data 1client server New port s_port opened for this transfer ‚ãÆclient latches on to s_port We turn next to the complications introduced by taking packet losses and reordering into account. 16.3 Fundamental Transport Issues The possibility of lost or delayed packets introduces several fundamental issues that any transport strategy must handle correctly for proper operation; we will revisit these in the context of TCP in 18.4 Anomalous 16.3 Fundamental Transport Issues 375
An Introduction to Computer Networks, Release 2.0.11 TCP scenarios. The issues we will consider include 
- old duplicate packets 
- lost Ô¨Ånal ACK 
- duplicated connection request 
- reboots In this section we will discuss these issues both in general and in particular how TFTP takes them into account. 16.3.1 Old Duplicate Packets Perhaps the trickiest issue is old duplicate packets: packets from the past arriving quite late, but which are mistakenly accepted as current. For a TFTP example, suppose the client chooses port 2000 and requests Ô¨Åle ‚Äúfoo‚Äù, and the server then chooses port 4000 for its child process. During this transfer, Data[2] gets duplicated (perhaps through timeout and retransmission) and one of the copies is greatly delayed. The other copy arrives on time, though, and the transfer concludes. Now, more-or-less immediately after, the client initiates a second request, this time for Ô¨Åle ‚Äúbar‚Äù. Fatefully, the client again chooses port 2000 and the server child process again chooses port 4000. At the point in the second transfer when the client is waiting for Data[2] from Ô¨Åle ‚Äúbar‚Äù, we will suppose the old-duplicate Data[2] from Ô¨Åle ‚Äúfoo‚Äù Ô¨Ånally shows up. There is nothing in the packet to indicate anything is amiss: the block number is correct, the destination port of 4000 ensures delivery to the current server child process, and the source port of 2000 makes the packet appear to be coming from the current client. The wrong Data[2] is therefore accepted as legitimate, and the Ô¨Åle transfer is corrupted. An old packet from a previous instance of the connection, as described above, is called an external old duplicate. An essential feature of the external case is that the connection is closed and then reopened a short time later, using the same port numbers at each end. As a connection is often deÔ¨Åned by its endpoint port numbers (more precisely, its socket addresses), we refer to ‚Äúreopening‚Äù the connection even if the second instance is completely unrelated. Two separate instances of a connection between the same socket addresses are sometimes known as incarnations of the connection, particularly in the context of TCP. Old duplicates can also be internal, from an earlier point in the same connection instance. For example, if TFTP allowed its 16-bit block numbers to wrap around, then a very old Data[3] might be accepted in lieu of Data[3+216]. Internal old duplicates are usually prevented ‚Äì or rendered improbable ‚Äì by numbering the data, either by block or by byte, and using sufÔ¨Åciently many bits that wrap-around is unlikely. TFTP prevents internal old duplicates simply by not allowing its 16-bit block numbers to wrap around; this is effective, but limits the maximum Ô¨Åle to 512B (216‚Äì1), or about 32 MB. If we were not concerned with old duplicates, TFTP‚Äôs stop-and-wait could make do with 1-bit sequence numbers ( 8.5 Exercises, exercise 8.5). Random Ports? 376 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 RFC 1350 states ‚ÄúThe TID‚Äôs [port numbers] chosen for a connection should be randomly chosen, so that the probability that the same number is chosen twice in immediate succession is very low.‚Äù A literal interpretation is that an implementation should choose a random 16-bit number and ask for that as its TID. But the author knows of no implementation that actually does this; all seem to create sockets ( egwith Java‚ÄôsDatagramSocket() ) and accept the port number assigned to the new socket by the operating system. That port number will not be ‚Äúrandom‚Äù in the statistical sense, but willbe very likely different from any recently used port. Should this be interpreted as noncompliance with the RFC? The author has no idea. TFTP‚Äôs defense against external old duplicates is based on requiring that both endpoints try to choose a different port for each separate transfer ( RFC 1350 states that each side should choose its port number ‚Äúrandomly‚Äù). As long as either endpoint succeeds in choosing a new port, external old duplicates cannot interfere; see exercise 7.0. If ports are chosen at random, the probability that both sides will chose the same pair of ports for the subsequent connection is around 1/232; if ports are assigned by the operating system, there is an implicit assumption that the OS will not reissue the same port twice in rapid succession. If a noncompliant implementation on one side reuses its port numbers, TFTP transfers are protected as long as the other side chooses a new port, though the random probability of failure rises to 1/216. Note that this issue represents a second, more fundamental reason for having the server choose a new port for each transfer, unrelated to making the implementer‚Äôs life easier. After enough time, port numbers will eventually be recycled, but we will assume old duplicates have a much smaller lifetime. Both the external and internal old-duplicate scenarios assume that the old duplicate was sent earlier, but was somehow delayed in transit for an extended period of time, while later packets were delivered normally. Exactly how this might occur remains unclear; perhaps the least far-fetched scenario is the following: 
- A Ô¨Årst copy of the old duplicate was sent 
- A routing error occurs; the packet is stuck in a routing loop 
- An alternative path between the original hosts is restored, and the packet is retransmitted successfully 
- Some time later, the packet stuck in the routing loop is released, and reaches its Ô¨Ånal destination Another scenario involves a link in the path that supplies link-layer acknowledgment: the packet was sent once across the link, the link-layer ACK was lost, and so the packet was sent again. Some mechanism is still needed to delay one of the copies. Most solutions to the old-duplicate problem assume some cap on just how late an old duplicate can be. In practical terms, TCP ofÔ¨Åcially once took this time limit to be 60 seconds, but implementations now usually take it to be 30 seconds. Other protocols often implicitly adopt the TCP limit. Once upon a time, IP routers were expected to decrement a packet‚Äôs TTL Ô¨Åeld by 1 for each second the router held the packet in its queue; in such a world, IP packets cannot be more than 255 seconds old. It is also possible to prevent external old duplicates by including a connection count parameter in the transport or application header. For each consecutive connection, the connection count is incremented by (at least) 1. A separate connection-count value must be maintained by each side; if a connection-count value is ever lost, a suitable backup mechanism based on delay might be used. As an example, see 18.5 TCP Faster Opening. 16.3 Fundamental Transport Issues 377
An Introduction to Computer Networks, Release 2.0.11 16.3.2 Lost Final ACK In most protocols, most packets will be acknowledged. The Ô¨Ånal packet (almost always an ACK), however, cannot itself be acknowledged, as then it would not be the Ô¨Ånal packet. Somebody has to go last. This leaves some uncertainty on the part of the sender: did the last packet make it through, or not? In the TFTP setting, suppose the server sends the Ô¨Ånal packet, Data[3]. The client receives it and sends ACK[3], and then exits as the transfer is done; however, the ACK[3] is lost. The server will eventually time out and retransmit Data[3] again. However, the client is no longer there to receive the packet! The server will continue to timeout and retransmit the Ô¨Ånal Data packet until it gives up; it will never receive conÔ¨Årmation that the transfer succeeded. More generally, if A sends a message to B and B replies with an acknowledgment that is delivered to A, then A and B both are both certain the message has been delivered successfully. B is notsure, however, that A knows this. An alternative formulation of the lost-Ô¨Ånal-ACK problem is the two-generals problem. Two generals wish to agree on a time to attack, by exchanging messages. However, the generals must attack together, or not at all. Because some messages may be lost, neither side can ever be completely sure of the agreement. If the generals are Alice and Bob ( 28.5.1 Alice and Bob ), the messages might look like this: 
- Alice sends: Attack at noon 
- Bob replies: Agreed ( ieACK) After Bob receives Alice‚Äôs message, both sides know that a noon attack has been proposed. After Bob‚Äôs reply reaches Alice, both sides know that the message has been delivered. If Alice‚Äôs message was an order to Bob, this would be sufÔ¨Åcient. But if Alice and Bob must cooperate, this is not quite enough: at the end of the exchange above, Bob does not know that Alice has received his reply; Bob might thus hesitate, fearing Alice might not know he‚Äôs on board. Alice, aware of this possibility, might hesitate herself. Alice might attempt to resolve this by acknowledging Bob‚Äôs ACK: 
- Alice replies: Ok, we‚Äôre agreed on noon But at this point Alice does not know if Bob has received this message. If Bob does not, Bob might still hesitate. Not knowing, Alice too might hesitate. There is no end. See [AEH75]. Mathematically, there is no perfect solution to the two-generals problem; the generals can never be certain they are in complete agreement to attack. Suppose, to the contrary, that a sequence of messages didbring certainty of agreement to both Alice and Bob. Let M 1,. .. , M nbe the shortest possible such sequence; without loss of generality we may assume Alice sent M n. Now consider what happens if this Ô¨Ånal message is lost. From Alice‚Äôs perspective, there is no change at all, so Alice must still be certain Bob has agreed. However, the now-shorter sequence M 1,. .. , M n-1cannot also bring certainty to Bob, as this sequence has length less than n, the supposed minimum here. So Bob is notcertain, and so Alice‚Äôs certainty is misplaced. In engineering terms, however, the probability of a misunderstanding can often be made vanishingly small. Typically, if Alice does not receive Bob‚Äôs reply promptly, she will resend her message at regular timeout intervals, until she does receive an ACK. If Bob can count on this behavior, he can be reasonably sure that one of his ACKs must have made it back after enough time has elapsed. 378 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 For example, if Bob knows Alice will try a total of six times if she does not receive a response, and Bob only receives Alice‚Äôs Ô¨Årst two message instances, the fact that Alice appears to have stopped repeating her transmissions is reasonable evidence that she has received Bob‚Äôs response. Alternatively, if each message/ACK pair has a 10% probability of failure, and Bob knows that Alice will retry her message up to six times over the course of a day, then by the end of the day Bob can conclude that the probability that all six of his ACKs failed is at most (0.1)6, or one in a million. It is not necessary in this case that Bob actually keep count of Alice‚Äôs retry attempts. For a TCP example, see 17 TCP Transport Basics, exercise 4.0. TFTP addresses the lost-Ô¨Ånal-ACK problem by recommending (though not requiring) that the receiver enter into a DALLY state when it has sent the Ô¨Ånal ACK. In this state, the receiver responds only to duplicates of the Ô¨Ånal DATA packet; its response is to retransmit the Ô¨Ånal ACK. While one lost Ô¨Ånal ACK is possible, multiple such losses are unlikely; sooner or later the sender should receive the Ô¨Ånal ACK and will then exit. The dally state will expire after an interval. This interval should be at least twice the sender‚Äôs timeout interval, allowing for the sender to make three tries with the Ô¨Ånal data packet in all. Note that the receiver has no direct way to determine the sender‚Äôs timeout value. Note also that dallying only provides increased assurance, not certainty: it is possible that all Ô¨Ånal ACKs were lost. The TCP analogue of dallying is the TIMEWAIT state ( 18.2 TIMEWAIT ), though TIMEWAIT also has another role related to prevention of old duplicates. 16.3.3 Duplicated Connection Request We would also like to be able to distinguish between duplicated ( egretransmitted) connection requests and close but separate connection requests, especially when the second of two separate connection requests represents the cancellation of the Ô¨Årst. Here is an outline in TFTP terms of the scenario we are trying to avoid: 
- The client sends RRQ(‚Äúfoo‚Äù) 
- The client changes its mind, or aborts, or reboots, or whatever 
- The client sends RRQ(‚Äúbar‚Äù) 
- The server responds with Data[1] from the Ô¨Årst RRQ, that is, with Ô¨Åle ‚Äúfoo‚Äù, while the client is expecting Ô¨Åle ‚Äúbar‚Äù In correct TFTP operation, it is up to the client to send the second RRQ(‚Äúbar‚Äù) from a new port. As long as the client does that, changing its mind is not a problem. The server might end up sending Data[1] for Ô¨Åle ‚Äúfoo‚Äù off into the void ‚Äì that is, to the Ô¨Årst client port ‚Äì until it times out, as TFTP doesn‚Äôt have a cancellation message exactly. But the request for Ô¨Åle ‚Äúbar‚Äù should succeed normally. One minor issue is that, when a TFTP application terminates, it may not have preserved anywhere a record of the port it used last, and so may be unable to guarantee that a new port is different from those used previously. But both strategies of 16.3.1 Old Duplicate Packets ‚Äì choosing a port number at random, and having the operating system assign one ‚Äì are quite effective here. TFTP does run into a somewhat unexpected issue, however, when the client sends a duplicate RRQ; typically this happens when the Ô¨Årst RRQ times out. It is certainly possible to implement a TFTP server so as to recognize that the second RRQ is a duplicate, perhaps by noting that it is from the same client socket address and contains the same Ô¨Ålename. In practice, however, this is incompatible with the simpliÔ¨Åed 16.3 Fundamental Transport Issues 379
An Introduction to Computer Networks, Release 2.0.11 implementation approach of 16.2 Trivial File Transport Protocol, TFTP in which the server starts a new child process for each RRQ received. What most TFTP server implementations do in this case is to start twosender processes, one for each RRQ received, from two ports s_port1 and s_port2. Both will send Data[1] to the receiver. The receiver is expected to ‚Äúlatch on‚Äù to the port of the Ô¨Årst Data[1] packet it receives, recording its source port. The second Data[1] will now appear to be from an incorrect port. The TFTP speciÔ¨Åcation requires that a receiver reply to any packets from an unknown port by sending an ERROR packet with the code ‚ÄúUnknown Transfer ID‚Äù (where ‚ÄúTransfer ID‚Äù means ‚Äúport number‚Äù); this causes the sender process that sent the later-arriving Data[1] to shut down. The sender process that sent the winning Data[1] will continue normally. Were it not for this duplicate-RRQ scenario, packets from an unknown port could probably be simply ignored. It is theoretically possible for a malicious actor on the LAN to take advantage of this TFTP ‚Äúlatching on‚Äù behavior to hijack anticipated RRQs. If the actor is aware that host C is about to request a Ô¨Åle via TFTP, it might send repeated copies of bad Data[1] to likely ports on C. When C does request a Ô¨Åle, it may receive the malicious Ô¨Åle instead of what it asked for. Because the malicious application must guess the client‚Äôs port number, though, this scenario appears to be of limited importance. However, many diskless devices do load a boot Ô¨Åle on startup via TFTP, and may do so from a predictable port number. 16.3.4 Reboots Any ongoing communications protocol has to take into account the possibility that one side may reboot in between messages from the other side. The primary issue is detection of the reboot, so the other side can close the now-broken connection. If the sending side of a TFTP connection reboots, packet exchange simply stops, assuming a typical receiver that does not retransmit on timeouts. If the receiving side reboots, the sender will continue to send data packets, but will receive no further acknowledgments. In most cases, the newly rebooted client will simply ignore them. The second issue with reboots is that the rebooting system typically loses all memory of what ports it has used recently, making it difÔ¨Åcult to ensure that it doesn‚Äôt reuse recently active ports. This leads to some risk of old duplicates. Here is a scenario, based on the one at the start of the previous section, in which a client reboot leads to receipt of the wrong Ô¨Åle. Suppose the client sends RRQ(‚Äúfoo‚Äù), but then reboots before sending ACK[1]. After reboot, the client then sends RRQ(‚Äúbar‚Äù), from the same port; after the reboot the client will be unable to guarantee not reopening a recently used port. The server, having received the RRQ(‚Äúfoo‚Äù), belatedly proceeds to send Data[1] for ‚Äúfoo‚Äù. The client latches on to this, and accepts Ô¨Åle ‚Äúfoo‚Äù while believing it is receiving Ô¨Åle ‚Äúbar‚Äù. In practical terms, this scenario seems to be of limited importance, though ‚Äúdiskless‚Äù devices often do use TFTP to request their boot image Ô¨Åle when restarting, and so might be potential candidates. 16.4 Other TFTP notes We now take a brief look at other aspects of TFTP unrelated to the fundamental transport issues above. We include a brief outline of an implementation. 380 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 16.4.1 TFTP and the Sorcerer TFTP uses a very straightforward implementation of stop-and-wait ( 8.1 Building Reliable Transport: Stopand-Wait ). Acknowledgment packets contain the block number of the data packet being acknowledged; that is, ACK[N] acknowledges Data[N]. In the original RFC 783 speciÔ¨Åcation, TFTP was vulnerable to the Sorcerer‚Äôs Apprentice bug ( 8.1.2 Sorcerer‚Äôs Apprentice Bug ). Correcting this problem was the justiÔ¨Åcation for updating the protocol in RFC 1350, eleven years later. The omnibus hosts-requirements document RFC 1123 (referenced by RFC 1350 ) describes the necessary change this way: Implementations MUST contain the Ô¨Åx for this problem: the sender ( ie, the side originating the DATA packets) must never resend the current DATA packet on receipt of a duplicate ACK. 16.4.2 TFTP States The TFTP speciÔ¨Åcation is relatively informal; more recent protocols are often described using Ô¨Ånite-state terminology. In each allowable state, such a speciÔ¨Åcation spells out the appropriate response to all packets. We can apply this approach to TFTP as well. Above we deÔ¨Åned a DALLY state, for the receiver only, with a speciÔ¨Åc response to arriving Data[N] packets. There are two other important conceptual states for TFTP receivers, which we might call UNLATCHED and ESTABLISHED. When the receiver-client Ô¨Årst sends RRQ, it does not know the port number from which the sender will send packets. We will call this state UNLATCHED, as the receiver has not ‚Äúlatched on‚Äù to the correct port. In this state, the receiver waits until it receives a packet from the sender that looks like a Data[1] packet; that is, it is from the sender‚Äôs IP address, it has a plausible length, it is a DATA packet, and its block number is 1. When this packet is received, the receiver records s_port, and enters the ESTABLISHED state. Once in the ESTABLISHED state, the receiver veriÔ¨Åes for all packets that the source port number is s_port. If a packet arrives from some other port, the receiver sends back to its source an ERROR packet with ‚ÄúUnknown Transfer ID‚Äù, but continues with the original transfer. Here is an outline, in java, of what part of the TFTP receiver source code might look like; the code here handles the ESTABLISHED state. Somewhat atypically, the code here times out and retransmits ACK packets if no new data is received in the interval TIMEOUT; generally timeouts are implemented only at the TFTP sender side. Error processing is minimal, though error responses aresent in response to packets from the wrong port as described in the previous section. For most of the other error conditions checked for, there is no deÔ¨Åned TFTP response. The variables state ,sendtime ,TIMEOUT ,thePacket ,theAddress ,thePort ,blocknum and expected_block would need to have been previously declared and initialized; sendtime represents the time the most recent ACK response was sent. Several helper functions, such as getTFTPOpcode() andwrite_the_data(), would have to be deÔ¨Åned. The remote port thePort would be initialized at the time of entry to the ESTABLISHED state; this is the port from which a packet must have been sent if it is to be considered valid. The loop here transitions to the DALLY state when a packet marking the end of the data has been received. 16.4 Other TFTP notes 381
An Introduction to Computer Networks, Release 2.0.11 // TFTP code forESTABLISHED state while(state == ESTABLISHED) { // check elapsed time if(System.currentTimeMillis() > sendtime + TIMEOUT) { retransmit_most_recent_ACK() sendtime = System.currentTimeMillis() // receive the next packet try{ s.receive(thePacket); } catch (SocketTimeoutException stoe) { continue; } // tryagain catch (IOException ioe) { System.exit(1); } // other errors if(thePacket.getAddress() != theAddress) continue; if(thePacket.getPort() != thePort) { send_error_packet(...); // Unknown Transfer ID; see √£√ëtext continue; } if(thePacket.getLength() < TFTP_HDR_SIZE) continue; // TFTP_HDR_SIZE = √£√ë4 opcode = thePacket.getData().getTFTPOpcode() blocknum = thePacket.getData().getTFTPBlock() if(opcode != DATA) continue; if(blocknum != expected_block) continue; write_the_data(...); expected_block ++; send_ACK(...); // andsave it too forpossible retransmission sendtime = System.currentTimeMillis(); datasize = thePacket.getLength() - TFTP_HDR_SIZE; if(datasize < MAX_DATA_SIZE) state = DALLY; // MAX_DATA_SIZE = 512 } Note that the check for elapsed time is quite separate from the check for the SocketTimeoutException. It is possible for the receiver to receive a steady stream of ‚Äúwrong‚Äù packets, so that it never encounters a SocketTimeoutException, and yet no ‚Äúgood‚Äù packet arrives and so the receiver must still arrange (as above) for a timeout and retransmission. 16.4.3 TFTP Throughput On a single physical Ethernet, the TFTP sender and receiver would alternate using the channel, with very little ‚Äúturnaround‚Äù time; the effective throughput would be close to optimal. As soon as the store-and-forward delays of switches and routers are introduced, though, stop-and-wait becomes a performance bottleneck. Suppose that the path from sender A to receiver B passes through two switches: A‚ÄîS1‚ÄîS2‚ÄîB, and that on all three links only the bandwidth delay is signiÔ¨Åcant. Because ACK packets are so much smaller than DATA packets, we can effectively ignore the ACK travel time from B to A. With these assumptions, the throughput is about a third of the underlying bandwidth. This is because only 382 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 one of the three links can be active at any given time; the other two must be idle. We could improve throughput threefold by allowing A to send three packets at a time: 
- packet 1 from A to S1 
- packet 2 from A to S1 while packet 1 goes from S1 to S2 
- packet 3 from A to S1 while packet 2 goes from S1 to S2 and packet 1 goes from S2 to B This amounts to sliding windows with a winsize of three. TFTP does not support this; in the next chapter we study TCP, which does. 16.5 Remote Procedure Call (RPC) A very different communications model, usually but not always implemented over UDP, is that of Remote Procedure Call, or RPC. The name comes from the idea that a procedure call is being made over the network; host A packages up a request, with parameters, and sends it to host B, which returns a reply. The term request/reply protocol is also used for this. The side making the request is known as the client, and the other side the server. They‚Äôre not quite procedure calls The RPC name is not entirely a good Ô¨Åt. You can‚Äôt pass pointers as parameters, for example, and true procedure calls seldom time out. See [TR88] for more. But the name has stuck. One common example is that of DNS: a host sends a DNS lookup request to its DNS server, and receives a reply. Other examples include password veriÔ¨Åcation, system information retrieval, database queries and Ô¨Åle I/O (below). RPC is also quite successful as the mechanism for interprocess communication within CPU clusters, perhaps its most time-sensitive application. While TCP can be used for processes like these, this adds the overhead of creating and tearing down a connection; in many cases, the RPC exchange consists of nothing further beyond the request and reply and so the TCP overhead would be nontrivial. RPC over UDP is particularly well suited for transactions where both endpoints are quite likely on the same LAN, or are otherwise situated so that packet losses are negligible. One issue with the use of UDP is that any desired acknowledgements have to be implemented within the RPC layer. This is not terribly difÔ¨Åcult; usually the reply serves to acknowledge the request, and all that is needed is another ACK after that. If the protocol is run over a LAN, it is reasonable to use a static timeout period, perhaps somewhere in the range of 0.5 to 1.0 seconds. The diagram below includes an ACK. 16.5 Remote Procedure Call (RPC) 383
An Introduction to Computer Networks, Release 2.0.11 client server Request Reply ACK Perhaps surprisingly, some RPC protocols omit the Ô¨Ånal ACK; see 16.5.2 Sun RPC below. At a minimum, not having a Ô¨Ånal ACK means that if the reply is lost, the client has to start the sequence over, and the reply has to be regenerated from scratch. It is essential that requests and replies be numbered (or otherwise identiÔ¨Åed), so that the client can determine which reply matches which request. This also means that the reply can serve to acknowledge the request; if reply[N] is not received; the requester retransmits request[N]. This can happen either if request[N] never arrived, or if it was reply[N] that got lost: client server Request[N] Reply[N]client server Request[N] Reply[N]Request[N]timeout Lost Request Lost ReplyReply[N] Request[N]timeout When the server creates reply[N] and sends it to the client, it must also keep a cached copy of the reply, until such time as ACK[N] is received. After sending reply[N], the server may receive ACK[N], indicating all is well, or may receive request[N] again, indicating that reply[N] was lost, or may experience a timeout, indicating that either reply[N] or ACK[N] was lost. In the latter two cases, the server should retransmit reply[N] and wait again for ACK[N]. Finally, let us suppose that the server host delivers to its request-processing application the Ô¨Årst copy of each request[N] to arrive, and that neither side crashes (or otherwise loses state in the middle of any one request/reply/ACK sequence). Let us also assume that no packet reordering occurs, and every request[N], reply[N] or ACK[N], retransmitted often enough, eventually makes it to its destination. We then have exactly-once semantics: while requests may be transmitted multiple times, they are processed (or ‚Äúexecuted‚Äù) once and only once. 384 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 16.5.1 Network File System In terms of total packet volume, the application making the greatest use of early RPC was Sun‚Äôs Network File System, or NFS; this allowed for a Ô¨Ålesystem on the server to be made available to clients. When the client opened a Ô¨Åle, the server would send back a Ô¨Åle handle that typically included the Ô¨Åle‚Äôs identifying ‚Äúinode‚Äù number. For read() operations, the request would contain the block number for the data to be read, and the corresponding reply would contain the data itself; blocks were generally 8 kB in size. For write() operations, the request would contain the block of data to be written together with the block number; the reply would contain an acknowledgment that it was received. Usually an 8 kB block of data would be sent as a single UDP/IPv4 packet, using IPv4 fragmentation by the sender for transmission over Ethernet. 16.5.2 Sun RPC The original simple model above is quite serviceable. However, in the RPC implementation developed by Sun Microsystems and documented in RFC 1831 (and now ofÔ¨Åcially known as Open Network Computing, or ONC, RPC), the Ô¨Ånal acknowledgment was omitted. As there are relatively few packet losses on a LAN, this was not quite as serious as it might sound, but it did have a major consequence: the server could now not afford to cache replies, as it would never receive an indication that it was ok to delete them. Therefore, the request was re-executed upon receipt of a second request[N], as in the right-hand ‚Äúlost reply‚Äù diagram above. This was often described as at-least-once semantics: if a client sent a request, and eventually received a reply, the client could be sure that the request was executed at least once, but if a reply got lost then the request might be transmitted more than once. Applications, therefore, had to be aware that this was a possibility. It turned out that for many requests, duplicate execution of the reply was not a problem. A request that has the same result (and same side effects on the server) whether executed once or executed twice is known as idempotent. While a request to read or write the next block of a Ô¨Åle is not idempotent, a request to read or write block 37 (or any other speciÔ¨Åc block) isidempotent. Most data queries are also idempotent; a second query simply returns the same data as the Ô¨Årst. Even Ô¨Åle open() operations are idempotent, or at least can be implemented as such: if a Ô¨Åle is opened the second time, the Ô¨Åle handle is simply returned a second time. Alas, there do exist fundamentally non-idempotent operations. File locking is one, or any form of exclusive Ô¨Åle open. Creating a directory is another, because the operation must fail if the directory already exists. Even opening a Ô¨Åle is not idempotent if the server is expected to keep track of how many open() operations have been called, in order to determine if a Ô¨Åle is still in use. So why did Sun RPC take this route? One major advantage of at-least-once semantics is that it allowed the server to be stateless. The server would not need to maintain any RPC state, because without the Ô¨Ånal ACK there is no server RPC state to be maintained; for idempotent operations the server would generally not have to maintain any application state either. The practical consequence of this was that a server could crash and, because there was no state to be lost, could pick up right where it left off upon restarting. Statelessness Inaction 16.5 Remote Procedure Call (RPC) 385
An Introduction to Computer Networks, Release 2.0.11 Back when the Loyola CS department used Sun NFS extensively, server crashes would bring people calmly out of their ofÔ¨Åces to Ô¨Ånd out what had happened; client-workstation processes doing I/O would have locked up. Everyone would mill about in the hall until the server was rebooted, at which point they would return to their work and were almost always able to pick up where they left off. If the server had not been stateless, users would have been quite a bit less happy. It is, of course, also possible to build recovery mechanisms into stateful protocols. And for all that at-least-once semantics might sound like an egregious and obsolete shortcut, it does tend to be fast. Note, too, that the exactly-once protocol outlined in the Ô¨Ånal paragraph of 16.5 Remote Procedure Call (RPC) includes the requirement that neither side crashes. A few approaches to the crash-and-reboot problem are reviewed in 16.5.4 RPC ReÔ¨Ånements. The lack of Ô¨Åle-locking and other non-idempotent I/O operations, along with the rise of cheap clientworkstation storage (and, for that matter, more-reliable servers), eventually led to the decline of NFS over RPC, though it has not disappeared. NFS can, if desired, also be run (statefully!) over TCP. Sun RPC also includes a data-encoding standard known as eXternal Data Representation, or XDR, eventually standardized in RFC 1832. It describes a way of encoding standard data types as sequences of bytes, ready for transmission. Integer values, for example, are encoded in big-endian format. Data transmitted via XDR is nottagged with its type, unlike, for example, the encoding of 26.12 SNMP and ASN.1 Encoding. This means the sender and receiver have to agree on the precise parameter type signature for each RPC call. With Sun RPC this was typically managed with rpcgen, a tool which takes an XDR-compatible representation of the parameter types and generates code for parameter packing and unpacking. 16.5.3 Serial Execution In some RPC systems, even those with explicit ACKs, requests are executed serially by the server. Serial execution is a necessity if request[N+1] serves as an implicit ACK[N]. Serial execution is a problem for Ô¨Åle I/O operations, as physical disk drives are generally most efÔ¨Åcient when the I/O operations can be reordered to suit the geometry of the disk. Disk drives commonly use the elevator algorithm to process requests: the read head moves from low-numbered tracks outwards to high-numbered tracks, pausing at each track for which there is an I/O request. Waiting for the Nth read to complete before asking the disk to start the N+1th one is slow. The best solution here, from the server application‚Äôs perspective, is to allow multiple outstanding requests and out-of-order replies. This complicates the RPC protocol, however. 16.5.4 RPC ReÔ¨Ånements One basic network-level improvement to RPC concerns the avoidance of IP-level fragmentation. While fragmentation is not a major performance problem on a single LAN, it may have difÔ¨Åculties over longer distances. One possible reÔ¨Ånement is an RPC-level large-message protocol, that fragments at the RPC layer and which supports a mechanism for retransmission, if necessary, only of those fragments that are actually lost. Another optimization might address the possibility that the client or the server might crash and reboot. To detect client restarts we can add to the client side a ‚Äúboot counter‚Äù, incremented on each reboot and then 386 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 rewritten to persistent storage. This value is then included in each request, and echoed back in each reply and ACK. This allows the server to distinguish between requests sent before and after a client reboot; such requests are conceptually unrelated and the mechanism here ensures they receive different identiÔ¨Åers. See 27.3.3 SNMPv3 Engines for a related example. On the server side, allowing for crashes and reboots is even more complicated. If the goal is simply to make the client aware that the server may have rebooted during a request/reply sequence, we might include the server‚Äôs boot counter in each reply[N]; if the client sees a change, there may be a problem with the current request. We might also include the client‚Äôs current estimate of the server‚Äôs boot counter in each request, and have the server deny requests for which there is a mismatch. In exceptional cases, we can liken requests to database transactions and include on the server side a databasestyle crash-recovery journal. The goal is to allow the server, upon restarting, to identify requests that were in progress at the time of the crash, and either to roll them back or to complete them. This is not trivial, and can only be done for restricted classes of requests ( egreads and writes). 16.5.5 gRPC The ‚Äúg‚Äù here is for Google. gRPC was designed for request/reply operations where the requests or replies may be large and complex, or where the client and the server are noton the same LAN; it is well suited for end-user requests to large servers. The underlying transport is TCP. More speciÔ¨Åcally, data is sent each way using HTTP/2, which has support for multiple data streams (though eventually gRPC seems likely to migrate to QUIC-based HTTP/3, which uses UDP ( 16.1.1 QUIC )). The use of TLS ( 29.5.2 TLS ) is also supported, for authentication and encryption; these are essential for long-haul connections but are less so within a datacenter. gRPC is also well-suited for cases ‚Äì even within a datacenter ‚Äì where requests are not idempotent and where lost responses could be serious. gRPC in effect focuses on the encoding portion of RPC; this is the part of SunRPC handled by XDR. It supports streamed data, though, which XDR does not. 16.5.6 Homa Homa ([MLAO18]), on the other hand, is notmeant for long-distance communications. It is intended to be a very high-performance RPC implementation for use exclusively within datacenters, generally where requests and replies are relatively small. The primary design goal is the minimization of latency. At 10 Gbps, one full-sized packet can be transmitted in about 1.2 ¬µsec, and the implementation of [MLAO18] achieves, at 80% network load, 15 ¬µsec delivery times for 99% of requests. Fast datacenter RPC is a very active research area, and Homa has quite a few predecessors. Among these are pHost ([GNKARS15]), pFabric ([AYSKMP13], and which requires special switches), and FastPass ([POBSF14], which requires a central scheduler). Queuing delay is the largest delay culprit here, and Homa addresses this by explicitly setting the Ethernet VLAN priority Ô¨Åeld for packets ( 3.2 Virtual LAN (VLAN) ). There are eight priority levels available. Homa adjusts packet priorities with the goal of giving the highest priority to responses that have the fewest remaining packets; this is known as Shortest Remaining Processing Time (SRPT) Ô¨Årst. Senders of data responses send the Ô¨Årst chunk of data (typically up to 10 KB) ‚Äúblindly‚Äù; that is, without a receiver-supplied priority. Blind transmission does not mean default priority, though; receivers continually 16.5 Remote Procedure Call (RPC) 387
An Introduction to Computer Networks, Release 2.0.11 monitor current trafÔ¨Åc conditions and piggyback their recommendations for blind-trafÔ¨Åc priority on other Homa trafÔ¨Åc. This Ô¨Årst data chunk also includes information about the total size of the data. After sending the Ô¨Årst chunk, a sender waits for a ‚ÄúGRANT‚Äù message from the receiver, which includes the receiver‚Äôs chosen priorities for the remaining data. The receiver sets the priorities because, in a typical datacenter, queues form primarily at the so-called ‚Äútop of rack‚Äù switches nearest to the receiver, and so it is the receiver that is best positioned to manage these queues. Homa‚Äôs use of priorities ‚Äì both blind and GRANT ‚Äì are what allow Homa data to leapfrog larger, non-Homa data Ô¨Çows. The use of priorities also largely prevents the ‚Äúincast‚Äù congestion problem (see 22.13.1 TCP Incast ) when a host sends out multiple requests and receives the corresponding replies all at the same time. For Sun NFS with 8 KB data blocks, blind priorities would be used frequently, though GRANT priorities would still come into play for multi-block messages. For message sizes in the range 10 KB - 1 MB, egthe Hadoop example of [MLAO18], most data would be transmitted under the aegis of GRANT priorities. In accordance with the SRPT strategy, the priorities for the packets of a large message would steadily increase as the message was transmitted. Homa, like SunRPC, does not support Ô¨Ånal acknowledgments of data; if a request is made and allthe response data packets are lost, then when the request is retransmitted the response will be evaluated again from scratch. This results in ‚Äúat-least-once‚Äù semantics, but signiÔ¨Åcantly simpliÔ¨Åes the overall protocol and tends thereby to improve throughput. However, if individual packets are lost, the receiver sends a RESEND message for the missing byte range. The sender will most likely not have cached the data, because, as with SunRPC, without a Ô¨Ånal ACK it cannot know when to delete the cached response, and so the response will again be evaluated from scratch. In sending GRANT requests, receivers engage in carefully calculated ‚Äúovercommitment‚Äù; that is, receivers grant more data transmissions than can be delivered immediately without queuing. This is because senders are not always able to send more data immediately, typically because they may also be in the process of sending other data to other receivers. 16.6 Epilog UDP does not get as much attention as TCP, but between avoidance of connection-setup overhead, avoidance of head-of-line blocking and high LAN performance, it holds its own. We also use UDP here to illustrate fundamental transport issues, both abstractly and for the speciÔ¨Åc protocol TFTP. We will revisit these fundamental issues extensively in the next chapter in the context of TCP; these issues played a major role in TCP‚Äôs design. 16.7 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.13 Solutions for UDP. 1.0. Perform the UDP simplex-talk experiments discussed at the end of 16.1.3 UDP Simplex-Talk. Can multiple clients have simultaneous sessions with the same server? 2.0. Suppose that both sides of a TFTP transfer implement retransmit-on-timeout and neither side implements retransmit-on-duplicate. What would happen in each of the following cases if the Ô¨Årst Data[3] packet 388 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 is lost? (a)‚ô¢. Sender timeout = receiver timeout = 2 seconds. (b). Sender timeout = 1 second; receiver timeout = 3 seconds. (c). Sender timeout = 3 seconds; receiver timeout = 1 second. Assume the actual transfer time is negligible in comparison to the timeout intervals, and that the retransmitted Data[3] is received successfully. 3.0. In the previous exercise, how do things change if the Ô¨Årst ACK[3] is the packet that is lost? 4.0. For each state below, spell out plausible responses for a TFTP receiver upon receipt of a Data[N] packet. Your answers may depend on N and the packet size. Indicate the events that cause a transition from one state to the next. The TFTP states were proposed in 16.4.2 TFTP States. (a). UNLATCHED (b). ESTABLISHED (c). DALLYING Example: upon receipt of an ERROR packet, TFTP would in all three states exit. 5.0. In the TFTP-receiver code in 16.4.2 TFTP States, explain why we must check thePacket. getLength() before extracting the opcode and block number. 6.0. Assume both the TFTP sender and the TFTP receiver implement retransmit-on-timeout but not retransmit-on-duplicate. Outline a speciÔ¨Åc TFTP scenario in which the TFTP receiver of 16.4.2 TFTP States sets a socket timeout interval but never encounters a ‚Äúhard‚Äù timeout ‚Äì that is, aSocketTimeoutException ‚Äì and yet must timeout and retransmit. Hint: arrange so the sender regularly times out and retransmits some packet, at an interval less than the receiver‚Äôs SocketTimeoutException time, but it is not the packet the receiver is waiting for. 7.0. At the end of 16.3.1 Old Duplicate Packets, we claimed that if either side in the TFTP protocol changed ports, the old-duplicate problem would not occur. (a). If the client (receiver) changes its port number on a subsequent connection, but the server (sender) does not, what prevents an old-duplicate data packet sent by the server from being accepted by the new client? (b). If the server changes its port number on a subsequent connection, but the client does not, what prevents an old-duplicate DATA[N] packet, with N>1, sent by the server from being accepted by the new client? 8.0. In part (b) of the previous exercise, it was claimed that an old-duplicate DATA[N] could not be accepted as valid by the new receiver provided N>1. Give an example in which an old-duplicate DATA[1] isaccepted as valid. 16.7 Exercises 389
An Introduction to Computer Networks, Release 2.0.11 9.0. Suppose a TFTP server implementation resends DATA[N] on receipt of a duplicate ACK[N-1], contrary to16.4.1 TFTP and the Sorcerer. It receives a Ô¨Åle request from a partially implemented TFTP client, that sends ACK[1] to the correct new port but then never increments the ACK number; the client‚Äôs response to DATA[N] is always ACK[1]. What happens? (Based on a true story.) 10.0. In the simple RPC protocol at the beginning of 16.5 Remote Procedure Call (RPC), suppose that the server sends reply[N] and experiences a timeout, receiving nothing back from the client. In the text we suggested that most likely this meant ACK[N] was lost. Give another loss scenario, involving the loss of two packets. Assume the client and the server have the same timeout interval. 11.0. Suppose a Sun RPC read() request ends up executing twice. Unfortunately, in between successive read() operations the block of data is updated by another process, so different data is returned. Is this a failure of idempotence? Why or why not? 12.0. Outline an RPC protocol in which multiple requests can be outstanding, and replies can be sent in any order. Assume that requests are numbered, and that ACK[N] acknowledges reply[N]. Should ACKs be cumulative? If not, what should happen if an ACK is lost? 13.0. Consider the request[N]/reply[N]/ACK[N] protocol of 16.5 Remote Procedure Call (RPC), under the assumption that requests are numbered sequentially, but packets may potentially be delivered out of order. Thus, request[5] may arrive again after ACK[5] has been sent. and the Ô¨Årst request[5] may even arrive after ACK[6] has been sent. (a). If requests are handled serially, as in 16.5.3 Serial Execution, what information does the server side need to maintain in order to avoid duplicate execution of a request? (b). What information does the server side need to maintain if requests are handled non-serially, that is, there can be multiple outstanding requests at any one time? Assume that if request[6] arrives before request[5], the server responds with reply[6] even though there is now a temporary gap in sequence numbers. 14.0. Suppose an RPC client maintains a boot counter as in 16.5.4 RPC ReÔ¨Ånements. Draw diagrams for cases (a) and (b), and indicate how the boot counter is used to resolve the situation. (a). The client sends request[N], but reboots before reply[N] is received. (b). The client sends request[N], and then immediately reboots and sends an unrelated request that just happens also to be numbered N. (c). What would happen in the scenario in part (b) if the reply[N] packet did notecho back the boot-counter value from the request[N] packet? 15.0. In this exercise we explore UDP connection state using netcat (16.1.4 netcat ). Let A and B be two hosts (not necessarily distinct!). (a). Verify that you can exchange messages between A and B after starting the following; -uis for UDP and-lis to create the server side (to ‚Äúlisten‚Äù). 390 16 UDP Transport
An Introduction to Computer Networks, Release 2.0.11 In a terminal on B: netcat -u -l 5432 In a terminal on A: netcat -u B 5432 (b). Now kill the netcat on A and restart it. A different local port is likely chosen by the second netcat; verify that communication fails. (c). Now repeat the process, but this time in addition specify the source port on A with the -poption: In a terminal on B: netcat -u -l 5432 In a terminal on A: netcat -u -p 2345 B 5432 Verify that killing and restarting the client on A allows communication to continue. 16.0. In this exercise we explore sending UDP packets through NAT routers ( 9.7 Network Address Translation ), usingnetcat (16.1.4 netcat ). Let A be an internal host, NR the public IP address of the NAT router, and C an outside host. We will initiate all connections by having A send to C at port 5432, which must not be Ô¨Årewalled (changing to a different port is straightforward). (a). Verify that you can send from A to C: In a terminal on C: netcat -u -l 5432 In a terminal on A: netcat -u C 5432 If this does not work, try changing port numbers or C‚Äôs Ô¨Årewall settings. (b). Try typing text into the terminal on C; netcat supports bidirectional communication. Does the output appear on A? (c). Through experimentation, estimate the allowable delay between the A-to-C packets and the C-to-A response. 1 minute? 5 minutes? 10 minutes? (d). Try to transmit the reply from C using an entirely separate pair of netcat sessions. For this to have any chance of working, A‚Äôs source port must be known; we will set it here to 40001. On C: as above On A:netcat -u -p 40001 C 5432 As soon as data has been transmitted successfully from A to C, try the reverse path. Both A-to-C netcat processes, above, must Ô¨Årst be terminated, to free the ports. Then: On A:netcat -u -l 40001 On C:netcat -u -p 5432 NR 40001 16.7 Exercises 391
An Introduction to Computer Networks, Release 2.0.11 392 16 UDP Transport
17 TCP TRANSPORT BASICS The standard transport protocols riding above the IP layer are TCP andUDP. As we saw in 16 UDP Transport, UDP provides simple datagram delivery to remote sockets, that is, to xhost,portypairs. TCP provides a much richer functionality for sending data to (connected) sockets. In this chapter we cover the basic TCP protocol; in the following chapter we cover some subtle issues related to potential data loss, some TCP implementation details, and then some protocols that serve as alternatives to TCP. TCP is quite different in several dimensions from UDP. TCP is stream-oriented, meaning that the application can write data in very small or very large amounts and the TCP layer will take care of appropriate packetization (and also that TCP transmits a stream of bytes, not messages or records; cf 18.15.2 SCTP ). TCP is connection-oriented, meaning that a connection must be established before the beginning of any data transfer. TCP is reliable, in that TCP uses sequence numbers to ensure the correct order of delivery and a timeout/retransmission mechanism to make sure no data is lost short of massive network failure. Finally, TCP automatically uses the sliding windows algorithm to achieve throughput relatively close to the maximum available. These features mean that TCP is very well suited for the transfer of large Ô¨Åles. The two endpoints open a connection, the Ô¨Åle data is written by one end into the connection and read by the other end, and the features above ensure that the Ô¨Åle will be received correctly. TCP also works quite well for interactive applications where each side is sending and receiving streams of small packets. Examples of this include ssh or telnet, where packets are exchanged on each keystroke, and database connections that may carry many queries per second. TCP even works reasonably well for request/reply protocols, where one side sends a single message, the other side responds, and the connection is closed. The drawback here, however, is the overhead of setting up a new connection for each request; a better application-protocol design might be to allow multiple request/reply pairs over a single TCP connection. Note that the connection-orientation and reliability of TCP represent abstract features built on top of the IP layer, which supports neither of them. The connection-oriented nature of TCP warrants further explanation. With UDP, if a server opens a socket (the OS object, with corresponding socket address), then any client on the Internet can send to that socket, via its socket address. Any UDP application, therefore, must be prepared to check the source address of each packet that arrives. With TCP, all data arriving at a connected socket must come from the other endpoint of the connection. When a server S initially opens a socket s, that socket is ‚Äúunconnected‚Äù; it is said to be in the LISTEN state. While it still has a socket address consisting of its host and port, a LISTENing socket will never receive data directly. If a client C somewhere on the Internet wishes to send data to s, it must Ô¨Årst establish a connection, which will be deÔ¨Åned by the socketpair consisting of the socket addresses (that is, thexIP_addr,portypairs) at both C and S. As part of this connection process, a new connected child socket s Cwill be created; it is s Cthat will receive any data sent from C. Usually, the server will also create a new thread or process to handle communication with s C. Typically the server will have multiple connected children of the original socket s, and, for each one, a process attached to it. If C1 and C2 both connect to s, two connected sockets at S will be created, s 1and s 2, and likely two separate processes. When a packet arrives at S addressed to the socket address of s, the source socket address will also be examined to determine whether the data is part of the C1‚ÄìS or the C2‚ÄìS connection, and thus whether a read on s 1or on s 2, respectively, will see the data. 393
An Introduction to Computer Networks, Release 2.0.11 If S is acting as an ssh server, the LISTENing socket listens on port 22, and the connected child sockets correspond to the separate user login connections; the process on each child socket represents the login process of that user, and may run for hours or days. In we likened TCP sockets to telephone connections, with the server like one high-volume phone number 800-BUY-NOWW. The unconnected socket corresponds to the number everyone dials; the connected sockets correspond to the actual calls. (This analogy breaks down, however, if one looks closely at the way such multi-operator phone lines are actually conÔ¨Ågured: each typically does have its own number.) TCP was originally deÔ¨Åned in RFC 793, with important updates in RFC 1122, dated October 1989. Since then there have been many miscellaneous updates; all of these have now been incorporated into a single speciÔ¨Åcation RFC 9293. 17.1 The End-to-End Principle The End-to-End Principle is spelled out in [SRC84]; it states in effect that transport issues are the responsibility of the endpoints in question and thus should not be delegated to the core network. This idea has been very inÔ¨Çuential in TCP design. Two issues falling under this category are data corruption and congestion. For the Ô¨Årst, even though essentially all links on the Internet have link-layer checksums to protect against data corruption, TCP still adds its own checksum (in part because of a history of data errors introduced within routers). For the latter, TCP is today essentially the only layer that addresses congestion management. Saltzer, Reed and Clark categorized functions that were subject to the End-to-End principle this way: The function in question can completely and correctly be implemented only with the knowledge and help of the application standing at the end points of the communication system. Therefore, providing that questioned function as a feature of the communication system itself is not possible. (Sometimes an incomplete version of the function provided by the communication system may be useful as a performance enhancement.) This does not mean that the backbone Internet should not concern itself with congestion; it means that backbone congestion-management mechanisms should not completely replace end-to-end congestion management. 17.2 TCP Header Below is a diagram of the TCP header. As with UDP, source and destination ports are 16 bits. The 4-bit Data Offset Ô¨Åeld speciÔ¨Åes the number of 32-bit words in the header; if no options are present its value is 5. As with UDP, the checksum covers the TCP header, the TCP data and an IP ‚Äúpseudo header‚Äù that includes the source and destination IP addresses. The checksum must be updated by a NAT router that modiÔ¨Åes any header values. (Although the IP and TCP layers are theoretically separate, and RFC 793 in some places appears to suggest that TCP can be run over a non-IP internetwork layer, RFC 793 also explicitly deÔ¨Ånes 4-byte addresses for the pseudo header. RFC 2460 ofÔ¨Åcially redeÔ¨Åned the pseudo header to allow IPv6 addresses.) 394 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 Source Port Destination Port Sequence Number Acknowledgment Number Window Size Urgent Pointer Padding OptionsChecksum Data OffsetReservedC W RE C EU R GS Y NP S HA C KR S TF I N0 8 16 24 32 The sequence andacknowledgment numbers are for numbering the data, at the byte level. This allows TCP to send 1024-byte blocks of data, incrementing the sequence number by 1024 between successive packets, or to send 1-byte telnet packets, incrementing the sequence number by 1 each time. There is no distinction between DATA and ACK packets; all packets carrying data from A to B also carry the most current acknowledgment of data sent from B to A. Many TCP applications are largely unidirectional, in which case the sender would include essentially the same acknowledgment number in each packet while the receiver would include essentially the same sequence number. It is traditional to refer to the data portion of TCP packets as segments. TCP History The clear-cut division between the IP and TCP headers did not spring forth fully formed. See [CK74] for a discussion of a proto-TCP in which the sequence number (but not the acknowledgment number) appeared in the equivalent of the IP header (perhaps so it could be used for fragment reassembly). The value of the sequence number, in relative terms, is the position of the Ô¨Årst byte of the packet in the data stream, or the position of what would be the Ô¨Årst byte in the case that no data was sent. The value of the acknowledgment number, again in relative terms, represents the byte position for the next byte expected. Thus, if a packet contains 1024 bytes of data and the Ô¨Årst byte is number 1, then that would be the sequence number. The data bytes would be positions 1-1024, and the ACK returned would have acknowledgment number 1025. The sequence and acknowledgment numbers, as sent, represent these relative values plus anInitial Sequence Number, or ISN, that is Ô¨Åxed for the lifetime of the connection. Each direction of a connection has its own ISN; see below. TCP acknowledgments are cumulative: when an endpoint sends a packet with an acknowledgment number of N, it is acknowledging receipt of all data bytes numbered less than N. Standard TCP provides no mechanism for acknowledging receipt of packets 1, 2, 3 and 5; the highest cumulative acknowledgment that could be sent in that situation would be to acknowledge packet 3. The TCP header deÔ¨Ånes some important Ô¨Çag bits; the brief deÔ¨Ånitions here are expanded upon in the sequel: 
- SYN: for SYNchronize; marks packets that are part of the new-connection handshake 17.2 TCP Header 395
An Introduction to Computer Networks, Release 2.0.11 
- ACK: indicates that the header Acknowledgment Ô¨Åeld is valid; that is, all but the Ô¨Årst packet 
- FIN: for FINish; marks packets involved in the connection closing 
- PSH: for PuSH; marks ‚Äúnon-full‚Äù packets that should be delivered promptly at the far end 
- RST: for ReSeT; indicates various error conditions 
- URG: for URGent; part of a now-seldom-used mechanism for high-priority data 
- CWR andECE: part of the Explicit Congestion NotiÔ¨Åcation mechanism, 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN) 17.3 TCP Connection Establishment TCP connections are established via an exchange known as the three-way handshake. If A is the client and B is the LISTENing server, then the handshake proceeds as follows: 
- A sends B a packet with the SYN bit set (a SYN packet) 
- B responds with a SYN packet of its own; the ACK bit is now also set 
- A responds to B‚Äôs SYN with its own ACK A B SYN SYN+ACK ACK TCP three-way handshake Normally, the three-way handshake is triggered by an application‚Äôs request to connect; data can be sent only after the handshake completes. This means a one-RTT delay before any data can be sent. The original TCP standard RFC 793 does allow data to be sent with the Ô¨Årst SYN packet, as part of the handshake, but such data cannot be released to the remote-endpoint application until the handshake completes. Most traditional TCP programming interfaces offer no support for this early-data option. There are recurrent calls for TCP to support data transmission within the handshake itself, so as to achieve request/reply turnaround comparable to that with RPC ( 16.5 Remote Procedure Call (RPC) ). We return to this in 18.5 TCP Faster Opening. The three-way handshake is vulnerable to an attack known as SYN Ô¨Çooding. The attacker sends a large number of SYN packets to a server B. For each arriving SYN, B must allocate resources to keep track of what appears to be a legitimate connection request; with enough requests, B‚Äôs resources may face exhaustion. SYN Ô¨Çooding is easiest if the SYN packets are simply spoofed, with forged, untraceable source-IP addresses; see spooÔ¨Ång at 9.1 The IPv4 Header, and 18.3.1 ISNs and spooÔ¨Ång below. SYN-Ô¨Çood attacks can also take the form of a large number of real connection attempts from a large number of real clients ‚Äì often compromised and pressed into service by some earlier attack ‚Äì but this requires considerably more resources 396 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 on the part of the attacker. See 18.15.2 SCTP for an alternative handshake protocol (unfortunately not available to TCP) intended to mitigate SYN-Ô¨Çood attacks, at least from spoofed SYNs. Toclose the connection, a superÔ¨Åcially similar exchange involving FIN packets may occur: 
- A sends B a packet with the FIN bit set (a FIN packet), announcing that it has Ô¨Ånished sending data 
- B sends A an ACK of the FIN 
- B may continue to send additional data to A 
- When B is also ready to cease sending, it sends its own FIN to A 
- A sends B an ACK of the FIN; this is the Ô¨Ånal packet in the exchange Here‚Äôs the ladder diagram for this: A B FIN ACK FIN ACKMore DATA (optional) A typical TCP close The FIN handshake is really more like two separate two-way FIN/ACK handshakes. We will return to TCP connection closing in 17.8.1 Closing a connection. Now let us look at the full exchange of packets in a representative connection, in which A sends strings ‚Äúabc‚Äù, ‚Äúdefg‚Äù, and ‚Äúfoobar‚Äù ( RFC 3092 ). B replies with ‚Äúhello‚Äù, and which point A sends ‚Äúgoodbye‚Äù and closes the connection. In the following table, relative sequence numbers are used, which is to say that sequence numbers begin with 0 on each side. The SEQ numbers in bold on the A side correspond to the ACK numbers in bold on the B side; they both count data Ô¨Çowing from A to B. 17.3 TCP Connection Establishment 397
An Introduction to Computer Networks, Release 2.0.11 A sends B sends 1 SYN, seq=0 2 SYN+ACK, seq=0, ack=1 (expecting) 3 ACK, seq=1, ack=1 (ACK of SYN) 4 ‚Äúabc‚Äù, seq=1, ack=1 5 ACK, seq=1, ack=4 6 ‚Äúdefg‚Äù, seq=4, ack=1 7 seq=1, ack=8 8 ‚Äúfoobar‚Äù, seq=8, ack=1 9 seq=1, ack=14, ‚Äúhello‚Äù 10 seq=14, ack=6, ‚Äúgoodbye‚Äù 11,12 seq=21, ack=6, FIN seq=6, ack=21 ;; ACK of ‚Äúgoodbye‚Äù, crossing packets 13 seq=6, ack=22 ;; ACK of FIN 14 seq=6, ack=22, FIN 15 seq=22, ack=7 ;; ACK of FIN (We will see below that this table is slightly idealized, in that real sequence numbers do notstart at 0.) Here is the ladder diagram corresponding to this connection: A B SYN SYN+ACK ACK ‚Äúabc‚Äù ACK ‚Äúdefg‚Äù ACK ‚Äúfoobar‚Äù ‚Äúhello‚Äù ‚Äúgoodbye‚Äù FIN ACK FIN ACKACKCrossing packets In terms of the sequence and acknowledgment numbers, SYNs count as 1 byte, as do FINs. Thus, the SYN counts as sequence number 0, and the Ô¨Årst byte of data (the ‚Äúa‚Äù of ‚Äúabc‚Äù) counts as sequence number 1. Similarly, the ack=21 sent by the B side is the acknowledgment of ‚Äúgoodbye‚Äù, while the ack=22 is the 398 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 acknowledgment of A‚Äôs subsequent FIN. Whenever B sends ACN=n, A follows by sending more data with SEQ=n. TCP does notin fact transport relative sequence numbers, that is, sequence numbers as transmitted do not begin at 0. Instead, each side chooses its Initial Sequence Number, orISN, and sends that in its initial SYN. The third ACK of the three-way handshake is an acknowledgment that the server side‚Äôs SYN response was received correctly. All further sequence numbers sent are the ISN chosen by that side plus the relative sequence number (that is, the sequence number as if numbering did begin at 0). If A chose ISN A=1000, we would add 1000 to all the bold entries above: A would send SYN(seq=1000), B would reply with ISN Band ack=1001, and the last two lines would involve ack=1022 and seq=1022 respectively. Similarly, if B chose ISN B=7000, then we would add 7000 to all the seqvalues in the ‚ÄúB sends‚Äù column and all the ackvalues in the ‚ÄúA sends‚Äù column. The table above up to the point B sends ‚Äúgoodbye‚Äù, with actual sequence numbers instead of relative sequence numbers, is below: A, ISN= 1000 B, ISN=7000 1 SYN, seq=1000 2 SYN+ACK, seq=7000, ack=1001 3 ACK, seq=1001, ack=7001 4 ‚Äúabc‚Äù, seq=1001, ack=7001 5 ACK, seq=7001, ack=1004 6 ‚Äúdefg‚Äù, seq=1004, ack=7001 7 seq=7001, ack=1008 8 ‚Äúfoobar‚Äù, seq=1008, ack=7001 9 seq=7001, ack=1014, ‚Äúhello‚Äù 10 seq=1014, ack=7006, ‚Äúgoodbye‚Äù If B had not been LISTENing at the port to which A sent its SYN, its response would have been RST (‚Äúreset‚Äù), meaning in this context ‚Äúconnection refused‚Äù. Similarly, if A sent data to B before the SYN packet, the response would have been RST. Finally, a RST can be sent by either side at any time to abort the connection. Sometimes routers along the path send ‚Äúspoofed‚Äù RSTs to tear down TCP connections they are conÔ¨Ågured to regard as undesired; see9.7.2 Middleboxes andRFC 3360. Worse, sometimes external attackers are able to tear down a TCP connection with a spoofed RST; this requires brute-force guessing the endpoint port numbers and the RST sender‚Äôs current SEQ value ( RFC 793 does not in general require the RST packet‚Äôs ACK value to match, but see exercise 9.0). In the days of 4 kB window sizes, guessing a valid SEQ value was a one-in-a-million chance, but window sizes have steadily increased ( 21.6 The High-Bandwidth TCP Problem ); a 4 MB window size makes SEQ guessing quite feasible. See also RFC 4953 and the RST-validation Ô¨Åx proposed inRFC 5961 ¬ß3.2. If A sends a series of small packets to B, then B has the option of assembling them into a full-sized I/O buffer before releasing them to the receiving application. However, if A sets the PSH bit on each packet, then B should release each packet immediately to the receiving application. In Berkeley Unix and most (if not all) BSD-derived socket-library implementations, there is in fact no way to set the PSH bit; it is set automatically for each write. (But even this is not guaranteed as the sender may leave the bit off or consolidate several PuSHed writes into one packet; this makes using the PSH bit as a record separator difÔ¨Åcult. In a series of runs of the program written to generate the WireShark packet trace, below, most of the time the strings ‚Äúabc‚Äù, ‚Äúdefg‚Äù, etcwere PuSHed separately but occasionally they were consolidated into one packet.) 17.3 TCP Connection Establishment 399
An Introduction to Computer Networks, Release 2.0.11 As for the URG bit, imagine a telnet (or ssh) connection, in which A has sent a large amount of data to B, which is momentarily stalled processing it. The application at A wishes to abort that processing by sending the interrupt character CNTL-C. Under normal conditions, the application at B would have to Ô¨Ånish processing all the pending data before getting to the CNTL-C; however, the use of the URG bit can enable immediate asynchronous delivery of the CNTL-C. The bit is set, and the TCP header‚Äôs Urgent Pointer Ô¨Åeld points to the CNTL-C in the current packet, far ahead in the normal data stream. The receiving application then skips ahead in its processing of the arriving data stream until it reaches the urgent data. For this to work, the receiving application process must have signed up to receive an asynchronous signal when urgent data arrives. The urgent data does appear as part of the ordinary TCP data stream, and it is up to the protocol to determine the start of the data that is to be considered urgent, and what to do with the unread, buffered data sent ahead of the urgent data. For the CNTL-C example in the telnet protocol ( RFC 854 ), the urgent data might consist of the telnet ‚ÄúInterrupt Process‚Äù byte, preceded by the ‚ÄúInterpret as Command‚Äù escape byte, and the earlier data is simply discarded. OfÔ¨Åcially, the Urgent Pointer value, when the URG bit is set, contains the offset from the start of the current packet data to the endof the urgent data; it is meant to tell the receiver ‚Äúyou should read up to this point as soon as you can‚Äù. The original intent was for the urgent pointer to mark the last byte of the urgent data, but ¬ß3.1 of RFC 793 got this wrong and declared that it pointed to the Ô¨Årst byte following the urgent data. This was corrected in RFC 1122, but most implementations to this day abide by the ‚Äúincorrect‚Äù interpretation. RFC 6093 discusses this and proposes, Ô¨Årst, that the near-universal ‚Äúincorrect‚Äù interpretation be accepted as standard, and, second, that developers avoid the use of the TCP urgent-data feature. 17.4 TCP and WireShark Below is a screenshot of the WireShark program displaying a tcpdump capture intended to represent the TCP exchange above. Both hosts involved in the packet exchange were Linux systems. Side A uses socket addressx10.0.0.3,45815 yand side B (the server) uses x10.0.0.1,54321 y. WireShark is displaying relative TCP sequence numbers. The Ô¨Årst three packets correspond to the threeway handshake, and packet 4 is the Ô¨Årst data packet. Every data packet has the Ô¨Çags [PSH, ACK] displayed. The data in the packet can be inferred from the WireShark Len Ô¨Åeld, as each of the data strings sent has a different length. 400 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 The packets are numbered the same as in the table above up through packet 8, containing the string ‚Äúfoobar‚Äù. At that point the table shows B replying by a combined ACK plus the string ‚Äúhello‚Äù; in fact, TCP sent the ACK alone and then the string ‚Äúhello‚Äù; these are WireShark packets 9 and 10 (note packet 10 has Len=5). Wireshark packet 11 is then a standalone ACK from A to B, acknowledging the ‚Äúhello‚Äù. WireShark packet 12 (the packet highlighted) then corresponds to table packet 10, and contains ‚Äúgoodbye‚Äù (Len=7); this string can be seen at the right side of the bottom pane. The table view shows A‚Äôs FIN (packet 11) crossing with B‚Äôs ACK of ‚Äúgoodbye‚Äù (packet 12). In the WireShark view, A‚Äôs FIN is packet 13, and is sent about 0.01 seconds after ‚Äúgoodbye‚Äù; B then ACKs them both with packet 14. That is, the table-view packet 12 does not exist in the WireShark view. Packets 11, 13, 14 and 15 in the table and 13, 14, 15 and 16 in the WireShark screen dump correspond to the connection closing. The program that generated the exchange at B‚Äôs side had to include a ‚Äúsleep‚Äù delay of 40 ms between detecting the closed connection (that is, reading A‚Äôs FIN) and closing its own connection (and sending its own FIN); otherwise the ACK of A‚Äôs FIN traveled in the same packet with B‚Äôs FIN. The ISN for A in this example was 551144795 and B‚Äôs ISN was 1366676578. The actual pcap packetcapture Ô¨Åle is at demo_tcp_connection.pcap. This pcap Ô¨Åle was generated by a TCP connection between two physical machines; for an alternative approach to observing TCP behavior see 30.2.2 Mininet WireShark Demos. 17.4 TCP and WireShark 401
An Introduction to Computer Networks, Release 2.0.11 17.5 TCP OfÔ¨Çoading In the Wireshark example above, the hardware involved used TCP checksum ofÔ¨Çoading, or TCO, to have the network-interface card do the actual checksum calculations; this permits a modest amount of parallelism. As a result, the checksums for outbound packets are wrong in the capture Ô¨Åle. WireShark has an option to disable the reporting of this. Despite the name, TCO can handle UDP packets as well. Most Ethernet (and some Wi-Fi) cards have the ability to calculate the Internet checksum ( 7.4 Error Detection ) over a certain range of bytes, and store the result (after taking the complement) at a designated offset. However, cards cannot, as a rule, handle the UDP and TCP ‚Äúpseudo headers‚Äù. So what happens is the host system calculates the pseudo-header checksum and stores it in the normal checksum Ô¨Åeld; the LAN card then includes this pseudo-header checksum value in its own checksum calculation, and the correct result is obtained. It is also possible, with many newer network-interface cards, to ofÔ¨Çoad the TCP segmentation process to the LAN hardware; that is, the kernel sends a very large TCP buffer ‚Äì perhaps 64 KB ‚Äì to the LAN hardware, along with a TCP header, and the LAN hardware divides the buffer into multiple TCP packets of at most 1500 bytes each. This is most useful when the application is writing data continuously and is known as TCP segmentation ofÔ¨Çoading, or TSO. The use of TSO requires TCO, but not vice-versa. TSO can be divided into large send ofÔ¨Çoading, LSO, for outbound trafÔ¨Åc, as above, and large receive ofÔ¨Çoading, LRO, for inbound. For inbound ofÔ¨Çoading, the network card accumulates multiple inbound packets that are part of the same TCP connection, and consolidates them in proper sequence to one much larger packet. This means that the network card, upon receiving one packet, must wait to see if there will be more. This wait is very short, however, at most a few milliseconds. SpeciÔ¨Åcally, all consolidated incoming packets must have the same TCP Timestamp value ( 18.4 Anomalous TCP scenarios ). TSO is of particular importance at very high bandwidths. At 10 Gbps, a system can send or receive close to a million packets per second, and ofÔ¨Çoading some of the packet processing to the network card can be essential to maintaining high throughput. TSO allows a host system to behave as if it were reading or writing very large packets, and yet the actual packet size on the wire remains at the standard 1500 bytes. On Linux systems, the status of TCO and TSO can be checked using the command ethtool --show-offload interface. TSO can be disabled with ethtool --offload interfacetso off. 17.6 TCP simplex-talk Here is a Java version of the simplex-talk server for TCP. As with the UDP version, we start by setting up the socket, here a ServerSocket calledss. This socket remains in the LISTEN state throughout. The mainwhile loop then begins with the call ss.accept() at the start; this call blocks until an incoming connection is established, at which point it returns the connected child socket s. Theaccept() call models the TCP protocol behavior of waiting for three-way handshakes initiated by remote hosts and, for each, setting up a new connection. Connections will be accepted from allIP addresses of the server host, egthe ‚Äúnormal‚Äù IP address, the loopback address 127.0.0.1 and, if the server is multihomed, any additional IP addresses. Unlike the UDP case ( 16.1.3.2 UDP and IP addresses ),RFC 1122 requires (¬ß4.2.3.7) that server response packets always be sent from the same server IP address that the client Ô¨Årst used to contact the server. (See 18 TCP Issues and Alternatives, exercise 5.0 for an example of non-compliance.) 402 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 A server application can process these connected children either serially or in parallel. The stalk version here can handle both situations, either one connection at a time ( THREADING = false ), or by creating a new thread for each connection ( THREADING = true ). Either way, the connected child socket is turned over toline_talker(), either as a synchronous procedure call or as a new thread. Data is then read from the socket‚Äôs associated InputStream using the ordinary read() call, versus the receive() used to read UDP packets. The main loop within line_talker() does not terminate until the client closes the connection (or there is an error). In the serial, non-threading mode, if a second client connection is made while the Ô¨Årst is still active, then data can be sent on the second connection but it sits in limbo until the Ô¨Årst connection closes, at which point control returns to the ss.accept() call, the second connection is processed, and the second connection‚Äôs data suddenly appears. In the threading mode, the main loop spends almost all its time waiting in ss.accept(); when this returns a child connection we immediately spawn a new thread to handle it, allowing the parent process to go back toss.accept(). This allows the program to accept multiple concurrent client connections, like the UDP version. The code here serves as a very basic example of the creation of Java threads. The inner class Talker has arun() method, needed to implement the runnable interface. To start a new thread, we create a new Talker instance; the start() call then begins Talker.run(), which runs for as long as the client keeps the connection open. The Ô¨Åle here is tcp_stalks.java. /*THREADED simplex-talk TCP server */ /*can handle multiple CONCURRENT client connections */ /*newline isto be included at client side */ import java.net. *; import java.io. *; public class tstalks { static public int destport = 5431; static public int bufsize = 512; static public boolean THREADING = true; static public void main(String args[]) { ServerSocket ss; Socket s; try{ ss = new ServerSocket(destport); } catch (IOException ioe) { System.err.println( "can't create server socket" ); return; } System.err.println( "server starting on port " + ss.getLocalPort()); while(true) { // accept loop try{ s = ss.accept(); } catch (IOException ioe) { System.err.println( "Can't accept" ); (continues on next page) 17.6 TCP simplex-talk 403
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) break; } if(THREADING) { Talker talk = new Talker(s); (new Thread(talk)).start(); }else{ line_talker(s); } } // accept loop } // end of main public static void line_talker(Socket s) { int port = s.getPort(); InputStream istr; try{ istr = s.getInputStream(); } catch (IOException ioe) { System.err.println( "cannot get input stream" ); // most √£√ëlikely cause: s was closed return; } System.err.println( "New connection from <" + s.getInetAddress().getHostAddress() + ","+ s.getPort() + ">"); byte[] buf = new byte[bufsize]; int len; while(true) { // while not done reading the socket try{ len = istr.read(buf, 0, bufsize); } catch (SocketTimeoutException ste) { System.out.println( "socket timeout" ); continue; } catch (IOException ioe) { System.err.println( "bad read" ); break; // probably a socket ABORT; treat asa close } if(len == -1) break; // other end closed gracefully String str = new String(buf, 0, len); System.out.print( ""+ port + ": "+ str); // str should contain √£√ënewline } // whilereading from s try{istr.close();} catch (IOException ioe) {System.err.println( "bad stream close" ); √£√ëreturn;} try{s.close();} catch (IOException ioe) {System.err.println( "bad socket close" ); √£√ëreturn;} System.err.println( "socket to port " + port + " closed" ); (continues on next page) 404 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) } // line_talker static class Talker implements Runnable { private Socket _s; public Talker (Socket s) { _s = s; } public void run() { line_talker(_s); } // run } // class Talker } 17.6.1 The TCP Client Here is the corresponding client tcp_stalkc.java. As with the UDP version, the default host to connect to islocalhost. We Ô¨Årst call InetAddress.getByName() to perform the DNS lookup. Part of the construction of the Socket object is the connection to the desired dest and destport. Within the main while loop, we use an ordinary write() call to write strings to the socket‚Äôs associated OutputStream. // TCP simplex-talk CLIENT injava import java.net. *; import java.io. *; public class stalkc { static public BufferedReader bin; static public int destport = 5431; static public void main(String args[]) { String desthost = "localhost"; if(args.length >= 1) desthost = args[0]; bin = new BufferedReader(new InputStreamReader(System. in)); InetAddress dest; System.err.print( "Looking up address of " + desthost + "..."); try{ dest = InetAddress.getByName(desthost); } catch (UnknownHostException uhe) { System.err.println( "unknown host: " + desthost); return; } System.err.println( " got it!" ); System.err.println( "connecting to port " + destport); Socket s; (continues on next page) 17.6 TCP simplex-talk 405
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) try{ s = new Socket(dest, destport); } catch(IOException ioe) { System.err.println( "cannot connect to <" + desthost + ","+ √£√ëdestport + ">"); return; } OutputStream sout; try{ sout = s.getOutputStream(); } catch (IOException ioe) { System.err.println( "I/O failure!" ); return; } //============================================================ while(true) { String buf; try{ buf = bin.readLine(); } catch (IOException ioe) { System.err.println( "readLine() failed" ); return; } if(buf == null) break; // user typed EOF character buf = buf + "\n"; // protocol requires sender includes \n byte[] bbuf = buf.getBytes(); try{ sout.write(bbuf); } catch (IOException ioe) { System.err.println( "write() failed" ); return; } } // while } } A Python3 version of the stalk client is available at tcp_stalkc.py. Here are some things to try with THREADING=false in the server: 
- start up two clients while the server is running. Type some message lines into both. Then exit the Ô¨Årst client. 
- start up the client before the server. 406 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 
- start up the server, and then the client. Kill the server and then type some message lines to the client. What happens to the client? (It may take a couple message lines.) 
- start the server, then the client. Kill the server and restart it. Now what happens to the client? With THREADING=true, try connecting multiple clients simultaneously to the server. How does this behave differently from the Ô¨Årst example above? See also exercise 13.0. 17.7 TCP and bind() The server version calls the ServerSocket() constructor, to create a socket in the LISTEN state; the local port must be speciÔ¨Åed here. The client version just calls Socket(), and the socket is then bound to a port by the operating system. It is also possible to create a client socket that is bound to a programmerspeciÔ¨Åed port; one application of this is to enable internal Ô¨Årewalls to identify the trafÔ¨Åc class by source port. More commonly, client sockets are assigned ephemeral ports by the system. The Linux ephemeral port range can be found in /proc/sys/net/ipv4/ip_local_port_range; as of 2022 it is 32768 to 60999. In C, sockets are created with socket(), bound to a port with bind(), and placed in the LISTEN state withlisten() or else connected to a server with connect(). For client sockets, the call to bind() may be performed implicitly by connect(). If two sockets on host A are connected via TCP to two different servers, B1 and B2, then it is possible that the operating system will assign the same local port to both sockets. On systems with an exceptional number of persistent outbound connections, such port reuse may be essential, as it is otherwise possible to run out of local ports. That said, port reuse is not an option if bind() is called explicitly, as at the time of the call to bind() the operating system does not yet know if the socket is to be used for LISTENing, for which a unique local port is necessary. To help deal with this, Linux has the socket option IP_BIND_ADDRESS_NO_PORT, which causes bind() to bind an IP address to the socket but defers port binding to a later connect(). Apple OS X provides the connectx() system call, which introduces similar functionality. See also this CloudÔ¨Çare blog post. Ephemeral ports were, originally, assigned more-or-less in sequence, skipping over values in use and ultimately wrapping around. This makes it easy, however, for adversaries to predict source-port numbers, so RFC 6056 proposed making a Ô¨Årst try at a new local-port number as follows: try0 = next_ephemeral + hash(local_addr, remote_addr, remote_port, secret_key) If that is not available, subsequent tries incremented this value successively, with appropriate wrapping to stay within the designated ephemeral-port range. (See 18.3.1 ISNs and spooÔ¨Ång for a related technique with ISN generation.) This had the advantage that any speciÔ¨Åc remote socket would see the local ports incremented successively, which makes port reuse unlikely until the entire ephemeral-port range has been cycled through. However, a different remote socket would get another, unrelated, port-number sequence, making it very difÔ¨Åcult for attackers to guess another connection‚Äôs port. Unfortunately, this elegant scheme introduced an unexpected problem: it enabled Ô¨Ångerprinting of the system that uses it, which lasted for the lifetime of the secret_key. This was done through the creation 17.7 TCP and bind() 407
An Introduction to Computer Networks, Release 2.0.11 of many ‚Äúprobe‚Äù connections, and observing the behavior of the local_port value; see [KK22] for details. The Ô¨Åx is to add a fast-changing timestamp to the hash arguments above, and to increment a failed port try by a random value between 1 and 7, rather than always by 1. 17.7.1 netcat again As with UDP ( 16.1.4 netcat ), we can use the netcat utility to act as either end of the TCP simplex-talk connection. As the client we can use netcat localhost 5431 while as the server we can use netcat -l -k 5431 Here (but not with UDP) the -koption causes the server to accept multiple connections in sequence. The connections are handled one at a time, as is the case in the stalk server above with THREADING=false. We can also use netcat to download web pages, using the HTTP protocol. The command below sends an HTTP GET request (version 1.1; RFC 2616 and updates) to retrieve part of the website for this book; it has been broken over two lines for convenience. echo -e 'GET /index.html HTTP/1.1 \r\nHOST: intronetworks.cs.luc.edu \r\n'| netcat intronetworks.cs.luc.edu 80 The\r\n represents the ofÔ¨Åcially mandatory carriage-return/newline line-ending sequence, though \nwill often work. The index.html identiÔ¨Åes the Ô¨Åle being requested; as index.html is the default it is often omitted, though the preceding /is still required. The webserver may support other websites as well via virtual hosting ( 10.1.2 nslookup and dig ); theHOST: speciÔ¨Åcation identiÔ¨Åes to the server the speciÔ¨Åc site we are looking for. Version 2 of HTTP is described in RFC 7540; its primary format is binary. (For production command-line retrieval of web pages, cURL and wget are standard choices.) 17.8 TCP state diagram A formal deÔ¨Ånition of TCP involves the state diagram, with conditions for transferring from one state to another, and responses to all packets from each state. The state diagram originally appeared in RFC 793; the following interpretation of the state diagram came from http://commons.wikimedia.org/wiki/File: Tcp_state_diagram_Ô¨Åxed.svg and was authored by Wikipedia users Sergiodc2, Marty Pauley, and DnetSvg. The blue arrows indicate the sequence of state transitions typically followed by the server; the brown arrows represent the client. Arrows are labeled with event / action; that is, we move from LISTEN to SYN_RECD upon receipt of a SYN packet; the action is to respond with SYN+ACK. 408 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 CLOSED (Start) LISTEN/- CLOSE/- LISTEN SYN RECEIVEDSYN SENTCONNECT/ (Step 1 of the 3-way-handshake) SYN SYN/SYN+ACK (Step 2 of the 3-way-handshake)unusual event client/receiver path server/sender path RST/- SYN/SYN+ACK (simultaneous open) SYN+ACK/ACK (Step 3 of the 3-way-handshake)Data exchange occurs ESTABLISHED FIN/ACKACK/-CLOSE/- SEND/ SYN CLOSE/ FIN CLOSE/ FIN CLOSING TIME WAIT CLOSEDFIN WAIT 1 FIN WAIT 2CLOSE WAIT LAST ACKCLOSE/ FINFIN/ACK FIN + ACK-of-FIN / ACK ACK-of-FIN / - FIN/ACK Timeout (Go back to start)Active CLOSE Passive CLOSE ACK/- ACK/- In general, this Ô¨Ånite-state-machine approach to protocol speciÔ¨Åcation has proven very effective, and is now used for most protocols. It makes it very clear to the implementer how the system should respond to each packet arrival. It is also a useful model for the implementation itself. Finally, we also observe that the TCP layer within an operating system cannot easily be modeled as anything other than a state machine; it must respond immediately to packet and program events, without indeÔ¨Ånite waiting, as the operating system must go on to other things. It is visually impractical to list every possible transition within the state diagram, full details are usually left to the accompanying text. For example, although this does not appear in the state diagram above, the perstate response rules of TCP require that in the ESTABLISHED state, if the receiver sends an ACK outside the current sliding window, then the correct response is to reply with one‚Äôs own current ACK. This includes the case where the receiver acknowledges data not yet sent. The ESTABLISHED state and the states below it are sometimes called the synchronized states, as in these states both sides have conÔ¨Årmed one another‚Äôs ISN values. Here is the ladder diagram for the 14-packet connection described above, this time labeled with TCP states. 17.8 TCP state diagram 409
An Introduction to Computer Networks, Release 2.0.11 A B SYN SYN+ACK ACK ‚Äúabc‚Äù ACK ‚Äúdefg‚Äù ACK ‚Äúfoobar‚Äù ‚Äúhello‚Äù ‚Äúgoodbye‚Äù FIN ACK FIN ACKLISTEN SYN_RECD ESTABLISHED CLOSEWAIT LAST_ACK CLOSEDCLOSED SYN_SENT ESTABLISHED FINWAIT_1 TIMEWAITFINWAIT_2ACK Although it essentially never occurs in practice, it is possible for each side to send the other a SYN, requesting a connection, simultaneously (that is, the SYNs cross on the wire). The telephony analogue occurs when each party dials the other simultaneously. On traditional land-lines, each party then gets a busy signal. On cell phones, your mileage may vary. With TCP, a single connection is created. With OSI TP4, two connections are created. The OSI approach is not possible in TCP, as a connection is determined only by the socketpair involved; if there is only one socketpair then there can be only one connection. It is possible to view connection states under either Linux or Windows with netstat -a. Most states are ephemeral, exceptions being LISTEN, ESTABLISHED, TIMEWAIT, and CLOSE_WAIT. One sometimes sees large numbers of connections in CLOSE_WAIT, meaning that the remote endpoint has closed the connection and sent its FIN, but the process at your end has not executed close() on its socket. Often this represents a programming error; alternatively, the process at the local end is still working on something. Given a local port number p in state CLOSE_WAIT on a Linux system, the (privileged) command lsof -i: pwill identify the process using port p. The reader who is implementing TCP is encouraged to consult RFC 793 and updates. For the rest of us, below are a few general observations about closing connections. 17.8.1 Closing a connection The ‚Äúnormal‚Äù TCP close sequence is as follows: 410 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 A B FINACK ACK Normal closeFINESTABLISHED FIN_WAIT_1ESTABLISHED FIN_WAIT_2 TIMEWAITCLOSE_WAIT LAST_ACK CLOSEDMore Data (optional) ACK of Data A‚Äôs FIN is, in effect, a promise to B not to send any more. However, A must still be prepared to receive data from B, hence the optional data shown in the diagram. A good example of this occurs when A is sending a stream of data to B to be sorted; A sends FIN to indicate that it is done sending, and only then does B sort the data and begin sending it back to A. This can be generated with the command, on A, cat thefile | ssh B sort. That said, the presence of the optional B-to-A data above following A‚Äôs FIN is relatively less common. In the diagram above, A sends a FIN to B and receives an ACK, and then, later, B sends a FIN to A and receives an ACK. This essentially amounts to two separate two-way closure handshakes. Either side may elect to close the connection, just as either party to a telephone call may elect to hang up. The Ô¨Årst side to send a FIN ‚Äì A in the diagram above ‚Äì takes the Active CLOSE path; the other side takes the Passive CLOSE path. In the diagram, active-closer A moves from state ESTABLISHED to FIN_WAIT_1 to FIN_WAIT_2 (upon receipt of B‚Äôs ACK of A‚Äôs FIN), and then to TIMEWAIT and Ô¨Ånally to CLOSED. Passive-closer B moves from ESTABLISHED to CLOSE_WAIT to LAST_ACK to CLOSED. Asimultaneous close ‚Äì having both sides send each other FINs before receiving the other side‚Äôs FIN ‚Äì is a little more likely than a simultaneous open, earlier above, though still not very. Each side would send its FIN and move to state FIN_WAIT_1. Then, upon receiving each other‚Äôs FIN packets, each side would send its Ô¨Ånal ACK and move to CLOSING. See exercises 5.0 and 6.0. A TCP endpoint is half-closed if it has sent its FIN (thus promising not to send any more data) and is waiting for the other side‚Äôs FIN; this corresponds to A in the diagram above in states FIN_WAIT_1 and FIN_WAIT_2. With the BSD socket library, an application can half-close its connection with the appropriate call toshutdown(). Unrelatedly, A TCP endpoint is half-open if it is in the ESTABLISHED state, but during a lull in the exchange of packets the other side has rebooted; this has nothing to do with the close protocol. As soon as the ESTABLISHED side sends a packet, the rebooted side will respond with RST and the connection will be fully closed. In the absence of the optional data from B to A after A sends its FIN, the closing sequence reduces to the left-hand diagram below: 17.8 TCP state diagram 411
An Introduction to Computer Networks, Release 2.0.11 A B FIN ACK FIN ACK Two TCP close scenarios with no B-to-A dataA B FIN FIN+ACK ACK If B is ready to close immediately, it is possible for B‚Äôs ACK and FIN to be combined, as in the righthand diagram above, in which case the resultant diagram superÔ¨Åcially resembles the connection-opening three-way handshake. In this case, A moves directly from FIN_WAIT_1 to TIMEWAIT, following the state-diagram link labeled ‚ÄúFIN + ACK-of-FIN‚Äù. In theory this is rare, as the ACK of A‚Äôs FIN is generated by the kernel but B‚Äôs FIN cannot be sent until B‚Äôs process is scheduled to run on the CPU. If the TCP layer adopts a policy of immediately sending ACKs upon receipt of any packet, this will never happen, as the ACK will be sent well before B‚Äôs process can be scheduled to do anything. However, if B delays its ACKs slightly (and if it has no more data to send), then it is possible ‚Äì and in fact not uncommon ‚Äì for B‚Äôs ACK and FIN to be sent together. Delayed ACKs, are, as we shall see below, a common strategy ( 18.8 TCP Delayed ACKs ). To create the scenario of 17.4 TCP and WireShark, it was necessary to introduce an artiÔ¨Åcial delay to prevent the simultaneous transmission of B‚Äôs ACK and FIN. 17.8.2 Calling close() Most network programming interfaces provide a close() method for ending a connection, based on the close operation for Ô¨Åles. However, it usually closes bidirectionally and so models the TCP closure protocol rather imperfectly. As we have seen in the previous section, the TCP close sequence is is followed more naturally if the activeclosing endpoint calls shutdown() ‚Äì promising not to send more, but allowing for continued receiving ‚Äì before the Ô¨Ånal close(). Here is what should happen at the application layer if endpoint A of a TCP connection wishes to initiate the closing of its connection with endpoint B: 
- A‚Äôs application calls shutdown(), thereby promising not to send any more data. A‚Äôs FIN is sent to B. A‚Äôs application is expected to continue reading, however. 
- The connection is now half-closed. On receipt of A‚Äôs FIN, B‚Äôs TCP layer knows this. If B‚Äôs application attempts to read more data, it will receive an end-of-Ô¨Åle indication (this is typically a read() or recv() operation that returns immediately with 0 bytes received). 
- B‚Äôs application is now done reading data, but it may or may not have more data to send. When B‚Äôs application is done sending, it calls close(), at which point B‚Äôs FIN is sent to A. Because the connection is already half-closed, B‚Äôs close() is really a second half-close, ending further transmission by B. 
- A‚Äôs application keeps reading until it too receives an end-of-Ô¨Åle indication, corresponding to B‚Äôs FIN. 412 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 
- The connection is now fully closed. No data has been lost. It is sometimes the case that it is evident to A from the application protocol that B will not send more data. In such cases, A might simply call close() instead ofshutdown(). This is risky, however, unless the protocol is crystal clear: if A calls close() and B later does send a little more data after all, or if B has already sent some data but A has not actually read it, A‚Äôs TCP layer may send RST to B to indicate that not all B‚Äôs data was received properly. RFC 1122 puts it this way: If such a host issues a CLOSE call while received data is still pending in TCP, or if new data is received after CLOSE is called, its TCP SHOULD send a RST to show that data was lost. If A‚Äôs RST arrives at B before all of A‚Äôs sent data has been processed by B‚Äôs application, it is entirely possible that data sent by A will be lost, that is, will never be read by B. In the BSD socket library, A can set the SO_LINGER option, which causes A‚Äôs close() to block until A‚Äôs data has been delivered to B (or until the SO_LINGER timeout, provided by the user when setting this option, has expired). However, SO_LINGER has no bearing on the issue above; post-close data from B to A will still cause A to send a RST. In the simplex-talk program at 17.6 TCP simplex-talk, the client does not call shutdown() (it implicitly callsclose() when it exits). When the client is done, the server calls s.close(). However, the fact that there is no data at all sent from the server to the client prevents the problem discussed above. It is sometimes the case that A is expected to send a large amount of data to B and then exit: byte[] bbuf = byte[1000000]; ... sout.write(bbuf); // Java OutputStream attached to the socket s s.close() In this case, the close() call is supposed to result in A sending all the data before actually terminating the connection. RFC 793 puts it this way: Closing connections is intended to be a graceful operation in the sense that outstanding SENDs will be transmitted (and retransmitted), as Ô¨Çow control permits, until all have been serviced. The Linux interpretation of this is given in the socket(7) man page: When the socket is closed as part of exit(2), it always lingers in the background. The linger time is not speciÔ¨Åed. If there is an explicit close(2) beforeexit(2), theSO_LINGER status above determines TCP‚Äôs behavior. Alternatively, A can send the data and then attempt to read from the socket. A will receive an end-of-Ô¨Åle indication (typically 0 bytes read) as soon as the other endpoint B closes. If B waits to close until it has read all the data, this end-of-Ô¨Åle indication will mean it is safe for A to call s.close(). However, B might equally well call shutdown() immediately on startup, as it does not intend to write any data, in which case A‚Äôs received end-of-Ô¨Åle is notan indication it is safe to close. See also exercises 13.0 and 15.0. 17.8 TCP state diagram 413
An Introduction to Computer Networks, Release 2.0.11 17.9 Epilog At this point we have covered the basic mechanics of TCP. The next chapter discusses, among other things, some of the subtle issues TCP must deal with in order to maintain reliability. 17.10 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0. Experiment with the TCP version of simplex-talk. How does the server respond differently with threading enabled and without, if two simultaneous attempts to connect are made, from two different client instances? 2.0. Trace the states visited if nodes A and B attempt to create a TCP connection by simultaneously sending each other SYN packets, that then cross in the network. Draw the ladder diagram, and label the states on each side. Hint: there should be two pairs of crossing packets. A SYN+ACK counts, in the state diagram, as an ACK. 3.0. Suppose nodes A and B are each behind their own NAT Ô¨Årewall ( 9.7 Network Address Translation ). ANAT_A Internet NAT_B B A and B attempt to connect to one another, using TCP; A uses source port 2000 and B uses 3000. A sends to the public IPv4 address of NAT_B, port 3000, and B sends to NAT_A, port 2000. Assume that neither NAT_A nor NAT_B changes the port numbers in outgoing packets, at least for the packets involved in this connection attempt. (a). Suppose A sends a SYN packet to (NAT_B, 3000). It will be rejected at NAT_B, as the connection was not initiated by B. However, a short while later, B sends a SYN packet to (NAT_A, 2000). Explain why this second SYN packet isdelivered to A. (b). Now suppose A and B attempt to connect simultaneously, each sending a SYN to the other. Show that the connection succeeds. 4.0. When two nodes A and B simultaneously attempt to connect to one another using the OSI TP4 protocol, two bidirectional network connections are created (rather than one, as with TCP). (a). Explain why this semantics is impossible with the existing TCP header. Hint: if a packet from xA,port1y arrives atxB,port2y, how would the receiver tell to which of the two possible connections it belongs? (b). Propose an additional Ô¨Åeld in the TCP header that would allow implementation of the TP4 semantics. 5.0. Simultaneous connection initiations are rare, but simultaneous connection termination is relatively common. How do two TCP nodes negotiate the simultaneous sending of FIN packets to one another? Draw the ladder diagram, and label the states on each side. Which node goes into TIMEWAIT state? Hint: there should be two pairs of crossing packets. 6.0. The state diagram at 17.8 TCP state diagram shows a dashed path from FIN_WAIT_1 to TIMEWAIT on receipt of FIN+ACK. All FIN packets contain a valid ACK Ô¨Åeld, but that is not what is meant here. Under what circumstances is this direct arc from FIN_WAIT_1 to TIMEWAIT taken? Explain why this arc can never be used during simultaneous close. Hint: consider the ladder diagram of a ‚Äúnormal‚Äù close. 414 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 7.0. (a) Suppose you see multiple connections on your workstation in state FIN_WAIT_1. What is likely going on? Whose fault is it? (b). What might be going on if you see connections languishing in state FIN_WAIT_2? 8.0. Suppose that, after downloading a Ô¨Åle, the client host is unplugged from the network, so it can send no further packets. The server‚Äôs connection is still in the ESTABLISHED state. In each case below, use the TCP state diagram to list all states that are reachable by the server. (a). Before being unplugged, the client was in state ESTABLISHED; ieit had notsent the Ô¨Årst FIN. (b). Before being unplugged the client had sent its FIN, and moved to FIN_WAIT_1. Eventually, the server connection here would in fact transition to CLOSED due to repeated timeouts. For this exercise, though, assume only transitions explicitly shown in the state diagram are allowed. 9.0. In 17.3 TCP Connection Establishment we noted that RST packets had to have a valid SEQ value, but that ‚Äú RFC 793 does not require the RST packet‚Äôs ACK value to match‚Äù. There is an exception for RST packets arriving at state SYN-SENT: ‚Äúthe RST is acceptable if the ACK Ô¨Åeld acknowledges the SYN‚Äù. Explain the reasoning behind this exception. 10.0. Suppose A and B create a TCP connection with ISN A=20,000 and ISN B=5,000. A sends three 1000byte packets (Data1, Data2 and Data3 below), and B ACKs each. Then B sends a 1000-byte packet DataB to A and terminates the connection with a FIN. In the table below, Ô¨Åll in the SEQ and ACK Ô¨Åelds for each packet shown. A sends B sends SYN, ISN A=20,000 SYN, ISN B=5,000, ACK=______ ACK, SEQ=______, ACK=______ Data1, SEQ=______, ACK=______ ACK, SEQ=______, ACK=______ Data2, SEQ=______, ACK=______ ACK, SEQ=______, ACK=______ Data3, SEQ=______, ACK=______ ACK, SEQ=______, ACK=______ DataB, SEQ=______, ACK=______ ACK, SEQ=_____, ACK=______ FIN, SEQ=______, ACK=______ 11.0. Suppose you are downloading a large Ô¨Åle, and there is a progress bar showing how much of the Ô¨Åle has been downloaded. For deÔ¨Åniteness, assume the progress bar moves 1 mm for each megabyte received by the application, and the throughput averages 0.5 MB per second (so the progress bar normally advances at a rate of 0.5 mm/sec). You see the progress bar stop advancing for an interval of time. Suddenly, it jumps forward 5 mm, and then resumes its steady 0.5 mm/sec advance. (a). Explain the jump in terms of a lost packet and subsequent timeout and retransmission. 17.10 Exercises 415
An Introduction to Computer Networks, Release 2.0.11 (b). Give an approximate value for the connection winsize, assuming only one packet was lost. 12.0. Suppose you are creating software for a streaming-video site. You want to limit the video read-ahead ‚Äì the gap between how much has been downloaded and how much the viewer has actually watched ‚Äì to approximately 1 MB; the server should pause in sending when necessary to enforce this. On the other hand, you do want the receiver to be able to read ahead by up to this much. You should assume that the TCP connection throughput will be higher than the actual video-data-consumption rate. (a). Suppose the TCP window size happens to be exactly 1 MB. If the receiver simply reads each video frame from the TCP connection, displays it, and then pauses brieÔ¨Çy before reading the next frame in accordance with the frame rate, explain how the Ô¨Çow-control mechanism of 18.10 TCP Flow Control will achieve the desired effect. (b). Applications, however, cannot control their TCP window size. Suppose the receiver application reads 1 MB ahead of what is being displayed, and then stops reading, until more can be displayed. Again, explain how the TCP Ô¨Çow-control mechanism will soon cause the sender to stop sending. (SpeciÔ¨Åcally, the sender would never send more than winsize+1MB ahead of what was necessary.) (It is also possible to implement sender pauses via an application-layer protocol.) 13.0. Modify the simplex-talk server of 17.6 TCP simplex-talk so thatline_talker() breaks out of thewhile loop as soon as it has printed the Ô¨Årst string received (or simply remove the while loop). Once out of thewhile loop, the existing code calls s.close(). (a). Start up the modiÔ¨Åed server, and connect to it with a client. Send a single message line, and use netstat to examine the TCP states of the client and server. What are these states? (b). Send two message lines to the server. What are the TCP states of the client and server? (c). Send three message lines to the server. Is there an error message at the client? (d). Send two message lines to the server, while monitoring packets with WireShark. The WireShark Ô¨Ålter expression tcp.port == 5431 may be useful for eliminating irrelevant trafÔ¨Åc. What FIN packets do you see? Do you see a RST packet? 14.0. Outline a scenario in which TCP endpoint A sends data to B and then calls close() on its socket, and after the connection terminates B has not received all the data, even though the network has not failed. In the style of 17.6.1 The TCP Client, A‚Äôs code might look like this: 416 17 TCP Transport Basics
An Introduction to Computer Networks, Release 2.0.11 s = new Socket(dest, destport); sout = s.getOutputStream(); sout.write(large_buffer) s.close() Hint: see 17.8.2 Calling close(). 17.10 Exercises 417
An Introduction to Computer Networks, Release 2.0.11 418 17 TCP Transport Basics
18 TCP ISSUES AND ALTERNATIVES In this chapter we cover some issues relating to TCP reliability, some technical issues relating to TCP efÔ¨Åciency, and Ô¨Ånally some outright alternatives to TCP. 18.1 TCP Old Duplicates Conceptually, perhaps the most serious threat facing the integrity of TCP data is external old duplicates (16.3 Fundamental Transport Issues ), that is, very late packets from a previous instance of the connection. Suppose a TCP connection is opened between A and B. One packet from A to B is duplicated and unduly delayed, with sequence number N. The connection is closed, and then another instance is reopened, that is, a connection is created using the same ports. At some point in the second connection, when an arriving packet with seq=N would be acceptable at B, the old duplicate shows up. Later, of course, B is likely to receive a seq=N packet from the new instance of the connection, but that packet will be seen by B as a duplicate (even though the data does not match), and (we will assume) be ignored. SYN SYN+ACK FIN FIN+ACK ACKACK SYN SYN+ACK ACKFirst instance of the connection Second instance of the connection B mistakenly accepts the old duplicate from the first instanceDelayed duplicate packet, redPort 100 Port 100Port 2000Port 2000A B For TCP, it is the actual sequence numbers, rather than the relative sequence numbers, that would have to match up. The diagram above ignores that. As with TFTP, coming up with a possible scenario accounting for the generation of such a late packet is not easy. Nonetheless, many of the design details of TCP represent attempts to minimize this risk. 419
An Introduction to Computer Networks, Release 2.0.11 Solutions to the old-duplicates problem generally involve setting an upper bound on the lifetime of any packet, the MSL, as we shall see in the next section. T/TCP ( 18.5 TCP Faster Opening ) introduced a connection-count Ô¨Åeld for this. TCP is also vulnerable to sequence-number wraparound: arrival of an old duplicates from the same instance of the connection. However, if we take the MSL to be 60 seconds, sequence-number wrap requires sending 232bytes in 60 seconds, which requires a data-transfer rate in excess of 500 Mbps. TCP offers a Ô¨Åx for this (Protection Against Wrapped Segments, or PAWS), but it was introduced relatively late; we return to this in 18.4 Anomalous TCP scenarios. 18.2 TIMEWAIT The TIMEWAIT state is entered by whichever side initiates the connection close; in the event of a simultaneous close, both sides enter TIMEWAIT. It is to last for a time 2 MSL, where MSL = Maximum Segment Lifetime is an agreed-upon value for the maximum lifetime on the Internet of an IP packet. Traditionally MSL was taken to be 60 seconds, but more modern implementations often assume 30 seconds (for a TIMEWAIT period of 60 seconds). One function of TIMEWAIT is to solve the external-old-duplicates problem. TIMEWAIT requires that between closing and reopening a connection, a long enough interval must pass that any packets from the Ô¨Årst instance will disappear. After the expiration of the TIMEWAIT interval, an old duplicate cannot arrive. A second function of TIMEWAIT is to address the lost-Ô¨Ånal-ACK problem ( 16.3 Fundamental Transport Issues ). If host A sends its Ô¨Ånal ACK to host B and this is lost, then B will eventually retransmit itsÔ¨Ånal packet, which will be its FIN. As long as A remains in state TIMEWAIT, it can appropriately reply to a retransmitted FIN from B with a duplicate Ô¨Ånal ACK. As with TFTP, it is possible (though unlikely) for the Ô¨Ånal ACK to be lost as well as all the retransmitted Ô¨Ånal FINs sent during the TIMEWAIT period; should this happen, one side thinks the connection closed normally while the other side thinks it did not. See exercise 4.0. TIMEWAIT only blocks reconnections for which both sides reuse the same port they used before. If A connects to B and closes the connection, A is free to connect again to B using a different port at A‚Äôs end. Conceptually, a host may have many old connections to the same port simultaneously in TIMEWAIT; the host must thus maintain for each of its ports a list of all the remote xIP_address,port ysockets currently in TIMEWAIT for that port. If a host is connecting as a client, this list likely will amount to a list of recently used ports; no port is likely to have been used twice within the TIMEWAIT interval. If a host is a server, however, accepting connections on a standardized port, and happens to be the side that initiates the active close and thus later goes into TIMEWAIT, then its TIMEWAIT list for that port can grow quite long. Generally, busy servers prefer to be free from these bookkeeping requirements of TIMEWAIT, so many protocols are designed so that it is the client that initiates the active close. In the original HTTP protocol, version 1.0, the server sent back the data stream requested by the http GET message ( 17.7.1 netcat again ), and indicated the end of this stream by closing the connection. In HTTP 1.1 this was Ô¨Åxed so that the client initiated the close; this required a new mechanism by which the server could indicate ‚ÄúI am done sending this Ô¨Åle‚Äù. HTTP 1.1 also used this new mechanism to allow the server to send back multiple Ô¨Åles over one connection. In an environment in which many short-lived connections are made from host A to the same port on server B, port exhaustion ‚Äì having all ports tied up in TIMEWAIT ‚Äì is a theoretical possibility. If A makes 1000 420 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 connections per second, then after 60 seconds it has gone through 60,000 available ports, and there are essentially none left. While this rate is high, early Berkeley-Unix TCP implementations often made only about 4,000 ports available to clients; with a 120-second TIMEWAIT interval, port exhaustion would occur with only 33 connections per second. If you use ssh to connect to a server and then issue the netstat -a command on your own host (or, more conveniently, netstat -a |grep -i tcp ), you should see your connection in ESTABLISHED state. If you close your connection and check again, your connection should be in TIMEWAIT. 18.3 The Three-Way Handshake Revisited As stated earlier in 17.3 TCP Connection Establishment, both sides choose an ISN; actual sequence numbers are the sum of the sender‚Äôs ISN and the relative sequence number. There are two original reasons for this mechanism, and one later one ( 18.3.1 ISNs and spooÔ¨Ång ). The original TCP speciÔ¨Åcation, as clariÔ¨Åed in RFC 1122, called for the ISN to be determined by a special clock, incremented by 1 every 4 microseconds. The most basic reason for using ISNs is to detect duplicate SYNs. Suppose A initiates a connection to B by sending a SYN packet. B replies with SYN+ACK, but this is lost. A then times out and retransmits its SYN. B now receives A‚Äôs second SYN while in state SYN_RECEIVED. Does this represent an entirely new request (perhaps A has suddenly restarted), or is it a duplicate? If A uses the clock-driven ISN strategy, B can tell ( almost certainly) whether A‚Äôs second SYN is new or a duplicate: only in the latter case will the ISN values in the two SYNs match. While there is no danger to data integrity if A sends a SYN, restarts, and sends the SYN again as part of a reopening the same connection, the arrival of a second SYN with a new ISN means that the original connection cannot proceed, because that ISN is now wrong. The receiver of the duplicate SYN should drop any connection state it has recorded so far, and restart processing the second SYN from scratch. The clock-driven ISN also originally added a second layer of protection against external old duplicates. Suppose that A opens a connection to B, and chooses a clock-based ISN N 1. A then transfers M bytes of data, closed the connection, and reopens it with ISN N 2. If N 1+ M < N 2, then the old-duplicates problem cannot occur: all of the absolute sequence numbers used in the Ô¨Årst instance of the connection are less than or equal to N 1+ M, and all of the absolute sequence numbers used in the second instance will be greater than N 2. Early Berkeley-Unix implementations of the socket library often allowed a second connection meeting the above ISN requirement to be reopened before TIMEWAIT would have expired; this potentially addressed the problem of port exhaustion. We might call this TIMEWAIT connection reuse. Of course, if the Ô¨Årst instance of the connection transferred data faster than the ISN clock rate, that is at more than 250,000 bytes/sec, then N 1+ M would be greater than N 2, and TIMEWAIT would have to be enforced. But in the era in which TCP was Ô¨Årst developed, sustained transfers exceeding 250,000 bytes/sec were not as common. Alternatively, the connection in TIMEWAIT might allow incoming connections that reuse the same ports, because in this case the host in TIMEWAIT can choose its own ISN to be greater than the Ô¨Ånal absolute sequence number of the previous instance of the connection. This second alternative is allowed by :rfc:1192:, ¬ß4.2.2.13. The three-way handshake was extensively analyzed by Dalal and Sunshine in [DS78]. The authors noted that with a two-way handshake, the second side receives no conÔ¨Årmation that its ISN was correctly received. 18.3 The Three-Way Handshake Revisited 421
An Introduction to Computer Networks, Release 2.0.11 The authors also observed that a four-way handshake ‚Äì in which the ACK of ISN Ais sent separately from ISN B, as in the diagram below ‚Äì could fail if one side restarted. A B SYNACK ACK Possible four-way handshake Less secure than three-way versionSYN For this failure to occur, assume that after sending the SYN in line 1, with ISN A1, A restarts. The ACK in line 2 is either ignored or not received. B now sends its SYN in line 3, but A interprets this as a new connection request; it will respond after line 4 by sending a Ô¨Åfth, SYN packet containing a different ISN A2. For B the connection is now ESTABLISHED, and if B acknowledges this Ô¨Åfth packet but fails to update its record of A‚Äôs ISN, the connection will fail as A and B would have different notions of ISN A. 18.3.1 ISNs and spooÔ¨Ång The clock-based ISN proved to have a signiÔ¨Åcant weakness: it often allowed an attacker to guess the ISN a remote host might use. It did not help any that an early version of Berkeley Unix, instead of incrementing the ISN 250,000 times a second, incremented it once a second, by 250,000 (plus something for each connection). By guessing the ISN a remote host would choose, an attacker might be able to mimic a local, trusted host, and thus gain privileged access. SpeciÔ¨Åcally, suppose host A trusts its neighbor B, and executes with privileged status commands sent by B; this situation was typical in the era of the rhost command. A authenticates these commands because the connection comes from B‚Äôs IP address. The bad guy, M, wants to send packets to A so as to pretend to be B, and thus get a privileged command invoked. The connection only needs to be started; if the ruse is discovered after the command is executed, it is too late. M can easily send a SYN packet to A with B‚Äôs IP address in the source-IP Ô¨Åeld; M can probably temporarily disable B too, so that A‚Äôs SYN-ACK response, which is sent to B, goes unnoticed. What is harder is for M to Ô¨Ågure out how to guess how to ACK ISN A. But if A generates ISNs with a slowly incrementing clock, M can guess the pattern of the clock with previous connection attempts, and can thus guess ISN Awith a considerable degree of accuracy. So M sends SYN to A with B as source, A sends SYN-ACK to B containing ISN A, and M guesses this value and sends ACK(ISN A+1) to A, again with B listed in the IP header as source, followed by a single-packet command. This TCP-layer IP-spooÔ¨Ång technique was Ô¨Årst described by Robert T Morris in [RTM85]; Morris went on to launch the Internet Worm of 1988 using unrelated attacks. The IP-spooÔ¨Ång technique was used in the 1994 Christmas Day attack against UCSD, launched from Loyola‚Äôs own apollo.it.luc.edu; the attack was associated with Kevin Mitnick though apparently not actually carried out by him. Mitnick was arrested a few months later. RFC 1948, in May 1996, introduced a technique for introducing a degree of randomization in ISN selection, while still ensuring that the same ISN would not be used twice in a row for the same connection. The ISN is 422 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 to be the sum of the 4-¬µs clock, C(t), and a secure hash of the connection information as follows (compare with the local-port algorithm of 17.7 TCP and bind() ): ISN = C(t) + hash(local_addr, local_port, remote_addr, remote_port, secret_key) Thesecret_key value is a random value chosen by the host on startup. While M, above, can poll A for its current ISN, and can probably guess the hash function and the Ô¨Årst four parameters above, without knowing the key it cannot determine (or easily guess) the ISN value A would have sent to B. Legitimate connections between A and B using the same port at each end, on the other hand, see the ISN increasing at the 4-¬µs rate, which potentially increases the chance of successful application of ‚ÄúTIMEWAIT connection reuse‚Äù as in 18.3 The Three-Way Handshake Revisited. RFC 5925 addresses spooÔ¨Ång and related attacks by introducing an optional TCP authentication mechanism: the TCP header includes an option containing a secure hash ( 28.6 Secure Hashes ) of the rest of the TCP header and a shared secret key. The need for key management limits when this mechanism can be used; the classic use case is BGP connections between routers ( 15 Border Gateway Protocol (BGP) ). Another approach to the prevention of spooÔ¨Ång attacks is to ask sites and ISPs to refuse to forward outwards any IP packets with a source address not from within that site or ISP. If an attacker‚Äôs ISP implements this, the attacker will be unable to launch spooÔ¨Ång attacks against the outside world. A concrete proposal can be found in RFC 2827. Unfortunately, it has been (as of 2015) almost entirely ignored. See also the discussion of SYN Ô¨Çooding at 17.3 TCP Connection Establishment, although that attack does not involve ISN manipulation. 18.4 Anomalous TCP scenarios TCP, like any transport protocol, must address the transport issues in 16.3 Fundamental Transport Issues. As we saw above, TCP addresses the Duplicate Connection Request (Duplicate SYN) issue by noting whether the ISN has changed. This is handled at the kernel level by TCP, versus TFTP‚Äôs application-level (and rather desultory) approach to handing Duplicate RRQs. TCP addresses Loss of Final ACK through TIMEWAIT: as long as the TIMEWAIT period has not expired, if the Ô¨Ånal ACK is lost and the other side resends its Ô¨Ånal FIN, TCP will still be able to reissue that Ô¨Ånal ACK. TIMEWAIT in this sense serves a similar function to TFTP‚Äôs DALLY state. External Old Duplicates, arriving as part of a previous instance of the connection, are prevented by TIMEWAIT, and may also be prevented by the use of a clock-driven ISN. Internal Old Duplicates, from the same instance of the connection, that is, sequence number wraparound, is only an issue for bandwidths exceeding 500 Mbps: only at bandwidths above that can 4 GB be sent in one 60-second MSL. TCP implementations now address this with PAWS: Protection Against Wrapped Segments (RFC 1323 ). PAWS adds a 32-bit ‚Äútimestamp option‚Äù to the TCP header. The granularity of the timestamp clock is left unspeciÔ¨Åed; one tick must be small enough that sequence numbers cannot wrap in that interval (egless than 3 seconds for 10,000 Mbps), and large enough that the timestamps cannot wrap in time MSL. On Linux systems the timestamp clock granularity is typically 1 to 10 ms; measurements on the author‚Äôs systems have been 4 ms. With timestamps, an old duplicate due to sequence-number wraparound can now easily be detected. 18.4 Anomalous TCP scenarios 423
An Introduction to Computer Networks, Release 2.0.11 The PAWS mechanism also requires ACK packets to echo back the sender‚Äôs timestamp, in addition to including their own. This allows senders to accurately measure round-trip times. Reboots are a potential problem as the host presumably has no record of what aborted connections need to remain in TIMEWAIT. TCP addresses this on paper by requiring hosts to implement Quiet Time on Startup: no new connections are to be accepted for 1*MSL. No known implementations actually do this; instead, they assume that the restarting process itself will take at least one MSL. This is no longer as certain as it once was, but serious consequences have not ensued. 18.5 TCP Faster Opening If a client wants to connect to a server, send a request and receive an immediate reply, TCP mandates one full RTT for the three-way handshake before data can be delivered. This makes TCP one RTT slower than UDP-based request-reply protocols. There have been periodic calls to allow TCP clients to include data with the Ô¨Årst SYN packet andhave it be delivered immediately upon arrival ‚Äì this is known as accelerated open. If there will be a series of requests and replies, the simplest way to deliver the data without a handshake delay is to pipeline all the requests and replies over one persistent connection; the handshake delay then applies only to the Ô¨Årst request. If the pipeline connection is idle for a long-enough interval, it may be closed, and then reopened later if necessary. An early accelerated-open proposal was T/TCP, or TCP for Transactions, speciÔ¨Åed in RFC 1644. T/TCP introduced a connection count TCP option, called CC; each participant would include a 32-bit CC value in its SYN; each participant‚Äôs own CC values were to be monotonically increasing. Accelerated open was allowed if the server side had the client‚Äôs previous CC in a cache, and the new CC value was strictly greater than this cached value. This ensured that the new SYN was not a duplicate of an older SYN. Unfortunately, this also bypasses the duplicate-SYN detection and modest validation of the client‚Äôs IP address provided by the full three-way handshake, worsening the spooÔ¨Ång problem of 18.3.1 ISNs and spooÔ¨Ång. If malicious host M wants to pretend to be B when sending a privileged request to A, all M has to do is send a single SYN+Data packet with an extremely large value for CC. Generally, the accelerated open succeeded as long as the CC value presented was larger that the value A had cached for B; it did not have to be larger by exactly 1. 18.5.1 TCP Fast Open The TCP Fast Open (TFO) mechanism, described in RFC 7413, involves a secure ‚Äúcookie‚Äù sent by the client as a TCP option; if a SYN+Data packet has a valid cookie, then the client has satisfactorily established its identity and the data may be released immediately to the receiving application. Cookies can be either 4 or 16 bytes (probably, though not necessarily, corresponding to IPv4 and IPv6), and are requested by the client through a previous TCP handshake with a cookie-request option in the SYN packet. If a client includes a still-valid cookie in the SYN packet of a subsequent connection, the data accompanying that SYN packet is immediately released by the server to the application; the three-way handshake still completes but the data does not wait for it. The same cookie can be reused multiple times. Cookies do have an expiration time, though, and also they are speciÔ¨Åc to the client IP address (though not to the TCP ports used). One implementation option is for 424 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 the server to use encryption (not hashing) of the client IP address; in this model the cookie expires when the encryption key expires. Because cookies must be requested ahead of time, TCP Fast Open is not fundamentally faster than the connection-pipeline option above, except that holding a TCP connection open uses more resources than simply storing a cookie. One likely application for TCP Fast Open is in accessing web servers. Web clients and servers already keep a persistent connection open for a while, but often ‚Äúa while‚Äù here amounts only to several seconds; TCP Fast Open cookies could remain active for much longer. Another potential use is for TCP-based DNS queries, for which there is no established mechanism for connection reuse. A serious practical problem with TCP Fast Open is that some middleboxes ( 9.7.2 Middleboxes ) remove TCP options they do not understand, or even block the connection attempt entirely. One consequence of this is that clients attempting to use TFO must log failures, and not attempt to reuse TFO again (at least for an appropriate time interval). Also, SYN Ô¨Çooding attacks are still possible; for example, a large number of compromised clients can obtain cookies legitimately, and then each reuse their cookie many times in short order. Alternatively, the cookie from oneclient can be distributed to a large number of other hosts which then spoof the original client‚Äôs IP address. To minimize the impact of such attacks, TFO requires that the fast-open option be ignored if the number of pending fast opens exceeds a given threshold. Connections would still open normally, but data would not be delivered to the server application until the three-way handshake completed. Finally, TFO does introduce a small possibility of duplicate data delivery. Consider, for example, the following sequence: 1. Then client sends a SYN with valid TFO cookie, and some data 2. The ACK from the server is lost, or is never sent 3. The server processes the data 4. The server reboots 5. The client times out, and retransmits its SYN+cookie+data, which arrives at the server The duplicate data arriving in the Ô¨Ånal step will be again processed by the server. This is not likely, but if either the client or the server cannot handle the possibility of duplication, then TFO should not be used. In particular, it must be possible to enable or disable TFO on a per-connection basis. Of course, if the request conveyed by the SYN+data is idempotent ( 16.5.2 Sun RPC ), the duplication should not matter. An alternative duplicate-data-delivery scenario involves the client sending a SYN+cookie+data packet and closing the connection. The client, then, goes into TIMEWAIT, but not the server. Meanwhile, the SYN+cookie+data packet somehow gets duplicated within the network, and this duplicate arrives at the server. This scenario (unlike the Ô¨Årst) is not prevented by arranging for the server‚Äôs cookie-generation key to become invalid after a reboot. 18.6 Path MTU Discovery TCP connections are more efÔ¨Åcient if they can keep large packets Ô¨Çowing between the endpoints. Once upon a time, TCP endpoints included just 512 bytes of data in each packet that was not destined for local delivery, to avoid fragmentation. TCP endpoints now typically engage in Path MTU Discovery which almost always 18.6 Path MTU Discovery 425
An Introduction to Computer Networks, Release 2.0.11 allows them to send larger packets; backbone ISPs are now usually able to carry 1500-byte packets. The Path MTU is the largest packet size that can be sent along a path without fragmentation. The IPv4 strategy is to send an initial data packet with the IPv4 DONT_FRAG bit set. If the ICMP message Frag_Required/DONT_FRAG_Set comes back, or if the packet times out, the sender tries a smaller size. If the sender receives a TCP ACK for the packet, on the other hand, indicating that it made it through to the other end, it might try a larger size. Usually, the size range of 512-1500 bytes is covered by less than a dozen discrete values; the point is not to Ô¨Ånd the exact Path MTU but to determine a reasonable approximation rapidly. IPv6 has no DONT_FRAG bit. Path MTU Discovery over IPv6 involves the periodic sending of larger packets; if the ICMPv6 message Packet Too Big is received, a smaller packet size must be used. RFC 1981 has details. 18.7 TCP Sliding Windows TCP implements sliding windows, in order to improve throughput. Window sizes are measured in terms of bytes rather than packets; this leaves TCP free to packetize the data in whatever segment size it elects. In the initial three-way handshake, each side speciÔ¨Åes the maximum window size it is willing to accept, in theWindow Size Ô¨Åeld of the TCP header. This 16-bit Ô¨Åeld can only go to 64 kB, and a 1 Gbps 100 ms bandwidthdelay product is 12 MB; as a result, there is a TCP Window Scale option that can also be negotiated in the opening handshake. The scale option speciÔ¨Åes a power of 2 that is to be multiplied by the actual Window Size value. In the WireShark example above, the client speciÔ¨Åed a Window Size Ô¨Åeld of 5888 (= 41472) in the third packet, but with a Window Scale value of 26= 64 in the Ô¨Årst packet, for an effective window size of 64 5888 = 256 segments of 1472 bytes. The server side speciÔ¨Åed a window size of 5792 and a scaling factor of 25= 32. TCP may either transmit a bulk stream of data, using sliding windows fully, or it may send slowly generated interactive data; in the latter case, TCP may never have even one full segment outstanding. In the following chapter we will see that a sender frequently reduces the actual TCP window size, in order to avoid congestion; the window size included in the TCP header is known as the Advertised Window Size. On startup, TCP does not send a full window all at once; it uses a mechanism called ‚Äúslow start‚Äù. 18.8 TCP Delayed ACKs TCP receivers are allowed brieÔ¨Çy to delay their ACK responses to new data. This offers perhaps the most beneÔ¨Åt for interactive applications that exchange small packets, such as ssh and telnet. If A sends a data packet to B and expects an immediate response, delaying B‚Äôs ACK allows the receiving application on B time to wake up and generate that application-level response, which can then be sent together with B‚Äôs ACK. Without delayed ACKs, the kernel layer on B may send its ACK before the receiving application on B has even been scheduled to run. If response packets are small, that doubles the total trafÔ¨Åc. The maximum ACK delay is 500 ms, according to RFC 1122 andRFC 2581, though 200 ms is more common. For bulk trafÔ¨Åc, delayed ACKs simply mean that the ACK trafÔ¨Åc volume is reduced. Because ACKs are cumulative, one ACK from the receiver can in principle acknowledge multiple data packets from the sender. Unfortunately, acknowledging too many data packets with one ACK can interfere with the self-clocking 426 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 aspect of sliding windows; the arrival of that ACK will then trigger a burst of additional data packets, which would otherwise have been transmitted at regular intervals. Because of this, the RFCs above specify that an ACK be sent, at a minimum, for every other data packet. For a discussion of how the sender should respond to delayed ACKs, see 19.2.1 TCP Reno Per-ACK Responses. The TCP ACK-delay time can usually be adjusted globally as a system parameter. Linux offers a TCP_QUICKACK option, as a Ô¨Çag to setsockopt(), to disable delayed ACKs on a per-connection basis, but only until the next TCP system call (including reads and writes). It must be invoked immediately after every receive operation to disable delayed ACKs entirely. This option is also not very portable. The TSO option of 17.5 TCP OfÔ¨Çoading, used at the receiver, can also reduce the number of ACKs sent. If every two arriving data packets are consolidated via TSO into a single packet, then the receiver will appear to the sender to be acknowledging every other data packet. The ACK delay introduced by TSO is, however, usually quite small. 18.9 Nagle Algorithm Like delayed ACKs, the Nagle algorithm ( RFC 896 ) also attempts to improve the behavior of interactive small-packet applications. It speciÔ¨Åes that a TCP endpoint generating small data segments ‚Äì segments of less than the maximum size ‚Äì should queue them until either it accumulates a full segment‚Äôs worth or receives an ACK for all the previously sent packets (small or not). If the full-segment threshold is not reached at the sender, this means that only one segment will be sent per RTT, containing all the data generated during that RTT. Bandwidth Conservation Delayed ACKs and the Nagle algorithm both originated in a bygone era, when bandwidth was in much shorter supply than it is today. In RFC 896, John Nagle writes (in 1984, well before TCP Reno, 19 TCP Reno and Congestion Management ) ‚ÄúIn general, we have not been able to afford the luxury of excess long-haul bandwidth that the ARPANET possesses, and our long-haul links are heavily loaded during peak periods. Transit times of several seconds are thus common in our network.‚Äù Today, it is unlikely that a modest number of small packets would cause detectable, let alone signiÔ¨Åcant, problems. That said, abandoning the Nagle algorithm has the potential to unleash onto the Internet backbone large numbers of small, mostly-header packets; [MM01] suggests ‚Äúit would be a mistake to stop using it.‚Äù As an example, suppose A wishes to send to B packets containing consecutive letters, starting with ‚Äúa‚Äù. The application on A generates these every 100 ms, but the RTT is 501 ms. At T=0, A transmits ‚Äúa‚Äù. The application on A continues to generate ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äúd‚Äù, ‚Äúe‚Äù and ‚Äúf‚Äù at times 100 ms through 500 ms, but A does not send them immediately. At T=501 ms, ACK(‚Äúa‚Äù) arrives; at this point A transmits its backlogged ‚Äúbcdef‚Äù. The ACK for this arrives at T=1002, by which point A has queued ‚Äúghijk‚Äù. The end result is that A sends a Ô¨Åfth as many separate packets as it would without the Nagle algorithm. If these letters are generated by a user typing them with telnet, and the ACKs also include the echoed responses, then if the user pauses the echoed responses will very soon catch up. The Nagle algorithm does not always interact well with delayed ACKs. If an application generates a 2 KB transaction that is divided between a full-sized packet and a followup small packet, then the Nagle algorithm means that the second packet cannot be sent until the Ô¨Årst is acknowledged. However, a receiver 18.9 Nagle Algorithm 427
An Introduction to Computer Networks, Release 2.0.11 using delayed ACKs may wait up to 500 ms to send the ACK that allows that second packet to be sent. This delays the entire transaction by the delayed-ACK time. Worse, this may happen to every one of a lengthy series of transactions. Internet Draft A Proposed ModiÔ¨Åcation to Nagle‚Äôs Algorithm addresses this by, in effect, always allowing senders to send onesmall packet without receiving an acknowledgment, to compensate for the possibility that a delayed-ACK receiver will need to receive that one small packet before it sends its ACK. More speciÔ¨Åcally, the modiÔ¨Åcation is to forbid the sending of small packets as long as there is an earlier unacknowledged small packet outstanding, versus the original rule forbidding the sending of small packets as long as there are anyearlier unacknowledged packets, small or not. For other examples, see exercises 1.0 and 2.0; the Ô¨Årst is an example of how the Nagle algorithm can have surprising user-interface consequences. The Nagle algorithm can usually be disabled on a per-connection basis, in the BSD socket library by calling setsockopt() with theTCP_NODELAY Ô¨Çag. 18.10 TCP Flow Control It is possible for a TCP sender to send data faster than the receiver can process it. When this happens, a TCP receiver may reduce the advertised Window Size value of an open connection, thus informing the sender to switch to a smaller window size. This provides support for Ô¨Çow control. The window-size reduction appears in the ACKs sent back by the receiver. A given ACK is not supposed to reduce the window size by so much that the upper end of the window gets smaller. A window might shrink from the byte range [20,000..28,000] to [22,000..28,000] but never to [20,000..26,000]. If a TCP receiver uses this technique to shrink the advertised window size to 0, this means that the sender may not send data. The receiver has thus informed the sender that, yes, the data was received, but that, no, more may not yet be sent. This corresponds to the ACK WAIT suggested in 8.1.3 Flow Control. Eventually, when the receiver is ready to receive data, it will send an ACK increasing the advertised window size again. If the TCP sender has its window size reduced to 0, and the ACK from the receiver increasing the window is lost, then the connection would be deadlocked. TCP has a special feature speciÔ¨Åcally to avoid this: if the window size is reduced to zero, the sender sends dataless packets to the receiver, at regular intervals. Each of these ‚Äúpolling‚Äù packets elicits the receiver‚Äôs current ACK; the end result is that the sender will receive the eventual window-enlargement announcement reliably. These ‚Äúpolling‚Äù packets are regulated by the so-called persist timer. 18.11 Silly Window Syndrome The silly-window syndrome is a term for a scenario in which TCP transfers only small amounts of data at a time. Because TCP/IP packets have a minimum Ô¨Åxed header size of 40 bytes, sending small packets uses the network inefÔ¨Åciently. The silly-window syndrome can occur when either by the receiving application consuming data slowly or when the sending application generating data slowly. As an example involving a slow-consuming receiver, suppose a TCP connection has a window size of 1000 bytes, but the receiving application consumes data only 10 bytes at a time, at intervals about equal to the RTT. The following can then happen: 
- The sender sends bytes 1-1000. The receiving application consumes 10 bytes, numbered 1-10. The receiving TCP buffers the remaining 990 bytes and sends an ACK reducing the window size to 10, 428 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 per18.10 TCP Flow Control. - Upon receipt of the ACK, the sender sends 10 bytes numbered 1001-1010, the most it is permitted. In the meantime, the receiving application has consumed bytes 11-20. The window size therefore remains at 10 in the next ACK. 
- the sender sends bytes 1011-1020 while the application consumes bytes 21-30. The window size remains at 10. The sender may end up sending 10 bytes at a time indeÔ¨Ånitely. This is of no beneÔ¨Åt to either side; the sender might as well send larger packets less often. The standard Ô¨Åx, set forth in RFC 1122, is for the receiver to use its ACKs to keep the window at 0 until it has consumed one full packet‚Äôs worth (or half the window, for small window sizes). At that point the sender is invited ‚Äì by an appropriate window-size advertisement in the next ACK ‚Äì to send another full packet of data. The silly-window syndrome can also occur if the sender is generating data slowly, say 10 bytes at a time. The Nagle algorithm, above, can be used to prevent this, though for interactive applications sending small amounts of data in separate but closely spaced packets may actually be useful. 18.12 TCP Timeout and Retransmission When TCP sends a packet containing user data (this excludes ACK-only packets), it sets a timeout. If that timeout expires before the packet data is acknowledged, it is retransmitted. Acknowledgments are sent for every arriving data packet (unless Delayed ACKs are implemented, 18.8 TCP Delayed ACKs ); this amounts to receiver-side retransmit-on-duplicate of 8.1.1 Packet Loss. Because ACKs are cumulative, and so a later ACK can replace an earlier one, lost ACKs are seldom a problem. For TCP to work well for both intra-server-room and trans-global connections, with RTTs ranging from well under 1 ms to close to 1 second, the length of the timeout interval must adapt. TCP manages this by maintaining a running estimate of the RTT, EstRTT. In the original version, TCP then set TimeOut = 2EstRTT (in the literature, the TCP TimeOut value is often known as RTO, for Retransmission TimeOut). EstRTT itself was a running average of periodically measured SampleRTT values, according to EstRTT = ùõºEstRTT + (1ùõº)SampleRTT for a Ô¨Åxed ùõº, 0<ùõº<1. Typical values of ùõºmight be ùõº=1/2 or ùõº=7/8. For ùõºclose to 1 this is ‚Äúconservative‚Äù in that EstRTT is slow to change. For ùõºcloser to 0, EstRTT is more volatile. There is a potential RTT measurement ambiguity: if a packet is sent twice, the ACK received could be in response to the Ô¨Årst transmission or the second. The Karn/Partridge algorithm resolves this: on packet loss (and retransmission), the sender 
- Doubles Timeout 
- Stops recording SampleRTT 
- Uses the doubled Timeout as EstRTT when things resume Setting TimeOut = 2 EstRTT proved too short during congestion periods and too long other times. Jacobson and Karels ([JK88]) introduced a way of calculating the TimeOut value based on the statistical variability of EstRTT. After each SampleRTT value was collected, the sender would also update EstDeviation according to 18.12 TCP Timeout and Retransmission 429
An Introduction to Computer Networks, Release 2.0.11 SampleDev = | SampleRTT ‚Äì EstRTT | EstDeviation = ùõΩEstDeviation + (1ùõΩ)SampleDev for a Ô¨Åxed ùõΩ, 0<ùõΩ<1. Timeout was then set to EstRTT + 4 EstDeviation. EstDeviation is an estimate of the so-called mean deviation; 4 mean deviations corresponds (for normally distributed data) to about 5 standard deviations. If the SampleRTT values were normally distributed (which they are not), this would mean that the chance that a non-lost packet would arrive outside the TimeOut period is vanishingly small. For further details, see [JK88] and [AP99]. In most implementations, a TCP sender maintains just one retransmission timer no matter how many packets are outstanding. Here is the recommended timer-management algorithm, from RFC 6298: - If a packet is sent and the timer is not running, restart it for time Timeout. 
- If an ACK arrives that acknowledges all outstanding data, turn off the timer. 
- If an ACK arrives that acknowledges new data, but not all outstanding data, reset the timer to time Timeout. If the sender transmits a steady stream of packets, none of which is lost, the last clause will ensure that the timer never Ô¨Åres, which is as desired. However, if a series of earlier ACKs arrives slowly, but just fast enough to keep resetting the timer, a lost packet may not time out until some multiple of Timeout has elapsed; while not ideal, this is not considered serious. See exercise 6.0. 18.13 KeepAlive There is no reason that a TCP connection should not be idle for a long period of time; ssh/telnet connections, for example, might go unused for days. However, there is the turned-off-at-night problem: a workstation might telnet into a server, and then be shut off (not shut down gracefully) at the end of the day. The connection would now be half-open, but the server would not generate any trafÔ¨Åc and so might never detect this; the connection itself would continue to tie up resources. KeepAlive in action One evening long ago, when dialed up (yes, that long ago) into the Internet, my phone line disconnected while I was typing an email message in an ssh window. I dutifully reconnected, expecting to Ô¨Ånd my message in the Ô¨Åle ‚Äúdead.letter‚Äù, which is what would have happened had I been disconnected while using the even-older tty dialup. Alas, nothing was there. I reconstructed my email as best I could and logged off. The next morning, there was my lost email in a Ô¨Åle ‚Äúdead.letter‚Äù, dated two hours after the initial crash! What had happened, apparently, was that the original ssh connection on the server side just hung there, half-open. Then, after two hours, KeepAlive kicked in, and aborted the connection. At that point ssh sent my mail program the HangUp signal, and the mail program wrote out what it had in ‚Äúdead.letter‚Äù. To avoid this, TCP supports an optional KeepAlive mechanism: each side ‚Äúpolls‚Äù the other with a dataless packet. The original RFC 1122 KeepAlive timeout was 2 hours, but this could be reduced to 15 minutes. If a connection failed the KeepAlive test, it would be closed. 430 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 Supposedly, some TCP implementations are not exactly RFC 1122 -compliant: either KeepAlives are enabled by default, or the KeepAlive interval is much smaller than called for in the speciÔ¨Åcation. 18.14 TCP timers To summarize, TCP maintains the following four kinds of timers. All of them can be maintained by a single timer list, above. 
- TimeOut: a per-segment timer; TimeOut values vary widely 
- 2MSL TIMEWAIT: a per-connection timer 
- Persist: the timer used to poll the receiving end when winsize = 0 
- KeepAlive, above 18.15 Variants and Alternatives One alternative to TCP is UDP with programmer-implemented timout and retransmission; many RPC implementations ( 16.5 Remote Procedure Call (RPC) ) do exactly this, with reasonable results. Within a LAN a static timeout of around half a second usually works quite well (unless the LAN has some tunneled links), and implementation of a simple timeout-retransmission mechanism is quite straightforward, although implementing adaptive timeouts as in 18.12 TCP Timeout and Retransmission is a bit more complex. QUIC (16.1.1 QUIC ) is an example of this strategy. We here consider four other protocols. The Ô¨Årst, MPTCP, is based on TCP itself. The second, SCTP, is a message-oriented alternative to TCP that is an entirely separate protocol. The last two, DCCP and QUIC, are attempts to create a TCP-like transport layer on top of UDP. 18.15.1 MPTCP Multipath TCP, or MPTCP, allows connections to use multiple network interfaces on a host, either sequentially or simultaneously. MPTCP architectural principles are outlined in RFC 6182; implementation details are in RFC 6824. To carry the actual trafÔ¨Åc, MPTCP arranges for the creation of multiple standard-TCP subÔ¨Çows between the sending and receiving hosts; these subÔ¨Çows typically connect between different pairs of IP addresses on the respective hosts. For example, a connection to a server can start using the client‚Äôs wired Ethernet interface, and continue via Wi-Fi after the user has unplugged. If the client then moves out of Wi-Fi range, the connection might continue via a mobile network. Alternatively, MPTCP allows the parallel use of multiple Ethernet interfaces on both client and server for higher throughput. MPTCP ofÔ¨Åcially forbids the creation of multiple TCP connections between a single pair of interfaces in order to simulate Highspeed TCP ( 22.5 Highspeed TCP );RFC 6356 spells out an MWTCP congestioncontrol algorithm to enforce this. 18.14 TCP timers 431
An Introduction to Computer Networks, Release 2.0.11 Suppose host A, with two interfaces with IP addresses A 1and A 2, wishes to connect to host B with IP addresses B 1and B 2. Connection establishment proceeds via the ordinary TCP three-way handshake, between one of A‚Äôs IP addresses, say A 1, and one of B‚Äôs, B 1. The SYN packets must each carry the MP_CAPABLE TCP option, to signal one another that MPTCP is supported. As part of the MP_CAPABLE option, A and B also exchange pseudorandom 64-bit connection keys, sent unencrypted; these will be used to sign later messages as in 28.6.1 Secure Hashes and Authentication. This Ô¨Årst connection is the initial subÔ¨Çow. Once the MPTCP initial subÔ¨Çow has been established, additional subÔ¨Çow connections can be made. Usually these will be initiated from the client side, here A, though the B side can also do this. At this point, however, A does not know of B‚Äôs address B 2, so the only possible second subÔ¨Çow will be from A 2to B 1. New subÔ¨Çows will carry the MP_JOIN option with their initial SYN packets, along with digital signatures signed by the original connection keys verifying that the new subÔ¨Çow is indeed part of this MPTCP connection. At this point A and B can send data to one another using both connections simultaneously. To keep track of data, each side maintains a 64-bit data sequence number, DSN, for the data it sends; each side also maintains a mapping between the DSN and the subÔ¨Çow sequence numbers. For example, A might send 1000-byte blocks of data alternating between the A 1and A 2connections; the blocks might have DSN values 10000, 11000, 12000, 13000,. .. . The A 1subÔ¨Çow would then carry blocks 10000, 12000, etc, numbering these consecutively (perhaps 20000, 21000,. .. ) with its own sequence numbers. The sides exchange DSN mapping information with a DSS TCP option. This mechanism means that all data transmitted over the MWTCP connection can be delivered in the proper order, and that if one subÔ¨Çow fails, its data can be retransmitted on another subÔ¨Çow. B can inform A of its second IP address, B 2, using theADD_ADDR option. Of course, it is possible that B 2 is not directly reachable by A; for example, it might be behind a NAT router. But if B 2isreachable, A can now open two more subÔ¨Çows A 1B2and A 2B2. All the above works equally well if either or both of A‚Äôs addresses is behind a NAT router, simply because the NAT router is able to properly forward the subÔ¨Çow TCP connections. Addresses sent from one host to another, such as B‚Äôs transmission of its address B 2, may be rendered invalid by NAT, but in this case A‚Äôs attempt to open a connection to B 2simply fails. Generally, hosts can be conÔ¨Ågured to use multiple subÔ¨Çows in parallel, or to use one interface only as a backup, when the primary interface is unplugged or out of range. APIs have been proposed that allow an control over MPTCP behavior on a per-connection basis. 18.15.2 SCTP The Stream Control Transmission Protocol, SCTP, is an entirely separate protocol from TCP, running directly above IP. It is, in effect, a message-oriented alternative to TCP: an application writes a sequence of messages and SCTP delivers each one as a unit, fragmenting and reassembling it as necessary. Like TCP, SCTP is connection-oriented and reliable. SCTP uses a form of sliding windows, and, like TCP, adjusts the window size to manage congestion. An SCTP connection can support multiple message streams; the exact number is negotiated at startup. A retransmission delay in one stream never blocks delivery in other streams. Within each stream, SCTP messages are sequentially numbered, and are normally delivered in order of message number. A receiver can request, however, to receive messages immediately upon successful delivery, that is, potentially out of order. Either way, the data within each message is guaranteed to be delivered in order and without loss. 432 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 Internally, message data is divided into SCTP chunks for inclusion in packets. One SCTP packet can contain data chunks from different messages and different streams; packets can also contain control chunks. Messages themselves can be quite large; there is no set limit. Very large messages may need to be received in multiple system calls ( egcalls torecvmsg() ). SCTP supports an MPTCP-like feature by which each endpoint can use multiple network interfaces. SCTP connections are set up using a four-way handshake, versus TCP‚Äôs three-way handshake. The extra packet provides some protection against so-called SYN Ô¨Çooding ( 17.3 TCP Connection Establishment ). The central idea is that if client A initiates a connection request with server B, then B allocates no resources to the connection until after B has received a response to its own message to A. This means that, at a minimum, A is a real host with a real IP address. The full four-way handshake between client A and server B is, in brief, as follows: 
- A sends B an INIT chunk (corresponding to SYN), along with a pseudorandom Tag A. 
- B sends A an INIT ACK, with Tag Band a state cookie. The state cookie contains all the information B needs to allocate resources to the connection, and is digitally signed ( 28.6.1 Secure Hashes and Authentication ) with a key known only to B. Crucially, B does notat this point allocate any resources to the incipient connection. 
- A returns the state cookie to B in a COOKIE ECHO packet. 
- B enters the ESTABLISHED state and sends a COOKIE ACK to A. Upon receipt, A enters the ESTABLISHED state. When B receives the COOKIE ECHO, it veriÔ¨Åes the signature. At this point B knows that it sent the cookie to A and received a response, so A must exist. Only then does B allocate memory resources to the connection. Spoofed INITs in the Ô¨Årst step cost B essentially nothing. The Tag Aand Tag Bin the Ô¨Årst two packets are called veriÔ¨Åcation tags. From this point on, B will include Tag Ain every packet it sends to A, and vice-versa. Although these tags are sent unencrypted, they nonetheless make it much harder for an attacker to inject data into the connection. Data can be included in the third and fourth packets above; ieA can begin sending data after one RTT. Unfortunately for potential SCTP applications, few if any NAT routers recognize SCTP; this limits the use of SCTP to Internet paths along which NAT is not used. In principle SCTP could simplify delivery of web pages, transmitting one page component per message, but lack of NAT support makes this infeasible. SCTP is also blocked by some middleboxes ( 9.7.2 Middleboxes ) on the grounds that it is an unknown protocol, and therefore suspect. While this is not quite as common as the NAT problem, it is common enough to prevent by itself the widespread adoption of SCTP in the general Internet. SCTP iswidely used for telecommunications signaling, both within and between providers, where NAT and recalcitrant middleboxes can be banished. 18.15.3 DCCP As we saw in 16.1.2 DCCP, DCCP is a UDP-based transport protocol that supports, among other things, connection establishment. While it is used much less often than TCP, it provides an alternative example of how transport can be done. 18.15 Variants and Alternatives 433
An Introduction to Computer Networks, Release 2.0.11 DCCP deÔ¨Ånes a set of distinct packet types, rather than TCP‚Äôs independent packet Ô¨Çags; this disallows unforeseen combinations such as TCP SYN+RST. Connection establishment involves Request and Respond; data transmission involves Data, ACK and DataACK, and teardown involves CloseReq, Close and Reset. While one cannot have, for example, a Respond+ACK, Respond packets do carry an acknowledgment Ô¨Åeld. Like TCP, DCCP uses a three-way handshake to open a connection; here is a diagram: A B Request Response ACK DCCP ‚Äúthree‚Äù-way handshake The fourth packet, DATA or ACK, is not considered part of the handshake itself.CLOSED REQUEST PARTOPENLISTEN RESPOND OPEN OPENDATA or ACK The OPEN state corresponds to TCP‚Äôs ESTABLISHED state. Like TCP, each side chooses an ISN (not shown in the diagram). Because packet delivery is not reliable, and because ACKs are not cumulative, the client remains in PARTOPEN state until it has conÔ¨Årmed that the server has received its ACK of the server‚Äôs Response. While in state PARTOPEN, the client can send ACK and DataACK but not ACK-less Data packets. Packets are numbered sequentially. The numbering includes all packets, not just Data packets, and is by packet rather than by byte. The DCCP state diagram is shown below. It is simpler than the TCP state diagram because DCCP does not support simultaneous opens. 434 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 CLOSED OPENLISTEN RESPONDREQUEST PARTOPEN CLOSEREQ CLOSING TIMEWAIT CLOSEDopen / Request Response / ACK Request / Response ACK / any ACK /passive open / server close / CloseReqServer side Client side client close / Close Reset / Close / Reset Close / Reset 2*MSL timer DCCP State Diagram To close a connection, one side sends Close and the other responds with Reset. Reset is used for normal close as well as for exceptional conditions. Because whoever sends the Close is then stuck with TIMEWAIT, the server side may send CloseReq to ask the client to send Close. There are also two special packet formats, Sync and SyncAck, for resynchronizing sequence numbers after a burst of lost packets. The other major TCP-like feature supported by DCCP is congestion control; see 21.3.3 DCCP Congestion Control. 18.15.4 QUIC Revisited Like DCCP, QUIC (see also 16.1.1 QUIC ) is a UDP-based transport protocol, aimed rather squarely at HTTP plus TLS ( 29.5.2 TLS ). The fundamental goal of QUIC is to provide TLS encryption protection with as little overhead as possible, in a manner that competes fairly with TCP in the presence of congestion. Opening a QUIC connection, encryption included, takes a single RTT. QUIC can also be seen, however, as a 18.15 Variants and Alternatives 435
An Introduction to Computer Networks, Release 2.0.11 complete rewrite of TCP from the ground up; a reading of speciÔ¨Åc features sheds quite a bit of light on how the corresponding TCP features have fared over the past thirty-odd years. As of 2021, QUIC was Ô¨Ånally edited to RFC status: 
- RFC 8999: Version-independent Properties of QUIC 
- RFC 9000: QUIC: A UDP-Based Multiplexed and Secure Transport (good overview) 
- RFC 9001: Using TLS to Secure QUIC 
- RFC 9002: QUIC Loss Detection and Congestion Control 
- RFC 9114: HTTP/3 (which uses QUIC) The move to base HTTP/3 on QUIC began ofÔ¨Åcially in 2018; RFC 9114 was published in June 2022. QUIC‚Äôs standardization may represent the beginning of the end for TCP, given that most Internet connections are for HTTP or HTTPS. Still, many TCP design issues ( 19 TCP Reno and Congestion Management, 20 Dynamics of TCP ,22 Newer TCP Implementations ) carry over very naturally to QUIC; a shift from TCP to QUIC should best be viewed as evolutionary. (And, by the same token, the 1995 standardization of IPv6 presumably represents the beginning of the end for IPv4, but that was over 25 years ago.) The design of QUIC was inÔ¨Çuenced by the fate of SCTP above; the latter, as a new protocol above IP, was often blocked by overly security-conscious middleboxes ( 9.7.2 Middleboxes ). The fact that the QUIC layer resides within an application (or within a library) rather than within the kernel has meant that QUIC is able to evolve much faster than TCP. The long-term consequences of having the transport layer live outside the kernel are not yet completely clear, however; it may, for example, make it easier for users to install unfair congestion-management schemes. 18.15.4.1 Headers We will start with the QUIC header. While there are some alternative forms, the basic header is diagrammed below, with a 1-byte Type Ô¨Åeld, an 8-byte Connection ID, and 4-byte Version and Packet Number Ô¨Åelds. Connection ID Version Packet NumberType Typical QUIC Long HeaderPayload.... Perhaps the most striking thing about this header is that 4-byte alignment ‚Äì used consistently in the IPv4, 436 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 IPv6, UDP and TCP headers ‚Äì has been completely abandoned. On most contemporary processors, the performance advantages of alignment are negligible; see the last paragraph at 9.1 The IPv4 Header. IP packets are identiÔ¨Åed as such by the Ethernet type Ô¨Åeld, and TCP and UDP packets are identiÔ¨Åed as such by the IPv4-header Protocol Ô¨Åeld. But QUIC packets are notidentiÔ¨Åed as such by any Ô¨Çag in the preceding IP or UDP headers; there is in fact no place in those headers for a QUIC marker to go. QUIC appears to an observer as just another form of UDP trafÔ¨Åc. This acts as a form of middlebox defense; QUIC packets cannot be identiÔ¨Åed as such in isolation. WireShark, sidebar below, identiÔ¨Åes QUIC packets by looking at the whole history of the connection, and even then must make some (educated) guesses. Middleboxes could do that too, but it would take work. The initial Connection ID consists of 64 random bits chosen by the client. The server, upon accepting the connection, may change the Connection ID; at that point the Connection ID is Ô¨Åxed for the lifetime of the connection. The Connection ID may be omitted for packets whose connection can be determined from the associated IP address and port values; this is signaled by the Type Ô¨Åeld. The Connection ID can also be used to migrate a connection to a different IP address and port, as might happen if a mobile device moves out of range of Wi-Fi and the mobile-data plan continues the communication. This may also happen if a connection passes through a NAT router. The NAT forwarding entry may time out (see the comment on UDP and inactivity at 9.7 Network Address Translation ), and the connection may be assigned a different outbound UDP port if it later resumes. QUIC uses the Connection ID to recognize that the reassigned connection is still the same one as before. TheVersion Ô¨Åeld gets dropped as soon as the version is negotiated. As part of the version negotiation, a packet might have multiple version Ô¨Åelds. Such packets put a random value into the low-order seven bits of theType Ô¨Åeld, as a prevention against middleboxes‚Äô blocking unknown types. This way, aggressive middlebox behavior should be discovered early, before it becomes widespread. QUIC-watching QUIC packets can be observed in WireShark by using the Ô¨Ålter string ‚Äúquic‚Äù. To generate QUIC trafÔ¨Åc, use a Chromium-based browser and go to a Google-operated site, say, google.com. Often the only nonencrypted Ô¨Åelds are the Type Ô¨Åeld and the packet number. It may be necessary to enable QUIC in the browser, done in Chrome via chrome://Ô¨Çags; see also chrome://net-internals. The packet number can be reduced to one or two bytes once the connection is established; this is signaled by theType Ô¨Åeld. Internally, QUIC uses packet numbers in the range 0 to 262; these internal numbers are not allowed to wrap around. The low-order 32 bits (or 16 bits or 8 bits) of the internal number are what is transmitted in the packet header. A packet receiver infers the high-order bits from the most recent acknowledgment. The initial packet number is to be chosen randomly in the range 0 to 232‚Äì1025. This corresponds to TCP‚Äôs use of Initial Sequence Numbers. Use of 16-bit or 8-bit transmitted packet numbers is restricted to cases where there can be no ambiguity. At a minimum, this means that the number of outstanding packets (the QUIC winsize) cannot exceed 27‚Äì1 for 8-bit packet numbering or 215‚Äì1 for 16-bit packet numbering. These maximum winsizes represent the ideal case where there is no packet reordering; smaller values are likely to be used in practice. (See 8.5 Exercises, exercise 9.0.) 18.15 Variants and Alternatives 437
An Introduction to Computer Networks, Release 2.0.11 18.15.4.2 Frames and streams Data in a QUIC packet is partitioned into one or more frames. Each frame‚Äôs data is preÔ¨Åxed by a simple frame header indicating its length and type. Some frames contain management information; frames containing higher-layer data are called STREAM frames. Each frame must be fully contained in one packet. The application‚Äôs data can be divided into multiple streams, depending on the application requirements. This is particularly useful with HTTP, as a client may request a large number of different resources (html, images, javascript, etc) simultaneously. Stream data is contained in STREAM frames. Streams are numbered, with Stream 0 reserved for the TLS cryptographic handshake. The HTTP/2 protocol has introduced its own notion of streams; these map neatly onto QUIC streams. The two low-order bits of each stream number indicate whether the stream was initiated by the client or by the server, and whether it is bior uni-directional. This design decision means that either side can create a stream and send data on it immediately, without negotiation; this is important for reducing unnecessary RTTs. Each individual stream is guaranteed in-order delivery, but there are no ordering guarantees between different streams. Within a packet, the data for a particular stream is contained in a frame for that stream. One packet can contain stream frames for multiple streams. However, if a packet is lost, streams that have frames contained in that packet are blocked until retransmission. Other streams can continue without interruption. This creates an incentive for keeping separate streams in separate packets. Stream frames contain the byte offset of the frame‚Äôs block of stream data (starting from 0), to enable in-order stream reassembly. TCP, as we have seen, uses this byte-numbering approach exclusively, though starting with the Initial Sequence Number rather than zero. QUIC‚Äôs stream-level numbering by byte is unrelated to its top-level numbering by packet. In addition to stream frames, there are a large number of management frames. Here are a few of them: 
- RST_STREAM: like TCP RST, but for one stream only. 
- MAX_DATA: this corresponds to the TCP advertised window size. As with TCP, it can be reduced to zero to pause the Ô¨Çow of data and thereby implement Ô¨Çow control. There is also a similar MAX_STREAM_DATA, applying per stream. 
- PING andPONG: to verify that the other endpoint is still responding. These serve as the equivalent of TCP KEEPALIVEs, among other things. 
- CONNECTION_CLOSE andAPPLICATION_CLOSE: these initiate termination of the connection; they differ only in that a CONNECTION_CLOSE might be accompanied by a QUIC-layer error or explanation message while an APPLICATION_CLOSE might be accompanied by, say, an HTTP error/explanation messge. 
- PAD: to pad out the packet to a larger size. 
- ACK: for acknowledgments, below. 18.15.4.3 Acknowledgments QUIC assigns a new, sequential packet number (the Packet ID ) to every packet, including retransmissions. TCP, by comparison, assigns sequence numbers to each byte. (QUIC stream frames do number data 438 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 by byte, as noted above.) Lost QUIC packets are retransmitted, but with a new packet number. This makes it impossible for a receiver to send cumulative acknowledgments, as lost packets will never be acknowledged. The receiver handles this as below. At the sender side, the sender maintains a list of packets it has sent that are both unacknowledged and also not known to be lost. These represent the packets in Ô¨Çight. When a packet is retransmitted, its old packet number is removed from this list, as lost, and the new packet number replaces it. To the extent possible given this retransmission-renumbering policy, QUIC follows the spirit of sliding windows. It maintains a state variable bytes_in_flight, corresponding to TCP‚Äôs winsize, listing the total size of all the packets in Ô¨Çight. As with TCP, new acknowledgments allow new transmissions. Acknowledgments themselves are sent in special acknowledgment frames. These begin with the number of the highest packet received. This is followed by a list of pairs, as long as will Ô¨Åt into the frame, consisting of the length of the next block of contiguous packets received followed by the length of the intervening gap of packets notreceived. The TCP Selective ACK ( 19.6 Selective Acknowledgments (SACK) ) option is similar, but is limited to three blocks of received packets. It is quite possible that some of the gaps in a QUIC ACK frame refer to lost packets that were long since retransmitted with new packet numbers, but this does not matter. The sender is allowed to skip packet numbers occasionally, to prevent the receiver from trying to increase throughput by acknowledging packets not yet received. Unlike with TCP, acknowledging an unsent packet is considered to be a fatal error, and the connection is terminated. As with TCP, there is a delayed-ACK timer, but, while TCP‚Äôs is typically 250 ms, QUIC‚Äôs is 25 ms. QUIC also includes in each ACK frame the receiver‚Äôs best estimate of the elapsed time between arrival of the most recent packet and the sending of the ACK it triggered; this allows the sender to better estimate the RTT. The primary advantage of the design decision not to reuse packet IDs is that there is never any ambiguity as to a retransmitted packet‚Äôs RTT, as there is in TCP ( 18.12 TCP Timeout and Retransmission ). Note, however, that because QUIC runs in a user process and not the kernel, it may not be able to respond immediately to an arriving packet, and so the time-delay estimate may be slightly short. ACK frames are not themselves acknowledged. This means that, in a one-way data Ô¨Çow, the receiver may have no idea if its ACKs are getting through (a TCP receiver may be in the same situation). The QUIC receiver may send a PING frame to the sender, which will respond not only with a matching PONG frame but also an ACK frame acknowledging the receiver‚Äôs recent acknowledgment packets. QUIC adjusts its bytes_in_flight value to manage congestion, much as TCP manages its winsize (or more properly its cwnd ,19 TCP Reno and Congestion Management ) for the same purpose. SpeciÔ¨Åcally, QUIC attempts to mimic the congestion response of TCP Cubic, 22.15 TCP CUBIC, and so should in theory compete fairly with TCP Cubic connections. However, it is straightforward to arrange for QUIC to model the behavior of any other Ô¨Çavor of TCP ( 22 Newer TCP Implementations ). 18.15.4.4 Connection handshake and TLS encryption The opening of a QUIC connection makes use of the TLS handshake, 29.5.2 TLS, speciÔ¨Åcally TLS v1.3, 29.5.2.4.3 TLS version 1.3. A client wishing to connect sends a QUIC Initial packet, containing the TLSClientHello message. The server responds (with a ServerHello ) in a QUIC Handshake packet. (There is also a Retry packet, for special situations.) The TLS negotiation is contained in QUIC‚Äôs 18.15 Variants and Alternatives 439
An Introduction to Computer Networks, Release 2.0.11 Stream 0. While the TLS and QUIC handshake rules are rather precise, there is as yet no formal statediagram description of connection opening. TheInitial packet also contains a set of QUIC transport parameters declared unilaterally by the client; the server makes a similar declaration in its response. These parameters include, among other things, the maximum packet size, the connection‚Äôs idle timeout, and initial value for MAX_DATA, above. An important feature of TLS v1.3 is that, if the client has connected to the server previously and still has the key negotiated in that earlier session, it can use that old key to send an encrypted application-layer request (in a STREAM frame) immediately following the Initial packet. This is called 0-RTT protection (or encryption). The advantage of this is that the client may receive an answer from the server within a single RTT, versus four RTTs for traditional TCP (one for the TCP three-way handshake, two for TLS negotiation, and one for the application request/reply). As discussed at 29.5.2.4.4 TLS v1.3 0-RTT mode, requests submitted with 0-RTT protection must be idempotent, to prevent replay attacks. Once the server‚Äôs Ô¨Årst Handshake packet makes it back to the client, the client is in possession of the key negotiated by the new session, and will encrypt everything using that going forward. This is known as the 1-RTT key, and all further data is said to be 1-RTT protected. The negotiated key is initially calculated by the TLS layer, which then exports it to QUIC. The QUIC layer then encrypts the entire data portion of its packets, using the format of RFC 5116. The QUIC header is not encrypted, but is still covered by an authentication checksum, making it impossible for middleboxes to rewrite anything. Such rewriting has been observed for TCP, and has sometimes complicated TCP evolution. The type Ô¨Åeld of a QUIC packet contains a special code to mark 0-RTT data, ensuring that the receiver will know what level of protection is in effect. When a QUIC server receives the ClientHello and sends off its ServerHello, it has not yet received any evidence that the client ‚Äúowns‚Äù the IP address it claims to have; that is, that the client is not spooÔ¨Ång its IP address ( 18.3.1 ISNs and spooÔ¨Ång ). Because of the idempotency restriction on responses to 0-RTT data, the server cannot give away privileges if spoofed in this way by a client. The server may, however, be an unwitting participant in a trafÔ¨Åc-ampliÔ¨Åcation attack, if the real client can trigger the sending by the server to a spoofed client of a larger response than the real client sends directly. The solution here is to require that the QUIC Initial packet, containing the ClientHello, be at least 1200 bytes. The server‚ÄôsHandshake response is likely to be smaller, and so represents no ampliÔ¨Åcation of trafÔ¨Åc. To close the connection, one side sends a CONNECTION_CLOSE orAPPLICATION_CLOSE. It may continue to send these in response to packets from the other side. When the other side receives the CLOSE packet, it should send its own, and then enter the so-called draining state. When the initiator of the close receives the other side‚Äôs echoed CLOSE, it too will enter the draining state. Once in this state, an endpoint may not send any packets. The draining state corresponds to TCP‚Äôs TIMEWAIT ( 18.2 TIMEWAIT ), for the purpose of any lost Ô¨Ånal ACKs; it should last three RTT‚Äôs. There is no need of a TIMEWAIT analog to prevent old duplicates, as a second QUIC connection will select a new Connection ID. QUIC connection closing has no analog of TCP‚Äôs feature in which one side sends FIN and the other continues to send data indeÔ¨Ånitely, 17.8.1 Closing a connection. This use of FIN, however, is allowed in bidirectional streams; the per-stream (and per-direction) FIN bit lives in the stream header. Alternatively, one side can send its request and close its stream, and the other side can then answer on a different stream. 440 18 TCP Issues and Alternatives
An Introduction to Computer Networks, Release 2.0.11 18.16 Epilog At this point we have covered the basic mechanics of TCP, but have one important topic remaining: how TCP manages its window size so as to limit congestion, while maintaining fairness. This turns out to be complex, and will be the focus of the next three chapters. 18.17 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0. A user moves the computer mouse and sees the mouse-cursor‚Äôs position updated on the screen. Suppose the mouse-position updates are being transmitted over a TCP connection with a relatively long RTT. The user attempts to move the cursor to a speciÔ¨Åc point. How will the user perceive the mouse‚Äôs motion (a). with the Nagle algorithm (b). without the Nagle algorithm 2.0. Host A sends two single-byte packets, one containing ‚Äúx‚Äù and the other containing ‚Äúy‚Äù, to host B. A implements the Nagle algorithm and B implements delayed ACKs, with a 500 ms maximum delay. The RTT is negligible. How long does the transmission take? Draw a ladder diagram. 3.0. Suppose you have fallen in with a group that wants to add to TCP a feature so that, if A and B1 are connected, then B1 can hand off its connection to a different host B2; the end result is that A and B2 are connected and A has received an uninterrupted stream of data. Either A or B1 can initiate the handoff. (a). Suppose B1 is the host to send the Ô¨Ånal FIN (or HANDOFF) packet to A. How would you handle appropriate analogues of the TIMEWAIT state for host B1? Does the fact that A is continuing the connection, just not with B1, matter? (b). Now suppose A is the party to send the Ô¨Ånal FIN/HANDOFF, to B1. What changes to TIMEWAIT would have to be made at A‚Äôs end? Note that A may potentially hand off the connection again and again, eg to B3, B4 and then B5. 4.0. Suppose A connects to B via TCP, and sends the message ‚ÄúAttack at noon‚Äù, followed by FIN. Upon receiving this, B is sure it has received the entire message. (a). What can A be sure of upon receiving B‚Äôs own FIN+ACK? (b). What can B be sure of upon receiving A‚Äôs Ô¨Ånal ACK? (c). What is A not absolutely sure of after sending its Ô¨Ånal ACK? 5.0. Host A connects to the Internet via Wi-Fi, receiving IPv4 address 10.0.0.2, and then opens a TCP 18.16 Epilog 441
An Introduction to Computer Networks, Release 2.0.11 connection conn1 to remote host B. After conn1 is established, A‚Äôs Ethernet cable is plugged in. A‚Äôs Ethernet interface receives IP address 10.0.0.3, and A automatically selects this new Ethernet connection as its default route. Assume that A now starts using 10.0.0.3 as the source address of packets it sends as part of conn1 (contrary to RFC 1122 ). Assume also that A‚Äôs TCP implementation is such that when a packet arrives from xBIP, BportytoxAIP, Aportyand this socketpair is to be matched to an existing TCP connection, the Ô¨Åeld A IPis allowed to be any of A‚Äôs IP addresses (that is, either 10.0.0.2 or 10.0.0.3); it does not have to match the IP address with which the connection was originally negotiated. (a). Explain why conn1 will now fail, as soon as any packet is sent from A. Hint: the packet will be sent from 10.0.0.3. What will B send in response? In light of the second assumption, how will A react to B‚Äôs response packet? (The author regularly sees connections fail this way. Perhaps some justiÔ¨Åcation for this behavior is that, at the time of establishment of conn1, A was not yet multihomed.) (b). Now suppose all four Ô¨Åelds of the socketpair ( xBIP, Bporty,xAIP, Aporty) are used to match an incoming packet to its corresponding TCP connection. The connection conn1 still fails, though not as immediately. Explain what happens. See also 10.2.5 ARP and multihomed hosts ,9 IP version 4 exercise 4.0, and 13 Routing-Update Algorithms exercise 16.0. 6.0. Draw a ladder diagram showing a lost packet transmitted at time T, and yet the retransmission timer does not go off until at least T + 3*Timeout (there is nothing special about 3 here). Assume that the algorithm of 18.12 TCP Timeout and Retransmission is used. Hint: show a series of ACKs for previous packets arriving at intervals of just under Timeout, causing a series of resets of the timer. 442 18 TCP Issues and Alternatives
19 TCP RENO AND CONGESTION MANAGEMENT This chapter addresses how TCP manages congestion, both for the connection‚Äôs own beneÔ¨Åt (to improve its throughput) and for the beneÔ¨Åt of other connections as well (which may result in our connection reducing its own throughput). Early work on congestion culminated in 1990 with the Ô¨Çavor of TCP known as TCP Reno. The congestion-management mechanisms of TCP Reno remain the dominant approach on the Internet today, though alternative TCPs are an active area of research and we will consider a few of them in 22 Newer TCP Implementations. The central TCP mechanism here is for a connection to adjust its window size. A smaller winsize means fewer packets are out in the Internet at any one time, and less trafÔ¨Åc means less congestion. A larger winsize means better throughput, up to a point. All TCPs reduce winsize when congestion is apparent, and increase it when it is not. The trick is in Ô¨Åguring out when and by how much to make these winsize changes. Many of the improvements to TCP have come from mining more and more information from the stream of returning ACKs. The Anternet The Harvester Ant Pogonomyrmex barbatus uses a mechanism related to TCP Reno to ‚Äúdecide‚Äù how many ants should be out foraging at any one time [PDG12]. The rate of ants leaving the nest to forage is closely tied to the rate of returning foragers; if foragers return quickly (meaning more food is available), the total number of foragers will increase (like the increasing winsize below). The ant algorithm is probabilistic, however, while most TCP algorithms are deterministic. Recall Chiu and Jain‚Äôs deÔ¨Ånition from 1.7 Congestion that the ‚Äúknee‚Äù of congestion occurs when the queue Ô¨Årst starts to grow, and the ‚Äúcliff‚Äù of congestion occurs when packets start being dropped. Congestion can be managed at either point, though dropped packets can be a signiÔ¨Åcant waste of resources. Some newer TCP strategies attempt to take action at the congestion knee (starting with 22.6 TCP Vegas ), but TCP Reno is a cliff-based strategy: packets must be lost before the sender reduces the window size. In25 Quality of Service we will consider some router-centric alternatives to TCP for Internet congestion management. However, for the most part these have not been widely adopted, and TCP is all that stands in the way of Internet congestive collapse. The Ô¨Årst question one might ask about TCP congestion management is just how did it get this job? A TCP sender is expected to monitor its transmission rate so as to cooperate with other senders to reduce overall congestion among the routers. While part of the goal of every TCP node is good, stable performance for its own connections, this emphasis on end-user cooperation introduces the prospect of ‚Äúcheating‚Äù: a host might be tempted to maximize the throughput of its own connections at the expense of others. Putting TCP nodes in charge of congestion among the core routers is to some degree like putting the foxes in charge of the henhouse. More accurately, such an arrangement has the potential to lead to the Tragedy of the Commons. Multiple TCP senders share a common resource ‚Äì the Internet backbone ‚Äì and while the backbone is most efÔ¨Åcient if every sender cooperates, each individual sender can improve its own situation by sending faster than allowed. Indeed, one of the arguments used by virtual-circuit routing adherents is that it provides support for the implementation of a wide range of congestion-management options under control of a central authority. 443
An Introduction to Computer Networks, Release 2.0.11 Nonetheless, TCP has been quite successful at distributed congestion management. In part this has been because system vendors do have an incentive to take the big-picture view, and in the past it has been quite difÔ¨Åcult for individual users to replace their TCP stacks with rogue versions. Another factor contributing to TCP‚Äôs success here is that most bad TCP behavior requires cooperation at the server end, and most server managers have an incentive to behave cooperatively. Servers generally want to distribute bandwidth fairly among their multiple clients, and ‚Äì theoretically at least ‚Äì a server‚Äôs ISP could penalize misbehavior. So far, at least, the TCP approach has worked remarkably well. 19.1 Basics of TCP Congestion Management TCP‚Äôs congestion management is window-based; that is, TCP adjusts its window size to adapt to congestion. The window size can be thought of as the number of packets out there in the network; more precisely, it represents the number of packets and ACKs either in transit or enqueued. An alternative approach often used for real-time systems is rate-based congestion management, which runs into an unfortunate difÔ¨Åculty if the sending rate momentarily happens to exceed the available rate. In the very earliest days of TCP, the window size for a TCP connection came from the AdvertisedWindow value suggested by the receiver, essentially representing how many packet buffers it could allocate. This value is often quite large, to accommodate large bandwidth delay products, and so is often reduced out of concern for congestion. When winsize is adjusted downwards for this reason, it is generally referred to as theCongestion Window, orcwnd (a variable name Ô¨Årst appearing in Berkeley Unix). Strictly speaking, winsize = min( cwnd, AdvertisedWindow). In newer TCP implementations, the variable cwnd may actually be used to mean the sender‚Äôs estimate of the number of packets in Ô¨Çight; see the sidebar at 19.4 TCP Reno and Fast Recovery. If TCP is sending over an idle network, the per-packet RTT will be RTT noLoad, the travel time with no queuing delays. As we saw in 8.3.2 RTT Calculations, (RTT‚ÄìRTT noLoad ) is the time each packet spends in the queue. The path bandwidth is winsize/RTT, and so the number of packets in queues is winsize  (RTT‚ÄìRTT noLoad ) / RTT. Usually all the queued packets are at the router at the head of the bottleneck link. Note that the sender can calculate this number (assuming we can estimate RTT noLoad; the most common approach is to assume that the smallest RTT measured corresponds to RTT noLoad ). TCP‚Äôs self-clocking ( iethat new transmissions are paced by returning ACKs) guarantees that, again assuming an otherwise idle network, the queue will build only at the bottleneck router. Self-clocking means that the rate of packet transmissions is equal to the available bandwidth of the bottleneck link. There are some spikes when a burst of packets is sent ( egwhen the sender increases its window size), but in the steady state self-clocking means that packets accumulate only at the bottleneck. We will return to the case of the non-otherwise-idle network in the next chapter, in 20.2 Bottleneck Links with Competition. The ‚Äúoptimum‚Äù window size for a TCP connection would be bandwidth RTT noLoad. With this window size, the sender has exactly Ô¨Ålled the transit capacity along the path to its destination, and has used none of the queue capacity. Actually, TCP Reno does not do this. Instead, TCP Reno does the following: 
- guesses at a reasonable initial window size, using a form of polling 444 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 
- slowly increases the window size if no losses occur, on the theory that maximum available throughput may not yet have been reached 
- rapidly decreases the window size otherwise, on the theory that if losses occur then drastic action is needed In practice, this usually leaves TCP‚Äôs window size well above the theoretical ‚Äúoptimum‚Äù. One interpretation of TCP‚Äôs approach is that there is a time-varying ‚Äúceiling‚Äù on the number of packets the network can accept. Each sender tries to stay near but just below this level. Occasionally a sender will overshoot and a packet will be dropped somewhere, but this just teaches the sender a little more about where the network ceiling is. More formally, this ceiling represents the largest cwnd that does not lead to packet loss, iethecwnd that at that particular moment completely Ô¨Ålls but does not overÔ¨Çow the bottleneck queue. We have reached the ceiling when the queue is full. In Chiu and Jain‚Äôs terminology, the far side of the ceiling is the ‚Äúcliff‚Äù, at which point packets are lost. TCP tries to stay above the ‚Äúknee‚Äù, which is the point when the queue Ô¨Årst begins to be persistently utilized, thus keeping the queue at least partially occupied; whenever it sends too much and falls off the ‚Äúcliff‚Äù, it retreats. The ceiling concept is often useful, but not necessarily as precise as it might sound. If we have reached the ceiling by gradually expanding the sliding-windows window size, then winsize will be as large as possible. But if the sender suddenly releases a burst of packets, the queue may Ô¨Åll and we will have reached a ‚Äútemporary ceiling‚Äù without fully utilizing the transit capacity. Another source of ceiling ambiguity is that the bottleneck link may be shared with other connections, in which case the ceiling represents our connection‚Äôs particular share, which may Ô¨Çuctuate greatly with time. Finally, at the point when the ceiling is reached, the queue is fulland so there are a considerable number of packets waiting in the queue; it is not possible for a sender to pull back instantaneously. It is time to acknowledge the existence of different versions of TCP, each incorporating different congestionmanagement algorithms. The two we will start with are TCP Tahoe (1988) and TCP Reno (1990); the names Tahoe and Reno were originally the codenames of the Berkeley Unix distributions that included these respective TCP implementations. The ideas behind TCP Tahoe came from a 1988 paper by Jacobson and Karels [JK88]; TCP Reno then reÔ¨Åned this a couple years later. TCP Reno is still in widespread use over twenty years later, and is still the undisputed TCP reference implementation, although some modest improvements (NewReno, SACK) have crept in. A common theme to the development of improved implementations of TCP is for one end of the connection (usually the sender) to extract greater and greater amounts of information from the packet Ô¨Çow. For example, TCP Tahoe introduced the idea that duplicate ACKs likely mean a lost packet; TCP Reno introduced the idea that returning duplicate ACKs are associated with packets that have successfully been transmitted but follow a loss. TCP Vegas ( 22.6 TCP Vegas ) introduced the Ô¨Åne-grained measurement of RTT, to detect when RTT > RTT noLoad. It is often helpful to think of a TCP sender as having breaks between successive windowfuls; that is, the sender sends cwnd packets, is brieÔ¨Çy idle, and then sends another cwnd packets. The successive windowfuls of packets are often called Ô¨Çights. The existence of any separation between Ô¨Çights is, however, not guaranteed. 19.1 Basics of TCP Congestion Management 445
An Introduction to Computer Networks, Release 2.0.11 19.1.1 The Somewhat-Steady State We will begin with the state in which TCP has established a reasonable guess for cwnd, comfortably below the Advertised Window Size, and which largely appears to be working. TCP then engages in some Ô¨Ånetuning. This TCP ‚Äústeady state‚Äù ‚Äì steady here in the sense of regular oscillation ‚Äì is usually referred to as thecongestion avoidance phase, though all phases of the process are ultimately directed towards avoidance of congestion. The central strategy is that when a packet is lost, cwnd should decrease rapidly, but otherwise should increase ‚Äúslowly‚Äù. This leads to slow oscillation of cwnd, which over time allows the average cwnd to adapt to long-term changes in the network capacity. As TCP Ô¨Ånishes each windowful of packets, it notes whether a loss occurred. The cwnd -adjustment rule introduced by TCP Tahoe and [JK88] is the following: 
- if there were no losses in the previous windowful, cwnd =cwnd +1 
- if packets were lost, cwnd =cwnd /2 We are informally measuring cwnd in units of full packets; strictly speaking, cwnd is measured in bytes and is incremented by the maximum TCP segment size. This strategy here is known as Additive Increase, Multiplicative Decrease, or AIMD; cwnd =cwnd +1 is the additive increase and cwnd =cwnd /2 is the multiplicative decrease. Typically, setting cwnd =cwnd /2 is a medium-term goal; in fact, TCP Tahoe brieÔ¨Çy sets cwnd =1 in the immediate aftermath of an actual timeout. With no losses, TCP will send successive windowfuls of, say, 20, 21, 22, 23, 24,. .. . This amounts to conservative ‚Äúprobing‚Äù of the network and, in particular, of the queue at the bottleneck router. TCP tries largercwnd values because the absence of loss means the current cwnd is below the ‚Äúnetwork ceiling‚Äù; that is, the queue at the bottleneck router is not yet overfull. If a loss occurs (including multiple losses in a single windowful), TCP‚Äôs response is to cut the window size in half. (As we will see, TCP Tahoe actually handles this in a somewhat roundabout way.) Informally, the idea is that the sender needs to respond aggressively to congestion. More precisely, lost packets mean the queue of the bottleneck router has Ô¨Ålled, and the sender needs to dial back to a level that will allow the queue to clear. If we assume that the transit capacity is roughly equal to the queue capacity (say each is equal to N), then we overÔ¨Çow the queue and drop packets when cwnd = 2N, and so cwnd =cwnd /2 leaves us with cwnd = N, which just Ô¨Ålls the transit capacity and leaves the queue empty. (When the sender sets cwnd =N, the actual number of packets in transit takes at least one RTT to fall from 2N to N.) Of course, assuming any relationship between transit capacity and queue capacity is highly speculative. On a 5,000 km Ô¨Åber-optic link with a bandwidth of 10 Gbps, the round-trip transit capacity would be about 60 MB, or 60,000 1kB packets. Most routers probably do not have queues that large. Queue capacities in excess of the transit capacity are common, however. On the other hand, a competing model of a long-haul high-bandwidth TCP path is that the queue size should be a small fraction of the bandwidth delay product. We return to this in 19.7 TCP and Bottleneck Link Utilization and21.5.1 Bufferbloat. Note that if TCP experiences a packet loss, and there is an actual timeout (as opposed to a packet loss detected by Fast Retransmit, 19.3 TCP Tahoe and Fast Retransmit ), then the sliding-window pipe has drained. No packets are in Ô¨Çight. No self-clocking can govern new transmissions. Sliding windows therefore needs to restart from scratch. The congestion-avoidance algorithm leads to the classic ‚ÄúTCP sawtooth‚Äù graph, where the peaks are at the points where the slowly rising cwnd crossed above the ‚Äúnetwork ceiling‚Äù. We emphasize that the 446 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 TCP sawtooth is speciÔ¨Åc to TCP Reno and related TCP implementations that share Reno‚Äôs additiveincrease/multiplicative-decrease mechanism. timecwnd TCP Sawtooth, red curve represents the network capacity During periods of no loss, TCP‚Äôs cwnd increases linearly; when a loss occurs, TCP sets cwnd =cwnd /2. This diagram is an idealization as when a loss occurs it takes the sender some time to discover it, perhaps as much as the TimeOut interval. The Ô¨Çuctuation shown here in the red ceiling curve is somewhat arbitrary. If there are only one or two other competing senders, the ceiling variation may be quite dramatic, but with many concurrent senders the variations may be smoothed out. For some TCP sawtooth graphs created through actual simulation, see 31.2.1 Graph of cwnd v time and 31.4.1 Some TCP Reno cwnd graphs. 19.1.1.1 A Ô¨Årst look at fairness The transit capacity of the path is more-or-less unvarying, as is the physical capacity of the queue at the bottleneck router. However, these capacities are also shared with other connections, which may come and go with time. This is why the ceiling does vary in real terms. If two other connections share a path with total capacity 60 packets, the ‚Äúfairest‚Äù allocation might be for each connection to get about 20 packets as its share. If one of those other connections terminates, the two remaining ones might each rise to 30 packets. And if instead a fourth connection joins the mix, then after equilibrium is reached each connection might hope for a fair share of 15 packets. Will this kind of ‚Äúfair‚Äù allocation actually happen? Or might we end up with one connection getting 90% of the bandwidth while two others each get 5%? Chiu and Jain [CJ89] showed that the additive-increase/multiplicative-decrease algorithm does indeed converge to roughly equal bandwidth sharing when two connections have a common bottleneck link, provided also that 19.1 Basics of TCP Congestion Management 447
An Introduction to Computer Networks, Release 2.0.11 
- both connections have the same RTT 
- during any given RTT, either both connections experience a packet loss, or neither connection does To see this, let cwnd1 andcwnd2 be the connections‚Äô congestion-window sizes, and consider the quantity cwnd1 ‚Äìcwnd2. For any RTT in which there is no loss, cwnd1 andcwnd2 both increment by 1, and so cwnd1 ‚Äìcwnd2 stays the same. If there is a loss, then both are cut in half and so cwnd1 ‚Äìcwnd2 is also cut in half. Thus, over time, the original value of cwnd1 ‚Äìcwnd2 is repeatedly cut in half (during each RTT in which losses occur) until it dwindles to inconsequentiality, at which point cwnd1cwnd2. Graphical and tabular versions of this same argument are in the next chapter, in 20.3 TCP Reno Fairness with Synchronized Losses. The second bulleted hypothesis above we may call the synchronized-loss hypothesis. While it is very reasonable to suppose that the two connections will experience the same number of losses as a long-term average, it is a much stronger statement to suppose that all loss events are shared by both connections. This behavior may not occur in real life and has been the subject of some debate; see [GV02]. We return to this point in 31.3 Two TCP Senders Competing. Fortunately, equal-RTT fairness still holds if each connection isequally likely to experience a packet loss: both connections will have the same loss rate, and so, as we shall see in 21.2 TCP Reno loss rate versus cwnd, will have the same cwnd. However, convergence to fairness may take rather much longer. In 20.3 TCP Reno Fairness with Synchronized Losses we also look at some alternative hypotheses for the unequal-RTT case. 19.2 Slow Start How do we make that initial guess as to the network capacity? What value of cwnd should we begin with? And even if we have a good target for cwnd, how do we avoid Ô¨Çooding the network sending an initial burst of packets? The TCP Reno answer is known as slow start. If you are trying to guess a number in a Ô¨Åxed range, you are likely to use binary search. Not knowing the range for the ‚Äúnetwork ceiling‚Äù, a good strategy is to guess cwnd =1 (orcwnd =2) at Ô¨Årst and keep doubling until you have gone too far. Then revert to the previous guess, which is known to have worked. At this point you are guaranteed to be within 50% of the true capacity. The actual slow-start mechanism is to increment cwnd by 1 for each ACK received. This seems linear, but that is misleading: after we send a windowful of packets ( cwnd many), we have received cwnd ACKs and so have incremented cwnd -many times, and so have set cwnd to (cwnd +cwnd ) = 2cwnd. In other words,cwnd =cwnd2 after each windowful is the same as cwnd +=1 after each packet. Assuming packets travel together in windowfuls, all this means cwnd doubles each RTT during slow start; this is possibly the only place in the computer science literature where exponential growth is described as ‚Äúslow‚Äù. It is indeed slower, however, than the alternative of sending an entire windowful at once. Here is a diagram of slow start in action. This diagram makes the implicit assumption that the no-load RTT is large enough to hold well more than the 8 packets of the maximum window size shown. 448 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 Data 1 Data 2-3Data 1 Data 4-7 Data 8-15 Slow Start with discrete packet flights For a different case, with a much smaller RTT, see 19.2.3 Slow-Start Multiple Drop Example. Eventually the bottleneck queue gets full, and drops a packet. Let us suppose this is after N RTTs, so cwnd =2N. Then during the previous RTT, cwnd =2N-1worked successfully, so we go back to that previous value by setting cwnd =cwnd /2. 19.2.1 TCP Reno Per-ACK Responses During slow start, incrementing cwnd by one per ACK received is equivalent to doubling cwnd after each windowful. We can Ô¨Ånd a similar equivalence for the congestion-avoidance phase, above. During congestion avoidance, cwnd is incremented by 1 after each windowful. To formulate this as a perACK increase, we spread this increment of 1 over the entire windowful, which of course has size cwnd. This amounts to the following upon each ACK received: 19.2 Slow Start 449
An Introduction to Computer Networks, Release 2.0.11 cwnd =cwnd + 1/cwnd This is a slight approximation, because cwnd keeps changing, but it works well in practice. Because TCP actually measures cwnd in bytes, Ô¨Çoating-point arithmetic is normally not required; see exercise 14.0. An exact equivalent to the per-windowful incrementing strategy is cwnd =cwnd + 1/cwnd 0, wherecwnd 0is the value of cwnd at the start of that particular windowful. Another, simpler, approach is to use cwnd += 1/cwnd, and to keep the fractional part recorded, but to use Ô¨Çoor( cwnd ) (the integer part of cwnd ) when actually sending packets. Most actual implementations keep track of cwnd in bytes, in which case using integer arithmetic is sufÔ¨Åcient untilcwnd becomes quite large. Ifdelayed ACKs are implemented ( 18.8 TCP Delayed ACKs ), then in bulk transfers one arriving ACK actually acknowledges two packets. RFC 3465 permits a TCP receiver to increment cwnd by 2/cwnd in that situation, which is the response consistent with incrementing cwnd by 1 upon receipt of enough ACKs to acknowledge an entire windowful. 19.2.2 Threshold Slow Start Sometimes TCP uses slow start even when it knows the working network capacity. After a packet loss and timeout, TCP knows that a new cwnd ofcwnd old/2 should work. If cwnd had been 100, TCP halves it to 50. The problem, however, is that after timeout there are no returning ACKs to self-clock the continuing transmission, and we do not want to dump 50 packets on the network all at once. So in restarting the Ô¨Çow TCP uses what might be called threshold slow start: it uses slow-start, but stops when cwnd reaches the target. SpeciÔ¨Åcally, on packet loss we set the variable ssthresh tocwnd /2; this is our new target for cwnd. We setcwnd itself to 1, and switch to the slow-start mode ( cwnd += 1 for each ACK). However, as soon ascwnd reachesssthresh, we switch to the congestion-avoidance mode ( cwnd += 1/cwnd for each ACK). Note that the transition from threshold slow start to congestion avoidance is completely natural, and easy to implement. TCP will use threshold slow-start whenever it is restarting from a pipe drain; that is, every time slow-start is needed after its very Ô¨Årst use. (If a connection has simply been idle, non-threshold slow start is typically used when trafÔ¨Åc starts up again.) Threshold slow-start can be seen as an attempt at combining rapid window expansion with self-clocking. By comparison, we might refer to the initial, non-threshold slow start as unbounded slow start. Note that unbounded slow start serves a fundamentally different purpose ‚Äì initial probing to determine the network ceiling to within 50% ‚Äì than threshold slow start. Here is the TCP sawtooth diagram above, modiÔ¨Åed to show timeouts and slow start. The Ô¨Årst two packet losses are displayed as ‚Äúcoarse timeouts‚Äù; the rest are displayed as if Fast Retransmit, below, were used. 450 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 timecwnd TCP Tahoe Sawtooth, red curve represents the network capacity Slow Start is used after each packet loss until ssthresh is reachedtimeout RFC 2581 allows slow start to begin with cwnd =2. 19.2.3 Slow-Start Multiple Drop Example Slow start has the potential to cause multiple dropped packets at the bottleneck link; packet losses continue for quite some time because the TCP sender is slow to discover them. The network topology is as follows, where the A‚ÄìR link is inÔ¨Ånitely fast and the R‚ÄìB link has a bandwidth in the R √ù√ëB direction of 1 packet/ms. A R Binfinitely fast 1 pkt/ms Assume that R has a queue capacity of 100, not including the packet it is currently forwarding to B, and that ACKs travel instantly from B back to A. In this and later examples we will continue to use the Data[N]/ACK[N] terminology of 8.2 Sliding Windows, beginning with N=1; TCP numbering is not done quite this way but the distinction is inconsequential. When A uses slow-start here, the successive windowfuls will almost immediately begin to overlap. A will send one packet at T=0; it will be delivered at T=1. The ACK will travel instantly to A, at which point A will send two packets. From this point on, ACKs will arrive regularly at A at a rate of one per second. Here is a brief chart: 19.2 Slow Start 451
An Introduction to Computer Networks, Release 2.0.11 Time A receives A sends R sends R‚Äôs queue 0 Data[1] Data[1] 1 ACK[1] Data[2],Data[3] Data[2] Data[3] 2 ACK[2] 4,5 3 4,5 3 ACK[3] 6,7 4 5..7 4 ACK[4] 8,9 5 6..9 5 ACK[5] 10,11 6 7..11 .. N ACK[N] 2N,2N+1 N+1 N+2 .. 2N+1 At T=N, R‚Äôs queue contains N packets. At T=100, R‚Äôs queue is full. Data[200], sent at T=100, will be delivered and acknowledged at T=200, giving it an RTT of 100. At T=101, R receives Data[202] and Data[203] and drops the latter one. Unfortunately, A‚Äôs timeout interval must of course be greater than the RTT, and so A will not detect the loss until, at an absolute minimum, T=200. At that point, A has sent packets up through Data[401], and the 100 packets Data[203], Data[205],. .. , Data[401] have all been lost. In other words, at the point when A Ô¨Årstreceives the news of one lost packet, in fact at least 100 packets have already been lost. Fortunately, unbounded slow start generally occurs only once per connection. 19.2.4 Summary of TCP so far So far we have the following features: 
- Unbounded slow start at the beginning 
- Congestion avoidance with AIMD once some semblance of a steady state is reached 
- Threshold slow start after each loss 
- Each threshold slow start transitioning naturally to congestion avoidance Here is a table expressing the slow-start and congestion-avoidance phases in terms of manipulating cwnd. phase cwnd change, loss cwnd change, no loss per window per window per ACK slow start cwnd /2 cwnd *= 2cwnd += 1 cong avoid cwnd /2 cwnd +=1cwnd += 1/cwnd Viewing cwnd Linux users can view the current values of cwnd andssthresh, as well as a host of other TCP statistics, using the command ss --tcp --info. The problem TCP often faces, in both slow-start and congestion-avoidance phases, is that when a packet is lost the sender will not detect this until much later (at least until the bottleneck router‚Äôs current queue has been sent); by then, it may be too late to avoid further losses. 452 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 19.2.5 The initial value of cwnd So far we have been assuming that slow start begins with cwnd equal to one packet, the value proposed in [JK88]. But 1988 was quite a while ago, and the initial cwnd has been creeping up. A decade later, RFC 2414 proposed setting the initial cwnd (used on startup and after a timeout) to between two and four packets, with a maximum size of 3 1460 (three packets if the maximum Ethernet packet size of 1500 bytes is used). For a while following, the most common value was two packets. Then RFC 6928, in 2013, proposed an ‚Äúexperimental‚Äù change to an initial value of up to ten packets, where each can be 1460 bytes. This ten-packet value is now in relatively common use. This increase in the initial cwnd value has mostly to do with the steadily increasing capacity of the Internet, and the decreased likelihood that a single extra packet will make a material difference. Still, the basic behavior of slow start remains the same. Non-Reno TCPs also implement something like slow start, though it may look slightly different. TCP BBR (22.16 TCP BBR ), for example, has a STARTUP mode that serves the same function as TCP Reno slow start. 19.3 TCP Tahoe and Fast Retransmit TCP Tahoe has one more important feature. Recall that TCP ACKs are cumulative; if packets 1 and 2 have been received and now Data[4] arrives, but not yet Data[3], all the receiver can (and must!) do is to send back another ACK[2]. Thus, from the sender‚Äôs perspective, if we send packets 1,2,3,4,5,6 and get back ACK[1], ACK[2], ACK[2], ACK[2], ACK[2], we can infer two things: 
- Data[3] got lost, which is why we are stuck on ACK[2] 
- Data 4,5 and 6 probably didmake it through, and triggered the three duplicate ACK[2]s (the three ACK[2]s following the Ô¨Årst ACK[2]). TheFast Retransmit strategy is to resend Data[N] when we have received three dupACKs for Data[N-1]; that is, four ACK[N-1]‚Äôs in all. Because this represents a packet loss, we also set ssthresh =cwnd /2, setcwnd =1, and begin the threshold-slow-start phase. The effect of this is typically to reduce the delay associated with the lost packet from that of a full timeout, typically 2 RTT, to just a little over a single RTT. The lost packet is now discovered before the TCP pipeline has drained. However, at the end of the next RTT, when the ACK of the retransmitted packet will return, the TCP pipeline willhave drained, hence the need for slow start. TCP Tahoe included all the features discussed so far: the cwnd +=1 andcwnd =cwnd /2 responses, slow start and Fast Retransmit. Fast Retransmit waits for the third dupACK to allow for the possibility of moderate packet reordering. Suppose packets 1 through 6 are sent, but they arrive in the order 1,3,4,2,6,5, perhaps due to a router along the way with an architecture that is strongly parallelized. Then the ACKs that would be sent back would be as follows: 19.3 TCP Tahoe and Fast Retransmit 453
An Introduction to Computer Networks, Release 2.0.11 Received Response Data[1] ACK[1] Data[3] ACK[1] Data[4] ACK[1] Data[2] ACK[4] Data[6] ACK[4] Data[5] ACK[6] Waiting for the third dupACK is in most cases a successful compromise between responsiveness to lost packets and reasonable evidence that the data packet in question is actually lost. However, a router that does more substantial delivery reordering would wreck havoc on connections using Fast Retransmit. In particular, consider the router R in the diagram below; when sending packets to B it might in principle wish to alternate on a packet-by-packet basis between the path via R1 and the path via R2. This would be a mistake; if the R1 and R2 paths had different propagation delays then this strategy would introduce major packet reordering. R should send all the packets belonging to any one TCP connection via a single path. AR BR1 R2 In the real world, routers generally go to considerable lengths to accommodate Fast Retransmit; in particular, use of multiple paths for a single TCP connection is almost universally frowned upon. Some actual data on packet reordering can be found in [VP97]; the author suggests that a switch to retransmission on the second dupACK would be risky. 19.4 TCP Reno and Fast Recovery Fast Retransmit requires a sender to set cwnd =1 because the pipe has drained and there are no arriving ACKs to pace transmission. Fast Recovery is a technique that often allows the sender to avoid draining the pipe, and to move from cwnd tocwnd /2 in the space of a single RTT. TCP Reno is TCP Tahoe with the addition of Fast Recovery. The idea is to use the arriving dupACKs to pace retransmission. We make the assumption that each arriving dupACK indicates that some data packet following the lost packet has been delivered successfully; it turns out not to matter which one. On discovery of the lost packet through Fast Retransmit, the goal is to set cwnd =cwnd /2; the next step is to Ô¨Ågure out how many dupACKs we have to wait for before we can resume transmissions of new data. Initially, at least, we assume that only one data packet is lost, though in the following section we will see that multiple losses can be handled via a slight modiÔ¨Åcation of the Fast Recovery strategy. During the recovery process, there is a problem with the direct use of sliding windows: the lost packet ‚Äúpins‚Äù the lower end of the window until that packet is successfully retransmitted, so for the duration of the 454 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 recovery period the window cannot slide forward. The ofÔ¨Åcial speciÔ¨Åcation of fast recovery, originally in RFC 2001 and now in RFC 5681, describes retransmission in terms of cwnd inÔ¨Çation anddeÔ¨Çation. If C is the value of cwnd at the time of the loss, then cwnd is steadily inÔ¨Çated to 1.5 C, allowing for the original window plus half a windowful more of new data. At the point the lost packet is successfully retransmitted, the window is ‚ÄúdeÔ¨Çated‚Äù to cwnd = C/2; at this point the lower end of the window slides forward by C, and the upper end of the window does not move. This all works out, but can be a bit hard to follow. Instead we will use the concept of Estimated FlightSize, or EFS, which is the sender‚Äôs best guess at the number of outstanding packets. Under normal circumstances, EFS is the same as cwnd. The crucial Fast Recovery observation is that EFS should be decremented by 1 for each arriving dupACK, because the arrival of each dupACK means one less packet is in transit, and so transmission of new packets can resume when EFS is reduced to half of the original value of cwnd. Our EFS approach is equivalent to the inÔ¨Çationary approach at least when slow start is not involved. Linux cwnd isEstimated FlightSize This chapter deÔ¨Ånes cwnd to be the sender winsize strictly construed. As such, packet N+ cwnd cannot be sent until packet N is ACKed. However, the Linux kernel actually uses cwnd as a synonym for Estimated FlightSize, which simpliÔ¨Åes the Fast-Recovery code. This usage applies, in fact, to all TCP varieties, though it often makes little difference. See tcp_cwnd_test(). We Ô¨Årst outline the general case, and then look at a speciÔ¨Åc example. Let cwnd = N, and suppose packet 1 is lost (packet numbers here may be taken as relative). Until packet 1 is retransmitted, the sender can only send up through packet N (Data[N] can be sent only after ACK[0] has arrived at the sender). The receiver will send N‚Äì1 dupACK[0]s representing packets 2 through N. At the point of the third dupACK, when the loss of Data[1] is discovered, the sender calculates as follows: EFS had been cwnd = N. Three dupACKs have arrived, representing three later packets no longer in Ô¨Çight, so EFS is now N‚Äì3. At this point the sender realizes a packet has been lost, which makes EFS = N‚Äì4 brieÔ¨Çy, but that packet is then immediately retransmitted, which brings EFS back to N‚Äì3. The sender expects at this point to receive N‚Äì4 more dupACKs, followed by one new ACK for the retransmission of the packet that was lost. This last ACK will be for the entire original windowful. The new target for cwnd is N/2 (for simplicity, we will assume N is even). So, we wait for N/2 ‚Äì 3 more dupACKs to arrive, at which point EFS is N‚Äì3‚Äì(N/2‚Äì3) = N/2. After this point the sender will resume sending new packets; it will send one new packet for each of the N/2‚Äì1 subsequently arriving dupACKs (recall that there are N‚Äì1 dupACKs in all). These new transmissions will be Data[N+1] through Data[N+(N/2‚Äì1)]. After the last of the dupACKs will come the ACK corresponding to the retransmission of the lost packet; it will be ACK[N], acknowledging all of the original windowful. At this point, there are N/2 ‚Äì 1 unacknowledged packets Data[N+1] through Data[N+(N/2)‚Äì1]. The sender now sends Data[N+N/2] and is thereby able to resume sliding windows with cwnd = N/2: the sender has received ACK[N] and has exactly one full windowful outstanding for the new value N/2 of cwnd. That is, we are right where we are supposed to be. Here is a diagram illustrating Fast Recovery for cwnd =10. Data[10] is lost. 19.4 TCP Reno and Fast Recovery 455
An Introduction to Computer Networks, Release 2.0.11 Data[9] Data[10] Data[11] Data[12] Data[13] Data[16] Data[17] Data[18]Data[14] Data[15] get ACK[9] / send Data[19] get Data[10]; send ACK[19]X send Data[21] send Data[22] send Data[23] send Data[24] sliding windows reestablished with cwnd = 5get Data[19]; send dupACK[9] / 19dupACK[9] dupACK[9] 3rd dupACK[9]; EFS=7; resend Data[10] EFS = 6 EFS = 5 EFS = 4; resume sending with Data[20]send dupACK[9] / 11send ACK[9] send dupACK[9] / 12 send dupACK[9] / 13 send dupACK[9] / 14 send dupACK[9] / 15 send dupACK[9] / 16send dupACK[9] / 15 send dupACK[9] / 17 send dupACK[9] / 18 send ACK[20] send ACK[21] send ACK[22] send Data[25]send ACK[23] send Data[26] send Data[27] Data[9] elicits the initial ACK[9], and the nine packets Data[11] through Data[19] each elicit a dupACK[9]. We denote the dupACK[9] elicited by Data[N] by dupACK[9]/N; these are shown along the upper right. Unless SACK TCP (below) is used, the sender will have no way to determine N or to tell these dupACKs apart. When dupACK[9]/13 (the third dupACK) arrives at the sender, the sender uses Fast Recovery to infer that Data[10] was lost and retransmits it. At this point EFS = 7: the sender has sent the original batch of 10 data packets, plus Data[19], and received one ACK and three dupACKs, for a total of 10+1-1-3 = 7. The sender has also inferred that Data[10] is lost (EFS ‚Äì= 1) but then retransmitted it (EFS += 1). Six more dupACK[9]‚Äôs are on the way. EFS is decremented for each subsequent dupACK arrival; after we get two more dupACK[9]‚Äôs, EFS is 5. The next dupACK[9] (dupACK[9]/16) reduces EFS to 4 and so allows us transmit Data[20] (which promptly bumps EFS back up to 5). We have 456 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 receive send dupACK[9]/16 Data[20] dupACK[9]/17 Data[21] dupACK[9]/18 Data[22] dupACK[9]/19 Data[23] We emphasize again that the TCP sender does not see the numbers 16 through 19 in the receive column above; it determines when to begin transmission by counting dupACK[9] arrivals. Working Backwards Figuring out when a fast-recovery sender should resume transmissions of new data is error-prone. Perhaps the simplest approach is to work backwards from the retransmitted lost packet: it should trigger at the receiver an ACK for the entire original windowful. When Data[10] above was lost, the ‚Äústuck‚Äù window was Data[10]-Data[19]. The retransmitted Data[10] thus triggers ACK[19]; when ACK[19] arrives, cwnd should be 10/2 = 5 so Data[24] should be sent. That in turn means the four packets Data[20] through Data[23] must have been sent earlier, via Fast Recovery. There are 10‚Äì1 = 9 dupACKs, so to send on the last four we must start with the sixth. The diagram above indeed shows new Fast Recovery transmissions beginning with the sixth dupACK. The next thing to arrive at the sender side is the ACK[19] elicited by the retransmitted Data[10]; at the point Data[10] arrives at the receiver, Data[11] through Data[19] have already arrived and so the cumulative-ACK response is ACK[19]. The sender responds to ACK[19] with Data[24], and the transition to cwnd =5 is now complete. During sliding windows without losses, a sender will send cwnd packets per RTT. If a ‚Äúcoarse‚Äù timeout occurs, typically it is not discovered until after at least one complete RTT of link idleness; there are additional underutilized RTTs during the slow-start phase. It is worth examining the Fast Recovery sequence shown in the illustration from the perspective of underutilized bandwidth. The diagram shows three round-trip times, as seen from the sender side. During the Ô¨Årst RTT, the ten packets Data[9]-Data[18] are sent. The second RTT begins with the sending of Data[19] and continues through sending Data[22], along with the retransmitted Data[10]. The third RTT begins with sending Data[23], and includes through Data[27]. In terms of recovery efÔ¨Åciency, the RTTs send 9, 5 and 5 packets respectively (we have counted Data[10] twice); this is remarkably close to the ideal of reducing cwnd to 5 instantaneously. Using the fast-recovery description in terms of inÔ¨Çation from RFC 5681, inÔ¨Çation would begin at the point the sender resumed transmitting new packets, at which point cwnd would be incremented for each dupACK. In the diagram above, at the instant labeled ‚Äúsend Data[24]‚Äù on the sender side, cwnd would momentarily become 15, representing the window Data[10]..Data[24]. As soon as the sender realized that the lost packet Data[10] had been acknowledged, via ACK[19], cwnd would immediately deÔ¨Çate to 5, representing the window Data[20]..Data[24]. For a diagram illustrating cwnd inÔ¨Çation and deÔ¨Çation, see 32.2.1 Running the Script. There is one more addition to Fast Recovery, described in RFC 3042, and known as Limited Transmit. As described above, transmission of new data begins with the third dupACK, which is the point at which Fast Recovery is initiated. Limited Transmit means that one new data packet is also transmitted for each of the Ô¨Årst two dupACKs, on a ‚Äújust in case‚Äù basis. These two transmissions are ‚Äúborrowed‚Äù against future 19.4 TCP Reno and Fast Recovery 457
An Introduction to Computer Networks, Release 2.0.11 forward motion of the send window; the value of cwnd is not changed and the Ô¨Årst ‚Äúskipped‚Äù packet is not retransmitted. If there was no packet loss, and the dupACKs simply resulted from packet reordering, no harm is done. Otherwise, Fast Retransmit gets a slightly earlier start. For small window sizes this can make a signiÔ¨Åcant difference. 19.5 TCP NewReno TCP NewReno, described in [JH96] and RFC 2582 (currently RFC 6582 ), is a modest tweak to Fast Recovery which greatly improves handling of the case when two or more packets are lost in a windowful. It is generally considered to be a part of contemporary TCP Reno. We again describe it in terms of Estimated FlightSize rather than in terms of cwnd inÔ¨Çation and deÔ¨Çation. If two data packets are lost and the Ô¨Årst is retransmitted, the receiver will acknowledge data up to just before the second packet, and then continue sending dupACKs of this until the second lost packet is also retransmitted. These ACKs of data up to just before the second packet are sometimes called partial ACKs, because retransmission of the Ô¨Årst lost packet did not result in an ACK of all the outstanding data. The NewReno mechanism uses these partial ACKs as evidence to retransmit later lost packets, and also to keep pacing the Fast Recovery process. In the diagram below, packets 1 and 4 are lost in a window 0..11 of size 12. Initially the sender will get dupACK[0]‚Äôs; the Ô¨Årst 11 ACKs (dashed lines from right to left) are ACK[0] and 10 dupACK[0]‚Äôs. When packet 1 is successfully retransmitted on receipt of the third dupACK[0], the receiver‚Äôs response will be ACK[3] (the heavy dashed line). This is the Ô¨Årst partial ACK (a full ACK would have been ACK[12]). On receipt of any partial ACK during the Fast Recovery process, TCP NewReno assumes that the immediately following data packet was lost and retransmits it immediately; the sender does not wait for three dupACKs because if the following data packet had not been lost, no instances of the partial ACK would ever have been generated, even if packet reordering had occurred. The TCP NewReno sender response here is, in effect, to treat each partial ACK as a dupACK[0], except that the sender also retransmits the data packet that ‚Äì based upon receipt of the partial ACK ‚Äì it is able to infer is lost. NewReno continues pacing Fast Recovery by whatever ACKs arrive, whether they are the original dupACKs or later partial ACKs or dupACKs. 458 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 data[0] 1 2 3 4 7 8 9 105 6 11 ACK[0] 3rd dupACK[0]; resend Data[1] Data[1] received; send ACK[3]X X send Data[13] send Data[14] send Data[15] send Data[16] resend Data[4] Also send Data[17] send Data[18] send Data[19] send Data[21]send Data[20] get ACK[17]; send Data[23]send Data[22]; now cwnd = 6Data[4] received; send ACK[16]Data[12]; send last dupACK[0] get ACK[18]; send Data[24] All is back to normalget ACK[19]; send Data[25]four dupACK[3]'s send ACK[17] When the receiver‚Äôs Ô¨Årst ACK[3] arrives at the sender, NewReno infers that Data[4] was lost and resends it; this is the second heavy data line. No dupACK[3]‚Äôs need arrive; as mentioned above, the sender can infer from the single ACK[3] that Data[4] is lost. The sender also responds as if another dupACK[0] had arrived, and sends Data[17]. The arrival of ACK[3] signals a reduction in the EFS by 2: one for the inference that Data[4] was lost, and 19.5 TCP NewReno 459
An Introduction to Computer Networks, Release 2.0.11 one as if another dupACK[0] had arrived; the two transmissions in response (of Data[4] and Data[17]) bring EFS back to where it was. At the point when Data[16] is sent the actual (not estimated) Ô¨Çightsize is 5, not 6, because there is one less dupACK[0] due to the loss of Data[4]. However, once NewReno resends Data[4] and then sends Data[17], the actual Ô¨Çightsize is back up to 6. There are four more dupACK[3]‚Äôs that arrive. NewReno keeps sending new data on receipt of each of these; these are Data[18] through Data[21]. The receiver‚Äôs response to the retransmitted Data[4] is to send ACK[16]; this is the cumulative of all the data received up to that moment. At the point this ACK arrives back at the sender, it had just sent Data[21] in response to the fourth dupACK[3]; its response to ACK[16] is to send the next data packet, Data[22]. The sender is now back to normal sliding windows, with acwnd of 6. Similarly, the Data[17] immediately following the retransmitted Data[4] elicits an ACK[17] (this is the Ô¨Årst Data[N] to elicit an exactly matching ACK[N] since the losses began), and the corresponding response to the ACK[17] is to continue with Data[23]. As with the previous Fast Recovery example, we consider the number of packets sent per RTT; the diagram shows four RTTs as seen from the sender side. RTT First packet Packets sent count Ô¨Årst Data[0] Data[0]-Data[11] 12 second Data[12] Data[12]-Data[15], Data[1] 5 third Data[16] Data[16]-Data[20], Data[4] 6 fourth Data[21] Data[21]-Data[26] 6 Again, after the loss is detected we segue to the new cwnd of 6 with only a single missed packet (in the second RTT). NewReno is, however, only able to send one retransmitted packet per RTT. Note that TCP Newreno, like TCPs Tahoe and Reno, is a sender-side innovation; the receiver does not have to do anything special. The next TCP Ô¨Çavor, SACK TCP, requires receiver-side modiÔ¨Åcation. 19.6 Selective Acknowledgments (SACK) A traditional TCP ACK is a cumulative acknowledgment of all data received up to that point. If Data[1002] is received but not Data[1001], then all the receiver can send is a duplicate ACK[1000]. This does indicate thatsomething following Data[1001] made it through, but nothing more. To provide greater speciÔ¨Åcity, TCP now provides a Selective ACK (SACK) option, implemented at the receiver. If this is available, the sender does not have to guess from dupACKs what has gotten through. The receiver can send an ACK that says: 
- All packets up through 1000 have been received (the cumulative ACK) 
- All packets up through 1050 have been received except for 1001, 1022, and 1035. The second line is the SACK part. Almost all TCP implementations now support this. SpeciÔ¨Åcally, SACKs include the following information; the additional data beyond the cumulative ACK is included in a TCP Option Ô¨Åeld. 
- The latest cumulative ACK 460 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 
- The three most recent blocks of consecutive packets received Thus, if we have lost 1001, 1022, 1035, and now 1051, and the highest received is 1060, the SACK might say: 
- All packets up through 1000 have been received 
- 1060-1052 have been received 
- 1050-1036 have been received 
- 1034-1023 have been received From this the sender know 1001s and 1022 were not received, but nothing about the packets in between. However, if the sender has been paying close attention to the previous SACKs received, it likely already knows that all packets 1002 through 1021 have been received. The term SACK TCP is typically used to mean that the receiving side supports selective ACKs, and the sending side is a straightforward modiÔ¨Åcation of TCP Reno to take advantage of them. In practice, selective ACKs provide at best a modest performance improvement in many situations; TCP NewReno does rather well, in moderate-loss environments. The paper [FF96] compares Tahoe, Reno, NewReno and SACK TCP, in situations involving from one to four packet losses in a single RTT. While Classic Reno performed poorly with two packet losses in a single RTT and extremely poorly with three losses, the three-loss NewReno and SACK TCP scenarios played out remarkably similarly. Only when connections experienced four losses in a single RTT did SACK TCP‚Äôs performance start to pull slightly ahead of that of NewReno. 19.7 TCP and Bottleneck Link Utilization Consider a TCP Reno sender with no competing trafÔ¨Åc. As cwnd saws up and down, what happens to throughput? Do those halvings of cwnd result in at least a dip in throughput? The answer depends to some extent on the size of the queue ahead of the bottleneck link, relative to the transit capacity of the path. As was discussed in 8.3.2 RTT Calculations, whencwnd is less than the transit capacity, the link is less than 100% utilized and the queue is empty. When cwnd is more than the transit capacity, the link is saturated (100% utilized) and the queue has about ( cwnd ‚Äì transit_capacity) packets in it. The diagram below shows two TCP Reno teeth; in the Ô¨Årst, the queue capacity exceeds the path transit capacity and in the second the queue capacity is a much smaller fraction of the total. 19.7 TCP and Bottleneck Link Utilization 461
An Introduction to Computer Networks, Release 2.0.11 Network ceiling QueueQueue TransitTransit 100% queue-filling phase queuefilling phaselink-unsaturated phase Queue capacity  Transit capacity Bottleneck link is always 100% utilizedQueue capacity < Transit capacity Bottleneck link is not 100% utilized In the Ô¨Årst diagram, the bottleneck link is always 100% utilized, even at the left edge of the teeth. In the second the interval between loss events (the left and right margins of the tooth) is divided into a linkunsaturated phase and a queue-Ô¨Ålling phase. In the unsaturated phase, the bottleneck link utilization is less than 100% and the queue is empty; in the later phase, the link is saturated and the queue begins to Ô¨Åll. Consider again the idealized network below, with an R‚ÄìB bandwidth of 1 packet/ms. A R Binfinitely fast 1 pkt/ms We Ô¨Årst consider the queue ¬•transit case. Assume that the total RTT noLoad delay is 100 ms, mostly due to propagation delay; this makes the bandwidth delay product 100 packets. The question for consideration is to what extent TCP Reno, once slow-start is over, sometimes leaves the R‚ÄìB link idle. The R‚ÄìB link will be saturated at all times provided A always keeps 100 packets in transit, that is, we always have cwnd¬•100 ( 8.3.2 RTT Calculations ). Ifcwnd min= 100, then cwnd max= 2cwnd min= 200. For this to be the maximum, the queue capacity must be at least 99, so that the path can accommodate 199 packets without loss: 100 packets in transit plus 99 packets in the queue. In general, TCP Reno never leaves the bottleneck link idle as long as the queue capacity in front of that link is at least as large as the path round-trip transit capacity. Now suppose instead that the queue size is 49, or about 50% of the transit capacity. Packet loss will occur whencwnd reaches 150, and so cwnd min= 75. Qualitatively this case is represented by the second diagram above, though the queue-to-network_ceiling proportion illustrated there is more like 1:8 than 1:3. There are now periods when the R‚ÄìB link is idle. During RTT intervals when cwnd =75, throughput will be 75% of the maximum and the R‚ÄìB link will be idle 25% of the time. 462 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 However,cwnd will be 75 just for the Ô¨Årst RTT following the loss. After 25 RTTs, cwnd will be back up to 100, and the link will be saturated. So we have 25 RTTs with an average cwnd of 87.5 (=&75+100)/2), meaning the link is 87.5% saturated, followed by 50 RTTs where the link is 100% saturated. The long-term average here is 95.8% utilization of the bottleneck link. This is not bad at all, given that using 10% of the link bandwidth on packet headers is almost universally considered reasonable. Furthermore, at the point whencwnd drops after a loss to cwnd min=75, the queue must have been full. It may take one or two RTTs for the queue to drain; during this time, link utilization will be even higher. If most or all of the time the bottleneck link is saturated, as in the Ô¨Årst diagram, it may help to consider the average queue size. Let the queue capacity be C queue and the transit capacity be C transit, with C queue > Ctransit. Thencwnd will vary from a maximum of C queue+Ctransit to a minimum of what works out to be (Cqueue-Ctransit)/2 + C transit. We would expect an average queue size about halfway between these, less the Ctransit term: 3/4Cqueue - 1/4Ctransit. If C queue=Ctransit, the expected average queue size should be about Cqueue/2. See exercises 12.0 and 12.5. 19.7.1 TCP Queue Sizes From the perspective of link utilization, the previous section suggests that router queues be larger rather than smaller. A queue capacity at least as large as transit capacity seems like an excellent choice. To conÔ¨Ågure a router this way, we Ô¨Årst make an educated guess at the average RTT, and then multiply this by the output bandwidth to get the desired queue capacity. For an average RTT of 50 ms, a bandwidth of 1 Gbps leads to a queue capacity of about 6 MB, or 4000 packets of 1500 bytes each. If the numbers rise to 100 ms and 10 Gbps, queue capacity needs to be 125 MB. Unfortunately, while large queues are helpful when the trafÔ¨Åc consists exclusively of bulk TCP transfers, they introduce proportionately large queuing delays that can wreak havoc on real-time trafÔ¨Åc. A bottleneck router with a queue size matching a Ô¨Çow‚Äôs bandwidth delay product will double the RTT for that Ô¨Çow, at points when the queue is full. Worse, if the goal is 100% TCP link utilization always, then the router queue must be sized for the highest-bandwidth Ô¨Çow with the longest RTT; shorter TCP connections will encounter a queue much larger than necessary. This problem of large queue capacity leading to excessive delay is known as bufferbloat; we will return to it at 21.5.1 Bufferbloat. Because of the delay problems brought on by large queues, TCP connections must sometimes pass through bottleneck routers with small queues. In this case a tooth of a TCP Reno connection is divided into a large link-unsaturated phase and a small queue-Ô¨Ålling phase. The need for large buffers, if near-100% queue utilization is the goal, is to a large degree speciÔ¨Åc to the TCP Reno sawtooth. Some other TCP implementations (in particular TCP Vegas, 22.6 TCP Vegas ), do not overÔ¨Åll the queue. However, TCP Vegas does not compete well with TCP Reno, at least with traditional FIFO queuing ( 20.1 A First Look At Queuing ) (but see 23.6.1 Fair Queuing and Bufferbloat ). The worst case for TCP link utilization is if the queue size is close to zero. Using again a bandwidth delay product 100 of packets, a zero-sized queue will mean that cwnd maxwill be 100 (or 101), and so cwnd min will be 50. Link utilization therefore ranges, over the lifetime of the tooth, from a low of 50/100 = 50% to a high of 100%; the average utilization is 75%. While this is not ideal, and while some non-Reno TCP variants have attempted to improve this Ô¨Ågure, 75% link utilization is not all that bad, and can be compared with the 10% of the bandwidth consumed as packet headers (though that Ô¨Ågure assumes 512 bytes of data 19.7 TCP and Bottleneck Link Utilization 463
An Introduction to Computer Networks, Release 2.0.11 per packet, which is low). (A literally zero-sized queue will not work at all well; one reason ‚Äì though not the only one ‚Äì is that TCP Reno sends a two-packet burst whenever cwnd is incremented.) TrafÔ¨Åc mix has a major inÔ¨Çuence on the appropriate queue size. For example, the analysis of the previous section assumed a single long-term TCP connection. The link-utilization situation improves with increasing numbers of TCP connections, at least if the losses are unsynchronized, because the halving of one connection‚Äôscwnd has a proportionately smaller impact on the total queue use. In [AKM04] it is shown that for a router with N TCP connections with unsynchronized losses, a queue size of (RTT averagebandwidth)/?N is sufÔ¨Åcient to keep the link almost always saturated. Larger values of N here are typically associated with ‚Äúcore‚Äù (backbone) routers. The paper [EGMR05] proposes even smaller buffer capacities, on the order of the logarithm of the maximum window size. The argument makes two important assumptions, however: Ô¨Årst, that we are willing to tolerate a link utilization somewhat less than 100% (though greater than 75%), and second, perhaps more importantly, that TCP is modiÔ¨Åed so as to spread out any packet bursts ‚Äì even bursts of size two ‚Äì over small intervals of time. There are other problems created by too-small queues, even if we are willing to accept 75% link utilization. Internet trafÔ¨Åc, not unlike city-bus trafÔ¨Åc, tends to ‚Äúbunch up‚Äù; queues serve as a way to keep these packet bunches from leading to unnecessary losses. For one example of unexpected trafÔ¨Åc bunching, see 31.4.1.3 Transient queue peaks. Increased trafÔ¨Åc randomization helps reduce the need for very large queues, but may increase the bunching effect. Internet ‚Äúcore‚Äù routers see more highly randomized trafÔ¨Åc than end-user or ‚Äúedge‚Äù routers; queues in the latter are often the most difÔ¨Åcult to conÔ¨Ågure. We will return to the issue of link utilization in 31.2.6 Single-sender Throughput Experiments and (for two senders) 31.3.10.2 Higher bandwidth and link utilization, using the ns simulator to get experimental data. See also exercise 12.0. Finally, the queue capacity does not necessarily have to remain static. We will return to this point in 21.5 Active Queue Management. Furthermore, many queue-size problems ultimately spring from the fact that all trafÔ¨Åc is being dumped into a single FIFO queue; we will look at alternative queuing strategies in23 Queuing and Scheduling. For a particular example related to bufferbloat, see 23.6.1 Fair Queuing and Bufferbloat. 19.8 Single Packet Losses Again assuming no competition on the bottleneck link, the TCP Reno additive-increase policy has a simple consequence: at the end of each tooth, only a single packet will be lost. To see this, let A be the sender, R be the bottleneck router, and B be the receiver: A R B Let T be the bandwidth delay at R, so that packets leaving R are spaced at least time T apart. A will therefore transmit packets T time units apart, except for those times when cwnd has just been incremented and A sends a pair of packets back-to-back. Let us call the second packet of such a back-to-back pair the ‚Äúextra‚Äù packet. To simplify the argument slightly, we will assume that the two packets of a pair arrive at R essentially simultaneously. 464 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 Only an extra packet can result in an increase in queue utilization; every other packet arrives after an interval T from the previous packet, giving R enough time to remove a packet from its queue. A consequence of this is that cwnd will reach the sum of the transit capacity and the queue capacity without R dropping a packet. (This is not necessarily the case if a cwnd this large were sent as a single burst.) Let C be this combined capacity, and assume cwnd has reached C. When A executes its next cwnd += 1 additive increase, it will as usual send a pair of back-to-back packets. The second of this pair ‚Äì the extra ‚Äì is doomed; it will be dropped when it reaches the bottleneck router. At this point there are C = cwnd ‚Äì 1 packets outstanding, all spaced at time intervals of T. Sliding windows will continue normally until the ACK of the packet just before the lost packet arrives back at A. After this point, A will receive only dupACKs. A has received C = cwnd ‚Äì1 ACKs since the last increment to cwnd, but must receive C+1 = cwnd ACKs in order to increment cwnd again. This will not happen, as no more new ACKs will arrive until the lost packet is transmitted. Following this, cwnd is reduced and the next sawtooth begins; the only packet that is lost is the ‚Äúextra‚Äù packet of the previous Ô¨Çight. See31.2.3 Single Losses for experimental conÔ¨Årmation, and exercise 15.0. 19.9 TCP Assumptions and Scalability In the TCP design portrayed above, several embedded assumptions have been made. Perhaps the most important is that every loss is treated as evidence of congestion. As we shall see in the next chapter, this fails for high-bandwidth TCP (when rare random losses become signiÔ¨Åcant); it also fails for TCP over wireless (either Wi-Fi or other), where lost packets are much more common than over Ethernet. See 21.6 The High-Bandwidth TCP Problem and21.7 The Lossy-Link TCP Problem. The TCPcwnd -increment strategy ‚Äì to increment cwnd by 1 for each RTT ‚Äì has some assumptions of scale. This mechanism works well for cross-continent RTT‚Äôs on the order of 100 ms, and for cwnd in the low hundreds. But if cwnd = 2000, then it takes 100 RTTs ‚Äì perhaps 20 seconds ‚Äì for cwnd to grow 10%; linear increase becomes proportionally quite slow. Also, if the RTT is very long, the cwnd increase is slow. The absolute set-by-the-speed-of-light minimum RTT for geosynchronous-satellite Internet is 480 ms, and typical satellite-Internet RTTs are close to 1000 ms. Such long RTTs also lead to slow cwnd growth; furthermore, as we shall see below, such long RTTs mean that these TCP connections compete poorly with other connections. See 21.8 The Satellite-Link TCP Problem. Another implicit assumption is that if we have a lot of data to transfer, we will send all of it in one single connection rather than divide it among multiple connections. The web http protocol violates this routinely, though. With multiple short connections, cwnd may never properly converge to the steady state for any of them; TCP Reno does not support carrying over what has been learned about cwnd from one connection to the next. A related issue occurs when a connection alternates between relatively idle periods and full-on data transfer; most TCPs set cwnd =1 and return to slow start when sending resumes after an idle period. Finally, TCP‚Äôs Fast Retransmit assumes that routers do not signiÔ¨Åcantly reorder packets. 19.9 TCP Assumptions and Scalability 465
An Introduction to Computer Networks, Release 2.0.11 19.10 TCP Parameters In TCP Reno‚Äôs Additive Increase, Multiplicative Decrease strategy, the increase increment is 1.0 and the decrease factor is 1/2. It is natural to ask if these values have some especial signiÔ¨Åcance, or what are the consequences if they are changed. Neither of these values plays much of a role in determining the average value of cwnd, at least in the short term; this is largely dictated by the path capacity, including the queue size of the bottleneck router. It seems clear that the exact value of the increase increment has no bearing on congestion; the per-RTT increase is too small to have a major effect here. The decrease factor of 1/2 may play a role in responding promptly to incipient congestion, in that it reduces cwnd sharply at the Ô¨Årst sign of lost packets. However, as we shall see in22.6 TCP Vegas, TCP Vegas in its ‚Äúnormal‚Äù mode manages quite successfully with an Additive Decrease strategy, decrementing cwnd by 1 at the point it detects approaching congestion (to be sure, it detects this well before packet loss), and, by some measures, responds better to congestion than TCP Reno. In other words, not only is the exact value of the AIMD decrease factor not critical for congestion management, but multiplicative decrease itself is not mandatory. There are two informal justiÔ¨Åcations in [JK88] for a decrease factor of 1/2. The Ô¨Årst is in slow start: if at the Nth RTT it is found that cwnd = 2Nis too big, the sender falls back to cwnd /2 = 2N-1, which is known to have worked without losses the previous RTT. However, a change here in the decrease policy might best be addressed with a concomitant change to slow start; alternatively, the reduction factor of 1/2 might be left still to apply to ‚Äúunbounded‚Äù slow start, while a new factor of ùõΩmight apply to threshold slow start. The second justiÔ¨Åcation for the reduction factor of 1/2 applies directly to the congestion avoidance phase; written in 1988, it is quite remarkable to the modern reader: If the connection is steady-state running and a packet is dropped, it‚Äôs probably because a new connection started up and took some of your bandwidth.. .. [I]t‚Äôs probable that there are now exactly two conversations sharing the bandwidth. I.e., you should reduce your window by half because the bandwidth available to you has been reduced by half. [JK88], ¬ßD Today, busy routers may have thousands of simultaneous connections. To be sure, Jacobson and Karels go on to state, ‚Äúif there are more than two connections sharing the bandwidth, halving your window is conservative ‚Äì and being conservative at high trafÔ¨Åc intensities is probably wise‚Äù. This advice remains apt today. But while they do not play a large role in setting cwnd or in avoiding ‚Äúcongestive collapse‚Äù, it turns out that these increase-increment and decrease-factor values of 1 and 1/2 respectively play a great role in fairness: making sure competing connections get the bandwidth allocation they ‚Äúshould‚Äù get. We will return to this in20.3 TCP Reno Fairness with Synchronized Losses, and also 21.4 AIMD Revisited. 19.11 Epilog TCP Reno‚Äôs core congestion algorithm is based on algorithms in Jacobson and Karel‚Äôs 1988 paper [JK88], now twenty-Ô¨Åve years old, although NewReno and SACK have been almost universally added to the standard ‚ÄúReno‚Äù implementation. There are also broad changes in TCP usage patterns. Twenty years ago, the vast majority of all TCP trafÔ¨Åc represented downloads from ‚Äúmajor‚Äù servers. Today, over half of all Internet TCP trafÔ¨Åc is peer-to-peer 466 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 rather than server-to-client. The rise in online video streaming creates new demands for excellent TCP real-time performance. In the next chapter we will examine the dynamic behavior of TCP Reno, focusing in particular on fairness between competing connections, and on other problems faced by TCP Reno senders. Then, in 22 Newer TCP Implementations, we will survey some attempts to address these problems. 19.12 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.14 Solutions for TCP Reno and Congestion Management. 1.0. Consider the following network, with each link other than the Ô¨Årst having a bandwidth delay of 1 packet/second. Assume ACKs travel instantly from B to R (and thus to A). Assume there are no propagation delays, so the RTT noLoad is 4; the bandwidth RTT product is then 4 packets. If A uses sliding windows with a window size of 6, the queue at R1 will eventually have size 2. A R1infinitely fast R21 pkt/sec R3 R4 B1 pkt/sec 1 pkt/sec 1 pkt/sec Suppose A uses threshold slow start ( 19.2.2 Threshold Slow Start ) withssthresh = 6, and with cwnd initially 1. Complete the table below until two rows after cwnd = 6; for these Ô¨Ånal two rows, cwnd has reachedssthresh and so A will send only one new packet for each ACK received. Assume the queue at R1 is large enough that no packets are dropped. How big will the queue at R1 grow? TA sends R1 queues R1 sends B receives/ACKs cwnd 01 1 1 1 2 3 42,3 3 2 1 2 5 3 2 6 7 84,5 5 4 2 3 Note that if, instead of using slow start, A simply sends the initial windowful of 6 packets all at once, then the queue at R1 will initially hold 6-1 = 5 packets. 2.0. Consider the following network from 19.2.3 Slow-Start Multiple Drop Example, with links labeled with bandwidths in packets/ms. Assume ACKs travel instantly from B to R (and thus to A). 19.12 Exercises 467
An Introduction to Computer Networks, Release 2.0.11 A R Binfinitely fast 1 pkt/ms A begins sending to B using unbounded slow start, beginning with Data[1] at T=0. Initially, cwnd = 1. Write out a table of packet transmissions and deliveries assuming R‚Äôs maximum queue size is 4 (not counting the packet currently being forwarded). Stop with the arrival at A of the Ô¨Årst dupACK triggered by the arrival at B of the packet that followed the Ô¨Årst packet that was dropped by R. No retransmissions will occur by then. TA sends R queues R drops R sends B receives/ACKs 0Data[1] Data[1] 3.0. Consider the network from exercise 2.0 above. A again begins sending to B using unbounded slow start, but this time R‚Äôs queue size is 2, not counting the packet currently being forwarded. Make a table showing all packet transmissions by A, all packet drops by R, and other columns as are useful. Assume noretransmission mechanism is used at all (no timeouts, no fast retransmit), and that A sends new data only when it receives new ACKs (dupACKs, in other words, do not trigger new data transmissions). With these assumptions, new data transmissions will eventually cease; continue the table until all transmitted data packets are received by B. 4.0. Suppose a connection starts with cwnd =1 and increments cwnd by 1 each RTT with no loss, and sets cwnd tocwnd /2, rounding down, on each RTT with at least one loss. Lost packets are not retransmitted, and propagation delays dominate so each windowful is sent more or less together. Packets 5, 13, 14, 23 and 30 are lost. What is the window size each RTT, up until the Ô¨Årst 40 packets are sent? What packets are sent each RTT? Hint: in the Ô¨Årst RTT, Data[1] is sent. There is no loss, so in the second RTT cwnd = 2 and Data[2] and Data[3] are sent. 5.0. Suppose TCP Reno is used to transfer a large Ô¨Åle over a path with bandwidth high enough that, during slow start,cwnd can be treated as doubling each RTT as in 19.2 Slow Start. Assume the receiver places no limits on window size. (a). How many RTTs will it take for the window size to Ô¨Årst reach ~8,000 packets (about 213), assuming unbounded slow start is used and there are no packet losses? (b). Approximately how many packets will have been sent and acknowledged by that point? (c). Now assume the bandwidth is 100 packets/ms and the RTT is 80 ms, making the bandwidth delay product 8,000 packets. What fraction of the total bandwidth will have been used by the connection up to the point where the window size reaches 8000? Hint: the total bandwidth is 8,000 packets per RTT. 6.0. (a) Repeat the diagram in 19.4 TCP Reno and Fast Recovery, done there with cwnd =10, for a window size of 8. Assume as before that the lost packet is Data[10]. There will be seven dupACK[9]‚Äôs, which it may be convenient to tag as dupACK[9]/11 through dupACK[9]/17. Be sure to indicate clearly when sending resumes. (b). Suppose you try to do this with a window size of 6. Is this window size big enough for Fast Recovery still to work? If so, at what dupACK[9]/N does new data transmission begin? If not, what goes wrong? 468 19 TCP Reno and Congestion Management
An Introduction to Computer Networks, Release 2.0.11 7.0. Suppose the window size is 100, and Data[1001] is lost. There will be 99 dupACK[1000]‚Äôs sent, which we may denote as dupACK[1000]/1002 through dupACK[1000]/1100. TCP Reno is used. (a). At which dupACK[1000]/N does the sender start sending new data? (b). When the retransmitted data[1001] arrives at the receiver, what ACK is sent in response? (c). When the acknowledgment in (b) arrives back at the sender, what data packet is sent? Hint: express EFS in terms of dupACK[1000]/N, for N ¬•1004. The third dupACK is dupACK[1000]/1004; what is EFS at that point after retransmission of Data[1001]? 8.0. Suppose the window size is 40, and Data[1001] is lost. Packet 1000 will be ACKed normally. Packets 1001-1040 will be sent, and 1002-1040 will each trigger a duplicate ACK[1000]. (a). What actual data packets trigger the Ô¨Årst three dupACKs? (The Ô¨Årst ACK[1000] is triggered by Data[1000]; don‚Äôt count this one as a duplicate.) (b). After the third dupACK[1000] has been received and the lost data[1001] has been retransmitted, how many packets/ACKs should the sender estimate as in Ô¨Çight? When the retransmitted Data[1001] arrives at the receiver, ACK[1040] will be sent back. (c). What is the Ô¨Årst Data[N] sent for which the response is ACK[N], for N>1000? (d). What is the Ô¨Årst N for which Data[N+20] is sent in response to ACK[N] (this represents the point when the connection is back to normal sliding windows, with a window size of 20)? 9.0. Recall ( 19.2 Slow Start ) that during slow start cwnd is incremented by 1 for each arriving ACK, resulting in the transmission of two new data packets. Suppose slow-start is modiÔ¨Åed so that, on each ACK, three new packets are sent rather than two; cwnd will now triple after each RTT, taking values 1, 3, 9, 27,. .. . (a). For each arriving ACK, by how much must cwnd now be incremented? (b). Suppose a path has mostly propagation delay. Progressively larger windowfuls are sent, with sizes successive powers of 3, until a cwnd is reached where a packet loss occurs. What window size can the sender be reasonably sure does work, based on earlier experience? 10.0. Suppose in the example of 19.5 TCP NewReno, Data[4] had notbeen lost. (a). When Data[1] is received, what ACK would be sent in response? (b). At what point in the diagram is the sender able to resume ordinary sliding windows with cwnd = 6? 19.12 Exercises 469
An Introduction to Computer Networks, Release 2.0.11 11.0. Suppose in the example of 19.5 TCP NewReno, Data[1] and Data[2] had been lost, but not Data[4]. (a). The third dupACK[0] is sent in response to what Data[N]? (b). When the retransmitted Data[1] reaches the receiver, ACK[1] is the response. When this ACK[1] reaches the sender, which Data packets are sent in response? 12.0. Suppose two TCP connections have the same RTT and share a bottleneck link, on which there is no other trafÔ¨Åc. The size of the bottleneck queue is negligible when compared to the bandwidth RTT noLoad product. Loss events occur at regular intervals, and are completely synchronized. Show that the two connections together will use 75% of the total bottleneck-link capacity, as in 19.7 TCP and Bottleneck Link Utilization (there done for a single connection). See also Exercise 16.0 of chapter 21 Further Dynamics of TCP. 13.0. In 19.7 TCP and Bottleneck Link Utilization we showed that, if the bottleneck router queue capacity was 50% of a TCP Reno connection‚Äôs transit capacity, and there was no other trafÔ¨Åc, then the bottleneck-link utilization would be 95.8%. (a). Suppose the queue capacity is 1/3 of the transit capacity. Show the bottleneck link utilization is 11/12, or 91.7%. Draw a diagram of the tooth, and Ô¨Ånd the relative lengths of the link-unsaturated and queue-Ô¨Ålling phases. You may round off cwnd maxto 4/3 the transit capacity (the value of cwnd just before the packet loss; the exact value of cwnd maxis higher by 1). (b).‚ô¢Derive a formula for the link utilization in terms of the ratio f<1 of queue capacity to transit capacity. Make the same simplifying assumption as in part (a). 14.0. In 19.2.1 TCP Reno Per-ACK Responses we stated that the per-ACK response of a TCP sender was to increment cwnd as follows: cwnd =cwnd + 1/cwnd (a). What is the corresponding formulation if the window size is in fact measured in bytes rather than packets? Let SMSS denote the sender‚Äôs maximum segment size, and let bwnd =SMSScwnd denote the congestion window as measured in bytes. Hint: solve this last equation for cwnd and plug the result in above. (b). What is the appropriate formulation of cwnd =cwnd + 1/cwnd if delayed ACKs are used ( 18.8 TCP Delayed ACKs ) and we still want cwnd to be incremented by 1 for each windowful? Assume we are back to measuring cwnd in packets. 15.0. In 19.8 Single Packet Losses we simpliÔ¨Åed the argument slightly by assuming that when A sent a pair of packets, they arrived at R ‚Äúessentially simultaneously‚Äù. Give a scenario in which it is not the ‚Äúextra‚Äù packet (the second of the pair) that is lost, but the packet that follows it. Hint: see 31.3.4.1 Single-sender phase effects. 470 19 TCP Reno and Congestion Management
20 DYNAMICS OF TCP In this chapter we introduce, Ô¨Årst and foremost, the possibility that there are other TCP connections out there competing with us for throughput. In 8.3 Linear Bottlenecks (and in 19.7 TCP and Bottleneck Link Utilization ) we looked at the performance of TCP through an uncontested bottleneck; now we allow for competition. Although the focus of this chapter is on TCP Reno, many, though not all, of the ideas presented here apply to non-Reno TCPs as well, and even to non-TCP mechanisms such as QUIC. Several non-Reno TCP alternatives are presented later in 22 Newer TCP Implementations. The following chapter continues this thread, with some more examples of TCP-Reno large-scale behavior. 20.1 A First Look At Queuing In what order do we transmit the packets in a router‚Äôs outbound-interface queue? The conventional answer is in the order of arrival; technically, this is FIFO (First-In, First-Out) queuing. What happens to a packet that arrives at a router whose queue for the desired outbound interface is full? The conventional answer is that it is dropped; technically, this is known as tail-drop. While FIFO tail-drop remains very important, there are alternatives. In an admittedly entirely different context (the IPv6 equivalent of ARP), RFC 4681 states, ‚ÄúWhen a queue overÔ¨Çows, the new arrival SHOULD replace the oldest entry.‚Äù This might be called ‚Äúhead drop‚Äù; it is not used for router queues. Queuing Theory While there are lots of queues in this chapter, we avoid the language of traditional queuing theory. That‚Äôs because queues created through sliding windows are highly deterministic in terms of both arrivals and departures (so-called ‚ÄúD/D/1‚Äù queues). From a queuing-theory perspective, interesting queues tend to have random ( egPoisson) arrival or service times, or both; eg‚ÄúM/M/1‚Äù queues. The advantage, for us, of sliding-windows queues is that they are a good deal more tractable than the general case. When analyzing independent messages, however, M/M/1 queuing is much more important; see for example [LK78]. An alternative drop-policy mechanism that hasbeen considered for router queues is random drop. Under this policy, if a packet arrives but the destination queue is full, with N packets waiting, then one of the N+1 packets in all ‚Äì the N waiting plus the new arrival ‚Äì is chosen at random for dropping. The most recent arrival has thus a very good chance of gaining an initial place in the queue, but also a reasonable chance of being dropped later on. See [AM90]. While random drop is seldom if ever put to production use its original form, it does resolve a peculiar synchronization problem related to TCP‚Äôs natural periodicity that can lead to starvation for one connection. This situation ‚Äì known as phase effects ‚Äì will be revisited in 31.3.4 Phase Effects. Mathematically, random-drop queuing is sometimes more tractable than tail-drop because a packet‚Äôs loss probability has little dependence on arrival-time race conditions with other packets. 471
An Introduction to Computer Networks, Release 2.0.11 20.1.1 Priority Queuing A quite different alternative to FIFO is priority queuing. We will consider this in more detail in 23.3 Priority Queuing, but the basic idea is straightforward: whenever the router is ready to send the next packet, it looks Ô¨Årst to see if it has any higher-priority packets to send; lower-priority packets are sent only when there is no waiting higher-priority trafÔ¨Åc. This can, of course, lead to complete starvation for the lower-priority trafÔ¨Åc, but often there are bandwidth constraints on the higher-priority trafÔ¨Åc ( egthat it amounts to less than 10% of the total available bandwidth) such that starvation does not occur. In an environment of mixed real-time and bulk trafÔ¨Åc, it is natural to use priority queuing to give the realtime trafÔ¨Åc priority service, by assignment of such trafÔ¨Åc to the higher-priority queue. This works quite well as long as, say, the real-time trafÔ¨Åc is less than some Ô¨Åxed fraction of the total; we will return to this in 25 Quality of Service. 20.2 Bottleneck Links with Competition So far we have been ignoring the fact that there are other TCP connections out there. A single connection in isolation needs not to overrun its bottleneck router and drop packets, at least not too often. However, once there are other connections present, then each individual TCP connection also needs to consider how to maximize its share of the aggregate bandwidth. Consider a simple network path, with bandwidths shown in packets/ms. The minimum bandwidth, or path bandwidth, is 3 packets/ms. 20.2.1 Example 1: linear bottleneck Below is the example we considered in 8.3 Linear Bottlenecks; bandwidths are shown in packets/ms. A R110 pkts/ms R26 pkts/ms R3 R4 B3 pkts/ms 3 pkts/ms 8 pkts/ms The bottleneck link for A √ëB trafÔ¨Åc is at R2, and the queue will form at R2‚Äôs outbound interface. We claimed earlier that if the sender uses sliding windows with a Ô¨Åxed window size, then the network will converge to a steady state in relatively short order. This is also true if multiple senders are involved; however, a mathematical proof of convergence may be more difÔ¨Åcult. 20.2.2 Example 2: router competition The bottleneck-link concept is a useful one for understanding congestion due to a single connection. However, if there are multiple senders in competition for a link, the situation is more complicated. Consider the following diagram, in which links are labeled with bandwidths in packets/ms: 472 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 R1 R2R3 C4 pkts/msA B10 pkts/ms 10 pkts/ms 10 pkts/ms2 pkts/ms For a moment, assume R3 uses priority queuing, with the B √ù√ëC path given priority over A √ù√ëC. If B‚Äôs Ô¨Çow to C is Ô¨Åxed at 3 packets/ms, then A‚Äôs share of the R3‚ÄìC link will be 1 packet/ms, and A‚Äôs bottleneck will be at R3. However, if B‚Äôs total Ô¨Çow rate drops to 1 packet/ms, then the R3‚ÄìC link will have 3 packets/ms available, and the bottleneck for the A‚ÄìC path will become the 2 packet/ms R1‚ÄìR3 link. Now let us switch to the more-realistic FIFO queuing at R3. If B‚Äôs Ô¨Çow is 3 packets/ms and A‚Äôs is 1 packet/ms, then the R3‚ÄìC link will be saturated, but just barely: if each connection sticks to these rates, no queue will develop at R3. However, it is no longer accurate to describe the 1 packet/ms as A‚Äôs share: if A wishes to send more, it will begin to compete with B. At Ô¨Årst, the queue at R3 will grow; eventually, it is quite possible that B‚Äôs total Ô¨Çow rate might drop because B is losing to A in the competition for R3‚Äôs queue. This latter effect is very real. In general, if two connections share a bottleneck link, they are competing for the bandwidth of that link. That bandwidth share, however, is precisely dictated by the queue share as of a short while before. R3‚Äôs Ô¨Åxed rate of 4 packets/ms means one packet every 250 ¬µs. If R3 has a queue of 100 packets, and in that queue there are 37 packets from A and 63 packets from B, then over the next 25 ms (= 100 250 ¬µs) R3‚Äôs trafÔ¨Åc to C will consist of those 37 packets from A and the 63 from B. Thus the competition between A and B for R3‚ÄìC bandwidth is Ô¨Årst fought as a competition for R3‚Äôs queue space. This is important enough to state as as rule: Queue-Competition Rule: in the steady state, if a connection utilizes fraction ùõº¬§1 of a FIFO router‚Äôs queue, then that connection has a share of ùõºof the router‚Äôs total outbound bandwidth. Below is a picture of R3‚Äôs queue and outbound link; the queue contains four packets from A and eight from B. The link, too, contains packets in this same ratio; presumably packets from B are consistently arriving twice as fast as packets from A. 20.2 Bottleneck Links with Competition 473
An Introduction to Computer Networks, Release 2.0.11 R3 CA B In the steady state here, A and B will use four and eight packets, respectively, of R3‚Äôs queue capacity. As acknowledgments return, each sender will replenish the queue accordingly. However, it is not in A‚Äôs longterm interest to settle for a queue utilization at R3 of four packets; A may want to take steps that will lead in this setting to a gradual increase of its queue share. Although we started the discussion above with Ô¨Åxed packet-sending rates for A and B, in general this leads to instability. If A and B‚Äôs combined rates add up to more than 4 packets/ms, R3‚Äôs queue will grow without bound. It is much better to have A and B use sliding windows, and give them each Ô¨Åxed window sizes; in this case, as we shall see, a stable equilibrium is soon reached. Any combination of window sizes is legal regardless of the available bandwidth; the queue utilization (and, if necessary, the loss rate) will vary as necessary to adapt to the actual bandwidth. If there are several competing Ô¨Çows, then a given connection may have multiple bottlenecks, in the sense that there are several routers on the path experiencing queue buildups. In the steady state, however, we can still identify the link (or Ô¨Årst link) with minimum bandwidth; we can call this link the bottleneck. Note that the bottleneck link in this sense can change with the sender‚Äôs winsize and with competing trafÔ¨Åc. 20.2.3 Example 3: competition and queue utilization In the next diagram, the bottleneck R‚ÄìC link has a normalized bandwidth of 1 packet per ms (or, more abstractly, one packet per unit time). The bandwidths of the A‚ÄìR and B‚ÄìR links do not matter, except they are greater than 1 packet per ms. Each link is labeled with the propagation delay, measured in the same time unit as the bandwidth; the delay thus represents the number of packets the link can be transporting at the same time, if sent at the bottleneck rate. 474 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 A BR Cdelay ddelay dA delay dB The network layout here, with the shared R‚ÄìC link as the bottleneck, is sometimes known as the singlebell topology. A perhaps-more-common alternative is the dumbbell topology of 20.3 TCP Reno Fairness with Synchronized Losses, though the two are equivalent for our purposes. Suppose A and B each send to C using sliding windows, each with Ô¨Åxed values of winsize w Aand w B. Suppose further that these winsize values are large enough to saturate the R‚ÄìC link. How big will the queue be at R? And how will the bandwidth divide between the A √ù√ëC and B√ù√ëC Ô¨Çows? For the two-competing-connections example above, assume we have reached the steady state. Let ùõºdenote the fraction of the bandwidth that the A √ù√ëC connection receives, and let ùõΩ= 1-ùõºdenote the fraction that the B√ù√ëC connection gets; because of our normalization choice for the R‚ÄìC bandwidth, ùõºandùõΩalso represent respective throughputs. From the Queue-Competition Rule above, these bandwidth proportions must agree with the queue proportions; if Q denotes the combined queue utilization of both connections, then that queue will have about ùõºQ packets from the A √ù√ëC Ô¨Çow and about ùõΩQ packets from the B √ù√ëC Ô¨Çow. We worked out the queue usage precisely in 8.3.2 RTT Calculations for a single Ô¨Çow; we derived there the following: queue_usage = winsize ‚Äì throughput RTT noLoad where we have here used ‚Äúthroughput‚Äù instead of ‚Äúbandwidth‚Äù to emphasize that this is the dynamic share rather than the physical transmission capacity. This equation remains true for each separate Ô¨Çow in the present case, where the RTT noLoad for the A√ù√ëC connection is 2(d A+d) (the factor of 2 is to account for the round-trip) and the RTT noLoad for the B√ù√ëC connection is 2(d B+d). We thus have ùõºQ = w A‚Äì 2ùõº(dA+d) ùõΩQ = w B‚Äì 2ùõΩ(dB+d) or, alternatively, ùõº[Q + 2d + 2d A] = w A ùõΩ[Q + 2d + 2d B] = w B If we add the Ô¨Årst pair of equations above, we can obtain the combined queue utilization: Q = w A+ w B‚Äì 2d ‚Äì 2( ùõºdA+ùõΩdB) The last term here, 2( ùõºdA+ùõΩdB), represents the number of A‚Äôs packets in Ô¨Çight on the A‚ÄìR link plus the number of B‚Äôs packets in Ô¨Çight on the B‚ÄìR link. 20.2 Bottleneck Links with Competition 475
An Introduction to Computer Networks, Release 2.0.11 We can solve these equations exactly for ùõº,ùõΩand Q in terms of the known quantities, but the algebraic solution is not particularly illuminating. Instead, we examine a few more-tractable special cases. 20.2.3.1 The equal-delays case We consider Ô¨Årst the special case of equal delays: dA= dB= d‚Äô. In this case the term ( ùõºdA+ùõΩdB) simpliÔ¨Åes to d‚Äô, and thus we have Q = w A+ w B‚Äì 2d ‚Äì 2d‚Äô. Furthermore, if we divide corresponding sides of the second pair of equations above, we get ùõº/ùõΩ= w A/wB; that is, the bandwidth (and thus the queue utilization) divides in exact accordance to the window-size proportions. If, however, d Ais larger than d B, then a greater fraction of the A √ù√ëC packets will be in transit, and so fewer will be in the queue at R, and so ùõºwill be somewhat smaller and ùõΩsomewhat larger. 20.2.3.2 The equal-windows case If we assume equal winsize values instead, w A= w B= w, then we get ùõº/ùõΩ= [Q + 2d + 2d B] / [Q + 2d + 2d A] The bandwidth ratio here is biased against the larger of d Aor d B. That is, if d A> d B, then more of A‚Äôs packets will be in transit, and thus fewer will be in R‚Äôs queue, and so A will have a smaller fraction of the the bandwidth. This bias is, however, not quite proportional: if we assume d Ais double d Band d B= d = Q/2, then ùõº/ùõΩ= 3/4, and A gets 3/7 of the bandwidth to B‚Äôs 4/7. Still assuming w A= w B= w, let us decrease w to the point where the link is just saturated, but Q=0. At this point ùõº/ùõΩ= [d+d B]/[d +d A]; that is, bandwidth divides according to the respective RTT noLoad values. As w rises, additional queue capacity is used and ùõº/ùõΩwill move closer to 1. 20.2.3.3 The Ô¨Åxed-w Bcase Finally, let us consider what happens if w BisÔ¨Åxed at a large-enough value to create a queue at R from the B‚ÄìC trafÔ¨Åc alone, while w Athen increases from zero to a point much larger than w B. Denote the number of B‚Äôs packets in R‚Äôs queue by Q B; with w A= 0 we have ùõΩ=1 and Q = Q B= w B‚Äì 2(d B+d) = throughput  (RTT ‚Äì RTT noLoad ). As w Abegins to increase from zero, the competition will decrease B‚Äôs throughput. We have ùõº= wA/[Q+2d+2d A];small changes in w Awill not lead to much change in Q, and even less in Q+2d+2d A, and so ùõºwill initially be approximately proportional to w A. For B‚Äôs part, increased competition from A (increased w A) will always decrease B‚Äôs share of the bottleneck R‚ÄìC link; this link is saturated and every packet of A‚Äôs in transit there must take away one slot on that link for a packet of B‚Äôs. This in turn means that B‚Äôs bandwidth ùõΩmust decrease as w Arises. As B‚Äôs bandwidth decreases, Q B=ùõΩQ = w B‚Äì 2ùõΩ(dB+d) must increase; another way to put this is as the transit capacity falls, the queue utilization rises. For Q B=ùõΩQ to increase while ùõΩdecreases, Q must be increasing faster than ùõΩ is decreasing. Finally, we can conclude that as w Agets large and ùõΩ√ë0, the limiting value for B‚Äôs queue utilization Q Bat R will be the entire windowful w B, up from its starting value (when w A=0) of w B‚Äì 2(d B+d). If d B+d had been 476 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 small relative to w B, then Q B‚Äôs increase will be modest, and it may be appropriate to consider Q Brelatively constant. We remark again that the formulas here are based on the assumption that the bottleneck bandwidth is one packet per unit time; see exercise 1.0 for the necessary adjustments for conventional bandwidth measurements. 20.2.3.4 The iterative solution Given d, d A, dB, wAand w B, one way to solve for ùõº,ùõΩand Q is to proceed iteratively. Suppose an initial xùõº,ùõΩyis given, as the respective fractions of packets in the queue at R. Over the next period of time, ùõº andùõΩmust (by the Queue Rule) become the bandwidth ratios. If the A‚ÄìC connection has bandwidth ùõº (recall that the R‚ÄìC connection has bandwidth 1.0, in packets per unit time, so a bandwidth fraction of ùõºmeans an actual bandwidth of ùõº), then the number of packets in bidirectional transit will be 2 ùõº(dA+d), and so the number of A‚ÄìC packets in R‚Äôs queue will be Q A= w A‚Äì 2ùõº(dA+d); similarly for Q B. At that point we will have ùõºnew= Q A/(QA+QB). Starting with an appropriate guess for ùõºand iterating ùõº√ëùõºnewa few times, if the sequence converges then it will converge to the steady-state solution. Convergence is not guaranteed, however, and is dependent on the initial guess for ùõº. One guess that often leads to convergence is w A/(wA+wB). 20.2.4 Example 4: cross trafÔ¨Åc and RTT variation In the following diagram, let us consider what happens to the A‚ÄìB trafÔ¨Åc when the C √ù√ëD link ramps up. Bandwidths shown are expressed as packets/ms and all queues are FIFO. (Because the bandwidth is not equal to 1.0, we cannot apply the formulas of the previous section directly.) We will assume that propagation delays are small enough that only an inconsequential number of packets from C to D can be simultaneously in transit at the bottleneck rate of 5 packets/ms. All senders will use sliding windows. A R1 R2 R3 B DC 100 pkts/ms100 pkts/ms 5 pkts/ms 100 pkts/ms2 pkts/ms 100 pkts/ms Let us suppose the A‚ÄìB link is idle, and the C √ù√ëD connection begins sending with a window size chosen so as to create a queue of 30 of C‚Äôs packets at R1 (if propagation delays are such that two packets can be in transit each direction, we would achieve this with winsize=34). Now imagine A begins sending. If A sends a single packet, is not shut out even though the R1‚ÄìR2 link is 100% busy. A‚Äôs packet will simply have to wait at R1 behind the 30 packets from C; the waiting time in the 20.2 Bottleneck Links with Competition 477
An Introduction to Computer Networks, Release 2.0.11 queue will be 30 packets (5 packets/ms) = 6 ms. If we change the winsize of the C √ù√ëD connection, the delay for A‚Äôs packets will be directly proportional to the number of C‚Äôs packets in R1‚Äôs queue. To most intents and purposes, the C √ù√ëD Ô¨Çow here has increased the RTT of the A √ù√ëB Ô¨Çow by 6 ms. As long as A‚Äôs contribution to R1‚Äôs queue is small relative to C‚Äôs, the delay at R1 for A‚Äôs packets looks more like propagation delay than bandwidth delay, because if A sends two back-to-back packets they will likely be enqueued consecutively at R1 and thus be subject to a single 6 ms queuing delay. By varying the C √ù√ëD window size, we can, within limits, increase or decrease the RTT for the A √ù√ëB Ô¨Çow. Let us return to the Ô¨Åxed C √ù√ëD window size ‚Äì denoted w C‚Äì chosen to yield a queue of 30 of C‚Äôs packets at R1. As A increases its own window size from, say, 1 to 5, the C √ù√ëD throughput will decrease slightly, but C‚Äôs contribution to R1‚Äôs queue will remain dominant. As in the argument at the end of 20.2.3.3 The Ô¨Åxed-wB case, small propagation delays mean that w Cwill not be much larger than 30. As w Aclimbs from zero to inÔ¨Ånity, C‚Äôs contribution to R1‚Äôs queue rises from 30 to at most w C, and so the 6ms delay for A √ù√ëB packets remains relatively constant even as A‚Äôs winsize rises to the point that A‚Äôs contribution to R1‚Äôs queue far outweighed C‚Äôs. (As we will argue in the next paragraphs, this can actually happen only if the R2‚ÄìR3 bandwidth is increased). Each packet from A arriving at R1 will, on average, face 30 or so of C‚Äôs packets ahead of it, along with anywhere from many fewer to many more of A‚Äôs packets. If A‚Äôs window size is 1, its one packet at a time will wait 6 ms in the queue at R1. If A‚Äôs window size is greater than 1 but remains small, so that A contributes only a small proportion of R1‚Äôs queue, then A‚Äôs packets will wait only at R1. Initially, as A‚Äôs winsize increases, the queue at R1 grows but all other queues remain empty. However, if A‚Äôs winsize grows large enough that its packets consume 40% of R1‚Äôs queue in the steady state, then this situation changes. At the point when A has 40% of R1‚Äôs queue, by the Queue Competition Rule it will also have a 40% share of the R1‚ÄìR2 link‚Äôs bandwidth, that is, 40% 5 = 2 packets/ms. Because the R2‚ÄìR3 link has a bandwidth of 2 packets/ms, the A‚ÄìB throughput can never grow beyond this. If the C‚ÄìD contribution to R1‚Äôs queue is held constant at 30 packets, then this point is reached when A‚Äôs contribution to R1‚Äôs queue is 20 packets. Because A‚Äôs proportional contribution to R1‚Äôs queue cannot increase further, any additional increase to A‚Äôs winsize must result in those packets now being enqueued at R2. We have now reached a situation where A‚Äôs packets are queuing up at both R1 and at R2, contrary to the single-sender principle that packets can queue at only one router. Note, however, that for any Ô¨Åxed value of A‚Äôs winsize, a small-enough increase in A‚Äôs winsize will result in either that increase going entirely to R1‚Äôs queue or entirely to R2‚Äôs queue. SpeciÔ¨Åcally, if w Arepresents A‚Äôs winsize at the point when A has 40% of R1‚Äôs queue (a little above 20 packets if propagation delays are small), then for winsize < w Aany queue growth will be at R1 while for winsize > w Aany queue growth will be at R2. In a sense the bottleneck link ‚Äúswitches‚Äù from R1‚ÄìR2 to R2‚ÄìR3 at the point winsize = w A. In the graph below, A‚Äôs contribution to R1‚Äôs queue is plotted in green and A‚Äôs contribution to R2‚Äôs queue is in blue. It may be instructive to compare this graph with the third graph in 8.3.3 Graphs at the Congestion Knee, which illustrates a single connection with a single bottleneck. 478 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 A's winsizequeue utilization WAR1 R2 In Exercise 8.0 we consider some minor changes needed if propagation delay is notinconsequential. 20.2.5 Example 5: dynamic bottlenecks The next example has two links offering potential competition to the A √ù√ëB Ô¨Çow: C√ù√ëD and E√ù√ëF. Either of these could send trafÔ¨Åc so as to throttle (or at least compete with) the A √ù√ëB trafÔ¨Åc. Either of these could choose a window size so as to build up a persistent queue at R1 or R3; a persistent queue of 20 packets would mean that A √ù√ëB trafÔ¨Åc would wait 4 ms in the queue. A R1 R2 R3 R4 DC 100 pkts/ms100 pkts/ms 5 pkts/ms 100 pkts/ms2 pkts/ms 5 pkts/msB100 pkts/msE F100 pkts/ms100 pkts/ms Despite situations like this, we will usually use the term ‚Äúbottleneck link‚Äù as if it were a precisely deÔ¨Åned concept. In Examples 2, 3 and 4 above, a better term might be ‚Äúcompetitive link‚Äù; for Example 5 we perhaps should say ‚Äúcompetitive link s. 20.2.6 Packet Pairs One approach for a sender to attempt to measure the physical bandwidth of the bottleneck link is the packetpairs technique: the sender repeatedly sends a pair of packets P1 and P2 to the receiver, one right after the other. The receiver records the time difference between the arrivals. Sooner or later, we would expect that P1 and P2 would arrive consecutively at the bottleneck router R, and be put into the queue next to each other. They would then be sent one right after the other on the bottleneck 20.2 Bottleneck Links with Competition 479
An Introduction to Computer Networks, Release 2.0.11 link; if T is the time difference in arrival at the far end of the link, the physical bandwidth is size(P1)/T. At least some of the time, the packets will remain spaced by time T for the rest of their journey. The theory is that the receiver can measure the different arrival-time differences for the different packet pairs, and look for the minimum time difference. Often, this will be the time difference introduced by the bandwidth delay on the bottleneck link, as in the previous paragraph, and so the ultimate receiver will be able to infer that the bottleneck physical bandwidth is size(P1)/T. Two things can mar this analysis. First, packets may be reordered; P2 might arrive before P1. Second, P1 and P2 can arrive together at the bottleneck router and be sent consecutively, but then, later in the network, the two packets can arrive at a second router R2 with a (transient) queue large enough that P2 arrives while P1 is in R2‚Äôs queue. If P1 and P2 are consecutive in R2‚Äôs queue, then the ultimate arrival-time difference is likely to reÔ¨Çect R2‚Äôs (higher) outbound bandwidth rather than R‚Äôs. Additional analysis of the problems with the packet-pair technique can be found in [VP97], along with a proposal for an improved technique known as packet bunch mode. 20.3 TCP Reno Fairness with Synchronized Losses This brings us to the question of just what isa ‚Äúfair‚Äù division of bandwidth. A starting place is to assume that ‚Äúfair‚Äù means ‚Äúequal‚Äù, though, as we shall see below, the question does not end there. For the moment, consider again two competing TCP Reno connections: Connection 1 (in blue) from A to C and Connection 2 (in green) from B to D, through the same bottleneck router R, and with the same RTT. The router R will use tail-drop queuing. R R2 DC A B The layout illustrated here, with the shared link somewhere in the middle of each path, is sometimes known as the dumbbell topology. For the time being, we will also continue to assume the TCP Reno synchronized-loss hypothesis: that in any one RTT either both connections experience a loss or neither does. (This assumption is suspect; we explore it further in 20.3.3 TCP Reno RTT bias and in 31.3 Two TCP Senders Competing ). This was the model reviewed previously in 19.1.1.1 A Ô¨Årst look at fairness; we argued there that in any RTT without a loss, the expression ( cwnd 1-cwnd 2) remained the same (both cwnd s incremented by 1), while in any RTT with a loss the expression ( cwnd 1-cwnd 2) decreased by a factor of 2 (both cwnd s decreased by factors of 2). Here is a graphical version of the same argument, as originally introduced in [CJ89]. We plot cwnd 1on the x-axis andcwnd 2on the y-axis. An additive increase of both (in equal amounts) moves the point (x,y) = (cwnd 1,cwnd 2) along the line parallel to the 45¬∞ line y=x; equal multiplicative decreases of both moves the point (x,y) along a line straight back towards the origin. If the maximum network capacity is Max, then a loss occurs whenever x+y exceeds Max, that is, the point (x,y) crosses the line x+y=Max. 480 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 cwnd1cwnd2cwnd1 = cwnd2 cwnd1+cwnd2 = Max MaxAdditive-increase follows this lineMultiplicative increase/decrease follows this line from/to origin initial state Beginning at the initial state, additive increase moves the state at a 45¬∞ angle up to the line x+y=Max, in small increments denoted by the small arrowheads. At this point a loss would occur, and the state jumps back halfway towards the origin. The state then moves at 45¬∞ incrementally back to the line x+y=Max, and continues to zigzag slowly towards the equal-shares line y=x. Any attempt to increase cwnd faster than linear will mean that the increase phase is not parallel to the line y=x, but in fact veers away from it. This will slow down the process of convergence to equal shares. Finally, here is a timeline version of the argument. We will assume that the A‚ÄìC path capacity, the B‚ÄìD path capacity and R‚Äôs queue size all add up to 24 packets, and that in any RTT in which cwnd 1+cwnd 2> 24, both connections experience a packet loss. We also assume that, initially, the Ô¨Årst connection has cwnd =20, and the second has cwnd =1. T A‚ÄìC B‚ÄìD 0 20 1 1 21 2 2 22 3 totalcwnd is 25; packet loss 3 11 1 4 12 2 5 13 3 Continued on next page 20.3 TCP Reno Fairness with Synchronized Losses 481
An Introduction to Computer Networks, Release 2.0.11 Table 1 ‚Äì continued from previous page T A‚ÄìC B‚ÄìD 6 14 4 7 15 5 8 16 6 9 17 7 10 18 8 second packet loss 11 9 4 12 10 5 13 11 6 14 12 7 15 13 8 16 14 9 17 15 10 third packet loss 18 7 5 19 8 6 20 9 7 21 10 8 22 11 9 23 12 10 24 13 11 25 14 12 fourth loss 26 7 6cwnd s are quite close. .. 32 13 12 loss 33 6 6cwnd s are equal So far, fairness seems to be winning. 20.3.1 Example 2: Faster additive increase Here is the same kind of timeline ‚Äì again with the synchronized-loss hypothesis ‚Äì but with the additiveincrease increment changed from 1 to 2 for the B‚ÄìD connection (but not for A‚ÄìC); both connections start withcwnd =1. Again, we assume a loss occurs when cwnd 1+cwnd 2> 24 482 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 T A‚ÄìC B‚ÄìD 0 1 1 1 2 3 2 3 5 3 4 7 4 5 9 5 6 11 6 7 13 7 8 15 8 9 17 Ô¨Årst packet loss 9 4 8 10 5 10 11 6 12 12 7 14 13 8 16 14 9 18 second loss 15 4 9 essentially where we were at T=9 The effect here is that the second connection‚Äôs average cwnd, and thus its throughput, is double that of the Ô¨Årst connection. Thus, changes to the additive-increase increment lead to very signiÔ¨Åcant changes in fairness. In general, an additive-increase value of ùõºincreases throughput, relative to TCP Reno, by a factor ofùõº. 20.3.2 Example 3: Longer RTT For the next example, we will return to standard TCP Reno, with an increase increment of 1. But here we assume that the RTT of the A‚ÄìC connection is double that of the B‚ÄìD connection, perhaps because of additional delay in the A‚ÄìR link. The longer RTT means that the Ô¨Årst connection sends packet Ô¨Çights only when T is even. Here is the timeline, where we allow the Ô¨Årst connection a hefty head-start. As before, we assume a loss occurs when cwnd 1+cwnd 2> 24. T A‚ÄìC B‚ÄìD 0 20 1 1 2 2 21 3 3 4 4 22 5 Ô¨Årst loss 5 2 6 11 3 7 4 8 12 5 9 6 10 13 7 11 8 Continued on next page 20.3 TCP Reno Fairness with Synchronized Losses 483
An Introduction to Computer Networks, Release 2.0.11 Table 2 ‚Äì continued from previous page T A‚ÄìC B‚ÄìD 12 14 9 13 10 14 15 11 second loss 15 5 16 7 6 17 7 18 8 8 B‚ÄìD has caught up 20 9 10 from here on only even values for T shown 22 10 12 24 11 14 third loss 26 5 8 B‚ÄìD is now ahead 28 6 10 30 7 12 32 8 14 34 9 16 fourth loss 35 8 36 4 9 38 5 11 40 6 13 42 7 15 44 8 17 Ô¨Åfth loss 45 8 46 4 9 exactly where we were at T=36 The interval 36 ¬§T<46 represents the steady state here; the Ô¨Årst connection‚Äôs average cwnd is 6 while the second connection‚Äôs average is (8+9+.. . +16+17)/10 = 12.5. Worse, the Ô¨Årst connection sends a windowful only half as often. In the interval 36 ¬§T<46 the Ô¨Årst connection sends 4+5+6+7+8 = 30 packets; the second connection sends 125. The cost of the Ô¨Årst connection‚Äôs longer RTT is quadratic; in general, as we argue more formally below, if the Ô¨Årst connection has RTT = ùúÜ> 1 relative to the second‚Äôs, then its bandwidth will be reduced by a factor of 1/ ùúÜ2. Is this fair? Early thinking was that there was something to Ô¨Åx here; see [F91] and [FJ92], ¬ß3.3 where the Constant-Rate window-increase algorithm is discussed. A more recent attempt to address this problem is TCP Hybla, [CF04]; discussed later in 22.12 TCP Hybla. Alternatively, we may simply deÔ¨Åne TCP Reno‚Äôs bandwidth allocation as ‚Äúfair‚Äù, at least in some contexts. This approach is particularly common when the issue at hand is making sure other TCP implementations ‚Äì and non-TCP Ô¨Çows ‚Äì compete for bandwidth in roughly the same way that TCP Reno does. While TCP Reno‚Äôs strategy is now understood to be ‚Äúgreedy‚Äù in some respects, ‚ÄúÔ¨Åxing‚Äù it in the Internet at large is generally recognized as a very difÔ¨Åcult option. 484 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 20.3.3 TCP Reno RTT bias Let us consider more carefully the way TCP allocates bandwidth between two connections sharing a bottleneck link with relative RTTs of 1 and ùúÜ>1. We claimed above that the slower connection‚Äôs bandwidth will be reduced by a factor of 1/ ùúÜ2; we will now show this under some assumptions. First, uncontroversially, we will assume FIFO droptail queuing at the bottleneck router, and also that the network ceiling (and hence cwnd at the point of loss) is ‚ÄúsufÔ¨Åciently‚Äù large. We will also assume, for simplicity, that the network ceiling C is constant. We need one more assumption: that most loss events are experienced by both connections. This is the synchronized losses hypothesis, and is the most debatable; we will explore it further in the next section. But Ô¨Årst, here is the general argument with this assumption. Let connection 1 be the faster connection, and assume a steady state has been reached. Both connections experience loss when cwnd 1+cwnd 2¬•C, because of the synchronized-loss hypothesis. Let c 1and c 2 denote the respective window sizes at the point just before the loss. Both cwnd values are then halved. Let N be the number of RTTs for connection 1 before the network ceiling is reached again. During this time c 1increases by N; c 2increases by approximately N/ ùúÜif N is reasonably large. Each of these increases represents half the corresponding cwnd; we thus have c 1/2 = N and c 2/2 = N/ ùúÜ. Taking ratios of respective sides, we get c 1/c2= N/(N/ ùúÜ) =ùúÜ, and from that we can solve to get c 1= CùúÜ/(1+ùúÜ) and c 2= C/(1+ ùúÜ). To get the relative bandwidths, we have to count packets sent during the interval between losses. Both connections have cwnd averaging about 3/4 of the maximum value; that is, the average cwnd s are 3/4 c 1 and 3/4 c 2respectively. Connection 1 has N RTTs and so sends about 3/4 c 1N packets. Connection 2, with its slower RTT, has only about N/ ùúÜRTTs (again we use the assumption that N is reasonably large), and so sends about 3/4 c 2N/ùúÜpackets. The ratio of these is c 1/(c2/ùúÜ) =ùúÜ2. Connection 1 sends fraction ùúÜ2/(1+ùúÜ2) of the packets; connection 2 sends fraction 1/(1+ ùúÜ2). 20.3.4 Synchronized-Loss Hypothesis The synchronized-loss hypothesis says that if two TCP connections share a bottleneck link, then whenever the queue is full, late-arriving packets from each connection will Ô¨Ånd it so, and be dropped. Once the queue becomes full, in other words, it stays full for long enough for each connection to experience a packet loss. The hypothesis is most relevent when, as is the case for TCP Reno, packet losses trigger changes to cwnd. This hypothesis is mostly a convenience for reasoning about TCP, and should not be taken as literal fact, though it is often ‚Äúlargely‚Äù true. That said, it is certainly possible to come up with hypothetical situations where losses are not synchronized. Recall that a TCP Reno connection‚Äôs cwnd is incremented by only 1 each RTT; losses generally occur when this single extra packet generated by the increment to cwnd arrives to Ô¨Ånd a full queue. Generally speaking, packets are leaving the queue about as fast as they are arriving; actual overfull-queue instants may be rare. It is certainly conceivable that, at least some of the time, one connection would overÔ¨Çow the queue by one packet, and halve its cwnd, in a short enough time interval that the other connection misses the queue-full moment entirely. Alternatively, if queue overÔ¨Çows lead to effectively random selection of lost packets (as would certainly be true for random-drop queuing, and might be true for tail-drop if there were sufÔ¨Åcient randomness in packet arrival times), then there is a Ô¨Ånite probability that all the lost packets at a given loss event come from the same connection. The synchronized-loss hypothesis is still valid if either or both connection experiences more than one packet loss, within a single RTT; the hypothesis fails only when one connection experiences no losses. 20.3 TCP Reno Fairness with Synchronized Losses 485
An Introduction to Computer Networks, Release 2.0.11 We will return to possible failure of the synchronized-loss hypothesis in 21.2.2 Unsynchronized TCP Losses. In 31.3 Two TCP Senders Competing we will consider some TCP Reno simulations in which actual measurement does not entirely agree with the synchronized-loss model. Two problems will emerge. The Ô¨Årst is that when two connections compete in isolation, a form of synchronization known as phase effects (31.3.4 Phase Effects ) can introduce a persistent perhaps-unexpected bias. The second is that the longer-RTT connection often does manage to miss out on the full-queue moment entirely, as discussed above in the second paragraph of this section. This results in a larger cwnd than the synchronized-loss hypothesis would predict. 20.3.5 Loss Synchronization The synchronized-loss hypothesis assumes alllosses are synchronized. There is another side to this phenomenon that is an issue even if only some reasonable fraction of loss events are synchronized: synchronized losses may represent a collective inefÔ¨Åciency in the use of bandwidth. In the immediate aftermath of a synchronized loss, it is very likely that the bottleneck link will go underutilized, as (at least) two connections using it have just cut their sending rate in half. Better utilization would be achieved if the loss events could be staggered, so that at the point when connection 1 experiences a loss, connection 2 is only halfway to its next loss. For an example, see exercise 18.0 in the following chapter. This loss synchronization is a very real effect on the Internet, even if losses are not necessarily allsynchronized. A major contributing factor to synchronization is the relatively slow response of all parties involved to packet loss. In the diagram above at 20.3 TCP Reno Fairness with Synchronized Losses, if A increments itscwnd leading to an overÔ¨Çow at R, the A‚ÄìR link is likely still full of packets, and R‚Äôs queue remains full, and so there is a reasonable likelihood that sender B will also experience a loss, even if its cwnd was not particularly high, simply because its packets arrived at the wrong instant. Congestion, unfortunately, takes time to clear. 20.3.6 Extreme RTT Ratios What happens to TCP Reno fairness if one TCP connection has a 100-fold-larger RTT than another? The short answer is that the shorter connection may get 10,000 times the throughput. The longer answer is that this isn‚Äôt quite as easy to set up as one might imagine. For the arguments above, it is necessary for the two connections to have a common bottleneck link: RA BC100 ms delay 0.1 ms delay1 ms delay10 pkts/ms In the diagram above, the A‚ÄìC connection wants its cwnd to be about 200 ms 10 packets/ms = 2,000 packets; it is competing for the R‚ÄìC link with the B‚ÄìD connection which is happy with a cwnd of 22. If R‚Äôs queue capacity is also about 20, then with most of the bandwidth the B‚ÄìC connection will experience a loss about every 20 RTTs, which is to say every 22 ms. If the A‚ÄìC link shares even a modest fraction of those losses, it is indeed in trouble. 486 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 However, the A‚ÄìC cwnd cannot fall below 1.0; to test the 10,000-fold hypothesis taking this constraint into account we would have to scale up the numbers on the B‚ÄìC link so the transit capacity there was at least 10,000. This would mean a 400 Gbps R‚ÄìC bandwidth, or else an unrealistically large A‚ÄìR delay. As a second issue, realistically the A‚ÄìC link is much more likely to have its bottleneck somewhere in the middle of its long path. In a typical real scenario along the lines of that diagrammed above, B, C and R are all local to a site, and bandwidth of long-haul paths is almost always less than the local LAN bandwidth within a site. If the A‚ÄìR path has a 1 packet/ms bottleneck somewhere, then it may be less likely to be as dramatically affected by B‚ÄìC trafÔ¨Åc. A few actual simulations using the methods of 31.3 Two TCP Senders Competing resulted in an average cwnd for the A‚ÄìC connection of between 1 and 2, versus a B‚ÄìC cwnd of 20-25, regardless of whether the two links shared a bottleneck or if the A‚ÄìC link had its bottleneck somewhere along the A‚ÄìR path. This may suggest that the A‚ÄìC connection was indeed saved by the 1.0 cwnd minimum. 20.4 Epilog TCP Reno‚Äôs core congestion algorithm is based on algorithms in Jacobson and Karel‚Äôs 1988 paper [JK88], now twenty-Ô¨Åve years old. There are concerns both that TCP Reno uses too much bandwidth (the greediness issue) and that it does not use enough (the high-bandwidth-TCP problem). In the next chapter we consider alternative versions of TCP that attempt to solve some of the above problems associated with TCP Reno. 20.5 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.15 Solutions for Dynamics of TCP. 1.0. In the section 20.2.3 Example 3: competition and queue utilization, we derived the formula Q = w A+ w B‚Äì 2d ‚Äì 2( ùõºdA+ùõΩdB) under the assumption that the bottleneck bandwidth was 1 packet per unit time. Give the formula when the bottleneck bandwidth is r packets per unit time. Hint: the formula above will apply if we measure time in units of 1/r; only the delays d, d Aand d Bneed to be re-scaled to refer to ‚Äúnormal‚Äù time. A delay d measured in ‚Äúnormal‚Äù time corresponds to a delay d1= rd measured in 1/r units. 2.0. Consider the following network, where the bandwidths marked are all in packets/ms. C is sending to D using sliding windows and A and B are idle. C 100 (continues on next page) 20.4 Epilog 487
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) A100R15R2100B 100 D Suppose the one-way propagation delay on the 100 packet/ms links is 1 ms, and the one-way propagation delay on the R1‚ÄìR2 link is 2 ms. The RTT noLoad for the C‚ÄìD path is thus about 8 ms, for a bandwidth delay product of 40 packets. If C uses winsize = 50, then the queue at R1 will have size 10. Now suppose A starts sending to B using sliding windows, also with winsize = 50. What will be the size of the queue at R1? Hint: by symmetry, the queue will be equally divided between A‚Äôs packets and C‚Äôs, and A and C will each see a throughput of 2.5 packets/ms. RTT noLoad, however, does not change. The number of packets in transit for each connection will be 2.5 packets/ms RTT noLoad. 3.0. In the previous exercise, give the average number of data packets (not ACKs) in transit on each individual link: (a).‚ô¢for the original case in which C is the only sender, with winsize = 50 (the only active links here are C‚ÄìR1, R1‚ÄìR2 and R2‚ÄìD). (b). for the new case in which B is also sending, also with winsize = 50. In this case all links are active. Each link will also have an equal number of ACK packets in transit in the reverse direction. Hint: since winsize¬•bandwidthdelay, packets are sent at the bottleneck rate. 4.0.‚ô¢Consider the following network, with links labeled with one-way propagation delays in milliseconds (so, ignoring bandwidth delay, A‚Äôs RTT noLoad is 40 ms and B‚Äôs is 20 ms). The bottleneck link is R‚ÄìD, with a bandwidth of 6 packets/ms. A 15 R5D B5 Initially B sends to D using a winsize of 120, the bandwidth round-trip-delay product for the B‚ÄìD path. A then begins sending as well, increasing its winsize until its share of the bandwidth is 2 packets/ms. What is A‚Äôs winsize at this point? How many packets do A and B each have in the queue at R? It is perhaps easiest to solve this by repeated use of the observation that the number of packets in transit on a connection is always equal to RTT noLoad times the actual bandwidth received by that connection. The algebraic methods of 20.2.3 Example 3: competition and queue utilization can also be used, but bandwidth there was normalized to 1; all propagation delays given here would therefore need to be multiplied by 6. 5.0. Consider the C‚ÄìD path from the diagram of 20.2.4 Example 4: cross trafÔ¨Åc and RTT variation: 488 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 C100R15R2100D Link numbers are bandwidths in packets/ms. Assume C is the only sender. (a).‚ô¢Give propagation delays for the links C‚ÄìR1 and R2‚ÄìD so that there will be an average of 5 packets in transit on the C‚ÄìR1 and R2‚ÄìD links, in each direction, if C uses a winsize sufÔ¨Åcient to saturate the bottleneck R1‚ÄìR2 link. (b). Give propagation delays for all three links so that, when C uses a winsize equal to the round-trip transit capacity, there are 5 packets each way on the C‚ÄìR1 link, 10 on the R1‚ÄìR2 link, and 20 on the R2‚ÄìD link. 6.0. Suppose we have the network layout below of 20.2.4 Example 4: cross trafÔ¨Åc and RTT variation, except that the R1‚ÄìR2 bandwidth is 6 packets/ms and the R2‚ÄìR3 bandwidth is 3 pkts/ms. The delays are as shown, making the C‚ÄìD RTT noLoad 10 ms and the A‚ÄìB RTT noLoad 16 ms. A connects to B and C connects to D. (a).‚ô¢Find window sizes w Aand w Cso that the A‚ÄìB and C‚ÄìD connections share the bottleneck R1‚ÄìR2 bandwidth equally, and there is no queue. (b). Show that increasing each of w Aand w Cby 30 packets leaves each connection with 30 packets in R1‚Äôs queue ‚Äì so the bandwidth is still shared equally ‚Äì and none in R2‚Äôs. Hint: As in (a), the A‚ÄìB bandwidth cannot exceed 3 packets/ms, and C‚Äôs packets can only accumulate at R1. To show A cannot have less than 50% of the bandwidth, observe that, if this happened, then A can have no queue at R2 (because packets now leave faster than they arrive), and so all of A‚Äôs extra packets must also queue at R1. A R1 R2 R3 B DC 6 pkts/ms 3 pkts/ms 3 ms delay 3 ms delay Links A‚ÄîR1, B‚ÄîR1, R2‚ÄîD, R3‚ÄîB: 1 ms propagation delay, 100 pkts/ms bandwidth 7.0. Suppose we have the network layout of the previous exercise, 6.0. Suppose also that the A‚ÄìB and C‚ÄìD connections have settled upon window sizes as in 6.0(b), so that each contributes 30 packets to R1‚Äôs queue. Each connection thus has 50% of the R1‚ÄìR2 bandwidth and there is no queue at R2. Now A‚Äôs winsize is incremented by 10, initially, at least, leading to A contributing more than 50% of R1‚Äôs queue. When the steady state is reached, how will these extra 10 packets be distributed between R1 and R2? 20.5 Exercises 489
An Introduction to Computer Networks, Release 2.0.11 Hint: As A‚Äôs winsize increases, A‚Äôs overall throughput cannot rise due to the bandwidth restriction of the R2‚ÄìR3 link. 8.0. Suppose we have the network layout of exercise 6.0, but modiÔ¨Åed so that the round-trip C‚ÄìD RTT noLoad is 5 ms. The round-trip A‚ÄìB RTT noLoad may be different. The R1‚ÄìR2 bandwidth is 6 packets/ms, so with A idle the C‚ÄìD throughput is 6 packets/ms. (a). Suppose that A and C have window sizes such that, with both transmitting, each has 30 packets in the queue at R1. What is C‚Äôs winsize? Hint: C‚Äôs throughput is now 3 packets/ms. (b). Now suppose C‚Äôs winsize, with A idle, is 60. In this case the C‚ÄìD transit capacity would be 5 ms 6 packets/ms = 30 packets, and so C would have 60‚Äì30 = 30 packets in R1‚Äôs queue. A then begins sending, with a winsize chosen so that A and C‚Äôs contributions to R1‚Äôs queue are equal; C‚Äôs winsize remains at 60. What will be C‚Äôs (and thus A‚Äôs) queue usage at R1? Hint: Ô¨Ånd the transit capacity for a throughput of 3 packets/ms. (c). Suppose the A‚ÄìB RTT noLoad is 10 ms. If C‚Äôs winsize is 60, Ô¨Ånd the winsize for A that makes A and C‚Äôs contributions to R1‚Äôs queue equal. 9.0. One way to address the reduced bandwidth TCP Reno gives to long-RTT connections is for all connections to use an increase increment of RTT2instead of 1; that is, everyone uses AIMD(RTT2,1/2) instead of AIMD(1,1/2) (or AIMD(k RTT2,1/2), where k is an arbitrary scaling factor that applies to everyone). (a). Construct a table in the style of of 20.3.2 Example 3: Longer RTT above, showing the result of two connections using this strategy, where one connection has RTT = 1 and the other has RTT = 2. Start the connections with cwnd =RTT2, and assume a loss occurs when cwnd 1+cwnd 2> 24. (b). Explain why this strategy might not be desirable if one connection is over a direct LAN with an RTT of 1 ms, while the second connection has a very long path and an RTT of 1.0 sec. (Hint: the cwnd -increment value for the short-RTT connection would have to apply whether or not the long-RTT connection was present.) 10.0. Suppose two 1 kB packets are sent as part of a packet-pair probe, and the minimum time measured between arrivals is 5 ms. What is the estimated bottleneck bandwidth? 11.0. Consider the following three causes of a 1-second network delay between A and B. In all cases, assume ACKs travel instantly from B back to A. (i) An intermediate router with a 1-second-per-packet bandwidth delay; all other bandwidth delays negligible (ii) An intermediate link with a 1-second propagation delay; all bandwidth delays negligible (iii) An intermediate router with a 100-ms-per-packet bandwidth delay, and a steadily replenished queue of 10 packets, from another source (as in the diagram in 20.2.4 Example 4: cross trafÔ¨Åc and RTT variation ). (a). Suppose that, in each of these cases, the packet-pair technique ( 20.2.6 Packet Pairs ) is used to measure the bandwidth. Assuming no packet reordering, what is the minimum time interval we could expect in each 490 20 Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 case? (b). What would be the corresponding values of the measured bandwidths, in packets per second? (For purposes of bandwidth measurement, you may assume that the ‚Äúnegligible‚Äù bandwidth delay in case (ii) is 0.01 sec.) 12.0. Suppose A sends packets to B using TCP Reno. The round-trip propagation delay is 1.0 seconds, and the bandwidth is 100 packets/sec (1 packet every 10 ms). (a). Give RTT actual when the window size has reached 100 packets. (b). Give RTT actual when the window size has reached 200 packets. 20.5 Exercises 491
An Introduction to Computer Networks, Release 2.0.11 492 20 Dynamics of TCP
21 FURTHER DYNAMICS OF TCP Is TCP Reno fair? Before we can ask that, we have to establish what we mean by fairness. We also look more carefully at the long-term behavior of TCP Reno (and Reno-like) connections, as the value of cwnd increases and decreases according to the TCP sawtooth. In particular we analyze the average cwnd; recall that the average cwnd divided by the RTT is the connection‚Äôs average throughput (we momentarily ignore here the fact that RTT is not constant, but the error this introduces is usually small). In the end, after establishing a fundamental relationship between TCP Reno cwnd and the packet loss rate, we end up declaring that maybe the best we can do is to assert that whatever TCP Reno does is ‚ÄúReno fair‚Äù, and establish a rule for ‚ÄúTCP [Reno] Friendliness‚Äù. The latter part of this chapter discusses ‚ÄúActive Queue Management‚Äù: the idea that routers can make some assumptions about TCP trafÔ¨Åc to better manage the Ô¨Çows passing through them. It turns out that routers can take advantage of TCP‚Äôs behavior to provide better overall performance. The chapter closes with the ‚Äúhigh-bandwidth TCP problem‚Äù and related TCP issues. 21.1 Notions of Fairness There are several deÔ¨Ånitions for fair allocation of bandwidth among Ô¨Çows sharing a bottleneck link. One isequal-shares fairness; another is what we might call TCP-Reno fairness: to divide the bandwidth the way TCP Reno would. There are additional approaches to deciding what constitutes a fair allocation of bandwidth. 21.1.1 Max-Min Fairness A natural generalization of equal-shares fairness to the case where some Ô¨Çows may be capped is maxmin fairness, in which no Ô¨Çow bandwidth can be increased without decreasing some smaller Ô¨Çow rate. Alternatively, we maximize the bandwidth of the smallest-capacity Ô¨Çow, and then, with that Ô¨Çow Ô¨Åxed, maximize the Ô¨Çow with the next-smallest bandwidth, etc. A more intuitive explanation is that we distribute bandwidth in tiny increments equally among the Ô¨Çows, until the bandwidth is exhausted (meaning we have divided it equally), or one Ô¨Çow reaches its externally imposed bandwidth cap. At this point we continue incrementing among the remaining Ô¨Çows; any time we encounter a Ô¨Çow‚Äôs external cap we are done with it. As an example, consider the following, where we have connections A‚ÄìD, B‚ÄìD and C‚ÄìD, and where the A‚ÄìR link has a bandwidth of 200 kbps and all other links are 1000 kbps. Starting from zero, we increment the allocations of each of the three connections until we get to 200 kbps per connection, at which point the A‚ÄìD connection has maxed out the capacity of the A‚ÄìR link. We then continue allocating the remaining 400 kbps equally between B‚ÄìD and C‚ÄìD, so they each end up with 400 kbps. 493
An Introduction to Computer Networks, Release 2.0.11 A CR D1000 kbps200 kbps 1000 kbpsB1000 kbps As another example, known as the parking-lot topology, suppose we have the following network: A B C D There are four connections: one from A to D covering all three links, and three single-link connections A‚ÄìB, B‚ÄìC and C‚ÄìD. Each link has the same bandwidth. If bandwidth allocations are incrementally distributed among the four connections, then the Ô¨Årst point at which any link bandwidth is maxed out occurs when all four connections each have 50% of the link bandwidth; max-min fairness here means that each connection has an equal share. 21.1.2 Proportional Fairness A bandwidth allocation of rates xr1,r2,.. . ,r Nyfor N connections satisÔ¨Åes proportional fairness if it is a legal allocation of bandwidth, and for any other allocation xs1,s2,.. . ,s Ny, the aggregate proportional change satisÔ¨Åes (r1‚Äìs1)/s1+ (r 2‚Äìs2)/s2+. .. + (r N‚ÄìsN)/sN< 0 Alternatively, proportional fairness means that the sum log(r 1)+log(r 2)+.. . +log(r N) is minimized. If the connections share only the bottleneck link, proportional fairness is achieved with equal shares. However, consider the following two-stage parking-lot network: A B C Suppose the A‚ÄìB and B‚ÄìC links have bandwidth 1 unit, and we have three connections A‚ÄìB, B‚ÄìC and A‚ÄìC. Then a proportionally fair solution is to give the A‚ÄìC link a bandwidth of 1/3 and each of the A‚ÄìB and B‚ÄìC links a bandwidth of 2/3 (so each link has a total bandwidth of 1). For any change b in the bandwidth for the A‚ÄìC link, the A‚ÄìB and B‚ÄìC links each change by - b. Equilibrium is achieved at the point where a 1% reduction in the A‚ÄìC link results in two 0.5% increases, that is, the bandwidths are divided in proportion 1:2. Mathematically, if x is the throughput of the A‚ÄìC connection, we are minimizing log(x) + 2log(1-x). 494 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 Proportional fairness partially addresses the problem of TCP Reno‚Äôs bias against long-RTT connections; speciÔ¨Åcally, TCP‚Äôs bias here is still not proportionally fair, but TCP‚Äôs response is closer to proportional fairness than it is to max-min fairness. See [HBT99]. 21.2 TCP Reno loss rate versus cwnd It turns out that we can express a connection‚Äôs average cwnd in terms of the packet loss rate, p,egp = 10-4= one packet lost in 10,000. The relationship comes by assuming that all packet losses are because the network ceiling was reached. We will also assume that, when the network ceiling is reached, only one packet is lost, although we can dispense with this by counting a ‚Äúcluster‚Äù of related losses (within, say, one RTT) as a single loss event. Let C represent the network ceiling ‚Äì so that when cwnd reaches C a packet loss occurs. While C is constant only for a very stable network, C usually does not vary by much; we will assume here that it is constant. Thencwnd varies between C/2 and C, with packet drops occurring whenever cwnd = C is reached. Let N = C/2. Then between two consecutive packet loss events, that is, over one ‚Äútooth‚Äù of the TCP connection, a total of N+(N+1)+. .. +2N packets are sent in N+1 Ô¨Çights; this sum can be expressed algebraically as 3/2 N(N+1)1.5 N2. The loss rate is thus one packet out of every 1.5 N2, and the loss rate p is 1/(1.5 N2). The average cwnd in this scenario is 3/2 N (that is, the average of N= cwnd minand 2N=cwnd max). If we let M = 3/2 N represent the average cwnd ,cwnd mean, we can express the above loss rate in terms of M: the number of packets between losses is 2/3 M2, and so p=3/2 M-2. Now let us solve this for M= cwnd mean in terms of p; we get M2= 3/2 p-1and thus M =cwnd mean = 1.225 p-1/2 where 1.225 is the square root of 3/2. Seen in this form, a given network loss rate sets the window size; this loss rate is ultimately tied to the network capacity. If we are interested in the maximum cwnd instead of the mean, we multiply the above by 4/3. From the above, the bandwidth available to a connection is now as follows (though RTT may not be constant): bandwidth = cwnd /RTT = 1.225/(RTT ?p) In [PFTK98] the authors consider a TCP Reno model that takes into account the measured frequency of coarse timeouts (in addition to fast-recovery responses leading to cwnd halving), and develop a related formula with additional terms. As the bottleneck queue capacity increases, both cwnd and the number of packets between losses (1/p) increase, connected as above. Once the queue is large enough that the bottleneck link is 100% utilized, however, the bandwidth no longer increases. Another way to view this formula is to recall that 1/p is the number of packets per tooth; that is, 1/p is the tooth ‚Äúarea‚Äù. Squaring both sides, the formula says that the TCP Reno tooth area is proportional to the square of the average tooth height (that is, to cwnd mean) as the network capacity increases (that is, as cwnd mean increases). 21.2 TCP Reno loss rate versus cwnd 495
An Introduction to Computer Networks, Release 2.0.11 21.2.1 Irregular teeth In the preceding, we assumed that all teeth were the same size. What if they are not? In [OKM96], this problem was considered under the assumption that every packet faces the same (small) loss probability (and so the intervals between packet losses are exponentially distributed). In this model, it turns out that the above formula still holds except the constant changes from 1.225 to 1.309833. To understand how irregular teeth lead to a bigger constant, imagine sending a large number K of packets which encounter n losses. If the losses are regularly spaced, then the TCP graph will have n equally sized teeth, each with K/n packets. But if the n losses are randomly distributed, some teeth will be larger and some will be smaller. The average tooth height will be the same as in the regularly-spaced case (see exercise 7.0). However, the number of packets in any one tooth is generally related to the square of the height of that tooth, and so larger teeth will count disproportionately more. Thus, the random distribution will have a higher total number of packets delivered and thus a higher mean cwnd. See also exercise 17.0, for a simple simulation that generates a numeric estimate for the constant 1.309833. Note that losses at uniformly distributed random intervals may not be an ideal model for TCP either; in the presence of congestion, loss events are far from statistical independence. In particular, immediately following one loss another loss is unlikely to occur until the queue has time to Ô¨Åll up. 21.2.2 Unsynchronized TCP Losses In20.3.3 TCP Reno RTT bias we considered a model in which all loss events are fully synchronized; that is, whenever the queue becomes full, both TCP Reno connections always experience packet loss. In that model, if RTT 2/RTT 1=ùúÜthencwnd 1/cwnd 2=ùúÜand bandwidth 1/bandwidth 2=ùúÜ2, wherecwnd 1and cwnd 2are the respective average values for cwnd. What happens if loss events for two connections do not have such a neat one-to-one correspondence? We will derive the ratio of loss events (or, more precisely, TCP loss responses ) for connection 1 versus connection 2 in terms of the bandwidth and RTT ratios, without using the synchronized-loss hypothesis. Note that we are comparing the total number of loss events (or loss responses) here ‚Äì the total number of TCP Reno teeth ‚Äì over a large time interval, and not the relative per-packet loss probabilities. One connection might have numerically more losses than a second connection but, by dint of a smaller RTT, send more packets between its losses than the other connection and thus have fewer losses per packet. Let losscount 1and losscount 2be the number of loss responses for each connection over a long time interval T. For i=1 and i=2, the ithconnection‚Äôs per-packet loss probability is p i= losscount i/(bandwidth iT) = (losscount iRTT i)/(cwnd iT). But by the result of 21.2 TCP Reno loss rate versus cwnd, we also have cwnd i= k/?pi, or p i= k2/cwnd i2. Equating, we get pi= k2/cwnd i2= (losscount iRTT i) / (cwnd iT) and so losscount i= k2T / (cwnd iRTT i) Dividing and canceling, we get losscount 1/losscount 2= (cwnd 2/cwnd 1)(RTT 2/RTT 1) 496 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 We will make use of this in 31.4.2.2 Relative loss rates. We can go just a little further with this: let ùõædenote the losscount ratio above: ùõæ= (cwnd 2/cwnd 1)(RTT 2/RTT 1) Therefore, as RTT 2/RTT 1=ùúÜ, we must have cwnd 2/cwnd 1=ùõæ/ùúÜand thus bandwidth 1/bandwidth 2= (cwnd 1/cwnd 2)(RTT 2/RTT 1) =ùúÜ2/ùõæ. Note that if ùõæ=ùúÜ, that is, if the longer-RTT connection has fewer loss events in exact inverse proportion to the RTT, then bandwidth 1/bandwidth 2=ùúÜ= RTT 2/RTT 1, and alsocwnd 1/cwnd 2= 1. 21.3 TCP Friendliness Suppose we are sending packets using a non-TCP real-time protocol. How are we to manage congestion? In particular, how are we to manage congestion in a way that treats other connections ‚Äì particularly TCP Reno connections ‚Äì fairly? For example, suppose we are sending interactive audio data in a congested environment. Because of the real-time nature of the data, we cannot wait for lost-packet recovery, and so must use UDP rather than TCP. We might further suppose that we can modify the encoding so as to reduce the sending rate as necessary ‚Äì that is, that we are using adaptive encoding ‚Äì but that we would prefer in the absence of congestion to keep the sending rate at the high end. We might also want a relatively uniform rateof sending; the TCP sawtooth leads to periodic variations in throughput that we may wish to avoid. Our application may not be windows-based, but we can still monitor the number of packets it has in Ô¨Çight on the network at any one time; if the packets are small, we can count bytes instead. We can use this count instead of the TCP cwnd. We will say that a given communications strategy is TCP Friendly if the number of packets on the network at any one time is approximately equal to the TCP Reno cwnd meanfor the prevailing packet loss rate p. Note that ‚Äì assuming losses are independent events, which is deÔ¨Ånitely not quite right but which is often Close Enough ‚Äì in a long-enough time interval, all connections sharing a common bottleneck can be expected to experience approximately the same packet loss rate. The point of TCP Friendliness is to regulate the number of the non-Reno connection‚Äôs outstanding packets in the presence of competition with TCP Reno, so as to achieve a degree of fairness. In the absence of competition, the number of any connection‚Äôs outstanding packets will be bounded by the transit capacity plus capacity of the bottleneck queue. Some non-Reno protocols ( egTCP Vegas, 22.6 TCP Vegas, or constantrate trafÔ¨Åc, 21.3.2 RTP ) may in the absence of competition have a loss rate of zero, simply because they never overÔ¨Çow the queue. Another way to approach TCP Friendliness is to start by deÔ¨Åning ‚ÄúReno Fairness‚Äù to be the bandwidth allocations that TCP Reno assigns in the face of competition. TCP Friendliness then simply means that the given non-Reno connection will get its Reno-Fair share ‚Äì not more, not less. We will return to TCP Friendliness in the context of general AIMD in 21.4 AIMD Revisited. 21.3 TCP Friendliness 497
An Introduction to Computer Networks, Release 2.0.11 21.3.1 TFRC TFRC, or TCP-Friendly Rate Control, RFC 3448, uses the loss rate experienced, p, and the formulas above to calculate a sending rate. It then allows sending at that rate; that is, TFRC is rate-based rather than windowbased. As the loss rate increases, the sending rate is adjusted downwards, and so on. However, adjustments are done more smoothly than with TCP, giving the application a more gradually changing transmission rate. From RFC 5348: TFRC is designed to be reasonably fair when competing for bandwidth with TCP Ô¨Çows, where we call a Ô¨Çow ‚Äúreasonably fair‚Äù if its sending rate is generally within a factor of two of the sending rate of a TCP Ô¨Çow under the same conditions. [emphasis added; a factor of two might not be considered ‚Äúclose enough‚Äù in some cases.] The penalty of having smoother throughput than TCP while competing fairly for bandwidth is that TFRC responds more slowly than TCP to changes in available bandwidth. TFRC senders include in each packet a sequence number, a timestamp, and an estimated RTT. The TFRC receiver is charged with sending back feedback packets, which serve as (partial) acknowledgments, and also include a receiver-calculated value for the loss rate over the previous RTT. The response packets also include information on the current actual RTT, which the sender can use to update its estimated RTT. The TFRC receiver might send back only one such packet per RTT. The actual response protocol has several parts, but if the loss rate increases, then the primary feedback mechanism is to calculate a new (lower) sending rate, using some variant of the cwnd = k/?p formula, and then shift to that new rate. The rate would be cut in half only if the loss rate p quadrupled. Newer versions of TFRC have a various features for responding more promptly to an unusually sudden problem, but in normal use the calculated sending rate is used most of the time. 21.3.2 RTP TheReal-Time Protocol, or RTP, is sometimes (though not always) coupled with TFRC. RTP is a UDPbased protocol for streaming time-sensitive data. Some RTP features include: 
- The sender establishes a rate(rather than a window size) for sending packets 
- The receiver returns periodic summaries of loss rates 
- ACKs are relatively infrequent 
- RTP is suitable for multicast use; a very limited ACK rate is important when every packet sent might have hundreds of recipients 
- The sender adjusts its cwnd -equivalent up or down based on the loss rate and the TCP-friendly cwnd =k/?p rule 
- Usually some sort of ‚Äústability‚Äù rule is incorporated to avoid sudden changes in rate As a common RTP example, a typical V oIP connection using a DS0 (64 kbps) rate might send one packet every 20 ms, containing 160 bytes of voice data, plus headers. 498 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 For a combination of RTP and TFRC to be useful, the underlying application must be rate-adaptive, so that the application can still function when the available rate is reduced. This is often not the case for simple V oIP encodings; see 25.11.4 RTP and VoIP. We will return to RTP in 25.11 Real-time Transport Protocol (RTP). The UDP-based QUIC transport protocol ( 16.1.1 QUIC ) uses a congestion-control mechanism compatible with Cubic TCP ( 22.15 TCP CUBIC ), which isn‚Äôt quite the same as TCP Reno. But QUIC could just as easily have used TFRC to achieve TCP-Reno-friendliness. 21.3.3 DCCP Congestion Control We saw DCCP earlier in 16.1.2 DCCP and18.15.3 DCCP. DCCP also includes a set of congestionmanagement ‚ÄúproÔ¨Åles‚Äù; a connection can choose the proÔ¨Åle that best Ô¨Åts its needs. The two standard ones are the TCP-Reno-like proÔ¨Åle ( RFC 4341 ) and the TFRC proÔ¨Åle ( RFC 4342 ). In the Reno-like proÔ¨Åle, every packet is acknowledged (though, as with TCP, ACKs may be sent on the arrival of every other Data packet). Although DCCP ACKs are not cumulative, use of the TCP-SACK-like ACK-vector format ensures that acknowledgments are received reliably except in extreme-loss situations. The sender maintains cwnd much as a TCP Reno sender would. It is incremented by one for each RTT with no loss, and halved in the event of packet loss. Because sliding windows is not used, cwnd does not represent a window size. Instead, the sender maintains an Estimated FlightSize ( 19.4 TCP Reno and Fast Recovery ), which is the sender‚Äôs best guess at the number of outstanding packets. In RFC 4341 this is referred to as the pipe value. The sender is then allowed to send additional packets as long as pipe < cwnd. The Reno-like proÔ¨Åle also includes a slow start mechanism. In the TFRC proÔ¨Åle, an ACK is sent at least once per RTT. Because ACKs are sent less frequently, it may occasionally be necessary for the sender to send an ACK of ACK. As with TFRC generally, a DCCP sender using the TFRC proÔ¨Åle has its rate limited, rather than its window size. DCCP provides a convenient programming framework for use of TFRC, complete with (at least in the Linux world), a traditional socket interface. The developer does not have to deal with the TFRC rate calculations directly. 21.4 AIMD Revisited TCP Tahoe chose an increase increment of 1 on no losses, and a decrease factor of 1/2 otherwise. Another approach to TCP Friendliness is to retain TCP‚Äôs additive-increase, multiplicative-decrease strategy, but to change the numbers. Suppose we denote by AIMD( ùõº,ùõΩ) the strategy of incrementing the window size byùõºafter a window of no losses, and multiplying the window size by (1ùõΩ)<1 on loss (so ùõΩ=0.1 means the window is reduced by10%). TCP Reno is thus AIMD(1,0.5). Any AIMD( ùõº,ùõΩ) protocol also follows a sawtooth, where the slanted top to the tooth has slope ùõº. All combinations of ùõº>0 and 0< ùõΩ<1 are possible. The dimensions of one tooth of the sawtooth are somewhat constrained by ùõºandùõΩ. Let h be the maximum height of the tooth and let w be the width (as measured in 21.4 AIMD Revisited 499
An Introduction to Computer Networks, Release 2.0.11 RTTs). Then, if the losses occur at regular intervals, the height of the tooth at the left (low) edge is (1ùõΩ)h and the total vertical difference is ùõΩh. This vertical difference must also be ùõºw, and so we get ùõºw =ùõΩh, or h/w = ùõº/ùõΩ; these values are labeled on the rightmost teeth in the diagram below. These equations mean that the proportions of the tooth (h to w) are determined by ùõºandùõΩ. Finally, the mean height of the tooth is (1-ùõΩ/2)h. We are primarily interested in AIMD( ùõº,ùõΩ) cases which are TCP Friendly ( 21.3 TCP Friendliness ). TCP friendliness means that an AIMD( ùõº,ùõΩ) connection with the same loss rate as TCP Reno will have the same meancwnd. Each tooth of the sawtooth represents one loss. The number of packets sent per tooth is, using h and w as in the previous paragraph, (1ùõΩ/2)hw. Geometrically, the number of packets sent per tooth is the area of the tooth, so two connections with the same per-packet loss rate will have teeth with the same area. TCP Friendliness means that two connections will have the same mean cwnd and thus the same average tooth height. If the teeth of two connections have the same area and the same average height, they must have the same width (in RTTs), and thus that the rates of loss per unit time must be equal, not just the rates of loss per number of packets. The diagram below shows a TCP Reno tooth (blue) together with some unfriendly AIMD( ùõº,ùõΩ) teeth on the left (red) and two friendly teeth on the right (green), the second friendly tooth is superimposed on the Reno tooth. h w/g1w=/g2h Unfriendly Reno Friendly Reno AIMD teeth; Reno tooth has guide-lines showing midpoints/g1w/2 3 2w The additional dashed lines within the central Reno tooth demonstrate the Reno 1 12 proportions, and show that the horizontal dashed line, representing cwnd mean, is at height 3/2 w, where w is, as before, the width. In the rightmost green tooth, superimposed on the Reno tooth, we can see that h = (3/2) w + ( ùõº/2)w. We already know h = ( ùõº/ùõΩ)w; setting these expressions equal, canceling the w and multiplying by 2 we get (3+ùõº) = 2ùõº/ùõΩ, orùõΩ= 2ùõº/(3+ùõº). Solving for ùõΩwe get ùõº= 3ùõΩ/(2-ùõΩ) orùõº1.5ùõΩfor small ùõΩ. As the reduction factor 1ùõΩgets closer to 1, the protocol can remain TCP-friendly by appropriately reducing ùõº; eg AIMD(1/5, 1/8). Having a small ùõΩmeans that a connection does not have sudden bandwidth drops when losses occur; this can be important for applications that rely on a regular rate of data transfer (such as voice). Such applications are sometimes said to be slowly responsive, in contrast to TCP‚Äôs cwnd =cwnd /2 rapid response. 500 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 21.4.1 AIMD and Convergence to Fairness While TCP-friendly AIMD( ùõº,ùõΩ) protocols will converge to fairness when competing with TCP Reno (with equal RTTs), a consequence of decreasing ùõΩis that fairness may take longer to arrive; here is an example. We will assume, as above in 20.3.3 TCP Reno RTT bias, that loss events for the two competing connections are synchronized. Recall that for two same-RTT TCP Reno connections (that is, AIMD( ùõº,ùõΩ) where ùõΩ=1/2), if the initial difference in the connections‚Äô respective cwnds is D, then D is reduced by half on each loss event. Now suppose we have two AIMD( ùõº,ùõΩ) connections with some other value of ùõΩ, and again with a difference D in theircwnd values. The two connections will each increase cwnd byùõºeach RTT, and so when losses are not occurring D will remain constant. At loss events, D will be reduced by a factor of 1ùõΩ. IfùõΩ=1/4, corresponding to ùõº=3/7, then at each loss event D will be reduced only to 3/4 D, and the ‚Äúhalf-life‚Äù of D will be almost twice as large. The two connections will still converge to fairness as D √ë0, but it will take twice as long. 21.5 Active Queue Management Active Queue Management (AQM) means that routers take some active steps to manage their queues. The primary goal of AQM is to reduce excessive queuing delays; cf 21.5.1 Bufferbloat. A secondary goal is to improve the performance of TCP connections ‚Äì which constitute the vast majority of Internet trafÔ¨Åc ‚Äì through the router. By signaling to TCP connections that they should reduce cwnd, overall queuing delays are also reduced. Generally routers manage their queues either by marking packets or by dropping them. All routers drop packets when there is no more space for new arrivals, but this falls into the category of active management when packets are dropped before the queue has run completely out of space. Queue management can be done at the congestion ‚Äúknee‚Äù, when queues just start to build (and when marking is more appropriate), or as the queue starts to become full and approaches the ‚Äúcliff‚Äù. Broadly speaking, the priority queuing and random drop mechanisms ( 20.1 A First Look At Queuing ) might be considered forms of AQM, at least if the goal was to manage the overall queue size. So might fair queuing and hierarchical queuing ( 23 Queuing and Scheduling ). The mechanisms most commonly associated with the AQM category, though, are RED, below, and its successors, especially CoDel. For a discussion of the potential beneÔ¨Åts of fair queuing to queue management, see 23.6.1 Fair Queuing and Bufferbloat. 21.5.1 Bufferbloat As we saw in 19.7 TCP and Bottleneck Link Utilization, TCP Reno connections are happiest when the queue capacity at the bottleneck router exceeds the bandwidth delay transit capacity. But it is easy to get carried away here. The calculations of 19.7.1 TCP Queue Sizes suggested that an optimum backbonerouter buffer size for TCP Reno might be hundreds of megabytes. Because RAM is cheap, and because more space is hard to say no to, queue sizes in the real world often tend to be at the larger end of the scale. Excessive delay due to excessive queue capacity is known as bufferbloat. Of course, ‚Äúexcessive‚Äù is a matter of perspective; if the only trafÔ¨Åc you‚Äôre interested in is bulk TCP Ô¨Çows, large queues are good. But if you‚Äôre interested in real-time trafÔ¨Åc like voice and interactive video, or even simply in fast web-page loads, bufferbloat becomes a problem. Large queues can also lead to delay variability, or jitter. 21.5 Active Queue Management 501
An Introduction to Computer Networks, Release 2.0.11 Backbone routers are one class of offender here, but not the only. Many residential routers have a queue capacity several times the average bandwidth delay product, meaning that queuing delay potentially becomes much larger than propagation delay. Even end-systems often have large queues; on Linux systems, the default queue size can be several hundred packets. All these delay-related issues do not play well with interactive trafÔ¨Åc or real-time trafÔ¨Åc ( RFC 7567 ). As a result, there are proposals for running routers with much smaller queues; see, for example, [WM05] and [EGMR05]. This may reduce the bottleneck link utilization of a single TCP Ô¨Çow to 75%. However, with multiple TCP Ô¨Çows having unsynchronized losses, the situation will often be much better. Still, for router managers, deciding on a queue capacity can be a vexing issue. The CoDel algorithm, below, offers great promise, but we start with some earlier strategies. 21.5.2 DECbit In the congestion-avoidance technique proposed in [RJ90], routers encountering early signs of congestion marked the packets they forwarded; senders used these markings to adjust their window size. The system became known as DECbit in reference to the authors‚Äô employer and was implemented in DECnet (closely related to the OSI protocol suite), though apparently there was never a TCP/IP implementation. The idea behind DECbit eventually made it into TCP/IP in the form of ECN, below, but while ECN ‚Äì like TCP‚Äôs other congestion responses ‚Äì applies control near the congestion cliff, DECbit proposed introducing control when congestion was still minimal, just above the congestion knee. DECbit was never a solution to bufferbloat; in the DECbit era, memory was expensive and queue capacities were seldom excessive. The DECbit mechanism allowed routers to set a designated ‚Äúcongestion bit‚Äù. This would be set in the data packet being forwarded, but the status of this bit would be echoed back in the corresponding ACK (otherwise the sender would never hear about the congestion). DECbit routers deÔ¨Åned ‚Äúcongestion‚Äù as an average queue size greater than 1.0; that is, congestion meant that the connection was just past the ‚Äúknee‚Äù. Routers would set the congestion bit whenever this average-queue condition was met. The target for DECbit senders would then be to have 50% of packets marked as ‚Äúcongested‚Äù. If fewer than 50% of packets were marked, cwnd would be incremented by 1; if more than 50% were marked, then cwnd would be decreased by a factor of 0.875. Note this is very different from the TCP approach in that DECbit begins marking packets at the congestion ‚Äúknee‚Äù while TCP Reno responds only to packet losses which occur at the ‚Äúcliff‚Äù. A consequence of this knee-based mechanism is that DECbit shoots for very limited queue utilization, unlike TCP Reno. At a congested router, a DECbit connection would attempt to keep about 1.0 packets in the router‚Äôs queue, while a TCP Reno connection might Ô¨Åll the remainder of the queue. Thus, DECbit would in principle compete poorly with any connection where the sender ignored the marked packets and simply tried to keep cwnd as large as possible. As we will see in 22.6 TCP Vegas, TCP Vegas also strives for limited queue utilization; in 31.5 TCP Reno versus TCP Vegas we investigate through simulation how fairly TCP Vegas competes with TCP Reno. 502 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN) ECN is the TCP/IP equivalent of DECbit, though the actual mechanics are quite different. The current version is speciÔ¨Åed in RFC 3168, modifying an earlier version in RFC 2481. The IP header contains a two-bit ECN Ô¨Åeld, consisting of the ECN-Capable Transport (ECT) bit and the Congestion Experienced (CE) bit; the ECN Ô¨Åeld is shown in 9.1 The IPv4 Header. The ECT bit is set by a sender to indicate to routers that it is able to use the ECN mechanism. (These are actually the older RFC 2481 names for the bits, but they will serve our purposes here.) The TCP header contains an additional two bits: the ECN-Echo bit (ECE) and the Congestion Window Reduced (CWR) bit; these are shown in the fourth row in 17.2 TCP Header. The original goal of ECN was to improve TCP throughput by eliminating most actual packet losses and the resultant timeouts. Bufferbloat was, again, not at issue. ECN and Middleboxes In the early days of ECN, some non-ECN-aware Ô¨Årewalls responded to packets with the CWR or ECE bits set by dropping them and returning a spoofed RST packet to the sender. See RFC 3360 for details and a discussion of this nonstandard use of RST. This is a good example of how ‚Äúmiddleboxes‚Äù are sometimes obstacles to protocol evolution; see 9.7.2 Middleboxes. Routers set the CE bit in the IP header when they might otherwise drop the packet (or possibly when the queue is at least half full, or in lieu of a RED drop, below). As in DECbit, receivers echo the CE status back to the sender in the ECE bit of the next ACK; the reason for using the ECE bit is that this bit belongs to the TCP header and thus the TCP layer can be assured of control of it. TCP senders treat ACKs with the ECE bit set the same as if a loss occurred: cwnd is cut in half. Because there is no actual loss, the arriving ACKs can still pace continued sliding-windows sending. The Fast Recovery mechanism is not needed. When the TCP sender has responded to an ECE bit (by halving cwnd ), it sets the CWR bit. Once the receiver has received a packet with the CE bit set in the IP layer, it sets the ECE bit in all subsequent ACKs until it receives a data packet with the CWR bit set. This provides for reliable communication of the congestion information, and helps the sender respond just once to multiple packet losses within a single windowful. Note that the initial packet marking is done at the IP layer, but the generation of the marked ACK and the sender response to marked packets is at the TCP layer (the same is true of DECbit though the layers have different names). Only a packet that would otherwise have been dropped has its CE bit set; the router does notmark all waiting packets once its queue reaches a certain threshold. Any marked packet must, as usual, wait in the queue for its turn to be forwarded. The sender Ô¨Ånds out about the congestion after one full RTT, versus one full RTT plus four packet transmission times for Fast Retransmit. A much earlier, ‚Äúlegacy‚Äù strategy was to require routers, upon dropping a packet, to immediately send back to the sender an ICMP Source Quench packet. This is a faster way (the fastest possible way) to notify a sender of a loss. It was never widely implemented, however, and was ofÔ¨Åcially deprecated by RFC 6633. Because ECN congestion is treated the same way as packet drops, ECN competes fairly with TCP Reno. RFC 3540 is a proposal (as of 2016 not yet ofÔ¨Åcial) to slightly amend the mechanism described above to 21.5 Active Queue Management 503
An Introduction to Computer Networks, Release 2.0.11 support detection of receivers who attempt to conceal evidence of congestion. A receiver would do this by not setting the ECE bit in the ACK when a data packet arrives marked as having experienced congestion. Such an unscrupulous (or incorrectly implemented) receiver may then gain a greater share of the bandwidth, because its sender maintains a larger cwnd than it should. The amendment also detects erasure of the ECE bit (or other ECN bits) by middleboxes. The new strategy, known as the ECN nonce, treats the ECN bits ECT and CE as a single unit. The value 00 is used by non-ECN-aware senders, and the value 11 is used by routers as the congestion marker. ECNaware senders mark data packets by randomly choosing 10 (known as ECT(0)) or 01 (known as ECT(1)). This choice encodes the nonce bit, with ECT(0) representing a nonce bit of 0 and ECT(1) representing 1; the nonce bit can also be viewed as the value of the second ECN bit. The receiver is now expected, in addition to setting the ECE bit, to also return the one-bit running sum of the nonce bits in a new TCP-header bit called the nonce-sum (NS) bit, which immediately precedes the CRW bit. This sum is over all data packets received since the previous packet loss or congestion-experienced packet. The point of this is that if the receiver attempts to conceal congestion by leaving the ECE bit zero, the receiver cannot properly set the NS bit, because it does not know which of ECT(0) or ECT(1) was used. For each packet marked as experiencing congestion, the receiver has a 50% chance of guessing correctly, but over time successful guessing becomes increasingly unlikely. If ECN-noncompliance is detected, the sender must now stop using ECN, and may choose a smaller cwnd as a precaution. Although we have described ECN as a mechanism implemented by routers, it can just as easily be implemented by switches, and is available in many commercial switches. 21.5.4 RED ‚ÄúTraditional‚Äù routers drop packets only when the queue is full; senders have no overt indication before then that the cliff is looming. ECN improves this by informing TCP connections of the impending cliff so they can reduce cwnd without actually losing packets. The idea behind Random Early Detection (RED) routers, introduced in [FJ93], is that the router is allowed to drop an occasional packet much earlier, say when the queue is less than half full. These early packet drops provide a signal to senders that they should slow down; we will call them signaling losses. While packets are indeed lost, they are dropped in such a manner that usually only one packet per windowful (per connection) will be lost. Classic TCP Reno, in particular, behaves poorly with multiple losses per window and RED is able to avoid such multiple losses. The primary goal of RED was, as with ECN, to improve TCP performance. Note that RED preceded ECN by six years. RED is, strictly speaking, a queuing discipline in the sense of 23.4 Queuing Disciplines; FIFO is another. It is often more helpful, however, to think of RED as a technique that an otherwise-FIFO router can use to improve the performance of TCP trafÔ¨Åc through it. Designing an early-drop algorithm is not trivial. A predecessor of RED known as Early Random Drop (ERD) gateways simply introduced a small uniform drop probability p, egp=0.01, once the queue had reached a certain threshold. This addresses the TCP Reno issue reasonably well, except that dropping with a uniform probability p leads to a surprisingly high rate of multiple drops in a cluster, or of long stretches with no drops. More uniformity was needed, but drops at regular intervals are too uniform. The actual RED algorithm does two things. First, the base drop probability ‚Äì p base‚Äì rises steadily from a minimum queue threshold q minto a maximum queue threshold q max(these might be 40% and 80% respectively of the absolute queue capacity); at the maximum threshold, the drop probability is still quite small. 504 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 The base probability p baseincreases linearly in this range according to the following formula, where p maxis the maximum RED-drop probability; the value for p maxproposed in [FJ93] was 0.02. pbase= pmax(avg_queuesize ‚Äì q min)/(q max‚Äì qmin) Second, as time passes after a RED drop, the actual drop probability p actual begins to rise, according to the next formula: pactual = pbase/ (1 ‚Äì countpbase) Here, count is the number of packets sent since the last RED drop. With count=0 we have p actual = p base, but p actual rises from then on with a RED drop guaranteed within the next 1/p basepackets. This provides a mechanism by which RED drops are uniformly enough spaced that it is unlikely two will occur in the same window of the same connection, and yet random enough that it is unlikely that the RED drops will remain synchronized with a single connection, thus targeting it unfairly. A signiÔ¨Åcant drawback to RED is that the choice of the various parameters is decidedly ad hoc. It is not clear how to set them so that TCP connections with both small and large bandwidth delay products are handled appropriately, or even how to set them for a given output bandwidth. The probability p baseshould, for example, be roughly 1/winsize, but winsize for TCP connections can vary by several orders of magnitude. RFC 2309, from 1998, recommended RED, but its successor RFC 7567 from 2015 has backed away from this, recommending instead that an appropriate AQM strategy be implemented, but that the details should be left to the discretion of the router manager. In25.8 RED with In and Out we will look at an application of RED to quality-of-service guarantees. 21.5.5 ADT The paper [SKS06] proposes the Adaptive Drop-Tail algorithm, in which the maximum queue capacity is adjusted at intervals (of perhaps 5 minutes) in order to maintain a speciÔ¨Åc desired link-utilization target (perhaps 95%). At the end of each interval, the available queue capacity is increased or decreased (perhaps by 5%) depending on whether the link utilization was under or over the target. ADT does not selectively drop packets otherwise; if there is space for an arriving packet within the current queue capacity, it is accepted. ADT does a good job adjusting the overall queue capacity to meet circumstances that change slowly; an example might be the size of the user pool. However, ADT does not respond to short-term Ô¨Çuctuations. In particular, it does not attempt to respond to Ô¨Çuctuations occurring within a single TCP Reno tooth. ADT also does not maintain additional queue space for transient packet bursts. 21.5.6 CoDel The CoDel queue-management algorithm (pronounced ‚Äúcoddle‚Äù) attempts, like RED, to use signaling losses to encourage connections to reduce their queue utilization. This allows CoDel to maintain a large total queue capacity, available to absorb bursts, while at the same time maintaining on average a much smaller level of actual queue utilization. To achieve this, CoDel is able to distinguish between transient queue spikes and ‚Äústanding-queue‚Äù utilization, persisting over multiple RTTs; the canonical example of the latter is TCP Reno‚Äôs queue buildup towards the right-hand edge of each sawtooth. Unlike RED, CoDel has essentially no tunable parameters, and adapts to a wide range of bandwidths and trafÔ¨Åc types. See [NJ12] and the Internet Draft draft-ietf-aqm-codel-06. Reducing bufferbloat is an explict goal of CoDel. 21.5 Active Queue Management 505
An Introduction to Computer Networks, Release 2.0.11 CoDel measures the minimum value of queue utilization over a designated short time period known as theInterval. The Interval is intended to be a little larger than most connection RTT noLoad values; it is typically 100 ms. Through this minimum-utilization statistic, CoDel will easily be able to detect a TCP Reno connection‚Äôs queue-building phase, which except for short-RTT connections will last for many Intervals. CoDel measures this queue utilization in terms of the time the packet spends in the queue (its ‚Äúsojourn time‚Äù) rather than the size of the queue in bytes. While these two measures are proportional at any one router, the use of time rather than space means that the CoDel algorithm is independent of the outbound bandwidth, and so does not need to be conÔ¨Ågured for that bandwidth. CoDel‚Äôs target for the minimum queue utilization is typically 5% of the interval, or 5 ms, although 10% is also reasonable. If the minimum utilization is smaller, no action is taken. If the minimum utilization becomes larger, then CoDel enters its ‚Äúdropping mode‚Äù, drops a packet, and begins scheduling additional packet drops. This lasts until the minimum utilization is again below the target, and CoDel returns to its ‚Äúnormal mode‚Äù. Once dropping mode begins, the second drop is scheduled for one Interval after the Ô¨Årst drop, though the second drop may not occur if CoDel is able to return to normal mode. While CoDel remains in dropping mode, additional packet drops are scheduled after times of Interval/?2, Interval/?3,etc; that is, the dropping rate accelerates in proportion to?n until the minimum time packets spend in the queue is small enough again that CoDel is able to return to normal mode. If the trafÔ¨Åc consists of a single TCP Reno connection, CoDel will drop one of its packets as soon as the queue utilization hits 5%. That will cause cwnd to halve, most likely making even a second packet drop unnecessary. If the connection‚Äôs RTT was approximately equal to the Interval, then its link utilization will be the same as if the queue capacity was Ô¨Åxed at 5% of the transit capacity, or 79% ( 19.12 Exercises, 13.0). However, if there are a modest number of unsynchronized TCP connections, the link-utilization rate climbs to above 90% ([NJ12], Ô¨Ågs 5 and 8). If the trafÔ¨Åc consists of several TCP Reno connections, a few drops should be all that are necessary to force most of the connections to halve their cwnd s, and thus greatly reduce their collective queue utilization. However, even if the trafÔ¨Åc consists of a single Ô¨Åxed-rate UDP connection, with too high a rate for the bottleneck, CoDel still works. In this case it drops as many packets as it needs to in order to drive down the queue utilization, and this cycle repeats as necessary. An additional feature of CoDel is that if the dropping mode is re-entered quickly, the dropping rate picks up where it left off. For an example application of CoDel, see 24.6 Limiting Delay. 21.6 The High-Bandwidth TCP Problem The TCP Reno algorithm has a serious consequence for high-bandwidth connections: the cwnd needed implies a very small ‚Äì unrealistically small ‚Äì packet-loss rate p. ‚ÄúNoise‚Äù losses (losses not due to congestion) are not frequent but no longer negligible; these keep the window signiÔ¨Åcantly smaller than it should be. The following table, from RFC 3649, is based on an RTT of 0.1 seconds and a packet size of 1500 bytes, for various throughputs. The cwnd values represent the bandwidth RTT products. 506 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 TCP Throughput (Mbps) RTTs between losses cwnd Packet Loss Rate P 1 5.5 8.3 0.02 10 55 83 0.0002 100 555 833 210-6 1000 5555 8333 210-8 10,000 55555 83333 210-10 Note the very small value of the loss probability needed to support 10 Gbps; this works out to a bit error rate of less than 210-14. For Ô¨Åber optic data links, alas, a physical bit error rate of 10-13is often considered acceptable; there is thus no way to support the window size of the Ô¨Ånal row above. (The use of errorcorrecting codes on OTN links, 6.2.3 Optical Transport Network, can reduce the bit error rate to less than 10-15.) Another source of ‚Äúnoise‚Äù losses are queue overÔ¨Çows within Ethernet switches; switches tend to have much shorter queues than routers. At 10 Gbps, a switch is forwarding one packet every microsecond; at that rate a burst does not have to last long to overrun the switch‚Äôs queue. Here is a similar table, expressing cwnd in terms of the packet loss rate: Packet Loss Rate P cwnd RTTs between losses 10-212 8 10-338 25 10-4120 80 10-5379 252 10-61,200 800 10-73,795 2,530 10-812,000 8,000 10-937,948 25,298 10-10120,000 80,000 The above two tables indicate that large window sizes require extremely small drop rates. This is the highbandwidth-TCP problem: how do we maintain a large window when a path has a large bandwidth delay product? The primary issue is that non-congestive (noise) packet losses bring the window size down, potentially far below where it could be. A secondary issue is that, even if such random drops are not signiÔ¨Åcant, the increase of cwnd to a reasonable level can be quite slow. If the network ceiling were about 2,000 packets, then the normal sawtooth return to the ceiling after a loss would take 1,000 RTTs. This is slow, but the sender would still average 75% throughput, as we saw in 19.7 TCP and Bottleneck Link Utilization. Perhaps more seriously, if the network ceiling were to double to 4,000 packets due to decreases in competing trafÔ¨Åc, it would take the sender an additional 2,000 RTTs to reach the point where the link was saturated. In the following diagram, the network ceiling and the ideal TCP sawtooth are shown in green. The ideal TCP sawtooth should range between 50% and 100% of the ceiling; in the diagram, ‚Äúnoise‚Äù or non-congestive losses occur at the red x‚Äôs, bringing down the throughput to a much lower average level. 21.6 The High-Bandwidth TCP Problem 507
An Introduction to Computer Networks, Release 2.0.11 timecwnd TCP without (green) and with (red) random losses In this diagram, red random losses occur 3-4 times as often as green congestion losses 21.7 The Lossy-Link TCP Problem Closely related to the high-bandwidth problem is the lossy-link problem, where one link on the path has a relatively high non-congestive-loss rate; the classic example of such a link is Wi-Fi. If TCP is used on a path with a 1.0% loss rate, then 21.2 TCP Reno loss rate versus cwnd indicates that the sender can expect an average cwnd of only about 12, no matter how high the bandwidth delay product is. The only difference between the lossy-link problem and the high-bandwidth problem is one of scale; the lossy-link problem involves unusually large values of p while the high-bandwidth problem involves circumstances where p is quite low but not low enough. For a given non-congestive loss rate p, if the bandwidthdelay product is much in excess of 1.22/?p then the sender will be unable to maintain a cwnd close to the network ceiling. 21.8 The Satellite-Link TCP Problem A third TCP problem, only partially related to the previous two, is that encountered by TCP users with very long RTTs. The most dramatic example of this involves satellite Internet links ( 4.4.2 Satellite Internet ). Communication each way involves routing the signal through a satellite in geosynchronous orbit; a round trip involves four up-or-down trips of ~36,000 km each and thus has a propagation delay of about 500ms. If we take the per-user bandwidth to be 1 Mbps (satellite ISPs usually provide quite limited bandwidth, though peak bandwidths can be higher), then the bandwidth delay product is about 40 packets. This is not especially high, even when typical queuing delays of another ~500ms are included, but the fact that it takes many seconds to reach even a moderate cwnd is an annoyance for many applications. Most ISPs provide an ‚Äúacceleration‚Äù mechanism when they can identify a TCP connection as a Ô¨Åle download; this usually involves transferring the Ô¨Åle over the satellite portion of the path using a proprietary protocol. However, this is not 508 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 much use to those using TCP connections that involve multiple bidirectional exchanges; egthose using VPN connections. 21.9 Epilog TCP Reno‚Äôs core congestion algorithm is based on algorithms in Jacobson and Karel‚Äôs 1988 paper [JK88], now twenty-Ô¨Åve years old. There are concerns both that TCP Reno uses too much bandwidth (the greediness issue) and that it does not use enough (the high-bandwidth-TCP problem). In the next chapter we consider alternative versions of TCP that attempt to solve some of the above problems associated with TCP Reno. 21.10 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.16 Solutions for Dynamics of TCP. 1.0. For each value ùõºorùõΩbelow, Ô¨Ånd the other value so that AIMD( ùõº,ùõΩ) is TCP-friendly. (a).ùõΩ= 1/5 (b).ùõΩ= 2/9 (c).ùõº= 1/5 Then pick the pair that has the smallest ùõº, and draw a sawtooth diagram that is approximately proportional: ùõºshould be the slope of the linear increase, and ùõΩshould be the decrease fraction at the end of each tooth. 2.0. Suppose two TCP Ô¨Çows compete. The Ô¨Çows have the same RTT. The Ô¨Årst Ô¨Çow uses AIMD( ùõº1,ùõΩ1) and the second uses AIMD( ùõº2,ùõΩ2); neither Ô¨Çow is necessarily TCP-Reno-friendly. The two connections, however, compete fairly with one another; that is, they have the same average packet-loss rates. Show that ùõº1/ùõΩ1= (2-ùõΩ2)/(2-ùõΩ1)ùõº2/ùõΩ2. Assume regular losses, and use the methods of 21.4 AIMD Revisited. Hint: Ô¨Årst, apply the argument there to show that the two Ô¨Çows‚Äô teeth must have the same width w and same average height. The average height is no longer 3w/2, but can still be expressed in terms of w, ùõºandùõΩ. Use a diagram to show that, for any tooth, average_height = h (1‚ÄìùõΩ/2), with h the right-edge height of the tooth. Then equate the two average heights of the h 1/ùõΩ1and h 2/ùõΩ2teeth. Finally, use the ùõºiw =ùõΩihirelationships to eliminate h 1and h 2. 3.0. Using the result of the previous exercise, show that AIMD( ùõº1,ùõΩ) is equivalent to (in the sense of competing fairly with) AIMD( ùõº2,0.5), with ùõº2=ùõº1(2‚ÄìùõΩ)/3ùõΩ. 4.0. Suppose two 1 kB packets are sent as part of a packet-pair probe, and the minimum time measured between arrivals is 5 ms. What is the estimated bottleneck bandwidth? 5.0. Consider again the three-link parking-lot network from 21.1.1 Max-Min Fairness: 21.9 Epilog 509
An Introduction to Computer Networks, Release 2.0.11 A B C D (a). Suppose we have twoend-to-end connections, in addition to one single-link connection for each link. Find the max-min-fair allocation. (b). Suppose we have a single end-to-end connection, and one B‚ÄìC and C‚ÄìD connection, but two A‚ÄìB connections. Find the max-min-fair allocation. 6.0. Consider the two-link parking-lot network: A B C Suppose there are two A‚ÄìC connections, one A‚ÄìB connection and one A‚ÄìC connection. Find the allocation that is proportionally fair. 7.0. Suppose we use TCP Reno to send K packets over R RTT intervals. The transmission experiences n not-necessarily-uniform loss events; the TCP cwnd graph thus has n sawtooth peaks of heights N 1through Nn. At the start of the graph, cwnd = A, and at the end of the graph, cwnd = B. Show that the sum N 1+. .. + N nis 2(R+A-B), and in particular the average tooth height is independent of the distribution of the loss events. 510 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 A RBR+A-B N1 N2N3N4 N4/2 N3/2 N2/2N1/2 8.0. Suppose the bandwidth delay product for a network path is 1000 packets. The only trafÔ¨Åc on the path is from a single TCP Reno connection. For each of the following cases, Ô¨Ånd the average cwnd and the approximate number of packets between losses (the reciprocal of the loss rate). Your answers, collectively, should reÔ¨Çect the formula in 21.2 TCP Reno loss rate versus cwnd. (a).‚ô¢The bottleneck queue capacity is close to zero. (b). The bottleneck queue capacity is 1000 packets. (c). The bottleneck queue capacity is 3000 packets. 9.0. Suppose TCP Reno has regularly spaced sawtooth peaks of the same height, but the packet losses come in pairs, with just enough separation that both losses in a pair are counted separately. N is large enough that the spacing between the two losses is negligible. The net effect is that each large-scale tooth ranges from height N/4 to N. As in 21.2 TCP Reno loss rate versus cwnd ,cwnd mean= K/?p for some constant K. Find the constant. Hint: from the given information one can determine both cwnd meanand the number of packets sent in one tooth. The loss rate is p = 2/(number of packets sent in one tooth). 21.10 Exercises 511
An Introduction to Computer Networks, Release 2.0.11 N N/4 3/4 N 10.0. As in the previous exercise, suppose a TCP transmission has large-scale teeth of height N. Between each pair of consecutive large teeth, however, there are K-1 additional losses resulting in K-1 additional tiny teeth; N is large enough that these tiny teeth can be ignored. A non-Reno variant of TCP is used, so that between these tiny teeth cwnd is assumed not to be cut in half; during the course of these tiny teeth cwnd does not change much at all. The large-scale tooth has width N/2 and height ranging from N/2 to N, and there are K losses per large-scale tooth. Find the ratio cwnd /(1/?p), in terms of K. When K=1 your answer should reduce to that derived in 21.2 TCP Reno loss rate versus cwnd. N N/2 N/2 K=4 shown 11.0. Suppose a TCP Reno tooth starts with cwnd = c, and contains N packets. Let w be the width of the tooth, in RTTs as usual. Show that w = (c2+ 2N)1/2‚Äì c. Hint: the maximum height of the tooth will be c+w, and so the average height will be c + w/2. Find an equation relating c, w and N, and solve for w using the quadratic formula. 12.0. Suppose we have a non-Reno implementation of TCP, in which the formula relating the cwnd c to the time t, as measured in RTTs since the most recent loss event, is c = t2(versus TCP Reno‚Äôs c = c 0+ t). The sawtooth then looks like the following: The number of packets sent in a tooth of width T RTTs, which is the reciprocal of the loss rate p, is now approximately T3/3 (this may be accepted on faith, or shown by integrating t2from 0 to T, or looking up the 512 21 Further Dynamics of TCP
An Introduction to Computer Networks, Release 2.0.11 formula for 12+ 22+. .. + T2). The average cwnd is therefore T3/3T = T2/3. Derive a formula expressing the average cwnd in terms of the loss rate p. Hint: the exponent for p should be ‚Äì2/3, versus ‚Äì1/2 in the formula in 21.2 TCP Reno loss rate versus cwnd. 13.0. Using the TCP assumptions of exercise 12.0 above, cwnd is incremented by about 2t per each RTT. Show that the cwnd increment rule can be expressed as cwnd +=ùõºcwnd1/2 and Ô¨Ånd the value of ùõº. 14.0. Using the same TCP assumptions as in exercise 12.0 above, show that cwnd is still proportional to p-2/3, where p is the loss rate, assuming the following: 
- the top boundary of each tooth follows the curve cwnd = c(t) = t2, as before. 
- each tooth has a right boundary at t=T and a left boundary at t=T 1, where c(T 1) = 0.5c(T). (In the previous exercise we assumed, in effect, that T 1= 0 and that cwnd dropped to 0 after each loss event; here we assume multiplicative decrease is in effect with ùõΩ=1/2.) The number of packets sent in one tooth is now k(T3‚Äì T13), and the mean cwnd is this divided by T‚ÄìT 1. T T1h h/2c(t) = t2 Note that as the teeth here become higher, they become proportionately narrower. Hint: show T 1= (0.5)0.5T, and then eliminate T 1from the above equations. 15.0. Suppose in a TCP Reno run each packet is equally likely to be lost; the number of packets N in each tooth will therefore be distributed exponentially. That is, N = -k log(X), where X is a uniformly distributed random number in the range 0<X<1 (k, which does not really matter here, is the mean interval between losses). Write a simple program that simulates such a TCP Reno run. At the end of the simulation, output an estimate of the constant C in the formula cwnd mean = C/?p. You should get a value of about 1.31, as in the formula in 21.2.1 Irregular teeth. Hint: There is no need to simulate packet transmissions; we simply create a series of teeth of random size, and maintain running totals of the number of packets sent, the number of RTT intervals needed to send them, and the number of loss events (that is, teeth). After each loss event (each tooth), we update: 
- total_packets += packets sent in this tooth 
- RTT_intervals += RTT intervals in this tooth 
- loss_events += 1 (one tooth = one loss event) If a loss event marking the end of one tooth occurs at a speciÔ¨Åc value of cwnd, the next tooth begins at height c =cwnd /2. If N is the random value for the number of packets in this tooth, then by the previous 21.10 Exercises 513
An Introduction to Computer Networks, Release 2.0.11 exercise the tooth width in RTTs is w = (c2+ 2N)1/2‚Äì c; the next peak (that is, loss event) therefore occurs whencwnd = c+w. Update the totals as above and go on to the next tooth. It should be possible to run this simulation for 1 million teeth in modest time. 16.0. Suppose two TCP connections have the same RTT and share a bottleneck link, for which there is no other competition. The size of the bottleneck queue is negligible when compared to the bandwidth  RTT noLoad product. Loss events occur at regular intervals. In Exercise 12.0 of the previous chapter, you were to show that if losses are synchronized then the two connections together will use 75% of the total bottleneck-link capacity Now assume the two TCP connections have no losses in common, and, in fact, alternate losses at regular intervals as in the following diagram. C Both connections have a maximum cwnd of C. When Connection 1 experiences a loss, Connection 2 will havecwnd = 75% of C, and vice-versa. (a). What is the combined transit capacity of the paths, in terms of C? (Because the queue size is negligible, the transit capacity is approximately the sum of the cwnd s at the point of loss.) (b). Find the bottleneck-link utilization. Hint: Again because the queue size is negligible, this is approximately the ratio of the average total cwnd to the transit capacity of part (a). It should be at least 85%. 514 21 Further Dynamics of TCP
22 NEWER TCP IMPLEMENTATIONS Since the rise of TCP Reno, several TCP alternatives to Reno have been developed; each attempts to address some perceived shortcoming of Reno. While many of them are very speciÔ¨Åc attempts to address the highbandwidth problem we considered in 21.6 The High-Bandwidth TCP Problem, some focus primarily or entirely on other TCP Reno foibles. One such issue is TCP Reno‚Äôs ‚Äúgreediness‚Äù in terms of queue utilization; another is the lossy-link problem ( 21.7 The Lossy-Link TCP Problem ) experienced by, say, Wi-Fi users. Generally speaking, a TCP implementation can respond to congestion at the cliff ‚Äì that is, it can respond to packet losses ‚Äì or can respond to congestion at the knee ‚Äì that is, it can detect the increase in RTT associated with the Ô¨Ålling of the queue. These strategies are sometimes referred to as loss-based anddelaybased, respectively; the latter term because of the rise in RTT. TCP implementers can tweak both the loss response ‚Äì the multiplicative decrease of TCP Reno ‚Äì and also the way TCP increases its cwnd in the absence of loss. There is a rich variety of options available. The concept of monitoring the RTT to avoid congestion at the knee was Ô¨Årst introduced in TCP Vegas (22.6 TCP Vegas ). One striking feature of TCP Vegas is that, in the absence of competition, the queue may never Ô¨Åll, and thus there may not be any congestive losses. The TCP sawtooth, in other words, is not inevitable. When losses do occur, most of the mechanisms reviewed here continue to use the TCP NewReno recovery strategy. As most of the implementations here are relatively recent, the senders can generally expect that the receiving end will support SACK TCP, which allows more rapid recovery from multiple losses. 22.1 Choosing a TCP on Linux On Linux systems, the TCP congestion-control mechanism can be set by writing an appropriate string to/proc/sys/net/ipv4/tcp_congestion_control (or, equivalently, by passing the string as a parameter to the sysctl net.ipv4.tcp_congestion_control command). The standard options on the author‚Äôs system as of 2013 are listed below (as of 2016, several are now only available if loaded explicitly, egwithmodprobe ). The list comes from /proc/sys/net/ipv4/ tcp_available_congestion_control. - highspeed 
- htcp 
- hybla 
- illinois 
- vegas 
- veno 
- westwood 
- bic 
- cubic 515
An Introduction to Computer Networks, Release 2.0.11 We review several of these below; see 22.4 A Roadmap for an overview. TCP Cubic is currently (2013) the default Linux congestion-control implementation; TCP Bic was a precursor. The TCP congestion-control mechanism can also be set on a per-connection basis. Non-root users can select any mechanism listed in /proc/sys/net/ipv4/tcp_allowed_congestion_control; entries in tcp_available_congestion_control can be copied to this by the root user. Many TCP Ô¨Çavors are not available by default, but can be loaded via modprobe. The modules containing the TCP implementations are generally in /lib/modules/$(uname -r)/kernel/net/ipv4. Executing ls tcp_* in this directory yields (on the author‚Äôs system in 2017) the following: 
- tcp_bic.ko 
- tcp_cdg.ko 
- tcp_dctcp.kp 
- tcp_highspeed.ko 
- tcp_htcp.ko 
- tcp_hybla.ko 
- tcp_illinois.ko 
- tcp_scalable.ko 
- tcp_vegas.ko 
- tcp_veno.ko 
- tcp_westwood.ko 
- tcp_yeah.ko To load, eg, TCP Vegas, use modprobe tcp_vegas (without the ‚Äú.ko‚Äù). This will last until the next reboot (or until the module is manually unloaded). At this point /proc/sys/net/ipv4/tcp_available_congestion_control will contain ‚Äúvegas‚Äù (not tcp_vegas). In the C language, we can select the Linux congestion control mechanism, after socket creation but before connection, by including the setsockopt() call below; see 28.2.2 An Actual Stack-OverÔ¨Çow Example and29.5.3 A TLS Programming Example for complete C examples (though without this call). #include <netinet/in.h> #include <netinet/tcp.h> ... char*cong_algorithm = "vegas"; int slen = strlen( cong_algorithm ) + 1; int rc = setsockopt( sock, IPPROTO_TCP, TCP_CONGESTION, cong_algorithm, slen); if(rc < 0) { / *error*/ } Checking the return code is essential to determine if the algorithm request succeeded. In Python3 (and Python2) we can do this as well; the Ô¨Åle below is also available at tcp_stalkc_cong.py. See also30.7.3.1 sender.py. 516 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 #!/usr/bin/python3 # stalk client allowing specification of the TCP congestion algorithm from socket import * from sys import argv default_host = "localhost" portnum = 5431 cong_algorithm = 'vegas' deftalk(): globalcong_algorithm rhost = default_host iflen(argv) > 1: rhost = argv[1] iflen(argv) > 2: cong_algorithm = argv[2] print("Looking up address of " + rhost + "...", end="") try: dest = gethostbyname(rhost) exceptgaierror asmesg: # host not found errno,errstr=mesg.args print("\n", errstr); return; print("got it: " + dest) addr=(dest, portnum) s = socket() TCP_CONGESTION = 13 # defined in /usr/include/netinet/tcp.h cong = bytes(cong_algorithm, 'ascii') try: s.setsockopt(IPPROTO_TCP, TCP_CONGESTION, cong) except OSError as mesg: errno, errstr = mesg.args print ('congestion mechanism {}not available: {}'.format(cong_ √£√ëalgorithm, errstr)) return res=s.connect_ex(addr) # make the actual connection ifres!=0: print("connect to port ", portnum, " failed" ) exit() while True: try: buf = input( "> ") except: break; buf = buf + "\n" s.send(bytes(buf, 'ascii')) talk() 22.1 Choosing a TCP on Linux 517
An Introduction to Computer Networks, Release 2.0.11 As of version 3.5, Python did not deÔ¨Åne the constant TCP_CONGESTION; the value 13 above was found in the C include Ô¨Åle mentioned in the comment. Fortunately, Python simply passes the parameters of s. setsockopt() to the underlying C call, and everything works. Supposedly TCP_CONGESTION is predeÔ¨Åned in Python 3.6. 22.2 High-Bandwidth Desiderata One goal of all TCP implementations that attempt to Ô¨Åx the high-bandwidth problem is to be unfair to TCP Reno: the whole point is to allow cwnd to increase more aggressively than is permitted by Reno. Beyond that, let us review what else a TCP version should do. First is the backwards-compatibility constraint: any new TCP should exhibit reasonable fairness with TCP Reno at lower bandwidth delay products. In particular, it should not ever have a signiÔ¨Åcantly lower cwnd than a competing TCP Reno would get. But also it should not take bandwidth unfairly from a TCP Reno connection: the above comment about unfairness to Reno notwithstanding, the new TCP, when competing with TCP Reno, should leave the Reno connection with about the same bandwidth it would have if it were competing with another Reno connection. This is possible because at higher bandwidth delay products TCP Reno does not efÔ¨Åciently use the available bandwidth; the new TCP should to the extent possible restrict itself to consuming this previously unavailable bandwidth rather than eating signiÔ¨Åcantly into the bandwidth of a competing TCP Reno connection. There is also the self-fairness issue: multiple connections using the new TCP should receive similar bandwidth allocations, at least with similar RTTs. For dissimilar RTTs, the bandwidth proportions should ideally be no worse than they would be under TCP Reno. Ideally, we also want relatively rapid convergence to fairness; fairness is something of a hollow promise if only connections transferring more than a gigabyte will beneÔ¨Åt from it. For TCP Reno, two connections halve the difference in their respective cwnds at each shared loss event; as we saw in 21.4.1 AIMD and Convergence to Fairness, slower convergence is possible. It is harder to hope for fairness between competing new implementations. However, at the very least, if new implementations tcp1 and tcp2 are competing, then neither should get less than TCP Reno would get. Some new TCPs make use of careful RTT measurements, and, as we shall see below, such measurements are subject to a considerable degree of noise. Any new TCP implementation should be reasonably robust in the face of inaccuracies in RTT measurement; a modest or transient measurement error should not make the protocol behave badly, in either the direction of low cwnd or of high. Finally, a new TCP should ideally try to avoid clusters of multiple losses at each loss event. Such multiple losses, for example, are a problem for TCP NewReno without SACK: as we have seen, it takes one RTT to retransmit each lost packet. Even with SACK, multiple losses complicate recovery. Yet if a new TCP increments cwnd by an amount N>1 after each RTT, then there is potential for the network ceiling to be exceeded by N within one RTT, making a cluster of N losses reasonably likely to occur. These losses are likely distributed among all connections, not just the new-TCP one. All TCPs addressing the high-bandwidth issue will need a cwnd -increment N that is fairly large, at least some of the time, apparently conÔ¨Çicting with this no-multiple-losses ideal. One trick is to reduce N when packet loss appears to be imminent. TCP Illinois and TCP Cubic do have mechanisms in place to reduce multiple losses. 518 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 22.3 RTTs The exact performance of some of the faster TCPs we consider ‚Äì for that matter, the exact performance of TCP Reno ‚Äì is inÔ¨Çuenced by the RTT. This may affect individual TCP performance and also competition between different TCPs. For reference, here are a few typical RTTs from Chicago to various other places: 
- US West Coast: 50-100 ms 
- Europe: 100-150 ms 
- Southeast Asia: 100-200 ms 22.4 A Roadmap We start with Highspeed TCP, an early and relatively simple attempt to address the high-bandwidth-TCP problem. After that is the group TCP Vegas, FAST TCP, TCP Westwood, TCP Illinois and Compound TCP. These all involve so-called delay-based congestion control, in which the sender carefully monitors the RTT for the minute increases that signal queuing. TCP Vegas, which dates from 1995, is the earliest TCP here and in fact predates widespread recognition of the high-bandwidth-TCP problem. Its goal ‚Äì then and now ‚Äì was to prove that one could build a TCP that, in the absence of competition, could transfer arbitrarily long streams of data with no losses and with 100% bottleneck-link utilization. The next group, consisting of TCP Veno, TCP Hybla and DCTCP, represent special-purpose TCPs. While TCP Veno may be a reasonable high-bandwidth TCP candidate, its primary goal is to improve TCP performance over lossy links such as Wi-Fi. TCP Hybla is targeted at satellite-Internet users with very long RTTs while DCTCP is for internal connections within a datacenter (which, among other things, have very short RTTs). The last triad represents newer, non-delay-based attempts to solve the high-bandwidth-TCP problem: HTCP, TCP Cubic and TCP BBR. TCP Cubic has become the default TCP on Linux. 22.5 Highspeed TCP An early proposed Ô¨Åx for the high-bandwidth-TCP problem is HighSpeed TCP, documented in RFC 3649 (Floyd, 2003). Highspeed TCP is sometimes called HS-TCP, but we use the longer name here to avoid confusion with the entirely unrelated H-TCP, below. Highspeed TCP adjusts the additive-increase and multiplicative-decrease parameters ùõºandùõΩso that, for larger values of cwnd, the rate of cwnd increase between losses is much faster, and the cwnd decrease at loss events is much smaller. This allows efÔ¨Åcient use of all the available bandwidth for large bandwidthdelay products. Correspondingly, when cwnd is in the range where TCP Reno works well, Highspeed TCP‚Äôs throughput is only modestly larger than TCP Reno‚Äôs, so the two compete relatively fairly. The threshold for Highspeed TCP diverging from TCP Reno is a loss rate less than 10‚Äì3, which for TCP Reno occurs when cwnd = 38. Beyond that point, Highspeed TCP gradually increases ùõºand decreases ùõΩ. The overall effect is to outperform TCP Reno by a factor N = N( cwnd ) according to the table below. This N 22.3 RTTs 519
An Introduction to Computer Networks, Release 2.0.11 can also be interpreted as the ‚Äúunfairness‚Äù of Highspeed TCP with respect to TCP Reno; fairness is arguably ‚Äúclose to‚Äù 1.0 until cwnd¬•1000, at which point TCP Reno is likely not using the full bandwidth available due to the high-bandwidth TCP problem. cwnd N(cwnd ) 1 1.0 10 1.0 100 1.4 1,000 3.6 10,000 9.2 100,000 23.0 An algebraic expression for N( cwnd ), for N¬•38, is N(cwnd ) = 0.23cwnd0.4 Atcwnd =38 this is about 1.0; for smaller cwnd we stick with N=1. To specify the details of Highspeed TCP, we start by considering a 10 Gbps link, which was the fastest generally available at the time Highspeed TCP was developed. If the RTT is 100 ms, then the bandwidth delay product works out to 83,000 packets. The central strategy of Highspeed TCP is to choose the desired loss rate for an average cwnd of 83,000 to be 1 packet in 107; this number was empirically determined. This is quite a bit larger than the corresponding TCP Reno loss rate of 1 packet in 5 109(21.6 The High-Bandwidth TCP Problem ); in this context, a larger congestion loss rate is better. The loss rate is the reciprocal of the tooth area; it turns out (below) that we have a great deal of latitude in choosing the tooth area by adjusting theùõºandùõΩwindow-growth parameters. After determining ùõºandùõΩforcwnd = 83,000, Highspeed TCP then uses interpolation to cover cwnd values in between 38 and 83,000. (The Highspeed TCP rules do extend to larger cwnd s too, but there is not necessarily an expectation that they will work well there.) We start with the TCP Reno relationship cwnd = 1.225p‚Äì0.5, from 21.2 TCP Reno loss rate versus cwnd (RFC 3649 uses a numerator of 1.20 in this formula.) We Ô¨Åt the relationship cwnd = kp‚Äìùõºto the above two pairs of ( cwnd ,p) values, (38,10‚Äì3) and (83000,10‚Äì7). This turns out to yield cwnd = 0.12p‚Äì0.835 From this we can derive the TCP Reno multiplier N( cwnd ) above, by using the TCP Reno relationship cwnd = 1.2Np‚Äì0.5for N synchronized connections, eliminating p and then solving for N. The next step is to deÔ¨Åne the additive-increase and multiplicative-decrease values ùõº=ùõº(cwnd ) and ùõΩ= ùõΩ(cwnd ), thus allowing us to build an actual implementation. While ùõºandùõΩare allowed to vary with cwnd, we will assume they do so only slowly, so that for any given steady-state connection the ùõºvalues are relatively constant (the ùõΩvalue is that at the maximum cwnd ). This gives us a standard AIMD tooth: 520 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 h ww=h w/2 average cwnd Whencwnd is 83,000 we want the loss rate to be 10‚Äì7, meaning that the area of the tooth, w cwnd = wh(1-ùõΩ/2), should be 107. From this we get w = 107/83,000 = 120.5 RTTs. We also have, very generally, ùõºw =ùõΩh, and combining this with cwnd = h(1-ùõΩ/2), we get ùõº=ùõΩh/w =cwnd(2ùõΩ/(1-ùõΩ/2))/w 1378ùõΩ/(1-ùõΩ/2). RFC 3649 suggests ùõΩ=0.1 at this cwnd, making ùõº= 73. The value of ùõΩfor values ofcwnd between 38 and 83,000 is determined by logarithmic interpolation between 0.5 and 0.1; the corresponding value of ùõº(cwnd ) is then set by the formula. The 1-in-107loss rate ‚Äì corresponding to a bit error rate of about one in 1.2 1011‚Äì is large enough that it is at least two orders of magnitude higher than the rate of noise-induced non-congestive packet losses. On the other hand, it is small enough that the Highspeed TCP derived from it competes reasonably fairly with TCP Reno, at least with bandwidth delay products small enough that TCP Reno alone performs reasonably well. It may be helpful to view Highspeed TCP in terms of the cwnd graph between losses. For ordinary TCP, the graph increases linearly. For Highspeed TCP, the graph is slightly convex (lying above its tangent). This means that there is a modest increase in the rateofcwnd increase, as time goes on (up to the point of packet loss). 22.5 Highspeed TCP 521
An Introduction to Computer Networks, Release 2.0.11 time tcwnd TCP Reno: cwnd(t) lineartime tcwnd Highspeed TCP: cwnd(t) convex This might be an appropriate time to point out that in TCP Reno, the cwnd -versustime graph between losses is actually slightly concave (lying below its tangent). We do get a strictly linear graph if we plot cwnd as a function of the count of elapsed RTTs, but the RTTs are themselves slowly increasing as a function of time once the queue starts Ô¨Ålling up. At that point, the cwnd -versus-time graph bends slightly down. If the bottleneck queue capacity matches the total path transit capacity, the RTTs for a full queue are about double the RTTs for an empty queue. In general, when Highspeed-TCP competes with a new TCP Reno Ô¨Çow it is N times as aggressive, and grabs N times the bandwidth, where N = N( cwnd ) is as above. For cwnd = 83,000, the formula above yields N = 21. This may be surprising, as for this value of cwnd Highspeed TCP is AIMD(73,0.1), which is equivalent to AIMD(459,0.5) (either via the formula above or by 21.10 Exercises, exercise 2.0). We might naively suppose that AIMD(459,0.5) would out-compete TCP Reno ‚Äì AIMD(1,0.5) ‚Äì by a factor of 459, by the reasoning of 20.3.1 Example 2: Faster additive increase. But this is true only if losses are synchronized, which, for such lopsided differences in ùõº, is manifestly not the case. Because Highspeed TCP uses the lion‚Äôs share of the queue, it encounters the lion‚Äôs share of loss events, and TCP Reno is able to do much better than theùõºvalues alone would suggest. Finally, with a little math we can compare Highspeed TCP with an AIMD-type Ô¨Çavor of TCP with an additive-increase rule (per RTT) of the form cwnd +=ùõºcwndk For TCP Reno, k=0, and in the example of exercises 12.0 and 13.0 of 21.10 Exercises we have k=1/2. For compatibility with Highspeed TCP, it turns out what we need is k=0.8. We will return to this in 22.10 Compound TCP, which intentionally mimics the behavior of Highspeed TCP when queue utilization is low. 22.6 TCP Vegas TCP Vegas, introduced in [BP95], is the only new TCP version we consider here that dates from the previous century. The goal was not directly to address the high-bandwidth problem, but rather to improve TCP throughput generally; indeed, in 1995 the high-bandwidth problem had not yet surfaced as a practical con522 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 cern. The ambitious goal of TCP Vegas is essentially to eliminate congestive losses, and to try to keep the bottleneck link 100% utilized at all times. Recall from 19.7 TCP and Bottleneck Link Utilization that, with a large queue, the average bottleneck-link utilization for TCP Reno can be as low as 75%. TCP Vegas achieves this improvement by, like DECbit, recognizing TCP congestion at the knee, that is, at the point where the bottleneck link has become saturated and further cwnd increases simply result in RTT increases. A TCP Vegas sender alone or in competition only with other TCP Vegas connections will seldom if ever approach the ‚Äúcliff‚Äù where packet losses occur. To accomplish this, no special router cooperation ‚Äì or even receiver cooperation ‚Äì is necessary. Instead, the sender uses careful monitoring of the RTT to keep track of the number of ‚Äúextra packets‚Äù ( iepackets sitting in queues) it has injected into the network. In the absence of competition, the RTT will remain constant, equal to RTT noLoad, untilcwnd has increased to the point when the bottleneck link has become saturated and the queue begins to Ô¨Åll ( 8.3.2 RTT Calculations ). By monitoring the bandwidth as well, a TCP sender can even determine the actual number of packets in the bottleneck queue, as bandwidth (RTT ‚Äì RTT noLoad ). TCP Vegas uses this information to attempt to maintain at all times a small but positive number of packets in the bottleneck queue. This TCP Vegas strategy is now often referred to as delay-based congestion control, as opposed to TCP Reno‚Äôs loss-based congestion control. TCP Reno‚Äôs periodic losses followed by the halving of cwnd is what leads to the ‚ÄúTCP sawtooth‚Äù; TCP Vegas, however, has no sawtooth. A TCP sender can readily measure its throughput. The simplest measurement is cwnd /RTT as in 8.3.2 RTT Calculations; this amounts to averaging throughput over an entire RTT. Let us denote this bandwidth estimate by BWE; for the time being we will accept BWE as accurate, though see 22.8.1 ACK Compression and Westwood+ below. TCP Vegas estimates RTT noLoad by the minimum RTT (RTT min) encountered during the connection. The ‚Äúideal‚Äù cwnd that just saturates the bottleneck link is BWE RTT noLoad. Note that BWE will be much more volatile than RTT min; the latter will typically reach its Ô¨Ånal value early in the connection, while BWE will Ô¨Çuctuate up and down with congestion (which will also act on RTT, but by increasing it). As in 8.3.2 RTT Calculations, any TCP sender can estimate queue utilization as queue_use = cwnd ‚Äì BWERTT noLoad =cwnd(1 ‚Äì RTT noLoad /RTT actual) TCP Vegas then adjusts cwnd regularly to maintain the following: ùõº¬§queue_use¬§ùõΩ which is the same as BWERTT noLoad +ùõº¬§cwnd¬§BWERTT noLoad +ùõΩ Typically ùõº= 2-3 packets and ùõΩ= 4-6 packets. We increment cwnd by 1 ifcwnd falls below the lower limit ( egif BWE has increased). Similarly, we decrement cwnd by 1 if BWE drops and cwnd exceeds BWERTT noLoad +ùõΩ. These adjustments are conceptually done once per RTT. Typically a TCP Vegas sender would also set cwnd =cwnd /2 if a packet were actually lost, though this does not necessarily happen nearly as often as with TCP Reno. TCP Vegas achieves its goal quite well. If one monitors the number of packets in queues, through real measurement or in simulation, the number does indeed stay between ùõºandùõΩ. In the absence of competition from TCP Reno, a single TCP Vegas connection will never experience congestive packet loss. This is a remarkable achievement. 22.6 TCP Vegas 523
An Introduction to Computer Networks, Release 2.0.11 The use of returning ACKs to determine BWE is subject to errors due to ‚ÄúACK compression‚Äù, 22.8.1 ACK Compression and Westwood+. This is generally not a major problem with TCP Vegas, however. 22.6.1 TCP Vegas versus TCP Reno Despite its striking ability to avoid congestive losses in the absence of competition, TCP Vegas encounters a potentially serious fairness problem when competing with TCP Reno, at least for the case when queue capacity exceeds or is close to the transit capacity ( 19.7 TCP and Bottleneck Link Utilization ). TCP Vegas will try to minimize its queue use, while TCP Reno happily Ô¨Ålls the queue. And whoever has more packets in the queue has a proportionally greater share of bandwidth. To make this precise, suppose we have two TCP connections sharing a bottleneck router R, the Ô¨Årst using TCP Vegas and the second using TCP Reno. Suppose further that both connections have a path transit capacity of 10 packets, and R‚Äôs queue capacity is 40 packets. If ùõº=3 and ùõΩ=5, TCP Vegas might keep an average of four packets in the queue. Unfortunately, TCP Reno then gobbles up most of the rest of the queue space, as follows. There are 40-4 = 36 spaces left in the queue after TCP Vegas takes its quota, and 10 in the TCP Reno connection‚Äôs path, for a total of 46. This represents the TCP Reno connection‚Äôs network ceiling, and is the point at which TCP Reno halves cwnd; thereforecwnd will vary from 23 to 46 with an average of about 34. Of these 34 packets, if 10 are in transit then 24 are in R‚Äôs queue. If on average R has 24 packets from the Reno connection and 4 from the Vegas connection, then the bandwidth available to these connections will also be in this same 6:1 proportion. The TCP Vegas connection will get 1/7 the bandwidth, because it occupies 1/7 the queue, and the TCP Reno connection will take the other 6/7. To put it another way, TCP Vegas is potentially too ‚Äúcivil‚Äù to compete with TCP Reno. Even worse, Reno‚Äôs aggressive queue Ô¨Ålling will eventually force the TCP Vegas cwnd to decrease; see Exercise 4.0 below. This Vegas-Reno fairness problem is most signiÔ¨Åcant when the queue size is an appreciable fraction of the path transit capacity. During periods when the queue is empty, TCPs Vegas and Reno increase cwnd at the same rate, so when the queue size is small compared to the path capacity, TCP Vegas and TCP Reno are much closer to being fair. In31.5 TCP Reno versus TCP Vegas we compare TCP Vegas with TCP Reno in simulation. With a transit capacity of 220 packets and a queue capacity of 10 packets, TCPs Vegas and Reno receive almost exactly the same bandwidth. TCP Reno‚Äôs advantage here assumes a router with a single FIFO queue. That advantage can disappear if a different queuing discipline is in effect. For example, if the bottleneck router used fair queuing (to be introduced in 23.5 Fair Queuing ) on a per-connection basis, then the TCP Reno connection‚Äôs queue greediness would not be of any beneÔ¨Åt, and both connections would get similar shares of bandwidth with the TCP Vegas connection experiencing lower delay. See 23.6.1 Fair Queuing and Bufferbloat. Let us next consider how TCP Vegas behaves when there is an increase in RTT due to the kind of cross trafÔ¨Åc shown in 20.2.4 Example 4: cross trafÔ¨Åc and RTT variation and again in the diagram below. Let A‚ÄìB be the TCP Vegas connection and assume that its queue-size target is 4 packets ( egùõº=3,ùõΩ=5). We will also assume that the RTT noLoad for the A‚ÄìB path is about 5ms and the RTT for the C‚ÄìD path is also low. As before, the link labels represent bandwidths in packets/ms, meaning that the round-trip A‚ÄìB transit capacity is 10 packets. 524 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 A R1 R2 R3 B DC 100 pkts/ms100 pkts/ms 5 pkts/ms 100 pkts/ms2 pkts/ms 100 pkts/ms Initially, in the absence of C‚ÄìD trafÔ¨Åc, the A‚ÄìB connection will send at a rate of 2 packets/ms (the R2‚ÄìR3 bottleneck), and maintain a queue of four packets at R2. Because the RTT transit capacity is 10 packets, this will be achieved with a window size of 10+4 = 14. Now let the C‚ÄìD trafÔ¨Åc start up, with a winsize so as to keep about four times as many packets in R1‚Äôs queue as A, once the new steady-state is reached. If all four of the A‚ÄìB connection‚Äôs ‚Äúqueue‚Äù packets end up now at R1 rather than R2, then C would need to contribute at least 16 packets. These 16 packets will add a delay of about 16/53ms; the A‚ÄìB path will see a more-or-less-Ô¨Åxed 3ms increase in RTT. A will also see a decrease in bandwidth due to competition; with C consuming 80% of R1‚Äôs queue, A‚Äôs share wll fall to 20% and thus its bandwidth will fall to 20% of the R1‚ÄìR2 link bandwidth, that is, 1 packet/ms. Denote this new value by BWE new. TCP Vegas will attempt to decrease cwnd so that cwndBWE newRTT noLoad + 4 A‚Äôs estimate of RTT noLoad, as RTT min, will not change; the RTT has gotten larger, not smaller. So we have BWE newRTT noLoad1 packet/ms5 ms = 5 packets; adding the 4 reserved for the queue, the new value ofcwnd is now about 9, down from 14. On the one hand, this new value of cwnd does represent 5 packets now in transit, plus 4 in R1‚Äôs queue; this is indeed the correct response. But note that this division into transit and queue packets is an average. The actual physical A‚ÄìB round-trip transit capacity remains about 10 packets, meaning that if the new packets were all appropriately spaced then none of them might be in any queue. 22.7 FAST TCP FAST TCP is closely related to TCP Vegas; the idea is to keep the Ô¨Åxed-queue-utilization feature of TCP Vegas to the extent possible, but to provide overall improved performance, in particular in the face of competition with TCP Reno. Details can be found in [JWL04] and [WJLH06]. FAST TCP is patented; see patent 7,974,195. As with TCP Vegas, the sender estimates RTT noLoad as RTT min. At regular short Ô¨Åxed intervals ( eg20ms) cwnd is updated via the following weighted average: cwnd new= (1-ùõæ)cwnd +ùõæ((RTT noLoad /RTT)cwnd +ùõº) where ùõæis a constant between 0 and 1 determining how ‚Äúvolatile‚Äù the cwnd update is ( ùõæ1 is the most volatile) and ùõºis a Ô¨Åxed constant, which, as we will verify shortly, represents the number of packets the 22.7 FAST TCP 525
An Introduction to Computer Networks, Release 2.0.11 sender tries to keep in the bottleneck queue, as in TCP Vegas. Note that the cwnd update frequency is not tied to the RTT. If RTT is constant for multiple consecutive update intervals, and is larger than RTT noLoad, the above will converge to a constant cwnd, in which case we can solve for it. Convergence implies cwnd new=cwnd = ((RTT noLoad /RTT)cwnd +ùõº), and from there we get cwnd(RTT‚ÄìRTT noLoad )/RTT = ùõº. As we saw in 8.3.2 RTT Calculations ,cwnd /RTT is the throughput, and so ùõº= throughput(RTT‚ÄìRTT noLoad ) is then the number of packets in the queue. In other words, FAST TCP, when it reaches a steady state, leaves ùõº packets in the queue. As long as this is the case, the queue will not overÔ¨Çow (assuming ùõºis less than the queue capacity). Whenever the queue is not full, though, we have RTT = RTT noLoad, in which case FAST TCP‚Äôs cwnd -update strategy reduces to cwnd new=cwnd +ùõæùõº. For ùõæ=0.5 and ùõº=10, this increments cwnd by 5. Furthermore, FAST TCP performs this increment at a speciÔ¨Åc rate independent of the RTT, egevery 20ms; for long-haul links this is much less than the RTT. FAST TCP will, in other words, increase cwnd very aggressively until the point when queuing delays occur and RTT begins to increase. When FAST TCP is competing with TCP Reno, it does not directly address the queue-utilization competition problem experienced by TCP Vegas. FAST TCP will try to limit its queue utilization to ùõº; TCP Reno, however, will continue to increase its cwnd until the queue is full. Once the queue begins to Ô¨Åll, TCP Reno will pull ahead of FAST TCP just as it did with TCP Vegas. However, FAST TCP does not reduce itscwnd in the face of TCP Reno competition as quickly as TCP Vegas. Additionally, FAST TCP can often offset this Reno-competition problem in other ways as well. First, the value of ùõºcan be increased from the value of around 4 packets originally proposed for TCP Vegas; in [TWHL05] the value ùõº=30 is suggested. Second, for high bandwidth delay products, the queue-Ô¨Ålling phase of a TCP Reno sawtooth (see 19.7 TCP and Bottleneck Link Utilization ) becomes relatively smaller. In the earlier link-unsaturated phase of each sawtooth, TCP Reno increases cwnd by 1 each RTT. As noted above, however, FAST TCP is allowed to increase cwnd much more rapidly in this earlier phase, and so FAST TCP can get substantially ahead of TCP Reno. It may fall back somewhat during the queue-Ô¨Ålling phase, but overall the FAST and Reno Ô¨Çows may compete reasonably fairly. 526 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 Network ceiling Queue capacity Transit/g1 FAST TCP cwnd curve (blue) superimposed on TCP Reno sawtooth The diagram above illustrates a FAST TCP graph of cwnd versus time, in blue; it is superimposed over one sawtooth of TCP Reno with the same network ceiling. Note that cwnd rises rapidly when it is below the path transit capacity, and then levels off sharply. 22.8 TCP Westwood TCP Westwood represents an attempt to use the RTT-monitoring strategies of TCP Vegas to address the high-bandwidth problem; recall that the issue there is to distinguish between congestive and non-congestive losses. TCP Westwood can also be viewed as a reÔ¨Ånement of TCP Reno‚Äôs cwnd =cwnd /2 strategy, which is a greater drop than necessary if the queue capacity at the bottleneck router is less than the transit capacity. It remains a form of loss-based congestion control. As in TCP Vegas, the sender keeps a continuous estimate of bandwidth, BWE, and estimates RTT noLoad by RTT min. The minimum window size to keep the bottleneck link busy is, again as in TCP Vegas, BWE  RTT noLoad. In TCP Vegas, BWE was calculated as cwnd /RTT; we will continue to use this for the time being but in fact TCP Westwood has used a wide variety of other algorithms, some of which are discussed in the following subsection, to infer the available average bandwidth from the returning ACKs. The core TCP Westwood innovation is to, on loss, reduce cwnd as follows: cwnd = max(cwnd /2, BWERTT noLoad ) ifcwnd > BWERTT noLoad no change, if cwnd¬§BWERTT noLoad The product BWE RTT noLoad represents what the sender believes is its current share of the ‚Äútransit capacity‚Äù of the path. This product represents how many packets can be in transit (rather than in queues) at the current bandwidth BWE. The RTT noLoad estimate as RTT minis relatively constant, but BWE may be 22.8 TCP Westwood 527
An Introduction to Computer Networks, Release 2.0.11 markedly reduced in the presence of competing trafÔ¨Åc. A TCP Westwood sender never drops cwnd below what it believes to be the current transit capacity for the path. Consider again the classic TCP Reno sawtooth behavior: 
- cwnd alternates between cwnd minandcwnd max= 2cwnd min. 
- cwnd maxtransit_capacity + queue_capacity (or at least the sender‚Äôs share of these) As we saw in 19.7 TCP and Bottleneck Link Utilization, if transit_capacity < cwnd min, then Reno does a reasonably good job keeping the bottleneck link saturated. However, if transit_capacity > cwnd min, then when Reno drops to cwnd min, the bottleneck link is not saturated until cwnd climbs to transit_capacity. For high-speed networks, this latter case is the more likely one. Westwood, on the other hand, would in that situation reduce cwnd only to transit_capacity, a smaller reduction. Thus TCP Westwood potentially better handles a wide range of router queue capacities. For bottleneck routers where the queue capacity is small compared to the transit capacity, TCP Westwood would in theory have a higher, Ô¨Åner-pitched sawtooth than TCP Reno: the teeth would oscillate between the network ceiling (= queue+transit) and the transit_capacity, versus Reno‚Äôs oscillation between the network ceiling and half the ceiling. In the event of a non-congestive (noise-related) packet loss, if it happens that cwnd is less than transit_capacity then TCP Westwood does not reduce the window size at all. That is, non-congestive losses withcwnd < transit_capacity have no effect. When cwnd > transit_capacity, losses reduce cwnd only to transit_capacity, and thus the link stays saturated. This can be useful in lossy wireless environments; see [MCGSW01]. In the largecwnd, high-bandwidth case, non-congestive packet losses can easily lower the TCP Reno cwnd to well below what is necessary to keep the bottleneck link saturated. In TCP Westwood, on the other hand, the average cwnd may be lower than it would be without the non-congestive losses, but it will be high enough to keep the bottleneck link saturated. TCP Westwood uses BWE RTT noLoad as aÔ¨Çoor for reducing cwnd. TCP Vegas shoots to have the actual cwnd be just a few packets above this. TCP Westwood is not any more aggressive than TCP Reno at increasing cwnd in no-loss situations. So while it handles the non-congestive-loss part of the high-bandwidth TCP problem very well, it does not particularly improve the ability of a sender to take advantage of a sudden large rise in the network ceiling. TCP Westwood is also potentially very effective at addressing the lossy-link problem, as most noncongestive losses would result in no change to cwnd. 22.8.1 ACK Compression and Westwood+ So far, we have been assuming that ACKs never encounter queuing delays. They in fact will not, ifthey are traveling in the reverse direction from all data packets. But while this scenario covers any single-sender model and also systems of two or more competing senders, real networks have more complicated trafÔ¨Åc patterns, and returning ACKs from an A √ù√ëB data Ô¨Çow can indeed experience queuing delays if there is third-party trafÔ¨Åc along some link in the B √ù√ëA path. Delay in the delivery of ACKs, leading to clustering of their arrival, is known as ACK compression; see [ZSC91] and [JM92] for examples. ACK compression causes two problems. First, arriving clusters of ACKs 528 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 trigger corresponding bursts of data transmissions in sliding-windows senders; the end result is an uneven data-transmission rate. Normally the bottleneck-router queue can absorb an occasional burst; however, if the queue is nearly full such bursts can lead to premature or otherwise unexpected losses. The second problem with late-arriving ACKs is that they can lead to inaccurate or Ô¨Çuctuating measurements of bandwidth, upon which both TCP Vegas and TCP Westwood depend. For example, if bandwidth is estimated as cwnd /RTT, late-arriving ACKs can lead to inaccurate calculation of RTT. The original TCP Westwood strategy was to estimate bandwidth from the spacing between consecutive ACKs, much as is done with the packet-pairs technique ( 20.2.6 Packet Pairs ) but smoothed with a suitable running average. This strategy turned out to be particularly vulnerable to ACK-compression errors. For TCP Vegas, ACK compression means that occasionally the sender‚Äôs cwnd may fail to be decremented by 1; this does not appear to be a signiÔ¨Åcant impact, perhaps because cwnd is changed by at most 1 each RTT. For Westwood, however, if ACK compression happens to be occurring at the instant of a packet loss, then a resultant transient overestimation of BWE may mean that the new post-loss cwnd is too large; at a point when cwnd was supposed to fall to the transit capacity, it may fail to do so. This means that the sender has essentially taken a congestion loss to be non-congestive, and ignored it. The inÔ¨Çuence of this ignored loss will persist ‚Äì through the much-too-high value of cwnd ‚Äì until the following loss event. To Ô¨Åx these problems, TCP Westwood has been amended to Westwood+, by increasing the time interval over which bandwidth measurements are made and by inclusion of an averaging mechanism in the calculation of BWE. Too much smoothing, however, will lead to an inaccurate BWE just as surely as too little. Suitable smoothing mechanisms are given in [FGMPC02] and [GM03]; the latter paper in particular examines several smoothing algorithms in terms of their resistance to aliasing effects: the tendency for intermittent measurement of a periodic signal (the returning ACKs) to lead to much greater inaccuracy than might initially be expected. One smoothing Ô¨Ålter suggested by [GM03] is to measure BWE only over entire RTTs, and then to keep a cumulative running average as follows, where BWM kis the measured bandwidth over the kth RTT: BWE k=ùõºBWE k-1+ (1‚Äì ùõº)BWM k A suggested value of ùõºis 0.9. For Westwood+ simulations, see [GM04]. 22.9 TCP Illinois The general idea behind TCP Illinois, described in [LBS06], is to use the usual AIMD( ùõº,ùõΩ) strategy but to have ùõº=ùõº(RTT) be a decreasing function of the current RTT, rather than a constant. When the queue is empty and RTT is equal to RTT noLoad, then ùõºwill be large, and cwnd will increase rapidly. Once RTT starts to increase, however, ùõºwill decrease rapidly, and the cwnd growth will level off. This leads to the same kind of concave cwnd graph as we saw above in FAST TCP; a consequence of this is that for most of the time between consecutive loss events cwnd is large enough to keep the bottleneck link close to saturated, and so to keep throughput high. The actual ùõº() function is not of RTT, but rather of delay, deÔ¨Åned to be RTT ‚Äì RTT noLoad. As with TCP Vegas, RTT noLoad is estimated by RTT min. As a connection progresses, the sender maintains continually updated values not only for RTT minbut also for RTT max. The sender then sets delay maxto be RTT max‚Äì RTT min. We are now ready to deÔ¨Åne ùõº(delay). We Ô¨Årst specify the highest value of ùõº,ùõºmax, and the lowest, ùõºmin. In 22.9 TCP Illinois 529
An Introduction to Computer Networks, Release 2.0.11 [LBS06] these are 10.0 and 0.1 respectively; in the Linux 3.5 kernel they are 10.0 and 0.3. We also deÔ¨Åne delay thresh to be 0.01delay max(the 0.01 is another tunable parameter). We then deÔ¨Åne ùõº(delay) as follows ùõº(delay) = ùõºmaxif delay¬§delay thresh ùõº(delay) = k 1/(delay+k 2) if delay thresh¬§delay¬§delay max where k 1and k 2are chosen so that, for the lower formula, ùõº(delay thresh) =ùõºmaxandùõº(delay max) =ùõºmin. In case there is a sudden spike in delay, delay maxis updated before the above is evaluated, so we always have delay¬§delay max. Here is a graph: /g1max /g1min delaythresh/g1 = /g1(delay)delaymax Whenever RTT = RTT noLoad, delay=0 and so ùõº(delay) = ùõºmax. However, as soon as queuing delay just barely starts to begin, we will have delay > delay thresh and so ùõº(delay) begins to fall ‚Äì rather precipitously ‚Äì toùõºmin. The value of ùõº(delay) is always positive, though, so cwnd will continue to increase (unlike TCP Vegas) until a congestive loss eventually occurs. However, at that point the change in cwnd is very small, which minimizes the probability that multiple packets will be lost. Note that, as with FAST TCP, the increase in delay is used to trigger the reduction in ùõº. TCP Illinois also supports having ùõΩbe a decreasing function of delay, so that ùõΩ(small_delay) might be 0.2 while ùõΩ(larger_delay) might match TCP Reno‚Äôs 0.5. However, the authors of [LBS06] explain that ‚Äúthe adaptation of ùõΩas a function of average queuing delay is only relevant in networks where there are non-congestion-related losses, such as wireless networks or extremely high speed networks‚Äù. 22.10 Compound TCP Compound TCP, or CTCP, is Microsoft‚Äôs entry into the advanced-TCP Ô¨Åeld, although it is now available for Linux as well; see [TSZS06]. The idea behind Compound TCP is to add a delay-based component to TCP Reno. To this end, CTCP supplements TCP Reno‚Äôs cwnd with a delay-based contribution to the window size known as dwnd; the total window size is then 530 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 winsize =cwnd +dwnd (As usual, winsize is also not allowed to exceed the receiver‚Äôs advertised window size.) The per-RTT increment of cwnd is now 1/winsize (though note that dwnd has a separate per-RTT increment). As in TCP Vegas, CTCP maintains RTT minas a stand-in for RTT noLoad, and also maintains a bandwidth estimate BWE = winsize/RTT actual. These allow estimation of the current number of packets in the queue, denoteddiff in [TSZS06], as diff =cwnd(1 ‚Äì RTT noLoad /RTT actual). Whendiff is less than ùõæpackets, where the parameter ùõæis conÔ¨Ågurable but ùõæ=30 is a good starting point, CTCP increases winsize (per RTT) according to the rule winsize += ùõºwinsizek where the exponent k is chosen to be 0.8. (In [TSZS06] this increase is achieved by having cwnd be incremented by 1, and dwnd byùõºwinsizek‚Äì 1.) This amounts to a fairly aggressive increase; for TCP Reno we have k=0. The choice of k=0.8 is intended to make CTCP competitive with Highspeed TCP; we will return to the justiÔ¨Åcation of this below. We will also choose ùõº=1/8, which we will take as given. The value ùõæ=30 here plays very roughly a similar role as Fast TCP‚Äôs ùõº, also 30, in that both represent a threshold for queue utilization. When CTCP encounters a loss, we set winsize = winsize (1‚ÄìùõΩ) While ùõΩis potentially conÔ¨Ågurable, typically we will have the usual ùõΩ=1/2. Finally we have the case where diff >ùõæ; that is, the queue has grown ‚ÄúsigniÔ¨Åcantly‚Äù. If dwnd is also positive, it is decremented. The variable cwnd continues to increase, but cwnd anddwnd will cancel each other out over the short term, leading to a roughly constant value for winsize. When dwnd drops to 0, however, this cancellation ends, and TCP Reno‚Äôs cwnd += 1 per RTT takes over; dwnd has no more effect until after the next packet loss. Considering all these cases, a rough graph of the growth of CTCP‚Äôs winsize is the following: diff <  winsize diff   dwnd > 0 diff   dwnd = 0 Compound TCP We next derive k=0.8 as the value that leads to fair competition with Highspeed TCP. To do this we need a modest bit of calculus; the derivation can be skipped if preferred. We start with a hypothetical TCP 22.10 Compound TCP 531
An Introduction to Computer Networks, Release 2.0.11 adjustingcwnd according to the rule cwnd +=ùõºcwnd0.8, per RTT, and show this TCP does indeed compete fairly with Highspeed TCP. If we measure time in RTTs, and denote cwnd by c = c(t), and extend c(t) to a continuous function of t, this increment rule becomes dc/dt = ùõºc0.8. Taking reciprocals, we get dt/dc = (1/ ùõº)c‚Äì0.8. We can now integrate both sides, which yields t = k 1c0.2(ignoring the constant of integration), or c = k 2t5. Integrating again, we get the number of packets in one tooth (the area) to be proportional to T6, where T is the time at the right edge of the tooth. (We are inappropriately ignoring the left edge of the tooth, but by the argument of exercise 14.0 in 21.10 Exercises this turns out not to matter.) This area is the reciprocal of the loss rate p. Solving for T, we get T proportional to (1/p)1/6. As the averagecwnd is proportional to T5(the area divided by T), by substitution we can conclude that cwnd is proportional to p‚Äì5/6= p‚Äì0.833(versus the original exponent in 22.5 Highspeed TCP of ‚Äì0.835). Calculating winsize0.8is hard to do rapidly, so in practice the exponent 0.75 is used. With that value the exponentiation can be done with two applications of a fast square-root algorithm. CTCP turns out to compete reasonably fairly one-on-one with Highspeed TCP, by virtue of the choice of k=0.8. However, when competing with a set of TCP Reno connections, CTCP leaves the Reno connections with nearly the same bandwidth they would have had if they were competing with one more TCP Reno connection instead. That is, CTCP resists ‚Äústealing‚Äù bandwidth. CTCP does, however, make effective use of the bandwidth that TCP Reno leaves unclaimed due to the high-bandwidth TCP problem. 22.11 TCP Veno TCP Veno ([FL03]) is a synthesis of TCP Vegas and TCP Reno, which attempts to use the RTT-monitoring ideas of TCP Vegas while at the same time remaining about as ‚Äúaggressive‚Äù as TCP Reno in using queue capacity. TCP Veno has generally been presented as an option to address TCP‚Äôs lossy-link problem, rather than the high-bandwidth problem per se. A TCP Veno sender estimates the number N of packets likely in the bottleneck queue as N queue =cwnd - BWERTT noLoad, like TCP Vegas. TCP Veno then modiÔ¨Åes the TCP Reno congestion-avoidance rule as follows, where the parameter ùõΩ, representing the queue-utilization value at which TCP Veno slows down, might be around 5. if N queue<ùõΩ,cwnd =cwnd + 1 each RTT if N queue¬•ùõΩ,cwnd =cwnd + 0.5 each RTT The above strategy makes cwnd growth less aggressive once link saturation is reached, but does continue to increase cwnd (half as fast as TCP Reno) until the queue is full and congestive losses occur. When a packet loss does occur, TCP Veno uses its current value of N queue to attempt to distinguish between non-congestive and congestive loss, as follows: if N queue<ùõΩ, the loss is probably notdue to congestion; set cwnd = (4/5)cwnd if N queue¬•ùõΩ, the loss probably isdue to congestion; set cwnd = (1/2)cwnd as usual The idea here is that most router queues will have a total capacity much larger than ùõΩ, so a loss with fewer thanùõΩlikely does not represent a queue overÔ¨Çow. Note that, by comparison, TCP Westwood leaves cwnd unchanged if it thinks the loss is not due to congestion, and its threshold for making that determination is Nqueue=0. If TCP Veno encounters a series of non-congestive losses, the above rules make it behave like AIMD(1,0.8). 532 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 Per the analysis in 21.4 AIMD Revisited, this is equivalent to AIMD(2,0.5); this means TCP Veno will be about twice as aggressive as TCP Reno in recovering from non-congestive losses. This would provide a deÔ¨Ånite improvement in lossy-link situations with modest bandwidth delay products, but may not be enough to make a major dent in the high-bandwidth problem. 22.12 TCP Hybla TCP Hybla ([CF04]) has one very speciÔ¨Åc focus: to address the TCP satellite problem ( 4.4.2 Satellite Internet ) of very long RTTs. TCP Hybla selects a more-or-less arbitrary ‚Äúreference‚Äù RTT, called RTT 0, and attempts to scale TCP Reno so as to behave like a TCP Reno connection with an RTT of RTT 0. In the paper [CF04] the authors suggest RTT 0= 25ms. Suppose a TCP Reno connection has, at a loss event at time t 0, reducedcwnd tocwnd min. TCP Reno will then increment cwnd by 1 for each RTT, until the next loss event. This Reno behavior can be equivalently expressed in terms of the current time t as follows: cwnd = (t‚Äìt 0)/RTT +cwnd min What TCP Hybla does is to use the above formula after replacing the actual RTT (or RTT noLoad ) with RTT 0. Equivalently, TCP Hybla deÔ¨Ånes the ratio of the two RTTs as ùúö= RTT/RTT 0, and then after each windowful (each time interval of length RTT) increments cwnd byùúö2instead of by 1. In the event that RTT < RTT 0,ùúö is set to 1, so that short-RTT connections are not penalized. Becausecwnd now increases each RTT by ùúö2, which can be relatively large, there is a good chance that when the network ceiling is reached there will be a burst of losses of size ~ ùúö2. Therefore, TCP Hybla strongly recommends that the receiving end support SACK TCP, so as to allow faster recovery from multiple packet losses. Another recommended feature is the use of TCP Timestamps; this is a standard TCP option that allows the sender to include its own timestamp in each data packet. The receiver is to echo back the timestamp in the corresponding ACK, thus allowing more accurate measurement by the receiver of the actual RTT. Finally, to further avoid having these relatively large increments to cwnd result in multiple packet losses, TCP Hybla recommends some form of pacing to smooth out the actual transmission times. Rather than sending out four packets upon receipt of an ACK, for example, we might estimate the time T to the next transmission batch ( egwhen the next ACK arrives) and send the packets at intervals of T/4. At the time TCP Hybla was developed, pacing was poorly supported, but see 22.16 TCP BBR below, where pacing is essential. TCP Hybla applies a similar ùúö-fold scaling mechanism to threshold slow start, when a value for ssthresh is known. But the initial unbounded slow start is much more difÔ¨Åcult to scale. Scaling at that point would mean repeatedly doubling cwnd and sending out Ô¨Çights of packets, before any ACKs have been received; this would likely soon lead to congestive collapse. 22.13 DCTCP Unlike the other TCP Ô¨Çavors in this chapter, Data Center TCP (DCTCP) is intended for use only by connections starting and ending within the same datacenter. DCTCP is notmeant to be used on the Internet at large, 22.12 TCP Hybla 533
An Introduction to Computer Networks, Release 2.0.11 as it makes no pretense of competing fairly with TCP Reno. DCTCP was Ô¨Årst described in [AGMPS10], and is now also speciÔ¨Åed in RFC 8257. A datacenter is a highly specialized networking environment. Round-trip times on the Internet at large might be 50-100 ms, but round-trip times in a datacenter are usually well under 1 ms. Communicating nodes in a datacenter are under common management, and so there is no ‚Äúchicken and egg‚Äù problem regarding software installation: if a new TCP feature is desired, it can be made available everywhere. Finally, cost-saving is an issue: datacenters have lots of switches and routers, and cheaper models generally have smaller queue capacities. Even with a 1-ms RTT, though, a 10 Gbps connection can have a bandwidth delay product of 1.25 MB (800 packets); we would like to have queues be much smaller than this. Recall that TCP Reno can be categorized as AIMD(1,0.5) ( 21.4 AIMD Revisited ). The basic idea of DCTCP is to use AIMD(1, ùõΩ) for values of ùõΩmuch smaller than 0.5. As the window-size reduction on packet loss is 1‚ÄìùõΩ, this means that cwnd is relatively constant. If the transit capacity of a path is M, then the queue space needed to keep the minimum cwnd at M (and thus to keep the bottleneck link 100% utilized) is M ùõΩ/(1-ùõΩ) MùõΩifùõΩ0. This small ùõΩcomes at the price of out-competing TCP Reno by a large margin. By Exercise 3.0 of 21.10 Exercises, AIMD(1, ùõΩ) is equivalent in terms of fairness to AIMD( ùõº,0.5) for ùõº= (2‚Äì ùõΩ)/3ùõΩ, and by the argument in20.3.1 Example 2: Faster additive increase an AIMD( ùõº,0.5) connection out-competes TCP Reno by a factor of ùõº. For ùõΩ= 1/8 we have ùõº= 5. For connections within a datacenter we can achieve fairness by implementing DCTCP everywhere, but introduction of DCTCP in the outside world DCTCP would be highly uncooperative. The next step is to specify ùõΩ. For the moment, we will make a simplifying assumption that there is only one connection, and no other trafÔ¨Åc; in this case, the queue utilization increases by 1 for each RTT (once the queue becomes nonempty). We now determine ùõΩdynamically: we simply count the number D of RTTs before the queue is sufÔ¨Åciently full, and let ùõΩ= 1/2D. (In [AGMPS10] and RFC 8257, 1/D is denoted by ùõº, making ùõΩ=ùõº/2, but to avoid confusion with the ùõºin AIMD( ùõº,ùõΩ) we will write out the DCTCP ùõºasalpha when we return to it below.) D RTTsWmin WmaxD DCTCP tooth We can now relate D to cwnd and to the amplitude of cwnd variation. Let W maxdenote the maximum cwnd, and W minthe minimum. Making the usual large-window simplifying assumptions, we have Wmin+ D = W max, becausecwnd increases by 1 each RTT Wmin= W max(1‚Äì1/2D) 534 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 Eliminating W minand solving, we get W max= 2D2, or D =?(Wmax/2). Note that D is also the amplitude of the queue variation, assuming we keep the bottleneck link saturated, and so is the absolute minimum queue capacity needed. If the goal is keeping the queue small, this compares quite favorably to TCP Reno, in which D = W min= W max/2. Now let K represent the maximum queue capacity; the next step is to relate K and D. We need to ensure that we can avoid having K be much larger than D. We have W max= TC + K, where TC is the transit capacity of the link, that is, bandwidth delay. We can express the minimum queue utilization as Q min= K ‚Äì D = K ‚Äì?((TC+K)/2. If we choose K = TC, which is necessary with TCP Reno to avoid underutilized bandwidth, we certainly will have K much larger than D. However, to ensure Q min¬•0 we need K =?((TC+K)/2, or K2= TC/2 + K/2, which, because TC is relatively large (perhaps 800 packets), simply requires K just a bit larger than?(TC/2). That is, K need not be much larger than D. At this point, we can afford to be more concerned with K‚Äôs being too small, and thereby allowing intervals where the bottleneck link is idle. Empirically, a workable value for the queue capacity K is around 65 for 10 Gbps Ethernet, which is moderately above?(TC/2) but still very affordable. It is large enough that link utilization remains near 100%. We now need to address the simplifying assumption that there was only one connection. First, there might be N connections, quite possibly synchronized. This means that the queue variation is N D. It also means D will be somewhat smaller, though, as the total cwnd will be increasing N times faster. A more serious issue is that there is also a lotof other trafÔ¨Åc in a datacenter, so much so that queue utilization is dominated by a more-or-less random component. Instead of measuring when the queue utilization reaches a set level, we must measure when the average utilization reaches that level. DCTCP achieves this with a clever application of ECN ( 21.5.3 Explicit Congestion NotiÔ¨Åcation (ECN) ). The use of ECN to detect queue fullness, rather than packet drops, has the added advantage of avoiding packet losses and timeouts. Within a datacenter, DCTCP may very well rely on switch-based ECN rather than router-based. In normal ECN, once the receiver has seen a packet with the CE bit, it is supposed to mark ECE (CE echo) in all returning ACKs until the sender acknowledges having responded to the congestion through the use of the CWR bit. DCTCP modiÔ¨Åes this by having the receiver mark only ACKs to packets that arrive with CE set. This allows the sender to gauge the severity of congestion: if every other data packet has its CE bit set, then half the returning ACKs will be marked. (Delayed ACKs may complicate this, as the two data packets being acknowledged may have different CE marks, but mostly this is both infrequent and not serious, and in any event DCTCP recommends sending two separate ACKs with different ECE marks in such a case.) Classically, having every other data packet marked should never happen: all data packets arriving at the router before the queue capacity K is reached should be unmarked, and all packets arriving after K is reached should be marked. But due to the random queue Ô¨Çuctuations described in the previous paragraph, this allunmarked-then-all-marked pattern may be riddled with exceptions. What the DCTCP sender does is to measure the average marking rate, using an averaging interval at least as long as one ‚Äútooth‚Äù. If there are D‚Äì1 unmarked RTTs and 1 marked RTT, then the average marking rate should be 1/D. This is exactly what DCTCP looks for: once a signiÔ¨Åcant number of marked ACKs arrives, indicating that congestion is experienced, the DCTCP sender looks at its running average of the marked fraction, and takes that to be 1/D. (More precisely, DCTCP denotes by alpha the marked fraction, and sets D = 1/ alpha, and thenùõΩ= 1/2D =alpha /2.) DCTCP then reduces its cwnd by 1‚Äì ùõΩas above. The actual algorithm does not involve the queue capacity K, as a TCP sender is unlikely to know K. While it is not part of DCTCP proper, another common conÔ¨Åguration choice for intra-data-center connections is to reduce the minimum TCP retransmission timeout (RTO). The RTO value is computed adaptively, 22.13 DCTCP 535
An Introduction to Computer Networks, Release 2.0.11 as in 18.12 TCP Timeout and Retransmission, but is subject to a minimum. As late as 2011, RFC 6298 recommended (but did not require) a minimum RTO of 1.0 seconds, which is three orders of magnitude too large for a datacenter. There is no global Linux minimum-RTO conÔ¨Åguration setting, but this can be altered on a per-destination basis using the ip route command: ip route change to 10.1.2.0/24 via 10.0.2.1 dev eth0 rto_min 20ms The actual RTO values of current TCP connections can be viewed using the Linux command ss --info. On recent versions of Windows, a global minimum RTO can be set, for the custom template, using netsh interface tcp set supplemental template=custom minRto=20 22.13.1 TCP Incast There is one particular congestion issue, mostly but not entirely exclusive to datacenters, that DCTCP does not handle directly: the TCP incast problem. Imagine one node sending out multiple simultaneous queries to ‚Äúhelper‚Äù nodes, and expecting more-or-less-simultaneous responses. One example might be a request for a large data block that has been distributed over multiple Ô¨Åle-server systems; another might be a MapReduce request for calculation results. Either way, all the respondents may reply at about the same time, and all the responses may arrive together at the router and lead to queue overÔ¨Çow and packet loss. DCTCP (and any other TCP) cannot be of much help if each individual connection may be sending only one packet. This is one reason for having a slightly larger queue capacity than the DCTCP analysis alone might suggest. The TCP incast problem is made much worse when (as is often the case) the helper-node requests must be executed serially; we saw this issue before with RPC in 16.5.3 Serial Execution. Sometimes serialization requirements can be eliminated through careful design; sometimes they cannot. 22.14 H-TCP H-TCP, or TCP-Hamilton, is described in [LSL05]. Like Highspeed-TCP it primarily allows for faster growth ofcwnd; unlike Highspeed-TCP, the cwnd increment is determined not by the size of cwnd but by the elapsed time since the previous loss event. The threshold for accelerated cwnd growth is generally set to be 1.0 seconds after the most recent loss event. Using an RTT of 50 ms, that amounts to 20 RTTs, suggesting that when cwnd minis less than 20 then H-TCP behaves very much like TCP Reno. The speciÔ¨Åc H-TCP acceleration rule Ô¨Årst deÔ¨Ånes a time threshold t L. If t is the elapsed time in seconds since the previous loss event, then for t ¬§tLthe per-RTT window-increment ùõºis 1. However, for t>t Lwe deÔ¨Åne ùõº(t) = 1 + 10(t‚Äìt L) + (t‚Äìt L)2/4 We then increment cwnd byùõº(t) after each RTT, or, equivalently, by ùõº(t)/cwnd after each received ACK. At t=t L+1 seconds (nominally 2 seconds), ùõºis 12. The quadratic term dominates the linear term when t‚Äìt L > 40. If RTT = 50 ms, that is 800 RTTs. Even ifcwnd is very large, growth is at the same rate as for TCP Reno until t>t L; one consequence of this is that, at least in the Ô¨Årst second after a loss event, H-TCP competes fairly with TCP Reno, in the sense that 536 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 both increase cwnd at the same absolute rate. H-TCP starts ‚Äúfrom scratch‚Äù after each packet loss, and does not re-enter its ‚Äúhigh-speed‚Äù mode, even if cwnd is large, until after time t L. A full H-TCP implementation also adjusts the multiplicative factor ùõΩas follows (the paper [LSL05] uses ùõΩto represent what we denote by 1‚Äì ùõΩ). The RTT is monitored, as with TCP Vegas. However, the RTT increase is not used for per-packet or per-RTT adjustments; instead, these measurements are used after each loss event to update ùõΩso as to have 1‚ÄìùõΩ= RTT min/RTT max The value 1‚Äì ùõΩis capped at a maximum of 0.8, and at a minimum of 0.5. To see where the ratio above comes from, Ô¨Årst note that RTT minis the usual stand-in for RTT noLoad, and RTT maxis, of course, the RTT when the bottleneck queue is full. Therefore, by the reasoning in 8.3.2 RTT Calculations, equation 5, 1‚Äì ùõΩis the ratio transit_capacity / (transit_capacity + queue_capacity). At a congestion event involving a single uncontested Ô¨Çow we have cwnd = transit_capacity + queue_capacity, and so after reducing cwnd to (1‚Äì ùõΩ)cwnd, we havecwnd new= transit_capacity, and hence (as in 19.7 TCP and Bottleneck Link Utilization ) the bottleneck link will remain 100% utilized after a loss. The cap on 1‚Äì ùõΩof 0.8 means that if the queue capacity is smaller than a quarter of the transit capacity then the bottleneck link willexperience some idle moments. When ùõΩis changed, H-TCP also adjusts ùõºtoùõº1= 2ùõΩùõº(t) so as to improve fairness with other H-TCP connections with different current values of ùõΩ. 22.15 TCP CUBIC TCP Cubic attempts, like Highspeed TCP, to solve the problem of efÔ¨Åcient TCP transport when bandwidthdelay is large. TCP Cubic allows very fast window expansion; however, it also makes attempts to slow the growth of cwnd sharply ascwnd approaches the current network ceiling, and to treat other TCP connections fairly. Part of TCP Cubic‚Äôs strategy to achieve this is for the window-growth function to slow down (become concave) as the previous network ceiling is approached, and then to increase rapidly again (become convex) if this ceiling is surpassed without losses. This concave-then-convex behavior mimics the graph of the cubic polynomial cwnd = t3, hence the name (TCP Cubic also improves an earlier TCP version known as TCP BIC). ab y = (xa)3 + b 22.15 TCP CUBIC 537
An Introduction to Computer Networks, Release 2.0.11 As mentioned above, TCP Cubic is currently (2013) the default Linux congestion-control implementation. TCP Cubic was originally documented in [HRX08]; it is now speciÔ¨Åed in RFC 8312. TCP Cubic has a number of interrelated features, in an attempt to address several TCP issues: 
- Reduction in RTT bias 
- TCP Friendliness when most appropriate 
- Rapid recovery of cwnd following its decrease due to a loss event, maximizing throughput 
- Optimization for an unchanged network ceiling (corresponding to cwnd max) 
- Rapid expansion of cwnd when a raised network ceiling is detected The eponymous cubic polynomial y=x3, appropriately shifted and scaled, is used to determine changes in cwnd. No special algebraic properties of this polynomial are used; the point is that the curve, while steadily increasing, is Ô¨Årst concave and then convex; the authors of [HRX08] write ‚Äú[t]he choice for a cubic function is incidental and out of convenience‚Äù. This y=x3polynomial has an inÔ¨Çection point at x=0 where the tangent line is horizontal; this is the point where the graph changes from concave to convex. We start with the basic outline of TCP Cubic and then consider some of the bells and whistles. We assume a loss has just occurred, and let W maxdenote the value of cwnd at the point when the loss was discovered. TCP Cubic then sets cwnd to 0.7Wmax; that is, TCP Cubic uses ùõΩ= 0.3 (originally ùõΩwas 0.2, but it has been adjusted to improve convergence to equilibrium between two Ô¨Çows). The corresponding ùõºfor TCPFriendly AIMD( ùõº,ùõΩ) would be ùõº=0.529, but TCP Cubic uses this ùõºonly in its TCP-Friendly adjustment, below. We now deÔ¨Åne a cubic polynomial W(t), a shifted and scaled version of w=t3. The parameter t represents the elapsed time since the most recent loss, in seconds. At time t>0 we set cwnd = W(t). The polynomial W(t), and thus the cwnd rate of increase, as in TCP Hybla, is no longer tied to the connection‚Äôs RTT; this is done to reduce the RTT bias that is so deeply ingrained in TCP Reno. We want the function W(t) to pass through the point representing the cwnd just after the loss, that is, xt,Wy =x0,0.7Wmaxy. We also want the inÔ¨Çection point to lie on the horizontal line y=W max. To fully determine the curve, it is at this point sufÔ¨Åcient to specify the value of t at this inÔ¨Çection point; that is, how far horizontally W(t) must be stretched. This horizontal distance from t=0 to the inÔ¨Çection point is represented by the constant K in the following equation; W(t) returns to its pre-loss value W maxat t=K. C is a second constant. W(t) = C(t‚ÄìK)3+ W max It sufÔ¨Åces algebraically to specify either C or K; the two constants are related by the equation obtained by plugging in t=0. K changes with each loss event, but it turns out that the value of C can be constant, not only for any one connection but for all TCP Cubic connections. TCP Cubic speciÔ¨Åes for C the ad hoc value 0.4; we can then set t=0 and, with a bit of algebra, solve to obtain K = (W max/2)1/3seconds If W max= 250, for example, K=5; if RTT = 100 ms, this is 50 RTTs. When each ACK arrives, TCP Cubic records the arrival time t, calculates W(t), and sets cwnd = W(t). At the next packet loss the parameters of W(t) are updated. 538 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 If the network ceiling does not change, the next packet loss will occur when cwnd again reaches the same Wmax; that is, at time t=K after the previous loss. As t approaches K and the value of cwnd approaches Wmax, the curve W(t) Ô¨Çattens out, so cwnd increases slowly. This concavity of the cubic curve, increasing rapidly but Ô¨Çattening near W max, achieves two things. First, throughput is boosted by keeping cwnd close to the available path transit capacity. In 19.7 TCP and Bottleneck Link Utilization we argued that if the path transit capacity is large compared to the bottleneck queue capacity (and this is the case for which TCP Cubic was designed), then TCP Reno averages 75% utilization of the available bandwidth. The bandwidth utilization increases linearly from 50% just after a loss event to 100% just before the next loss. In TCP Cubic, the initial rapid rise in cwnd following a loss means that the average will be much closer to 100%. Another important advantage of the Ô¨Çattening is that whencwnd is Ô¨Ånally incremented to the point of loss, it likely is just over the network ceiling; the connection has an excellent chance that only one or two packets are lost rather than a large burst. This facilitates the NewReno Fast Recovery algorithm, which TCP Cubic still uses if the receiver does not support SACK TCP. Once t>K, W(t) becomes convex, and in fact begins to increase rapidly. In this region, cwnd > W max, and so the sender knows that the network ceiling has increased since the previous loss. The TCP Cubic strategy here is to probe aggressively for additional capacity, increasing cwnd very rapidly until the new network ceiling is encountered. The cubic increase function is in fact quite aggressive when compared to any of the other TCP variants discussed here, and time will tell what strategy works best. As an example in which the TCP Cubic approach seems to pay off, let us suppose the current network ceiling is 2,000 packets, and then (because competing connections have ended) increases to 3,000. TCP Reno would take 1,000 RTTs for cwnd to reach the new ceiling, starting from 2,000; if one RTT is 50 ms that is 50 seconds. To Ô¨Ånd the time t-K that TCP Cubic will need to increase cwnd from 2,000 to 3,000, we solve 3000 = W(t) = C (t‚ÄìK)3+ 2000, which works out to t-K 13.57 seconds (recall 2000 = W(K) here). The constant C=0.4 is determined empirically. The cubic inÔ¨Çection point occurs at t = K = (W maxùõΩ/C)1/3. A larger C reduces the time K between the a loss event and the next inÔ¨Çection point, and thus the time between consecutive losses. If W max= 2000, we get K=10 seconds when ùõΩ=0.2 and C=0.4. If the RTT were 50 ms, 10 seconds would be 200 RTTs. For TCP Reno, on the other hand, the interval between adjacent losses is W max/2 RTTs. If we assume a speciÔ¨Åc value for the RTT, we can compare the Reno and Cubic time intervals between losses; for an RTT of 50 ms we get Wmax Reno Cubic 2000 50 sec 10 sec 250 6.2 sec 5 sec 54 1.35 sec 3 sec For smaller RTTs, the basic TCP Cubic strategy above runs the risk of being at a competitive disadvantage compared to TCP Reno. For this reason, TCP Cubic makes a TCP-Friendly adjustment in the windowsize calculation: on each arriving ACK, cwnd is set to the maximum of W(t) and the window size that TCP Reno would compute. The TCP Reno calculation can be based on an actual count of incoming ACKs, or be based on the formula (1ùõΩ)Wmax+ùõºt/RTT. Note that this adjustment is only ‚Äúhalf-friendly‚Äù: it guarantees that TCP Cubic will not choose a window size smaller than TCP Reno‚Äôs, but places no restraints on the choice of a larger window size. Broadly speaking, however, in the range of smaller bandwidth delay products where TCP Reno performs well, TCP Cubic relies on its TCP-Friendly adjustment to keep up; that is, its ‚Äúnative‚Äù window increase is less aggressive than 22.15 TCP CUBIC 539
An Introduction to Computer Networks, Release 2.0.11 TCP Reno‚Äôs. TCP Cubic is only supposed to be more aggressive than TCP Reno in settings where the latter is not aggressive enough. A consequence of the TCP-Friendly adjustment is that, on networks with modest bandwidth delay products, TCP Cubic behaves exactly like TCP Reno. TCP Cubic also has a provision to detect if a given W maxislower than the previous value, suggesting increasing congestion; in this situation, cwnd is lowered by an additional factor of 1‚Äì ùõΩ/2. This is known as fast convergence, and helps TCP Cubic adapt more quickly to reductions in available bandwidth. Another way to gain a sense of how TCP Cubic compares with TCP Reno is to look at the rate of cwnd increase, per unit time. If W is the current window size, we can express this rate as W/t. For TCP Reno, if we take t to be the time between successive ACKs (as determined by the bottleneck bandwidth), W is 1/W. For TCP Cubic, we will estimate W/t by the derivative (from calculus) dW/dt of the formula W(t) = C(t‚ÄìK)3+ W max, as above. This means dW/dt = 3C(t-K)2. In order to get numbers we can actually compare, we need to look at speciÔ¨Åc scenarios in which we can evaluate K. At the left edge of the Cubic tooth, t=0. For the Ô¨Årst scenario, suppose the bandwidth is 1 packet/ms, and the delay is 50 ms, making the bandwidthdelay product 50 packets. We will also assume a queue capacity of 50 packets, so W max= 100. This means that for TCP Reno, W increases by 1/50 per ms at the left edge of the tooth, where W = 50; the window growth rate W/t is then (1/50)/(1/1000) = 20 packets/sec. At the right edge of the tooth, where W = 100, W/t = (1/100)/(1/1000) = 10 packets/sec. For TCP Cubic, we use the formula above, and recall C = 0.4, so W/t1.2(t-K)2. At the left edge of the tooth, where t=0, this evaluates to 1.2K2. Recalling that K = (W max/2)1/3, we get K=3.68, and so W/t16 packets/sec. This is comparable to TCP Reno. (At the inÔ¨Çection point of the cubic curve, though, where t=K, we always get W/t = 0.) Now let‚Äôs switch to a second, higher-bandwidth scenario, where the bottleneck bandwidth is increased to 100 packets/ms, the delay is again 50 ms, and the queue capacity is 3000 packets. The bandwidth delay product is now 5000 packets, and so W max= 5000 + 3000 = 8,000. As before, for TCP Reno W/t ranges from (1/4,000)/(1/100,000) = 25 at the left edge of the tooth to 12.5 at the right edge; that is, the rate of cwnd growth is not much different from what it was in the Ô¨Årst scenario. For TCP Cubic, on the other hand, we now have K = 40001/316, and so, at the left edge of the tooth where t=0, we have W/t1.2(K)2 300. That is, cwnd is increasing at a rate of 300 packets/sec, which is twelve times more aggressive than TCP Reno. The following graph is taken from [RX05], and shows TCP Cubic connections competing with each other and with TCP Reno. 540 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 The diagram shows four connections, all with the same RTT. Two are TCP Cubic and two are TCP Reno. The red connection, cubic-1, was established and with a maximum cwnd of about 4000 packets when the other three connections started. Over the course of 200 seconds the two TCP Cubic connections reach a fair equilibrium; the two TCP Reno connections reach a reasonably fair equilibrium with one another, but it is much lower than that of the TCP Cubic connections. On the other hand, here is a graph from [LSM07], showing the result of competition between two Ô¨Çows using an earlier version of TCP Cubic over a low-speed connection. One connection has an RTT of 160ms and the other has an RTT a tenth that. The bottleneck bandwidth is 1 Mbit/sec, meaning that the bandwidth delay product for the 160ms connection is 13-20 packets (depending on the packet size used). 22.15 TCP CUBIC 541
An Introduction to Computer Networks, Release 2.0.11 Note that the longer-RTT connection (the solid line) is almost completely starved, once the shorter-RTT connection starts up at T=100. This is admittedly an extreme case, and there have been more recent Ô¨Åxes to TCP Cubic, but it does serve as an example of the need for testing a wide variety of competition scenarios. 22.16 TCP BBR TCP BBR returns to the central idea of TCP Vegas: to measure the available bandwidth and RTT min, and to base the number of in-Ô¨Çight packets on the measured bandwidth delay product. ‚ÄúBBR‚Äù here stands for Bottleneck Bandwidth and RTT; it is described in [CGYJ16] and in an Internet Draft. There are some large differences from TCP Vegas, however; ultimately, these differences enable TCP BBR to compete reasonably fairly with TCP Reno. One important difference is that TCP BBR does not engage in the high-precision monitoring of RTT for increases above RTT noLoad. As a result, TCP BBR does not Ô¨Åt the TCP Vegas delaybased congestion-control model; it is for that reason sometimes referred to as congestion-based congestion control. TCP BBR is, in practice, rate-based rather than window-based; that is, at any one time, TCP BBR sends at a given calculated rate, instead of sending new data in direct response to each received ACK. Each arriving ACK does potentially update the current rate, much as each arriving ACK in TCP Reno slides the sender‚Äôs window forwards; however, the connection between arriving ACKs and new data transmissions is decidedly indirect. Rate-based sending requires some form of pacing support by the underlying LAN layer, so that packets 542 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 can be sent at equal time intervals. On a 10 Gbps link, this time interval can be as small as a microsecond; conventional timers don‚Äôt work well at these time scales. Linux TCP BBR implementations generally use the pacing support built into the so-called Fair Queuing (FQ) queuing discipline (which is not actually a true Fair Queuing implementation in the sense of 23.5 Fair Queuing ). Throughout the lifetime of a connection, TCP BBR maintains an estimate for RTT min, which is nominally the stand-in for RTT noLoad except that it may go up in the presence of competition; see below. TCP BBR also maintains a current bandwidth estimate, which we denote BWE. As with TCP Vegas, BWE is much more volatile than RTT minas it better reÔ¨Çects the current degree of bandwidth competition. After each RTT, TCP BBR records the throughput during that RTT; BWE is then the maximum of the last ten per-RTT throughput measurements. That BWE is the maximum rate recorded over the past ten RTTs, rather than the average, will be important below. The fundamental congestion indicators for TCP BBR are changes to its BWE and RTT minestimates; packet losses are not used directly as evidence of congestion. As we shall see below, TCP BBR reduces its sending rate in response to decreases in BWE; this is TCP BBR‚Äôs primary congestion response. When losses do occur, TCP BBR does enter a recovery mode, but it is much less conservative than TCP Reno‚Äôs halving ofcwnd. TCP BBR‚Äôs initial response to a loss is to limit the number of packets in Ô¨Çight (FlightSize) to the number currently in Ô¨Çight, which allows it to continue to send new data at the rate of arriving ACKs. This is not necessarily a reduction in FlightSize, and, if it is, FlightSize may be allowed to grow, even if additional losses are discovered. Overall, this strategy is quite effective at handling non-congestive losses without losing throughput. In its core state, known as PROBE_BW, TCP BBR continually updates BWE as above and then sets its base sending rate to BWE. It then sets its cwnd target (or, more properly, its FlightSize target, as losses may have occurred) to 2 BWERTT min. This results in a bottleneck queue utilization equal to the transit capacity. If the actual available bandwidth does not change, then sending at rate BWE will send new packets at exactly the rate of returning ACKs and so FlightSize will not change. TCP BBR does allow for faster initial growth (see STARTUP mode, below) to reach the FlightSize target. If the actual available bandwidth falls, BWE will not reÔ¨Çect that for ten RTTs. As a result, TCP BBR may for a while send faster than the rate of returning ACKs. If this happens, the bottleneck queue utilization will rise. Eventually, BWE will fall to match the rate of returning ACKs. Similarly, if the actual available bandwidth rises, queue utilization will fall. However, it will not fall to zero ‚Äì and so cause sending to starve ‚Äì in a single RTT unless the bandwidth doubles, and after that the increased bandwidth will be reÔ¨Çected in the updated BWE. TCP BBR must, like every TCP Ô¨Çavor, regularly probe to see if additional bandwidth is available. TCP BBR does this by periodically (currently every eight RTTs, where RTT is measured as RTT min) increasing its sending rate by an additional factor of 1.25; that is, it sets a variable pacing_gain to 1.25 and sends at the new rate pacing_gain BWE. The increase lasts one RTT interval. Ifthere was no competition, and if the bottleneck link was fully utilized, this pacing_gain increase results in no change to BWE. All that happens is that the queue builds up, and the 1.25-fold larger Ô¨Çight of packets results in an RTT that is also 1.25 times larger. In the next RTT interval, TCP BBR sets pacing_gain to 0.75, which causes the newly created additional queue to dissipate. After that it resumes its regular rate, that is, with pacing_gain = 1.0, for the next six RTT intervals. Consider, however, what happens if TCP BBR is competing, perhaps with TCP Reno. Increasing the sending rate by a factor of 1.25 now results in greater queue (or bottleneck link) utilization, which results in an immediate increase in BWE for that RTT. At this point, recall that BWE is the maximum of the last ten per-RTT measurements; the end result is that BWE is set to this elevated value for the next ten RTTs. In 22.16 TCP BBR 543
An Introduction to Computer Networks, Release 2.0.11 the following RTT, pacing_gain drops to 0.75 as before, but this time TCP BBR has measured a larger BWE, and this change to BWE persists. Here is a concrete example of BWE increase. To simplify the analysis, we will assume TCP BBR‚Äôs FlightSize is BWERTT min, dropping the factor of 2. Suppose a TCP BBR connection and a TCP Reno connection share a bottleneck link with a bandwidth of 2 packets/ms. The RTT min(= RTT noLoad ) of each connection is 80 ms, making the transit capacity 160 packets. Finally, suppose that each connection has 80 packets in Ô¨Çight, exactly Ô¨Ålling the transit capacity but with no queue utilization (so RTT min= RTT actual). Over the course of the eight-RTT pacing_gain cycle, the Reno connection‚Äôs cwnd rises by 8, to 88 packets. This means the total queue utilization is now 8 packets, divided on average between BBR and Reno in the proportion 80 to 88. Now the BBR cycle with pacing_gain =1.25 arrives; for the next RTT, the BBR connection has 80 1.25 = 100 packets in Ô¨Çight. The total number of packets in Ô¨Çight is now 188. The RTT climbs to 188/2 = 94 ms, and the next BBR BWE measurement is 100 packets in 94 ms, or 1.064 packets/ms (the precise value may depend on exactly when the measurement is recorded). For the following RTT, pacing_gain drops to 0.75, but the higher BWE persists. For the rest of the pacing_gain cycle, TCP BBR calculates a base rate corresponding to 1.064 80 = 85 packets in Ô¨Çight per RTT, which is close to the TCP Reno cwnd. See also exercise 14.0. TCP BBR also has another mechanism, arguably more important in the long run, for maintaining its fair share of the bandwidth. Periodically (every ~10 seconds), TCP BBR connections re-measure RTT min, entering PROBE_RTT mode. In this state the number of packets in Ô¨Çight drops to four, and stays there for at least one RTT actual as measured for these four packets (with a minimum of 200 ms). Afterwards the connection returns to PROBE_ BW mode with a freshly estimated RTT min. The value of BWE is picked up where it was left off, so that if RTT minincreases, then so does the sending rate BWE RTT min. A certain amount of potential throughput is ‚Äúwasted‚Äù during these PROBE_RTT intervals, but as they amount to ~200 ms out of every 10 sec, or 2%, the impact is negligible. If, during the PROBE_RTT mode, competing connections keep some packets in the bottleneck queue, then the queuing delay corresponding to those packets will be incorporated into the new RTT minmeasurement; because of this, RTT minmay signiÔ¨Åcantly exceed RTT noLoad and thus cause TCP BBR to send at a more competitive rate. Suppose, for example, that in the BBR-vs-Reno scenario above, Reno has gobbled up a total of 240 spots in the bottleneck queue, thus increasing the RTT for both connections to (240+80)/2 = 160. During a PROBE_RTT cycle, TCP BBR will drop its link utilization essentially to zero, but TCP Reno will still have 240 packets in transit, so TCP BBR will measure RTT minas 240/2 = 120 ms. After the PROBE_RTT phase is over, TCP BBR will increase its sending rate by 50% over what it had been when RTT minwas 80. Note that, in any one RTT, we can either measure bottleneck bandwidth orRTT, but not both. If the number of packets in Ô¨Çight is larger than the transit capacity then the packet return rate reÔ¨Çects the bottleneck bandwidth. Conversely, we can measure RTT minonly if the number of packets in Ô¨Çight is smaller than the transit capacity. When a connection is Ô¨Årst opened, a TCP BBR connection is in STARTUP mode, which is similar to TCP Reno‚Äôs slow start. In this mode, pacing_gain is 2.89 (2/log(2)) consistently, which leads to exponential growth of the number of packets in Ô¨Çight. STARTUP mode ends when an additional RTT yields no improvement in BWE. At this point TCP BBR has overÔ¨Ålled the queue substantially (just as a TCP Reno connection does in slow start), and so the connection enters DRAIN mode to reduce the queue. This is accomplished by settingpacing_gain = 1/2.89. The connection transitions from DRAIN to PROBE_RTT when the number of packets in Ô¨Çight drops to 2 BWERTT min. 544 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 Below is a diagram of TCP BBR competing with TCP Reno in a setting where the bottleneck queue capacity is eight times the bandwidth delay product, which is 40 ms 10 Mbps = 50 KB. It was produced using the Mininet network emulator, 30.7.6 TCP Competition: Reno vs BBR. The large queue capacity was contrived speciÔ¨Åcally to be beneÔ¨Åcial to TCP Reno, in that in a similar setting with a queue capacity approximately equal to the bandwidth delay product TCP BBR often ends up quite a bit ahead of TCP Reno. Such large queues are, however, a not-uncommon real-world situation on high-capacity backbone links ( 21.5.1 Bufferbloat ). Acting alone, Reno‚Äôs cwnd would range between 4.5 and 9 times the bandwidthdelay product, which works out to keeping the queue over 70% full on average. The lower part of the diagram shows each connection‚Äôs share of the 10 Mbps (1.25 MBps) bottleneck bandwidth. The upper part shows the number of packets ‚Äúin Ô¨Çight‚Äù (for TCP Reno, outside of Fast Recovery, that is of course cwnd ). The Reno sawtooth pattern is clearly visible. A dominant feature of the graph is the spikes every 10 seconds (down for BBR, correspondingly up for Reno) caused by TCP BBR‚Äôs periodic PROBE_RTT mode. 0500 KBps1000 KBps 0 50 100 150 200 250 300Reno BBR For the Ô¨Årst ten seconds, TCP Reno does indeed run away with all the bandwidth. But after the Ô¨Årst PROBE_RTT event TCP BBR begins to catch up, and the two tie at around T=60 seconds. After that Reno mostly stays a little ahead of TCP BBR, typically with about 58% of the bandwidth versus BBR‚Äôs 42%, but the point here is that, even in circumstances favorable to Reno, BBR does not collapse. It is evident from the graph, particularly during the Ô¨Årst 60 seconds, that the PROBE_RTT intervals do not lead to sudden jumps in throughput. Almost all of the change in throughput occurs during the PROBE_BW intervals. That said, it is the PROBE_RTT interval at T=10 that triggers the ensuing turnaround in throughput. In addition to the sharp PROBE_RTT spikes every 10 seconds, we also see smaller spikes at a rate of about 6 every 10 seconds. These represent the pacing-gain cycling within BBR‚Äôs PROBE_BW phase. If eight RTT mintimes amount to 10/6 seconds, then RTT minmust be about 200 ms. When the queue is completely full, RTT actual is 940 ms = 360 ms, but during TCP BBR‚Äôs PROBE_RTT cycles RTT actual does indeed drop considerably, which accounts for the 200 ms value. This value is then used as RTT minfor the next ten seconds. 22.16 TCP BBR 545
An Introduction to Computer Networks, Release 2.0.11 Experimental results in [CGYJ16] indicate that TCP BBR has been much more successful than TCP Cubic in addressing the high-bandwidth TCP problem on parts of Google‚Äôs network. This is presumably because TCP BBR does not necessarily reduce throughput at all when faced with occasional non-congestive losses. 22.17 Epilog TCP Reno‚Äôs core congestion algorithm is based on algorithms in Jacobson and Karel‚Äôs 1988 paper [JK88], now (2017) approaching thirty years old. There are concerns both that TCP Reno uses too much bandwidth (the greediness issue) and that it does not use enough (the high-bandwidth-TCP problem). There are also broad changes in TCP usage patterns. Twenty years ago, the vast majority of all TCP trafÔ¨Åc represented downloads from ‚Äúmajor‚Äù servers. Today, over half of all Internet TCP trafÔ¨Åc is peer-to-peer rather than server-to-client. The rise in online video streaming creates new demands for excellent TCP real-time performance. So which TCP version to use? That depends on circumstances; some of the TCPs above are primarily intended for relatively speciÔ¨Åc environments; for example, TCP Hybla for satellite links and TCP Veno for mobile devices (including wireless laptops). If the sending and receiving hosts are under common management, and especially if intervening trafÔ¨Åc patterns are relatively stable, one can run a few simple throughput-comparison experiments to Ô¨Ånd which TCP version works best. But there are two problems with this experimental approach. First, intervening trafÔ¨Åc patterns are often not stable; a TCP version that worked well in one trafÔ¨Åc environment might perform poorly in another. TCP Vegas, after all, does well in a Vegas-only environment; problems arise only when there is competing TCP Reno trafÔ¨Åc, or the equivalent. Second, and perhaps more seriously, the best-performing TCP version might achieve its throughput at the expense of other users‚Äô TCP trafÔ¨Åc. As a simple example, consider the effect of simply increasing the TCP Reno additive-increase value, perhaps from AIMD(1,0.5) to AIMD( 10,0.5). As we saw in 20.3.1 Example 2: Faster additive increase, this gives the faster-incrementing TCP an unfair (in fact tenfold) advantage. If the goal is to Ô¨Ånd a TCP version that allusers will be happy with, this will not be effective. Then there is the question of what TCP to use on a server that is serving up large volumes of data, to a range of disparate hosts and with a wide variety of competing-trafÔ¨Åc scenarios. Here, experimentation is even more difÔ¨Åcult. Many trials will be needed to determine reliably which TCP version works best in the most cases, even ignoring the impact on competing trafÔ¨Åc. These issues suggest a need for continued research into how to update and improve TCP, and Internet congestion-management generally. Finally, while most new TCPs are designed to hold their own in a Reno world, there is some question that perhaps we would all be better off with a radical rather than incremental change. Might TCP Vegas be a better choice, if only the queue-grabbing greediness of TCP Reno could be restrained? Questions like these are today entirely hypothetical, but it is not impossible to envision an Internet backbone that implemented non-FIFO queuing mechanisms ( 23 Queuing and Scheduling ) that fundamentally changed the rules of the game. 546 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 22.18 Exercises 1.0. How would TCP Vegas respond if it estimated RTT noLoad = 100ms, with a bandwidth of 1 packet/ms, and then due to a routing change the RTT noLoad increased to 200ms without changing the bandwidth? What cwnd would be chosen? Assume no competition from other senders. 2.0. Suppose a TCP Vegas connection from A to B passes through a bottleneck router R. The RTT noLoad is 50 ms and the bottleneck bandwidth is 1 packet/ms. (a). If the connection keeps 4 packets in the queue ( egùõº=3,ùõΩ=5), what will RTT actual be? What value of cwnd will the connection choose? What will be the value of BWE? (b). Now suppose a competing (non-Vegas) connection keeps 6 packets in the queue to the Vegas connection‚Äôs 4, eventually meaning that the other connection will have 60% of the bandwidth. What will be the Vegas connection‚Äôs steady-state values for RTT actual,cwnd and BWE? 3.0. Suppose a TCP Vegas connection has R as its bottleneck router. The transit capacity is M, and the queue utilization is currently Q>0 (meaning that the transit path is 100% utilized, although not necessarily by the TCP Vegas packets). The current TCP Vegas cwnd iscwnd V. Using the formulas from 8.3.2 RTT Calculations, show that the number of packets TCP Vegas calculates are in the queue, queue_use, is queue_use =cwnd VQ/(Q+M) 4.0. Suppose that at time T=0 a TCP Vegas connection and a TCP Reno connection share the same path, and each has 100 packets in the bottleneck queue, exactly Ô¨Ålling the transit capacity of 200. TCP Vegas uses ùõº=1,ùõΩ=2. By the previous exercise, in any RTT with cwnd VTCP Vegas packets and cwnd RTCP Reno packets in Ô¨Çight and cwnd V+cwnd R>200, N queue iscwnd V/(cwnd V+cwnd R) multiplied by the total queue utilizationcwnd V+cwnd R‚Äì200. Continue the following table, where T is measured in RTTs, up through the next two RTTs where cwnd V isnotdecremented; that is, Ô¨Ånd the next two rows where the TCP Vegas queue share is less than 2. (After each of these RTTs, cwnd Vis not decremented.) This can be done either with a spreadsheet or by simple algebra. Note that the TCP Reno cwnd Rwill always increment. Tcwnd Vcwnd RTCP Vegas queue share 0100 100 0 1101 101 1 2102 102 2 3101 103 (101/204)x4 = 1.980 < ùõΩ 4101 104 Vegas has (101/205) 5 = 2.463 packets in queue 5100 105 Vegas has (100/205) 5 = 2.435 packets in queue 699 106 (99/205)5 = 2.439 This exercise attempts to explain the linear decrease in the TCP Vegas graph in the diagram in 31.5 TCP Reno versus TCP Vegas. Competition with TCP Reno means not only that cwnd Vstops increasing, but in fact it decreases by 1 most RTTs. 22.18 Exercises 547
An Introduction to Computer Networks, Release 2.0.11 5.0. Suppose that, as in the previous exercise, a FAST TCP connection and a TCP Reno connection share the same path, and at T=0 each has 100 packets in the bottleneck queue, exactly Ô¨Ålling the transit capacity of 200. The FAST TCP parameter ùõæis 0.5. The FAST TCP and TCP Reno connections have respective cwnd s ofcwnd Fandcwnd R. You may use the fact that, as long as the queue is nonempty, RTT/RTT noLoad = (cwnd F+cwnd R)/200. Find the value of cwnd Fat T=40, where T is counted in units of 20 ms until T = 40, using ùõº=4,ùõº=10 and ùõº=30. Assume RTT 20 ms as well. Use of a spreadsheet is recommended. The table here uses ùõº=10. Tcwnd Fcwnd R 0100 100 1105 101 2108.47 102 3110.77 103 4112.20 104 6.0. Suppose A sends to B as in the layout below. The packet size is 1 kB and the bandwidth of the bottleneck R‚ÄìB link is 1 packet / 10 ms; returning ACKs are thus normally spaced 10 ms apart. The RTT noLoad for the A‚ÄìB path is 200 ms. ARB C However, large amounts of trafÔ¨Åc are also being sent from C to A; the bottleneck link for that path is R‚ÄìA with bandwidth 1 kB / 5 ms. The queue at R for the R‚ÄìA link has a capacity of 40 kB. ACKs are 50 bytes. (a). What is the maximum possible arrival time difference on the A‚ÄìB path for ACK[0] and ACK[20], if there are no queuing delays at R in the A √ëB direction? ACK[0] should be forwarded immediately by R; ACK[20] should have to wait for 40 kB at R (b). What is the minimum possible arrival time difference for the same ACK[0] and ACK[20]? 7.0. Suppose a TCP Veno and a TCP Reno connection compete along the same path; there is no other trafÔ¨Åc. Both start at the same time with cwnd s of 50; the total transit capacity is 160. Both share the next loss event. The bottleneck router‚Äôs queue capacity is 60 packets; sometimes the queue Ô¨Ålls and at other times it is empty. TCP Veno‚Äôs parameter ùõΩis zero, meaning that it shifts to a slower cwnd increment as soon as the queue just begins Ô¨Ålling. (a). In how many RTTs will the queue begin Ô¨Ålling? (b). At the point the queue is completely Ô¨Ålled, how much larger will the Reno cwnd be than the Veno cwnd ? 8.0. Suppose two connections use TCP Hybla. They do not compete. The Ô¨Årst connection has an RTT of 100 ms, and the second has an RTT of 1000 ms. Both start with cwnd min= 0 (literally meaning that nothing 548 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 is sent the Ô¨Årst RTT). (a). How many packets are sent by each connection in four RTTs (involving three cwnd increases)? (b). How many packets are sent by each connection in four seconds? Recall 1+2+3+.. . +N = N(N+1)/2. 9.0. Suppose that at time T=0 a TCP Illinois connection and a TCP Reno connection share the same path, and each has 100 packets in the bottleneck queue, exactly Ô¨Ålling the transit capacity of 200. The respective cwnd s arecwnd Iandcwnd R. The bottleneck queue capacity is 100. Find the value of cwnd Iat T=50, where T is the number of elapsed RTTs. At this point cwnd Ris, of course, 150. Tcwnd Icwnd R 0100 100 1101 101 2? 102 You may assume that the delay, RTT ‚Äì RTT noLoad, is proportional to queue_utilization = cwnd I+cwnd R‚Äì 200ùõº. Using this expression to represent delay, delay max= 100 and so delay thresh = 1. When calculating ùõº(delay), assume ùõºmax= 10 and ùõºmin= 0.1. 10.0. Assume that a TCP connection has an RTT of 50 ms, and the time between loss events is 10 seconds. (a). For a TCP Reno connection, what is the bandwidth delay product? (b). For an H-TCP connection, what is the bandwidth delay product? 11.0. For each of the values of W maxbelow, Ô¨Ånd the change in TCP Cubic‚Äôs cwnd over one 100 ms RTT at each of the following points: i. Immediately after the previous loss event, when t = 0. ii. At the midpoint of the tooth, when t=K/2 iii. At the point when cwnd has returned to W max, at t=K (a). W max= 250 (making K=5) (b). W max= 2000 (making K=10) 12.0. Suppose a TCP Reno connection is competing with a TCP Cubic connection. There is no other trafÔ¨Åc. All losses are synchronized. In this setting, once the steady state is reached, the cwnd graphs for one tooth will look like this: 22.18 Exercises 549
An Introduction to Computer Networks, Release 2.0.11 r c0.8 c 0.5 r One tooth, TCP Cubic v TCP Reno(r/2)√óRTT (c/2)1/3 Let c be the maximum cwnd of the TCP Cubic connection (c=W max) and let r be the maximum of the TCP Reno connection. Let M be the network ceiling, so a loss occurs when c+r reaches M. The width of the tooth for TCP Reno is (r/2) RTT, where RTT is measured in seconds; the width of the TCP Cubic tooth is (c/2)1/3. For the examples here, ignore the TCP-Friendly feature of TCP Cubic. (a). If M = 200 and RTT = 50 ms = 0.05 sec, show that at the steady state r 130.4 and c = M‚Äìr 69.6. (b). Find equilibrium r and c (to the nearest integer) for M=1000 and RTT = 50 ms. Hint: use of a spreadsheet or scripting language makes trial-and-error quite practical. (c). Find equilibrium r and c for M = 1000 and RTT = 100 ms. 13.0. Suppose a TCP Westwood connection has the path A R1 R2 B. The R1‚ÄìR2 link is the bottleneck, with bandwidth 1 packet/ms, and RTT noLoad is 200 ms. At T=0, with cwnd = 300 so the queue at R1 has 100 A‚ÄìB packets, the R1 R2 throughput for A‚Äôs packets falls to 1 packet / 2 ms, perhaps due to competition. At that same time, and perhaps also due to competition, a single A‚ÄìB packet is lost at R1. (a). Suppose A responds to the loss using the original BWE of 1 packet/ms. What transit capacity will A calculate, and how will A update its cwnd ? (b). Now suppose A uses the new throughput of 1 packet / 2 ms as its BWE. What transit capacity will A calculate, and how will A update its cwnd ? (c). Suppose A calculates BWE as cwnd /RTT. What value of BWE does A obtain by measuring the RTT of the packet just before the one that was lost? 550 22 Newer TCP Implementations
An Introduction to Computer Networks, Release 2.0.11 14.0. In 22.16 TCP BBR we estimated the impact on TCP BBR‚Äôs BWE value during the interval when pacing_gain =1.25. Suppose now that the BBR and Reno connections each have 800 packets in transit, instead of 80. Assume the bottleneck bandwidth rises tenfold to 20 packets/ms, so RTT noLoad is still 80 ms. During the 8-RTT pacing-gain cycle, Reno increases its cwnd to 808. IfBWE is measured at the optimum point after BBR‚Äôs pacing_gain =1.25 rate increase, what is the new value of BWE? 22.18 Exercises 551
An Introduction to Computer Networks, Release 2.0.11 552 22 Newer TCP Implementations
23 QUEUING AND SCHEDULING Is giving all control of congestion to the TCP layer really the only option? As the Internet has evolved, so have situations in which we may not want routers handling all trafÔ¨Åc on a Ô¨Årst-come, Ô¨Årst-served basis. TrafÔ¨Åc with delay bounds ‚Äì so-called real-time trafÔ¨Åc, often involving either voice or video ‚Äì is likely to perform much better with preferential service, for example; we will turn to this in 25 Quality of Service. But even without real-time trafÔ¨Åc, we might be interested in guaranteeing that each of several customers gets an agreed-upon fraction of bandwidth, regardless of what the other customers are receiving. If we trust only to TCP Reno‚Äôs bandwidth-allocation mechanisms, and if one customer has one connection and another has ten, then the bandwidth received may also be in the ratio of 1:10. This may make the Ô¨Årst customer quite unhappy. The fundamental mechanism for achieving these kinds of trafÔ¨Åc-management goals in a shared network is through queuing; that is, in deciding how the routers prioritize the trafÔ¨Åc waiting in their queues. In this chapter and the following we will take a look at what router-based strategies are available in the toolbox. This chapter is mostly concerned with so-called fair queuing, in which the bandwidth assigned to idle senders is reapportioned to the other, active senders. The following chapter, 24 Token Bucket Rate Limiting, deals with bandwidth caps, in which there is no reapportioning of the bandwidth of idle senders. Finally, in 25 Quality of Service we will see how some of these ideas have been applied to develop distributed quality-of-service options. Previously, in 20.1 A First Look At Queuing, we looked at FIFO queuing ‚Äì both tail-drop and random-drop variants ‚Äì and (brieÔ¨Çy) at priority queuing. These are examples of queuing disciplines, a catchall term for anything that supports a way to accept and release packets. The RED gateway strategy ( 21.5.4 RED ) could qualify as a separate queuing discipline, too, although one closely tied to FIFO. Queuing disciplines provide tools for meeting administratively imposed constraints on trafÔ¨Åc. Two senders, for example, might be required to share an outbound link equally, or in the proportion 60%-40%, even if one participant would prefer to use 100% of the bandwidth. Alternatively, a sender might be required not to send in bursts of more than 10 packets at a time. Closely allied to the idea of queuing is scheduling: deciding what packets get sent when. Scheduling may take the form of sending someone else‚Äôs packets right now, or it may take the form of delaying packets that are arriving too fast. While priority queuing is one practical alternative to FIFO queuing, we will also look at so-called fair queuing, in both Ô¨Çat and hierarchical forms. Fair queuing provides a straightforward strategy for dividing bandwidth among multiple senders according to preset percentages. Also introduced here is the token-bucket mechanism, which can be used for trafÔ¨Åc scheduling but also for trafÔ¨Åc description. Some of the material here ‚Äì in particular that involving fair queuing and the Parekh-Gallager theorem ‚Äì may give this chapter a more mathematical feel than earlier chapters. Mostly, however, this is conÔ¨Åned to the proofs; the claims themselves are more straightforward. 553
An Introduction to Computer Networks, Release 2.0.11 23.1 Queuing and Real-Time TrafÔ¨Åc One application for advanced queuing mechanisms is to support real-time transport ‚Äì that is, trafÔ¨Åc with delay constraints on delivery. In its original conception, the Internet was arguably intended for non-time-critical transport. If you wanted to place a digital phone call where every (or almost every) byte was guaranteed to arrive within 50 ms, your best bet might be to use the (separate) telephone network instead. And, indeed, having an entirely separate network for real-time transport is deÔ¨Ånitely a workable solution. It is, however, expensive; there are many economies of scale to having just a single network. There is, therefore, a great deal of interest in Ô¨Åguring out how to get the Internet to support real-time trafÔ¨Åc directly. The central strategy for mixing real-time and bulk trafÔ¨Åc is to use queuing disciplines to give the real-time trafÔ¨Åc the service it requires. Priority queuing is the simplest mechanism, though the fair-queuing approach below offers perhaps greater Ô¨Çexibility. We round out the chapter with the Parekh-Gallager theorem, which provides a precise delay bound for realtime trafÔ¨Åc that shares a network with bulk trafÔ¨Åc. All that is needed is that the real-time trafÔ¨Åc satisÔ¨Åes a token-bucket speciÔ¨Åcation and is assigned bandwidth guarantees through fair queuing; the volume of bulk trafÔ¨Åc does not matter. This is exactly what is needed for real-time support. While this chapter contains some rather elegant theory, it is not at all clear how much it is put into practice today, at least for real-time trafÔ¨Åc at the ISP level. We will return to this issue in the following chapter. Realtime trafÔ¨Åc management hasseen widespread adoption in settings where trafÔ¨Åc must be divided among multiple ‚Äúguest users‚Äù: the hotel industry, for example, and perhaps for students. And most ISPs apply bandwidth caps to most of their customers. But we acknowledge that the more advanced forms of real-time trafÔ¨Åc management have in general seen relatively limited adoption. 23.2 TrafÔ¨Åc Management Even if none of your trafÔ¨Åc has real-time constraints, you still may wish to allocate bandwidth according to administratively determined percentages. For example, you may wish to give each of three departments an equal share of download (or upload) capacity, or you may wish to guarantee them shares of 55%, 35% and 10%. If you are an ISP, or the manager of a public Wi-Fi access point, you might wish to guarantee that everyone gets a roughly equal share of the available bandwidth, or, alternatively, that no one gets more bandwidth than they paid for. If you want any unused capacity to be divided among the non-idle users, fair queuing is the tool of choice, though in some contexts it may beneÔ¨Åt from cooperation from your ISP. If the users are more like customers receiving only the bandwidth they pay for, you might want to enforce Ô¨Çat caps even if some bandwidth thus goes unused; token-bucket Ô¨Åltering would then be the way to go. If bandwidth allocations are not only by department (or customer) but also by workgroup (or customer-speciÔ¨Åc subcategory), then hierarchical queuing offers the necessary control. In general, network management divides into managing the hardware and managing the trafÔ¨Åc; the tools in this chapter address this latter component. These tools can be used internally by ISPs and at the customer/ISP interconnection, but trafÔ¨Åc management often makes good economic sense even when entirely 554 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 contained within a single organization. Unlike support for real-time trafÔ¨Åc, above, use of trafÔ¨Åc management is widespread throughout the Internet, though often barely visible. 23.3 Priority Queuing To get started, let us Ô¨Åll in the details for priority queuing, which we looked at brieÔ¨Çy in 20.1.1 Priority Queuing. Here a given outbound interface can be thought of as having two (or more) physical queues representing different priority levels. Packets are placed into the appropriate subqueue based on some packet attribute, which might be an explicit priority tag, or which might be the packet‚Äôs destination socket. Whenever the outbound link becomes free and the router is able to send the next packet, it always looks Ô¨Årst to the higher-priority queue; if it is nonempty then a packet is dequeued from there. Only if the higher-priority queue is empty is the lower-priority queue served. Note that priority queuing is nonpreemptive: if a high-priority packet arrives while a low-priority packet is being sent, the latter is not interrupted. Only when the low-priority packet has Ô¨Ånished transmission does the router again check its high-priority subqueue(s). Priority queuing can lead to complete starvation of low-priority trafÔ¨Åc, but only if the high-priority trafÔ¨Åc consumes 100% of the outbound bandwidth. Often we are able to guarantee (for example, through admission control) that the high-priority trafÔ¨Åc is limited to a designated fraction of the total outbound bandwidth. 23.4 Queuing Disciplines As an abstract data type, a queuing discipline is simply a data structure that supports the following operations: 
- enqueue() 
- dequeue() 
- is_empty() Note that the enqueue() operation includes within it a way to handle dropping a packet in the event that the queue is full. For FIFO queuing, the enqueue() operation needs only to know the correct outbound interface; for priority queuing enqueue() also needs to be told ‚Äì or be able to infer ‚Äì the packet‚Äôs priority classiÔ¨Åcation. We may also in some cases Ô¨Ånd it convenient to add a peek() operation to return the next packet that would be dequeued if we were actually to do that, or at least to return some important statistic ( egsize or arrival time) about that packet. As with FIFO and priority queuing, any queuing discipline is always tied to a speciÔ¨Åc outbound interface. In that sense, any queuing discipline has a single output. On the input side, the situation may be more complex. The FIFO queuing discipline has a single input stream, though it may be fed by multiple physical input interfaces: the enqueue() operation puts all packets in the same physical queue. A queuing discipline may, however, have multiple input streams; we will call these classes, orsubqueues, and will refer to the queuing discipline itself as classful. Priority queues, for example, have an input class for each priority level. 23.3 Priority Queuing 555
An Introduction to Computer Networks, Release 2.0.11 When we want to enqueue a packet for a classful queuing discipline, we must Ô¨Årst invoke a classiÔ¨Åer ‚Äì possibly external to the queuing discipline itself ‚Äì to determine the input class. (In the Linux documentation, what we have called classiÔ¨Åers are often called Ô¨Ålters .) For example, if we wish to use a priority queue to give priority to V oIP packets, the classiÔ¨Åer‚Äôs job is to determine which arriving packets are in fact V oIP packets (perhaps taking into account things like size or port number or source host), so as to be able to provide this information to the enqueue() operation. The classiÔ¨Åer might also take into account pre-existing trafÔ¨Åc reservations, so that packets that belong to Ô¨Çows with reservations get preferred service, or else packet tags that have been applied by some upstream router; we return to both of these in 25 Quality of Service. The number and conÔ¨Åguration of classes is often Ô¨Åxed at the time of queuing-discipline creation; this is typically the case for priority queues. Abstractly, however, the classes can also be dynamic; an example of this might be fair queuing (below), which often supports a conÔ¨Åguration in which a separate input class is created on the Ô¨Çy for each separate TCP connection. FIFO and priority queuing are both work-conserving, meaning that the associated outbound interface is not idle unless the queue is empty. A non-work-conserving queuing discipline might, for example, artiÔ¨Åcially delay some packets in order to enforce an administratively imposed bandwidth cap. Non-work-conserving queuing disciplines are often called trafÔ¨Åc shapers; see 24 Token Bucket Rate Limiting below for an example. Because delayed packets have to be assigned transmission times, and kept somewhere until that time is reached, shaping tends to be more complex internally than other queuing mechanisms. 23.5 Fair Queuing An important alternative to FIFO and priority is fair queuing. Where FIFO and its variants have a single input class and put all the incoming trafÔ¨Åc into a single physical queue, fair queuing maintains a separate logical FIFO subqueue for each input class; we will refer to these as the per-class subqueues. Division into classes can be Ô¨Åne-grained ‚Äì ega separate class for each TCP connection ‚Äì or coarse-grained ‚Äì ega separate class for each arrival interface, or a separate class for each designated internal subnet. Suppose for a moment that all packets are the same size; this makes fair queuing much easier to visualize. In this (special) case ‚Äì sometimes called Nagle fair queuing, and proposed in RFC 970 ‚Äì the router simply services the per-class subqueues in round-robin fashion, sending one packet from each in turn. If a per-class subqueue is empty, it is simply skipped over. If all per-class subqueues are always nonempty this resembles time-division multiplexing ( 6.2 Time-Division Multiplexing ). However, unlike time-division multiplexing if one of the per-class subqueues does become empty then it no longer consumes any outbound bandwidth. Recalling that all packets are the same size, the total bandwidth is then divided equally among the nonempty per-class subqueues; if there are K such queues, each will get 1/K of the output. Fair queuing was extended to streams of variable-sized packets in [DKS89], [LZ89] and [LZ91]. Since then there has been considerable work in trying to Ô¨Ågure out how to implement fair queuing efÔ¨Åciently and to support appropriate variants. There are two broad approaches to fair queuing for variable-sized packets. The newer approach is to be concerned only with long-term bandwidth guarantees consistent with the assigned bandwidth fractions, as in23.5.5 DeÔ¨Åcit Round Robin and23.5.6 Stochastic Fair Queuing. The original approaches, 23.5.3 Bitby-bit Round Robin and23.5.4 The GPS Model, provide these same bandwidth assurances, but they also make short-term delay guarantees: each time we choose the next packet to send, we choose the one that is the most ‚Äúentitled‚Äù to be next, where a packet‚Äôs ‚Äúentitlement‚Äù decreases as it gets larger or if its Ô¨Çow has sent recent previous packets. SpeciÔ¨Åcally, packet-transmission choices are made according to the calculated 556 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 ‚Äúvirtual Ô¨Ånishing time‚Äù, 23.5.2 Virtual Finishing Times. We will sometimes refer to this original approach asreal-time fair queuing. Real-time fair queuing has potentially signiÔ¨Åcant beneÔ¨Åts for real-time trafÔ¨Åc management. In particular, we can identify a speciÔ¨Åc delay guarantee; see 23.5.4.7 Finishing-Order Bound. That said, in today‚Äôs world where packet-transmission times might be one microsecond but an application‚Äôs packet-delay requirements might be several milliseconds ‚Äì that is, thousands of times larger ‚Äì real-time fair queuing is not always necessary. 23.5.1 Weighted Fair Queuing An extension of fair queuing is weighted fair queuing (WFQ), where instead of giving each class an equal share, we assign each class a different percentage. For example, we might assign bandwidth percentages of 10%, 30% and 60% to three different departments. If all three subqueues are active, each gets the listed percentage. If the 60% subqueue is idle, then the others get 25% and 75% respectively, preserving the 1:3 ratio of their allocations. If the 10% subqueue is idle, then the other two subqueues get 33.3% and 66.7%. If all packets are the same size, weighted fair queuing is, conceptually, a straightforward generalization of fair queuing, although the actual implementation details are sometimes nontrivial as the round-robin implementation above naturally yields equal shares. If we have two per-class subqueues that are to receive allocations of 40% and 60% (that is, in the ratio 2:3), and all packets are the same size, then we could implement WFQ by having one per-class subqueue send two packets and the other three. Or we might intermingle the two: class 1 sends its Ô¨Årst packet, class 2 sends its Ô¨Årst packet, class 1 sends its second, class 2 sends its second and its third. If the allocation is to be in the ratio 1:?2, the Ô¨Årst sender might always send 1 packet while the second might send in a pattern ‚Äì an irregular one ‚Äì that averages?2: 1, 2, 1, 2, 1, 1, 2,. .. . 23.5.2 Virtual Finishing Times In the real world, however, packets are far from being equal-sized, and mixing bulk and real-time trafÔ¨Åc tends to make the size variation worse. In this case, fair queuing and weighted fair queuing are still possible but we have a little more work to do. This is an important practical case, as fair queuing is often used when one input class consists of small-packet real-time trafÔ¨Åc, and should not be ‚Äúpenalized‚Äù for sending small packets. The strategy we will introduce Ô¨Årst ‚Äì the strategy of ‚Äúreal-time‚Äù fair queuing ‚Äì is to transmit packets in order of a calculated virtual Ô¨Ånishing time, which beneÔ¨Åts Ô¨Çows with smaller packets and Ô¨Çows that have not sent packets recently. WFQ algorithms based on virtual Ô¨Ånishing times are what were referred to above as real-time fair queuing. For the non-real-time approach, see 23.5.5 DeÔ¨Åcit Round Robin. We present two mechanisms for handling different-sized packets using virtual Ô¨Ånishing times; the two are ultimately equivalent. The Ô¨Årst ‚Äì 23.5.3 Bit-by-bit Round Robin ‚Äì is a straightforward extension of the round-robin idea, and the second ‚Äì 23.5.4 The GPS Model ‚Äì uses a ‚ÄúÔ¨Çuid‚Äù model of simultaneous packet transmission. Both mechanisms share the idea of a ‚Äúvirtual clock‚Äù that runs at a rate inversely proportional to the number of active subqueues; as we shall see, the point of varying the clock rate in this way is so that the virtual-clock time at which a given packet would theoretically Ô¨Ånish transmission does not depend on activity in any of the other subqueues. Finally, we present the quantum algorithm ‚Äì 23.5.5 DeÔ¨Åcit Round Robin ‚Äì which is a more-efÔ¨Åcient approximation to either of the exact algorithms, but which ‚Äì being an approximation ‚Äì no longer satisÔ¨Åes the 23.5 Fair Queuing 557
An Introduction to Computer Networks, Release 2.0.11 same small-scale delay constraints. For a straightforward generalization of the round-robin idea to different packet sizes, we start with a simpliÔ¨Åcation: let us assume that each per-class subqueue is always active, where a subqueue is active if it is nonempty whenever the router looks at it. If each subqueue is always active for the equal -sized-packets case, then packets are transmitted in order of increasing (or at least nondecreasing) cumulative data sent by each subqueue. In other words, every subqueue gets to send its Ô¨Årst packet, and only then do we go on to begin transmitting second packets, and so on. Still assuming each subqueue is always active, we can handle different -sized packets by the same idea. For packet P, let C Pbe the cumulative number of bytes that will have been sent by P‚Äôs subqueue as of the endof P. Then we simply need to send packets in nondecreasing order of C P. Variable-packet-sized fair queuing with all subqueues active Packet transmission order: Q1, P1, R1, Q2, Q3, R2, Q4, Q5, P2, R3, R4, P3, Q6 P1 P3 Q1P2 Q2 Q3 Q4 Q5 Q6 R1 R2 R3 R4 In the diagram above, transmission in nondecreasing order of C Pmeans transmission in left-to-right order of the vertical lines marking packet divisions, egQ1, P1, R1, Q2, Q3, R2,. .. . This ensures that, in the long run, each subqueue gets an equal share of bandwidth. A completely equivalent strategy, better suited for generalization to the case where not all subqueues are always active, is to send each packet in nondecreasing order of virtual Ô¨Ånishing times, calculated for each packet with the assumption that only that packet‚Äôs subqueue is active. The virtual Ô¨Ånishing time F Pof packet P is equal to C Pdivided by the output bandwidth. We use Ô¨Ånishing times rather than starting times because if one packet is very large, shorter packets in other subqueues that would Ô¨Ånish sooner should be sent Ô¨Årst. 23.5.2.1 A Ô¨Årst virtual-Ô¨Ånish example As an example, suppose there are two subqueues, P and Q. Suppose further that a stream of 1001-byte packets P 1, P2, P3,. .. arrives for P, and a stream of 400-byte packets Q 1, Q2, Q3,. .. arrives for Q; each stream is steady enough that each subqueue is always active. Finally, assume the output bandwidth is 1 byte per unit time, and let T=0 be the starting point. For the P subqueue, the virtual Ô¨Ånishing times calculated as above would be P 1at 1001, P 2at 2002, P 3 at 3003, etc; for Q the Ô¨Ånishing times would be Q 1at 400, Q 2at 800, Q 3at 1200, etc. So the order of transmission of all the packets together, in increasing order of virtual Ô¨Ånish, will be as follows: 558 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 Packet virtual Ô¨Ånish actual Ô¨Ånish Q1 400 400 Q2 800 800 P1 1001 1801 Q3 1200 2201 Q4 1600 2601 Q5 2000 3001 P2 2002 4002 For each packet we have calculated in the table above its virtual Ô¨Ånishing time, and then its actual wallclock Ô¨Ånishing time assuming packets are transmitted in nondecreasing order of virtual Ô¨Ånishing time (as shown). Because both subqueues are always active, and because the virtual Ô¨Ånishing times assumed each subqueue received 100% of the output bandwidth, in the long run the actual Ô¨Ånishing times will be about double the virtual times. This, however, is irrelevant; all that matters is the relative virtual Ô¨Ånishing times. 23.5.2.2 A second virtual-Ô¨Ånish example For the next example, however, we allow a subqueue to be idle for a while and then become active. In this situation virtual Ô¨Ånishing times do not work quite so well, at least when based directly on wallclock time. We return to our initial simpliÔ¨Åcation that all packets are the same size, which we take to be 1 unit; this allows us to apply the round-robin mechanism to determine the transmission order and compare this to the virtual-Ô¨Ånish order. Assume there are three queues P, Q and R, and P is empty until wallclock time 20. Q is constantly busy; its Kth packet Q K, starting with K=1, has virtual Ô¨Ånishing time F K= K. For the Ô¨Årst case, assume R is completely idle. When P‚Äôs Ô¨Årst packet P 1arrives at time 20, its virtual Ô¨Ånishing time will be 21. At time 20 the head packet in Q will be Q 21; the two packets therefore have identical virtual Ô¨Ånishing times. And, encouragingly, under round-robin queue service P 1and Q 21will be sent in the same round. For the second case, however, suppose R is also constantly busy. Up until time 20, Q and R have each sent 10 packets; their next packets are Q 11and R 11, each with a virtual Ô¨Ånishing time of T=11. When P‚Äôs Ô¨Årst packet arrives at T=20, again with virtual Ô¨Ånishing time 21, under round-robin service it should be sent in the same round as Q 11and R 11. Yet their virtual Ô¨Ånishing times are off by a factor of about two; queue P‚Äôs stretch of inactivity has left it far behind. Virtual Ô¨Ånishing times, as we have been calculating them so far, simply do not work. The trick, as it turns out, is to measure elapsed time not in terms of packet-transmission times ( iewallclock time), but rather in terms of rounds of round-robin transmission. This amounts to scaling the clock used for measuring arrival times; counting in rounds rather than packets means that we run this clock at rate 1/N when N subqueues are active. If we do this in case 1, with N=1, then the Ô¨Ånishing times are unchanged. However, in case 2, with N=2, packet P 1arrives after 20 time units but only 10 rounds; the clock runs at half rate. Its calculated Ô¨Ånishing time is thus 11, exactly matching the Ô¨Ånishing times of the two long-queued packets Q 11and R 11with which P 1shares a round-robin transmission round. We formalize this in the next section, extending the idea to include both variable-sized packets and sometimes-idle subqueues. Note that only the clock that measures arrival times is scaled; we do not scale the calculated transmission times. 23.5 Fair Queuing 559
An Introduction to Computer Networks, Release 2.0.11 23.5.3 Bit-by-bit Round Robin Imagine sending a single bitat a time from each active input subqueue, in round-robin fashion. While not directly implementable, this certainly meets the goal of giving each active subqueue equal service, even if packets are of different sizes. We will use bit-by-bit round robin, or BBRR, as a way of modeling packetÔ¨Ånishing times, and then, as in the previous example, send the packets the usual way ‚Äì one full packet at a time ‚Äì in order of increasing BBRR-calculated virtual Ô¨Ånishing times. It will sometimes happen that a larger packet is being transmitted at the point a new, shorter packet arrives for which a smaller Ô¨Ånishing time is computed. The current transmission is not interrupted, though; the algorithm is non-preemptive. The trick to making the BBRR approach workable is to Ô¨Ånd an ‚Äúinvariant‚Äù formulation of Ô¨Ånishing time that does not change as later packets arrive, or as other subqueues become active or inactive. To this end, taking the lead from the example of the previous section, we deÔ¨Åne the ‚Äúrounds counter‚Äù R(t), where t is the time measured in units of the transmission time for one bit. When there are any active (nonempty or currently transmitting) input subqueues, R(t) counts the number of round-robin 1-bit cycles that have occurred since the last time all the subqueues were empty. If there are K active input subqueues, then R(t) increments by 1 as t increments by K; that is, R(t) grows at rate 1/K. An important attribute of R(t) is that, if a packet of size S bits starts transmission via BBRR at R 0= R(t 0), then it will Ô¨Ånish when R(t) = R 0+S,regardless of whether any other input subqueues become active or become empty. For any packet actively being sent via BBRR, R(t) increments by 1 for each bit of that packet sent. If for a given round-robin cycle there are K subqueues active, then K bits will be sent in all, and R(t) will increment by 1. To calculate the virtual BBRR Ô¨Ånishing time of a packet P, we Ô¨Årst record R P= R(t P) at the moment of arrival. We now compute the BBRR-Ô¨Ånishing R-value F Pas follows; we can think of this as a ‚Äútime‚Äù measured via the rounds counter R(t). That is, R(t) represents a ‚Äúvirtual clock‚Äù that happens sometimes to run slow. Let S be the size of the packet P in bits. If P arrived on a previously empty input subqueue, then its BBRR transmission can begin immediately, and so its Ô¨Ånishing R-value F Pis simply R P+S. If the packet‚Äôs subqueue was nonempty, we look up the (future) Ô¨Ånishing R-value of the packet immediately ahead of P in its subqueue, say F prev; the Ô¨Ånishing R-value of P is then F P= F prev+ S. This is sometimes described as: Start = max(R(now), F prev) FP= Start + S (S = packet size, measured in bits) As each new packet P arrives, we calculate its BBRR-Ô¨Ånishing R-value F P, and then send packets the conventional one-packet-at-a-time way in increasing order of F P. As stated above, F Pwill not change if other subqueues empty or become active, thus changing the rate of the rounds-counter R(t). The router maintaining R(t) does not have to increment it on every bit; it sufÔ¨Åces to update it whenever a packet arrives or a subqueue becomes empty. If the previous value of R(t) was R prev, and from then to now exactly K subqueues were nonempty, and M bit-times have elapsed according to the wall clock, then the current value of R(t) is R prev+ M/K. 23.5.3.1 BBRR example As an example, suppose the fair queuing router has three input subqueues P, Q and R, initially empty. The following packets arrive at the wall-clock times shown. 560 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 Packet Queue Size Arrival time, t P1 P 1000 0 P2 P 1000 0 Q1 Q 600 800 Q2 Q 400 800 Q3 Q 400 800 R1 R 200 1200 R2 R 200 2100 At t=0, we have R(t)=0 and we assign Ô¨Ånishing R-values F(P 1)=1000 to P 1and F(P 2) = F(P 1)+1000 = 2000 to P 2. Transmission of P 1begins. When the three Q packets arrive at t=800, we have R(t)=800 as well, as only one subqueue has been active. We assign Ô¨Ånishing R-values for the newly arriving Q 1, Q2and Q 3of F(Q 1) = 800+600 = 1400, F(Q 2) = 1400+400 = 1800, and F(Q 3) = 1800+400 = 2200. At this point, BBRR begins serving two subqueues, so the R(t) rate is cut in half. At t=1000, transmission of packet P 1is completed; R(t) is 800 + 200/2 = 900. The smallest Ô¨Ånishing R-value on the books is F(Q 1), at 1400, so Q 1is the second packet transmitted. Q 1‚Äôs real Ô¨Ånishing time will be t = 1000+600 = 1600. At t=1200, R 1arrives; transmission of Q 1is still in progress. R(t) is 800 + 400/2 = 1000; we calculate F(R 1) = 1000 + 200 = 1200. Note this is less than the Ô¨Ånishing R-value for Q 1, which is currently transmitting, but Q1is not preempted. At this point (t=1200, R(t)=1000), the R(t) rate falls to 1/3. At t=1600, Q 1has Ô¨Ånished transmission. We have R(t) = 1000 + 400/3 = 1133. The next smallest Ô¨Ånishing R-value is F(R 1) = 1200 so transmission of R 1commences. At t=1800, R 1Ô¨Ånishes. We have R(1800) = R(1200) + 600/3 = 1000 + 200 = 1200 (3 subqueues have been busy since t=1200). Queue R is now empty, so the R(t) rate rises from 1/3 to 1/2. The next smallest Ô¨Ånishing R-value is F(Q 2)=1800, so transmission of Q 2begins. It will Ô¨Ånish at t=2200. At t=2100, we have R(t) = R(1800) + 300/2 = 1200 + 150 = 1350. R 2arrives and is assigned a Ô¨Ånishing time of F(R 2) = 1350 + 200 = 1550. Again, transmission of Q 2is not preempted even though F(R 2) < F(Q 2). The R(t) rate again falls to 1/3. At t=2200, Q 2Ô¨Ånishes. R(t) = 1350 + 100/3 = 1383. The next smallest Ô¨Ånishing R-value is F(R 2)=1550, so transmission of R 2begins. At t=2400, transmission of R 2ends. R(t) is now 1350 + 300/3 = 1450. The next smallest Ô¨Ånishing R-value is F(P 2) = 2000, so transmission of P 2begins. The R(t) rate rises to 1/2, as queue R is again empty. At t=3400, transmission of P 2ends. R(t) is 1450 + 1000/2 = 1950. The only remaining unsent packet is Q 3, with F(Q 3)=2200. We send it. At t=3800, transmission of Q 3ends. R(t) is 1950 + 400/1 = 2350. To summarize: 23.5 Fair Queuing 561
An Introduction to Computer Networks, Release 2.0.11 Packet send-time, wall clock tcalculated Ô¨Ånish RvalueR-value when sentR-value at Ô¨Ånish P1 0 1000 0 900 Q1 1000 1400 900 1133 R1 1600 1200* 1133 1200 Q2 1800 1800 1200 1383 R2 2200 1550* 1383 1450 P2 2400 2000 1450 1950 Q3 3400 2200 1950 2350 Packets arrive, begin transmission and Ô¨Ånish in ‚Äúreal‚Äù time. However, the number of queues active in real time affects the rate of the rounds-counter R(t); this value is then attached to each packet as it arrives as its virtual Ô¨Ånishing time, and determines the order of packet transmission. The change in R-value from start to Ô¨Ånish exactly matches the packet size when the packet is ‚Äúvirtually sent‚Äù via BBRR. When the packet is sent as an indivisible unit, as in the table above, the change in R-value is usually much smaller, as the R-clock runs slower whenever at least two subqueues are in use. The calculated-Ô¨Ånish R-values are not in fact increasing, as can be seen at the starred (*) values. This is because, for example, R 1was not yet available when it was time to send Q 1. Computationally, maintaining the R-value counter is inconsequential. The primary performance issue with BBRR simulation is the need to Ô¨Ånd the smallest R-value whenever a new packet is to be sent. If n is the number of packets waiting to be sent, then we can do this in time O(log(n)) by keeping the R-values sorted in an appropriate data structure. The BBRR approach assumes equal weights for each subqueue; this does not generalize completely straightforwardly to weighted fair queuing as the number of subqueues cannot be fractional. If there are two queues, one which is to have weight 40% and the other 60%, we could use BBRR with Ô¨Åve subqueues, two of which (2/5) are assigned to the 40%-subqueue and the other three (3/5) to the 60% subqueue. But this becomes increasingly awkward as the fractions become less simple; the GPS model, next, is a better option. 23.5.4 The GPS Model An almost-equivalent model to BBRR is the generalized processor sharing model, or GPS; it was Ô¨Årst developed as an application to CPU scheduling. In this approach we imagine the packets as liquid, and the outbound interface as a pipe that has a certain total capacity. The head packets from each subqueue are all squeezed into the pipe simultaneously, each at its designated fractional rate. The GPS model is essentially an ‚ÄúinÔ¨Ånitesimal‚Äù variant of BBRR. The GPS model has an advantage of generalizing straightforwardly to weighted fair queuing. Other Ô¨Çuid models have also been used in the analysis of networks, egfor the study of TCP, though we do not consider these here. See [MW00] for one example. For the GPS model, assume there are N input subqueues, and the ith subqueue, 0 ¬§i<N, is to receive fraction ùõºi> 0, where ùõº0+ùõº1+. .. + ùõºN‚Äì1=1. If at some point a set A of input subqueues is active, say A = {0,2,4}, then subqueue 0 will receive fraction ùõº0/(ùõº0+ùõº2+ùõº4), and subqueues 2 and 4 similarly. The router forwards packets from each active subqueue simultaneously, each at its designated rate. 562 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 The GPS model (and the BBRR model) provides an ideal degree of isolation between input Ô¨Çows: each Ô¨Çow is insulated from any delay due to packets on competing Ô¨Çows. The ith Ô¨Çow receives bandwidth of at least ùõºiand packets wait only for other packets belonging to the same Ô¨Çow. When a packet arrives for an inactive subqueue, forwarding begins immediately, interleaved with any other work the router is doing. TrafÔ¨Åc on other Ô¨Çows can reduce the real rate of a Ô¨Çow, but not its virtual rate. While GPS is convenient as a model, it is even less implementable, literally, than BBRR. As with BBRR, though, we can use the GPS model to determine the order of one-packet-at-a-time transmission. As each real packet arrives, we calculate the time it would Ô¨Ånish, if we were using GPS. Packets are then transmitted under WFQ one at a time, in order of increasing GPS Ô¨Ånishing time. In lieu of the BBRR rounds counter R(t), a virtual clock VC(t) is used that runs at an increased rate 1/ ùõº ¬•1 where ùõºis the sum of the ùõºifor the active subqueues. That is, if subqueues 0, 2 and 4 are active, then the VC(t) clock runs at a rate of 1/( ùõº0+ùõº2+ùõº4). If all the ùõºiare equal, each to 1/N, then VC(t) always runs N times faster than R(t), and so VC(t) = N R(t); the VC clock runs at wallclock speed when all input subqueues are active and speeds up as subqueues become idle. For any one active subqueue i, the GPS rate of transmission relative to the virtual clock (that is, in units of bits per virtual-second) is always equal to fraction ùõºiof the full output-interface rate. That is, if the output rate is 10 Mbps and an active Ô¨Çow has fraction ùõº= 0.4, then it will always transmit at 4 bits per virtual microsecond. When all the subqueues are active, and the VC clock runs at wallclock speed, the Ô¨Çow‚Äôs actual rate will be 4 bits/¬µsec. When the subqueue is active alone, its speed measured by a real clock will be 10 bit/¬µsec but the virtual clock will run 2.5 times faster so 10 bits/¬µsec is 10 bits per 2.5 virtual microseconds, or 4 bits per virtual microsecond. To make this claim more precise, let A be the set of active queues, and let ùõºagain be the sum of the ùõºjfor j in A. Then VC(t) runs at rate 1/ ùõºand active subqueue i‚Äôs data is sent at rate ùõºi/ùõºrelative to wallclock time. Subqueue i‚Äôs transmission rate relative to virtual time is thus ( ùõºi/ùõº)/(1/ùõº) =ùõºi. As other subqueues become inactive or become active, the VC(t) rate and the actual transmission rate move in lockstep. Therefore, as with BBRR, a packet P of size S on subqueue i that starts transmission at virtual time T will Ô¨Ånish at T + S/(r ùõºi) by the VC clock, where r is the actual output rate of the router, regardless of what is happening in the other subqueues. In other words, VC-calculated Ô¨Ånishing times are invariant. To round out the calculation of Ô¨Ånishing times, suppose packet P of size S arrives on an active GPS subqueue i. The previous packet in that subqueue will have Ô¨Ånishing time F prev, and that will become P‚Äôs start time. If P arrives on an inactive subqueue, GPS allows it to begin transmitting immediately. Either way, as with BBRR, P‚Äôs start time is P start= max(VC(now), F prev), and its Ô¨Ånishing time on the VC clock is F P= P start+ S/(rùõºi). In one expression, this Ô¨Ånishing time is FP= max(VC(now), F prev) + S/(rùõºi) In23.8.1.1 WFQ with non-FIFO subqueues below, we will consider WFQ routers that, as part of a hierarchy, are in effect only allowed to transmit intermittently. In such a case, the virtual clock should be suspended whenever output is blocked. This is perhaps easiest to see for the BBRR scheduler: the roundscounter RR(t) is to increment by 1 for each bit sent by each active subqueue. When no bits may be sent, the clock should not increase. As an example of what happens if this is not done, suppose R has two subqueues A and B; the Ô¨Årst is empty and the second has a long backlog. R normally processes one packet per second. At T=0/VC=0, R‚Äôs output is suspended. Packets in the second subqueue b 1, b2, b3,. .. have virtual Ô¨Ånishing times 1, 2, 3,. .. . At T=10, R resumes transmission, and packet a 1arrives on the A subqueue. If R‚Äôs virtual clock had been suspended 23.5 Fair Queuing 563
An Introduction to Computer Networks, Release 2.0.11 for the interval 0 ¬§T¬§10, a 1would be assigned Ô¨Ånishing time T=1 and would have priority comparable to b1. If R‚Äôs virtual clock had continued to run, a 1would be assigned Ô¨Ånishing time T=11 and would not be sent until b 11reached the head of the B queue. 23.5.4.1 The WFQ scheduler To schedule actual packet transmission under weighted fair queuing, we calculate upon arrival each packet‚Äôs virtual-clock Ô¨Ånishing time assuming it were to be sent using GPS. Whenever the sender is ready to start transmission of a new packet, it selects from the available packets the one with the smallest GPS-Ô¨Ånishingtime value. By the argument above, a packet‚Äôs GPS Ô¨Ånishing time does not depend on any later arrivals or idle periods on other subqueues. As with BBRR, small but later-arriving packets might have smaller virtual Ô¨Ånishing times, but a packet currently being transmitted will not be interrupted. 23.5.4.2 Finishing Order under GPS and WFQ We now look at the order in which packets Ô¨Ånish transmission under GPS versus WFQ. The goal is to provide in 23.5.4.7 Finishing-Order Bound a tight bound on how long packets may have to wait under WFQ compared to GPS. We emphasize again: 
- GPS Ô¨Ånishing time: the theoretical Ô¨Ånishing time based on parallel multi-packet transmissions under the GPS model 
- WFQ Ô¨Ånishing time: the real Ô¨Ånishing time assuming packets are sent sequentially in increasing order of calculated GPS Ô¨Ånishing time One way to view this is as a quantiÔ¨Åcation of the informal idea that WFQ provides a natural priority for smaller packets, at least smaller packets sent on previously idle subqueues. This is quite separate from the bandwidth guarantee that a given small-packet input class might receive; it means that small packets are likely to leapfrog larger packets waiting in other subqueues. The quantum algorithm, below, provides long-term WFQ bandwidth guarantees but does notprovide the same delay assurances. First, if all subqueues are always active (or if a Ô¨Åxed subset of subqueues is always active), then packets Ô¨Ånish under WFQ in the same order as they do under GPS. This is because under WFQ packets are transmitted in the order of GPS Ô¨Ånishing times according the virtual clock, and if all subqueues are always active the virtual clock runs at a rate identical to wallclock time (or, if a Ô¨Åxed subset of subqueues is always active, at a rate proportional to wallclock time). If all subqueues are always active, we can assume that all packets were in their subqueues as of time T=0; the Ô¨Ånishing order is the same as long as each packet arrived before its subqueue went inactive. Finally, if all subqueues are always active then each packet Ô¨Ånishes at least as early under WFQ as under GPS. To see this, let P jbe the jth packet to Ô¨Ånish, under either GPS or WFQ. At the time when P jÔ¨Ånishes under WFQ, the router R will have devoted 100% of its output bandwidth exclusively to P 1through P j. When P jÔ¨Ånishes under GPS, R will also have transmitted P 1through P j, and may have transmitted fractions of later packets as well. Therefore, the P jÔ¨Ånishing time under GPS cannot be earlier. The Ô¨Ånishing order and the relative GPS/WFQ Ô¨Ånishing times may change, however, if ‚Äì as will usually be the case ‚Äì some subqueues are sometimes idle; that is, if packets sometimes ‚Äúarrive late‚Äù for some subqueues. 564 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 23.5.4.3 GPS Example 1 As a Ô¨Årst example we return to the scenario of 23.5.2.1 A Ô¨Årst virtual-Ô¨Ånish example. The router‚Äôs two subqueues are always active; each has an allocation of ùõº=50%. Packets P 1, P2, P3,. .. , all of size 1001, wait in the Ô¨Årst queue; packets Q 1, Q2, Q3,. .. , all of size 400, wait in the second queue. Output bandwidth is 1 byte per unit time, and T=0 is the starting point. The router‚Äôs virtual clock runs at wallclock speed, as both subqueues are always active. If F irepresents the virtual Ô¨Ånishing time of R i, then we now calculate F ias F i-1+ 400/ ùõº= Fi-1+ 800. The virtual Ô¨Ånishing times of P 1, P2,etcare similarly at multiples of 2002. Packet virtual Ô¨Ånish actual Ô¨Ånish time Q1 800 400 Q2 1600 800 P1 2002 1801 Q3 2400 2201 Q4 3200 2601 Q5 4000 3001 P2 4004 4002 In the table above, the ‚Äúvirtual Ô¨Ånish‚Äù column is simply double that of the BBRR version, reÔ¨Çecting the fact that the virtual Ô¨Ånishing times are now scaled by a factor of 1/ ùõº= 2. The actual Ô¨Ånish times are identical to what we calculated before. Note that, in every case, the actual WFQ Ô¨Ånish time is always less than or equal to the virtual GPS Ô¨Ånish time. 23.5.4.4 GPS Example 2 If the router has only a single active subqueue, with share ùõºand packets P 1, P2, P3,. .. , then the calculated virtual-clock packet Ô¨Ånishing times will be equal to the time on the virtual clock at the point of actual Ô¨Ånish, at least if this has been the case since the virtual clock last restarted at T=VC=0. Let r be the output rate of the router, let S 1, S2, S3be the sizes of the packets and let F 1, F2, F3be their virtual Ô¨Ånishing times with F0=0. Then Fi= F i-1+ S i/(rùõº) = S 1/(rùõº) +. .. + S i/(rùõº) The ith packet‚Äôs actual Ô¨Ånishing time A iis (S 1+. .. + S i)/r, which is ùõºFi. But the virtual clock runs fast by a factor of 1/ ùõº, so the actual Ô¨Ånishing time on the virtual clock is A i/ùõº= F i. 23.5.4.5 GPS Example 3 The next example illustrates a smaller but later-arriving packet, in this case Q 2, that Ô¨Ånishes ahead of P 2 under GPS but not under WFQ. P 2can be said to leapfrog Q2and R 1under WFQ. Suppose packets P 1, Q1, P2, Q2and R 1arrive at a router at the following times T, and with the following lengths L. The output bandwidth is 1 length unit per time unit; that is, r=1. The total number of length units is 24. Each subqueue is allocated an equal share of the bandwidth; egùõº=1/3. 23.5 Fair Queuing 565
An Introduction to Computer Networks, Release 2.0.11 subqueue 1 subqueue 2 subqueue 3 P1: T=0, L=1 Q1: T=0, L=2 R1: T=10, L=5 P2: T=2, L=10 Q2: T=4, L=6 Under WFQ, we send P 1and then Q 1; Q1is second because its Ô¨Ånishing time is later. When Q 1Ô¨Ånishes the wallclock time is T=3. At this point, P 2is the only packet available to send; it Ô¨Ånishes at T=13. Up until T=10, we have two packets in progress under GPS (because Q 1Ô¨Ånishes under GPS at T=4 and Q2arrives at T=4), and so the GPS clock runs at rate 3/2 of wallclock time and the BBRR clock runs at rate 1/2 of wallclock time. At T=4, when Q 2arrives, the BBRR clock is at 2 and the VC clock is at 6 and we calculate the BBRR Ô¨Ånishing time as 2+6=8 and the GPS Ô¨Ånishing time as 6+6/(1/3) = 24. At T=10, the BBRR clock is at 5 and the GPS clock is 15. R 1arrives then; we calculate its BBRR Ô¨Ånishing time as 5+5=10 and its GPS Ô¨Ånishing time as 15+5/ ùõº= 30. Because Q 2has the earlier virtual-clock Ô¨Ånishing time, WFQ sends it next after P 2, followed by R 1. Here is a diagram of transmission under GPS. The chart itself is scaled to wallclock times. The BBRR clock is on the scale below; the VC clock always runs three times faster. The circled numbers represent the size of the portion of the packet sent in the intervals separated by the dotted vertical lines; for each packet, these add up to the packet‚Äôs total size. Note that, while the transmission order under WFQ is P 1, Q1, P2, Q2, R1, the Ô¨Ånishing order under GPS is P1, Q1, Q2, R1, P2. That is, P 2managed to leapfrog Q 2and R 1under WFQ by the simple expedient of being the only packet available for transmission at T=3. 23.5.4.6 GPS Example 4 As a second example of leapfrogging, suppose we have the following arrivals; in this scenario, the smaller but later-arriving R 1Ô¨Ånishes ahead of P 1and Q 2under GPS, but not under WFQ. subqueue 1 subqueue 2 subqueue3 P1: T=0, L=1000 Q1: T=0, L=200 R1: T=600, L=100 Q2: T=0, L=300 566 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 The following diagram shows how the packets shared the link under GPS over time. As can be seen, the GPS Ô¨Ånishing order is Q 1, R1, Q2, P1. Under WFQ, the transmission order is Q 1, Q2, P1, R1, because when Q 2Ô¨Ånishes at T=500, R 1has not yet arrived. 23.5.4.7 Finishing-Order Bound These examples bring us to the following delay-bound claim, due to Parekh and Gallager [PG93] (see also [PG94]); we will make use of it below in 24.12 Parekh-Gallager Theorem. It is arguably the deepest part of the Parekh-Gallager theorem. Claim: For any packet P, the wallclock Ô¨Ånishing time of P at a router R under WFQ cannot be later than the wallclock Ô¨Ånishing time of P at R under GPS by more than the time R needs to transmit the maximum-sized packet that can appear. Expressed symbolically, if F WFQ and F GPSare the Ô¨Ånishing times for P under WFQ and GPS, R‚Äôs outbound transmission rate is r, and L maxis the maximum packet size that can appear at R, then FWFQ¬§FGPS+ L max/r This is the best possible bound; L max/r is the time packet P must wait if it has arrived an instant too late and another packet of size L maxhas started instead. Note that, if a packet‚Äôs subqueue is inactive, the packet starts transmitting immediately upon arrival under GPS; however, GPS may send the packet relatively slowly. To prove this claim, let us number the packets P 1through P kin order of WFQ transmission, starting from the most recent point when at least one subqueue of the router became active. (Note that these packets may be spread over multiple input subqueues.) For each i, let F ibe the Ô¨Ånishing time of P iunder WFQ, let G ibe the Ô¨Ånishing time of P iunder GPS, and let L ibe the length of P i; note that, for each i, F i+1= L i+1/r + F i. If P kÔ¨Ånishes after P 1through P k-1under GPS, then the argument above ( 23.5.4.2 Finishing Order under GPS and WFQ ) for the all-subqueues-active case still applies to show P kcannot Ô¨Ånish earlier under GPS than it does under WFQ; that is, we have F k¬§Gk. Otherwise, some packet P iwith i<k must Ô¨Ånish after P kunder GPS; P ihasleapfrogged Pkunder WFQ, presumably because P kwas late in arriving. Let P mbe the most recent (largest m<k) such leapfrogger 23.5 Fair Queuing 567
An Introduction to Computer Networks, Release 2.0.11 packet, so that P mÔ¨Ånishes after P kunder GPS but P m+1through P k-1Ô¨Ånish earlier (or are tied); this was illustrated above in 23.5.4.5 GPS Example 3 for k=5 and m=3. We next claim that none of the packets P m+1through P kcould have yet arrived at R at the time T startwhen Pmbegan transmission under WFQ. If some P iwith i>m were present at time T start, then the fact that it is transmitted after P munder WFQ would imply that the calculated GPS Ô¨Ånishing time came after that of P m. But, as we argued earlier, calculated virtual-clock GPS Ô¨Ånishing times are always the actual virtual-clock GPS Ô¨Ånishing times, and we cannot have P iÔ¨Ånishing both ahead of P mand behind it. Recalling that F mis the Ô¨Ånishing time of P munder WFQ, the time T startabove is simply F mLm/r. Between Tstartand G k, all the packets P m+1through P kmust arrive and then, under GPS, depart. The absolute minimum time to send these packets under GPS is the WFQ time for end-to-end transmission, which is (L m+1+ .. + L k)/r = F kFm. Therefore we have GkTstart¬•FkFm Gk- (F mLm/r)¬•FkFm Fk¬§Gk+ L m/r¬§Gk+ L max/r The last line represents the desired conclusion. 23.5.5 DeÔ¨Åcit Round Robin The BBRR approach has cost O(log n), where n is the number of packets waiting, as outlined earlier. One simple though approximate alternative that has O(1) performance, introduced in [SV96], is to assign to each input subqueue a numeric quantum, proportional to that subqueue‚Äôs bandwidth share. Each subqueue will also maintain a deÔ¨Åcit representing the cumulative amount of data it was granted permission to send in the past, but wasn‚Äôt able to because the next packet was too large. The deÔ¨Åcit will always be smaller than the size of the subqueue‚Äôs head packet. When the subqueue is idle (and so there is no head packet), the deÔ¨Åcit is always set to zero. At each round, a subqueue will be granted permission to send one additional quantum‚Äôs worth of data; that is, the quantum will be added to the subqueue‚Äôs deÔ¨Åcit. If this new deÔ¨Åcit is larger than the size of the head packet, then that packet can be sent, and possibly other packets as well. The sizes of the packets sent are then subtracted from the deÔ¨Åcit, which will now be smaller than the size of the new head packet, and the next subqueue gets a turn. Often the quantum is a number of bytes at least as large as the network MTU; in this case, each subqueue will be permitted to send at least one packet on each round. This is not a requirement, however; often the quantum is small enough that it takes many rounds to accumulate enough of a deÔ¨Åcit to send a packet. One cost of the quantum approach is that the elegant delay bound of 23.5.4.7 Finishing-Order Bound no longer holds. In fact, a sender may be forced to wait for all other senders each to send a full quantum, even if the sender has only a small packet. In particular, deÔ¨Åcit round robin is not the same as BBRR, even if the deÔ¨Åcit is one bit. The subqueues are always granted the opportunity to send in the round-robin order, rather than in the order of virtual Ô¨Ånishing time. See exercises 3.0 and 5.5. If the quantum is 1000 bytes, the deÔ¨Åcit is zero, and we have two 600-byte packets in the current subqueue, then the Ô¨Årst of the packets is sent and the deÔ¨Åcit is updated to 400. If instead we have two 400-byte packets then the deÔ¨Åcit is 0 as sending the two 400-byte packets empties the subqueue. If the quantum is 1000 bytes and a steady stream of 600-byte packets arrives on one subqueue, they are sent as follows 568 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 round quantum + prev deÔ¨Åcit packets sent new deÔ¨Åcit 1 1000 1 400 2 1400 2,3 200 3 1200 4,5 0 In three rounds the subqueue has been allowed to send 5 600 = 3000 bytes, which is exactly 3 quantums. We can implement weighted fair queuing using the quantum algorithm by adjusting the quantum sizes in proportion to the weights; that is, if the weights are 40% and 60% then the respective quanta might be 1000 bytes and 1500 bytes. If the quantums are Q 1, Q2,. .. , Q n, and if all subqueues are always nonempty, then in the long run the ith subqueue will get a share of the bandwidth equal to Q i/(Q1+Q2+. .. +Q n), just as with other implementations of fair queuing. 23.5.6 Stochastic Fair Queuing Stochastic fair queuing ([McK90]) is a different kind of approximation to fair queuing. The ideal is to give each individual TCP connection through a router its own fair queuing class, so that no connection has an incentive to keep more than the minimum number of packets in the queue. However, managing so many separate (and dynamically changing) input classes is computationally intensive. Instead, a hash function is used to put each TCP connection (as identiÔ¨Åed by its socketpair) into one of several buckets (the current default bucket count in the Linux implementation is 1024). The buckets are then used as the fair queuing input queues. If two connections hash to the same bucket, each will get only half the bandwidth to which it is entitled. Therefore the hash function has some additional parameters that allow it to be perturbed at regular intervals; after perturbation, socketpairs that formerly hashed to the same bucket likely now will not. The Linux implementation of stochastic fair queuing, known as sfq, uses the quantum-based deÔ¨Åcit round robin algorithm, above. The different subqueues ultimately share a common pool of buffer space. If a packet arrives when no more space is available, the last packet from the fullest subqueue is dropped. This rewards Ô¨Çows that hold their queue utilization to modest levels. 23.6 Applications of Fair Queuing As we saw in the Queue-Competition Rule in 20.2.2 Example 2: router competition, if a bottleneck router uses FIFO queuing then it pays a sender, in terms of the bandwidth it receives, to keep as many packets as possible in the queue. Fair queuing, however, can behave quite differently. If fair queuing is used with a separate class for each connection, this ‚Äúgreediness‚Äù beneÔ¨Åt evaporates; a sender need only keep its per-class queue nonempty in order to be guaranteed its assigned share. Senders can limit their queue utilization to one or two packets without danger of being crowded out by competing trafÔ¨Åc. As another application of fair queuing, suppose we have three web servers, A, B and C, to which we want to give equal shares at sending along a 6 Mbps link; all three reach the link through router R. One way would be to have R restrict them each to 2 Mbps, but that is not work-conserving: if one server is idle then its share goes unused. Fair queuing offers a better approach: if all three are busy, each gets 2 Mbps; if two are busy then each gets 3 Mbps, and if only one is busy it gets 6 Mbps. ISPs can also use this strategy internally whenever they have several Ô¨Çows competing for one outbound link. 23.6 Applications of Fair Queuing 569
An Introduction to Computer Networks, Release 2.0.11 A B CR ISP6 Mbps Unfortunately, this strategy does not work quite as well with three receivers. Suppose now A, B and C are three pools of users ( egthree departments), and we want to give them each equal shares of an inbound 6 Mbps link: A B CR ISP6 Mbps Inbound fair queuing could be used at the ISP (upstream) end of the connection, where the three Ô¨Çows would be identiÔ¨Åed by their destinations. But the ISP router is likely not ours to play with. Once the three Ô¨Çows arrive at R, it is too late to allocate shares; there is nothing to do but deliver the trafÔ¨Åc to A, B and C respectively. We will return to this scenario below in 24.4.1 Applications of Token Bucket. Note that for these kinds of applications we need to be able to designate administratively how packets get classiÔ¨Åed into different input classes. The automatic classiÔ¨Åcation of stochastic fair queuing, above, will not help here. 23.6.1 Fair Queuing and Bufferbloat Fair Queuing provides an alternative approach, at least theoretically, to the bufferbloat problem (21.5.1 Bufferbloat ). In 21.5 Active Queue Management we discussed how FIFO routers can reduce excessive queue utilization. However, if a router abandoned FIFO and instead awarded each Ô¨Çow its own fair-queuing subqueue, there would no longer be an incentive for a TCP implementation to, like TCP Reno and TCP Cubic ( 22.15 TCP CUBIC ), grab as much queue capacity as possible. The queue-competition rule of 20.2.2 Example 2: router competition would simply no longer apply. N connections through the router would each receive 1/N of the bottleneck bandwidth, as long as each Ô¨Çow kept at least one packet in its subqueue at all times (WFQ might change the proportions, but not the principle). If end users could count on fair queuing in routers, the best transit choice would become something like TCP Vegas ( 22.6 TCP Vegas ), which keeps a minimum number of packets in the bottleneck queue. TCP Vegas is largely unused today because it competes poorly in FIFO queues with TCP Reno ( 22.6.1 TCP Vegas versus TCP Reno ), but all that would change with fair queuing. Depending on the maximum subqueue capacity, TCP Reno users in this new routing regime might receive as little as 75% of their ‚Äúfair share‚Äù through the bottleneck router ( 19.7 TCP and Bottleneck Link Utilization ), but the link itself would never go idle as long as there were active TCP Vegas connections. Real-time UDP Ô¨Çows (perhaps carrying V oIP trafÔ¨Åc) would also see negligible queuing delay, even in the face of large Ô¨Åle transfers going on in parallel. As a practical matter, a fair-queuing subqueue for each separate TCP connection is expensive. A reasonable compromise, however, might be to use stochastic fair queuing, or SFQ ( 23.5.6 Stochastic Fair Queuing ). A 570 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 drop policy of dropping from the fullest subqueue further penalizes TCP Reno and TCP Cubic, and favors TCP Vegas. In31.5 TCP Reno versus TCP Vegas there is a graph of the relative bandwidth utilization of TCP Reno versus TCP Vegas. In that graph, TCP Reno outperforms Vegas by at least fourfold for much of the range, and achieves tenfold greater throughput with the largest queue capacities. Here, by comparison, is a chart when the bottleneck queue is changed from FIFO to SFQ: In this graph, TCP Vegas always gets more bandwidth than TCP Reno, though for large queue capacities Reno comes within 80%. The TCP Reno connection still faces bufferbloat problems, but the TCP Vegas connection is unaffected. (In the ns-2 simulator, SFQ‚Äôs total queue capacity is set with, eg,Queue/SFQ set maxqueue_ 100 .) Of course, most routers do notenable fair queuing, stochastic or otherwise, at least not at the level of individual Ô¨Çows. So Reno and Cubic still win, and Vegas remains an unfruitful choice. Fair queuing costs money, as there is more packet processing involved. Compounding this is the chicken-and-egg problem: if one ISP did implement fair queuing, end users would stick to TCP Reno and TCP Cubic because they worked best everywhere else, and the beneÔ¨Åts of using protocols like TCP Vegas would go unrealized. 23.7 Hierarchical Queuing Any queuing discipline with multiple input classes can participate in a hierarchy: the root queuing discipline is at the top of the tree, and each of its input classes is fed by a separate lower-level queuing discipline. 23.7 Hierarchical Queuing 571
An Introduction to Computer Networks, Release 2.0.11 Hierarchical Queuing Today Hierarchical queuing is admittedly somewhat out of fashion. One of its original goals was to support complex sharing of a single link by multiple data streams with different bandwidth and delay requirements; in today‚Äôs Internet providers often instead address this with the brute-force approach of excess bandwidth capacity. That said, hierarchical queuing offers an elegant and economical alternative, and despite today‚Äôs trends it is not clear if providing excess capacity will always be an option in the future. The usual understanding of hierarchical queuing is that the non-leaf queuing-discipline nodes are ‚Äúvirtual‚Äù: they do not store data, but only make decisions as to which subqueue to serve. The only physical output interface is at the root, and all physical queues are attached to the leaf nodes. Each non-leaf queuingdiscipline node is allowed to peek at what packet, if any, its subqueues would dequeue if asked, but any node will only actually dequeue a packet unless that packet will immediately be forwarded up the tree to the root. This might be referred to as a leaf-storage hierarchy. An immediate corollary is that leaf nodes, which now hold all the physical packets, will do all the packet dropping. While it is possible to chain real queue objects together so as to construct composite queuing structures in which packets are notstored only at leaf nodes, the resultant internal-node-storage hierarchy will often not function as might be expected. For example, a priority-queuing node that immediately forwards each arriving packet on to its parent has forfeited any control over the priority order of resultant packet transmissions. Similarly, if we try to avoid the complexity of hierarchical fair queuing, below, by connecting separate WFQ nodes into a tree so that the internal nodes immediately forward upwards, then either packets simply pile up at the root, or else the interior links become the bottleneck. For a case where the internal-node-storage approach does work, see the end of 24.9.1 Hierarchical Token Bucket. 23.7.1 Generic Hierarchical Queuing One simple way to enable lazy dequeuing is to require the following properties of the hierarchy‚Äôs component queuing disciplines: 
- each node supports a peek() operation to return the packet it would dequeue if asked, and 
- any non-leaf node can determine what packet it would itself dequeue simply by calling peek() on each of its child nodes With these in place, a dequeue operation at the root node will trigger a depth-Ô¨Årst traversal of the entire tree throughpeek() calls; eventually one leaf node will dequeue its packet and pass it up towards the root. We will call this generic hierarchical queuing. Note that if we have a peek() operation at the leaf nodes, and the second property above for the interior nodes, we can recursively deÔ¨Åne peek() at every node. Many homogeneous queuing-discipline hierarchies support this generic mechanism. Priority queuing, as in the example below, works, because when a parent node needs to dequeue a packet it Ô¨Ånds the highest-priority nonempty child subqueue, dequeues a packet from it, and sends it up the tree. In this case we do not even needpeek(); all we need is is_empty(). Homogeneous FIFO queuing also works, assuming packets are each tagged with their arrival time, because for a node to determine which packet it would dequeue it needs only to peek() at its child nodes and Ô¨Ånd the earliest-arriving packet. 572 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 As an example of where generic hierarchical queuing does not work, imagine a hierarchy consisting of a queuing discipline node that sends the smallest packet Ô¨Årst, fed by two FIFO child nodes. The parent would have to dequeue all packets from the child nodes to determine the smallest. Fair queuing is not quite well-behaved with respect to generic hierarchical queuing. There is no problem if all packets are the same size; in this case the nodes can all implement simple round-robin selection in which they dequeue a packet from the next nonempty child node and send it immediately up the tree. Suppose, however, that to accommodate packets of different sizes all nodes use BBRR (or GPS). The problem is that without timely notiÔ¨Åcation of when packets arrive at the leaf nodes, the interior nodes may not have enough information to determine the correct rates for their virtual clocks. An appropriate fair-queuing-speciÔ¨Åc modiÔ¨Åcation to the generic hierarchical-queuing algorithm is below in 23.8.1 A Hierarchical Weighted Fair Queuing Algorithm. 23.7.2 Hierarchical Examples Our Ô¨Årst example is of hierarchical priority queuing, though as we shall see shortly it turns out in this case that the hierarchy collapses. It may still serve to illustrate some principles, however. The basic queuingdiscipline unit will be the two-input-class priority queue, with priorities 0 (high) and 1 (low). We put one of these at the root, named QR, and then put a pair of such queues (QA and QB in the diagram) at the next level down, each feeding into one of the input classes of the root queue. QR QA QB0 1 0 1 0 1 A B C D Generic Hierarchical Queuing We now need a classiÔ¨Åer, as with any multi-subqueue queuing discipline, to place input packets into one of the four leaf nodes labeled A, B, C and D above; these likely represent FIFO queues. Once everything is set up, the dequeuing rules are as follows: 
- at the root node, we Ô¨Årst see if packets are available in the left (high-priority) subqueue, QA. If so, we dequeue the packet from there; if not, we go on to the right subqueue QB. 
- at either QA or QB, we Ô¨Årst see if packets are available in the left input, labeled 0; if so, we dequeue from there. Otherwise we see if packets are available from the right input, labeled 1. If neither, then the subqueue is empty. The catch here is that hierarchical priority queuing collapses to a single layer, with four priority levels 00, 01, 10, and 11 (from high to low): 23.7 Hierarchical Queuing 573
An Introduction to Computer Networks, Release 2.0.11 root 00 01 10 11 A B C D Flattened Priority Hierarchy For many other queuing disciplines, however, the hierarchy does not collapse. One example of this might be a root queuing discipline PQR using priority queuing, with two leaf nodes using fair queuing, FQA and FQB. (Fair queuing does not behave well as an interior node without modiÔ¨Åcation, as mentioned above, but it can participate in generic hierarchical queuing just Ô¨Åne as a leaf node.) PQR FQA FQB A1 A2 B1 B2 Priority / Fair Queuing Hierarchy Senders A1 and A2 receive all the bandwidth, if either is active; when this is the case, B1 and B2 get nothing. If both A1 and A2 are active, they get equal shares. Only when neither is active do B1 and B2 receive anything, in which case they divide the bandwidth fairly between themselves. The root node will check on each dequeue() operation whether FQA is nonempty; if it is, it dequeues a packet from FQA and otherwise dequeues from FQB. Because the root does not dequeue a packet from FQA or FQB unless it is about to return it via its own dequeue() operation, those two child nodes do not have to decide which of their internal per-class queues to service until a packet is actually needed. 23.8 Hierarchical Weighted Fair Queuing Hierarchical weighted fair queuing is an elegant mechanism for addressing naturally hierarchical bandwidthallocation problems, even if it cannot be properly implemented by ‚Äúgeneric‚Äù methods of creating queuingdiscipline hierarchies. There are, fortunately, algorithms speciÔ¨Åc to hierarchical fair queuing. The Ô¨Çuid model provides a convenient reference point for determining the goal of hierarchical weighted fair queuing. Each interior node divides its bandwidth according to the usual one-level WFQ mechanism: if N is an interior node, and if the ith child of N is guaranteed fraction ùõºiof N‚Äôs bandwidth, and A is the set of N‚Äôs 574 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 currently active children, then WFQ on N will allocate to active child j the fraction ùõΩjequal to ùõºjdivided by the sum of the ùõºifor i in A. Now we can determine the bandwidth allocated to each leaf node L. Suppose L is at level r (with the root at level 0), and let ùõΩkbe the fraction currently assigned to the node on the root-to-L path at level 0<k ¬§r. Then L should receive service in proportion to ùõΩ1.. .ùõΩr, the product of the fractions along the root-to-L path. For example, in the following hierarchical model, leaf node L1 should receive fraction 70% 40%50% = 14% of the total bandwidth. FQR FQA FQB FQC FQD L1 L270% 30% 50%60% 50%... ... Hierarchical Weighted Fair Queuing L1 is guaranteed 70%√ó40%√ó50% = 14%40% If, however, node FQD becomes inactive, then FQA will assign 100% to FQC, in which case L1 will receive 70%100%50% = 35% of the bandwidth. Alternatively, we can stick to real packets, but simplify by assuming all are the same size andthat child allocations are all equal; in this situation we canimplement hierarchical fair queuing via generic hierarchical queuing. Each parent node simply dequeues from its child nodes in round-robin fashion, skipping over any empty children. Hierarchical fair queuing ‚Äì unlike priority queuing ‚Äì does notcollapse; the hierarchical queuing discipline is not equivalent to any one-level queuing discipline. Here is an example illustrating this. We form a tree of three fair queuing nodes, labeled FQR, FQA and FQB in the diagram. Each grants 50% of the bandwidth to each of its two input classes. 23.8 Hierarchical Weighted Fair Queuing 575
An Introduction to Computer Networks, Release 2.0.11 FQR FQ ABCD ABCDFQA FQB Hierarchical Flat50%50% 50%50% 50%25%25%25%25%50% If all four input classes A,B,C,D are active in the hierarchical structure, then each gets 25% of the total bandwidth, just as for a Ô¨Çat four-input-class fair queuing structure. However, consider what happens when only A, B and C are active, and D is idle. In the Ô¨Çat model, A, B and C each get 33%. In the hierarchical model, however, as long as FQA and FQB are both active then each gets 50%, meaning that A and B split FQA‚Äôs allocation and receive 25% each, while C gets 50%. As an application, suppose two teams, Left and Right, are splitting a shared outbound interface; each team has contracted to receive at least 50% of the bandwidth. Each team then further subdivides its allocation among its own members, again using fair queuing. When A, B and C are the active senders, then Team Right ‚Äì having active sender C ‚Äì still expects to receive its contractual 50%. Team Right may in fact have decided to silence D speciÔ¨Åcally so C would receive the full allocation. Hierarchical fair queuing can notbe implemented by computing a Ô¨Ånishing time for each packet at the point it arrives. By way of demonstration, consider the following example from [BZ97], with root node R and interior child node C. R C A B D80% 20% 15/16 1/16 When A is idle, B gets 4 times D's bandwidth When A is active, B gets 1/4 D's bandwidth If A is idle, but B and D are active, then B should receive four times the bandwidth of D. Assume for a moment that a large number of packets have arrived for each of B and D. Then B should receive the entire 80% of C‚Äôs share. If transmission order can be determined by packet arrival, then the order will look something like d1, b1, b2, b3, b4, 576 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 d2, b5, b6, b7, b8,. .. Now suppose A wakes up and becomes active. At that point, B goes from receiving 80% of the total bandwidth to 5% = 80% 1/16, or from four times D‚Äôs bandwidth to a quarter of it. The new b/d relative rate, not showing A‚Äôs packets, must be b1, d1, d2, d3, d4, b2, d5, d6, d7, d8,. .. The relative frequencies of B and D packets have been reversed by the arrival of later A packets. The packet order for B and D is thus dependent on later arrivals on another queue entirely, and thus cannot be determined at the point the packets arrive. After presenting the actual hierarchical-WFQ virtual-clock algorithm, we will return to this example in 23.8.1.2 A Hierarchical-WFQ Example. 23.8.1 A Hierarchical Weighted Fair Queuing Algorithm The GPS-based WFQ scheduling algorithm is almost suitable for use in the generic-hierarchical-queuing framework; two adjustments must be made. The Ô¨Årst adjustment is that each non-leaf node must be notiÔ¨Åed whenever any of its formerly empty subqueues becomes active; the second adjustment is a modiÔ¨Åcation to how ‚Äì and more importantly when ‚Äì a packet‚Äôs virtual Ô¨Ånishing time is calculated. As for the active-subqueue notiÔ¨Åcation, each non-leaf node can, of course, check after dequeuing each packet which of its subqueues is active, using the is_empty() operation. This, however, may be significantly after the subqueue-activating packet has arrived. A WFQ node needs to know the exact time when a subqueue becomes active both to record the GPS virtual-clock start time for the subqueue, and to know when to change the rate of its virtual clock. Addition of this subqueue-activation notiÔ¨Åcation to hierarchical queuing is straightforward. When a previously empty leaf node receives a packet, it must send a notiÔ¨Åcation to its parent. Each interior node of the hierarchy must in turn forward any received subqueue-activation notiÔ¨Åcation to itsparent, provided that none of its other child subqueues were already active. If the interior node already had other active subqueues, then that node is itself active and no new notiÔ¨Åcation needs to be sent. In this way, when a leaf node becomes active, the news will be propagated towards the root of the tree until either the root or an already-active interior node is reached. To complete the hierarchical WFQ algorithm, we next describe how to modify the algorithm of 23.5.4 The GPS Model to support subqueues of anytype ( egFIFO, priority, or in our case hierarchical subtrees), provided inactive subqueues notify the WFQ parent when they become active. 23.8.1.1 WFQ with non-FIFO subqueues Suppose we want to implement WFQ where the per-class subqueues, instead of being FIFO, can be arbitrary queuing disciplines; again, the case in which we are interested is when the subqueue represents a subhierarchy. The order of dequeuing from each subqueue might be changed by later arrivals ( egas in priority queuing), and packets in the subqueues might even disappear (as with random-drop queuing). (We will assume that nonempty subqueues can only become empty through a dequeuing operation; this holds in all the cases we will consider here.) The original WFQ algorithm envisioned labeling each packet P with its Ô¨Ånishing time as follows: 23.8 Hierarchical Weighted Fair Queuing 577
An Introduction to Computer Networks, Release 2.0.11 FP= max(VC(now), F prev) + S/ ùõºi Clearly, this labeling of each packet upon its arrival is incompatible with subqueues that might place laterarriving packets ahead of earlier-arriving ones. The original algorithm must be modiÔ¨Åed. It turns out, though, that a WFQ node need only calculate Ô¨Ånishing times F Pfor the packets P that are at the heads of each of its subqueues, and even that needs only to be done at the time the dequeue() operation is invoked. No waiting packet needs to be labeled. In fact, a packet P1 might be at the head of one subqueue, and be passed over in a dequeue() operation for too large an F P1, only to be replaced by a different packet P2 during the next dequeue() call. It is sufÔ¨Åcient for the WFQ node to maintain, for each of its subqueues, a variable NextStart, representing the virtual-clock time (according to the WFQ node‚Äôs own virtual clock) to be used as the start time for that subqueue‚Äôs next transmitted packet. A subqueue‚Äôs NextStart value serves as the max(VC(now), F prev) of the single-layer WFQ formula above. When the parent WFQ node is called upon to dequeue a packet, it calls thepeek() operation on each of its subqueues and then calculates the Ô¨Ånishing time F Pfor the packet Pcurrently at the head of each subqueue as NextStart + size(P)/ ùõº, where ùõºis the bandwidth fraction assigned to the subqueue. If a formerly inactive subqueue becomes active, it by hypothesis notiÔ¨Åes the parent WFQ node. The parent node records the time on its virtual clock as the NextStart value for that subqueue. Whenever a subqueue is called upon to dequeue packet P, its NextStart value is updated to NextStart + size(P)/ ùõº, the virtual-clock Ô¨Ånishing time for P. The active-subqueue notiÔ¨Åcation is also exactly what is necessary for the WFQ node to maintain its virtual clock. If A is the set of active subqueues, and the ith subqueue has bandwidth share ùõºi, then the clock is to run at rate equal to the reciprocal of the sum of the ùõºifor i in A. This rate needs to be updated whenever a subqueue becomes active or inactive. In the Ô¨Årst case, the parent node is notiÔ¨Åed by hypothesis, and the second case happens only after a dequeue() operation. There is one more modiÔ¨Åcation needed for non-root WFQ nodes: we must suspend their virtual clocks when they are not ‚Äútransmitting‚Äù, following the argument at the end of 23.5.4 The GPS Model. Non-root nodes do not have real interfaces and do not physically transmit. However, we can say that an interior node N is logically transmitting when the root node R is currently transmitting a packet from leaf node L, and N lies on the path from R to L. Note that all interior nodes on the path from R to L will be logically transmitting simultaneously. For a speciÔ¨Åc non-root node N, whenever it is called upon at time T to dequeue a packet P, its virtual clock should run during the wallclock interval from T to T + size(P)/r, where r is the root node‚Äôs physical bandwidth. The virtual Ô¨Ånishing time of P need not have any direct correspondence to this actual Ô¨Ånishing time T + size(P)/r. The rate of N‚Äôs virtual clock in the interval from T to T + size(P)/r will depend, of course, on the number of N‚Äôs active child nodes. We remark that the dequeue() operation described above is relatively inefÔ¨Åcient; each dequeue() operation by the root results in recursive traversal of the entire tree. There have been several attempts to improve the algorithm performance. Other algorithms have also been used; the mechanism here has been taken from [BZ97]. 23.8.1.2 A Hierarchical-WFQ Example Let us consider again the example at the end of 23.8 Hierarchical Weighted Fair Queuing: 578 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 R C A B D80% 20% 15/16 1/16 When A is idle, B gets 4 times D's bandwidth When A is active, B gets 1/4 D's bandwidth Assume that all packets are of size 1 and R transmits at rate 1 packet per second. Initially, suppose FIFO leaf nodes B and D have long backlogs ( egb1, b2, b3,. .. ) but A is idle. Both of R‚Äôs subqueues are active, so R‚Äôs virtual clock runs at the wall-clock rate. C‚Äôs virtual clock is running 16 fast, though. R‚Äôs NextStart values for both its subqueues are 0. The Ô¨Ånishing time assigned by R to d iwill be 5i. Whenever packet d ireaches the head of the D queue, R‚ÄôsNextStart for D will be 5(i-1). (Although we claimed in the previous section that hierarchical WFQ nodes shouldn‚Äôt need to assign Ô¨Ånishing times beyond that for the current head packet, for FIFO subqueues this is safe.) At least during the initial A-idle period, whenever R checks C‚Äôs subqueue, if b iis the head packet then R‚Äôs NextStart for C will be 1.25(i-1) and the calculated virtual Ô¨Ånishing time will be 1.25i. If ties are decided in B‚Äôs favor then in the Ô¨Årst ten seconds R will send b1, b2, b3, b4,d1, b5, b6, b7, b8,d2 During the ten seconds needed to send the ten packets above, all of the packets dequeued by C come from B. Having only one active subqueue puts C is in the situation of 23.5.4.4 GPS Example 2, and so its packets‚Äô calculated Ô¨Ånishing times will exactly match C‚Äôs virtual-clock value at the point of actual Ô¨Ånish. C dequeues eight packets, so its virtual clock runs for only those 8 of the 10 seconds during which one of the b iis being transmitted. As a result, packet b iÔ¨Ånishes at time 16i by C‚Äôs virtual clock. At T=10, C‚Äôs virtual clock is 816 = 128. Now, at T=10, as the last of the ten packets above completes transmission, let subqueue A become backlogged with a 1, a2, a3,etc. C will assign a Ô¨Ånishing time of 128 + 1.0667i to a i(1.0667 = 16/15); C has already assigned a virtual Ô¨Ånishing time of 9 16=144 to b 9. None of the virtual Ô¨Ånishing times assigned by C to the remaining b iwill change. At this point the virtual Ô¨Ånishing times for C‚Äôs packets are as follows: 23.8 Hierarchical Weighted Fair Queuing 579
An Introduction to Computer Networks, Release 2.0.11 packet C Ô¨Ånishing time R Ô¨Ånishing time a1 128 + 1.0667 10 + 1.25 a2 128 + 21.0667 10 + 2.50 a3 128 + 31.0667 10 + 3.75 a4 128 + 41.0667 10 + 5. .. a15 128 + 151.0667 = 144 10 + 151.25 b9 144 10 + 161.25 = 30 During the time the 16 packets in the table above are sent from C, R will also send four of D‚Äôs packets, for a total of 20. The virtual Ô¨Ånishing times assigned by C to b 9and b 10have notchanged, but note that the virtual Ô¨Ånishing times assigned to these packets by R are now very different from what they would have been had A remained idle. With A idle, these Ô¨Ånishing times would have been F 9= 11.25 and F 10= 12.50, etc. Now, with A active, it is a 1and a 2that Ô¨Ånish at 11.25 and 12.50; b 9will now be assigned by R a Ô¨Ånishing time of 30 and b 10 will be assigned a Ô¨Ånishing time of 50. R is still assigning successive Ô¨Ånishing times at increments of 1.25 to packets dequeued by C, but B‚Äôs contributions to this stream of packets have been bumped far back. R‚Äôs assignments of virtual Ô¨Ånishing times to the d iare immutable, as are C‚Äôs assignments of virtual Ô¨Ånishing times, but R can notassign a Ô¨Ånal virtual Ô¨Ånishing time to any of C‚Äôs packets (that is, A‚Äôs or B‚Äôs) until the packet has reached the head of C‚Äôs queue. R assigns Ô¨Ånishing times to C‚Äôs packets in the order they are dequeued, and until a packet is dequeued by C it is subject to potential preemption. 23.9 Epilog If we want to use 100% the outbound bandwidth, but divide it among several senders according to a predetermined ratio, fair queuing is the tool to use. This allows us, for example, to dedicate 25% of the outbound bandwidth to V oIP trafÔ¨Åc, in such a way that nothing is wasted if nobody is using V oIP at the moment. One catch is that fair queuing is harder to apply to the inbound bandwidth. Fair queuing also has applications to the routing of ordinary packets; for example, if routers implement fair queuing of TCP trafÔ¨Åc on a per-connection basis, then senders will have no incentive to maximize queue utilization and TCP Reno will lose its competitive advantage. Of course, implementing fair queuing on a per-connection basis becomes tricky if there are thousands of connections. 23.10 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.17 Solutions for Queuing and Scheduling. 1.0. Suppose a router uses fair queuing with three input classes, and uses the quantum algorithm of 23.5.2 Virtual Finishing Times. The Ô¨Årst class sends packets of size 900 bytes, the second sends packets of 400 bytes, and the third sends packets of 200 bytes. List what would be sent by each Ô¨Çow in each of the Ô¨Årst Ô¨Åve rounds. 580 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 2.0. Suppose we attempt to simulate BBRR as follows with the following strategy we will call SBBRR. Each subqueue has a bit-position marker that advances by one bit for each bit we have available to send from that queue. If three queues are active, then in the time it takes us to send three bits, each marker would advance by one bit. When all the bits of a packet have been ticked through, the packet is sent. (a). Explain why this is not the same as BBRR fair queuing (even with equal-sized packets). (b). Is it the same as BBRR if all input queues are active? 3.0. Suppose we modify the SBBRR strategy of the previous exercise so that, if the output link is ever idle, and no packet has yet had all its bits ticked through by the bit-position marker, then we immediately send the packet with the fewest bits remaining to be ticked through by the bit-position marker. Suppose packets P1 of size 1000 and P2 of size 100 arrive on subqueue 1. Just after P1 begins transmission, packet Q1 of size 400 arrives on subqueue 2. Fair queuing should send the packets in the order P1, Q1, P2; show that the mechanism described here does not do that. 4.0. Suppose we attempt to implement fair queuing by calculating the Ô¨Ånishing time F jfor P j, the jth packet in subqueue i, as follows. 
- Start j+1= max(F j, now) (‚Äúnow‚Äù by wallclock time) 
- F j= Start j+ NLj where N is the total number of subqueues, active or not. (a). Suppose a router has three subqueues; ieN=3. The outbound bandwidth is 1 size unit / 1 time unit. At T=0, packets P1, P2, P3, P4 and P5 arrive for subqueue 1, each of size 1 unit. At T=2 (by which point P1 and P2 will have Ô¨Ånished transmission), packets Q1 and Q2 arrive on subqueue 2, also of size 1. What Ô¨Ånishing times will all the packets be assigned? In what order will they be transmitted? (b). Is this strategy approximately equivalent to fair queuing if we are given that all subqueues of the router are always active? 5.0. Suppose we modify the strategy of the previous exercise by letting N be the number of active subqueues at the time of arrival of packet P j. What happens if we have three input subqueues, and at T=0 Ô¨Åve packets arrive for subqueue 1, and at T=1 Ô¨Åve packets arrive for subqueue 2. Assume all ten packets are of size 1, and the output bandwidth is again 1 size unit per time unit. 6.0‚ô¢. Suppose we have three active subqueues. The Ô¨Årst is Ô¨Ålled with packets of size 190, the second with packets of size 1000 and the third with packets of size 1200. (a). Give the transmission order assuming the packets are transmitted according to BBRR. Continue until three packets of size ¬•1000 bytes are sent. (b). Give the transmission order assuming the packets are transmitted according to deÔ¨Åcit round-robin, with a quantum of 1000. As before, continue until three packets of size ¬•1000 bytes are sent. 7.0. The following packets all arrive at time T=0 at a router with an output rate of one size unit per time unit. 23.10 Exercises 581
An Introduction to Computer Networks, Release 2.0.11 Subqueue 1: P1 of size 100, P2 of size 500, P3 of size 400 Subqueue 2: Q1 of size 300, Q2 of size 200, Q3 of size 600 Subqueue 3: R1 of size 400, R2 of size 500 (a). Find the BBRR virtual Ô¨Ånishing time of each packet (b). Give the actual wallclock Ô¨Ånishing time of each packet, if the packets were sent via BBRR 8.0. Show that byte-by-byte round-robin leads to the same transmission order as bit-by-bit round-robin (BBRR). As with BBRR, assume byte-by-byte round-robin involves calculation of virtual Ô¨Ånishing times, and sending packets based on these. 9.0. Calculate GPS Ô¨Ånishing times for the following packets, all present at T=0. There are two subqueues, and their bandwidth fractions are ùõºandùõΩwhere ùõº= (?5-1)/20.618 and ùõΩ=ùõº2= 1-ùõº. The packet sizes for the two subqueues are as follows (they follow the Fibonacci sequence, except 2 appears twice): ùõº: 2, 3, 8, 21, 55, 144, 377, 987 ùõΩ: 1, 2, 5, 13, 34, 89, 233, 610 Hint: you will have to evaluate ùõºto more decimal places than is shown here. 10.0. Suppose a WFQ router has two subqueues, each with a bandwidth fraction of ùõº=50%. The router transmits 1 byte per ms. Initially, the subqueues are empty and T=0 and the GPS virtual clock is 0. At that moment a packet P1 of size 1000 bytes arrives at the Ô¨Årst subqueue. At T=500, a similarly sized packet P2 arrives at the second subqueue. Give, for each of P1 and P2, (a). Its Ô¨Ånishing time under the GPS virtual clock (b). Its wallclock Ô¨Ånishing time (c). The value of the GPS virtual clock at the moment of WFQ Ô¨Ånishing. 11.0. Suppose a router has three subqueues, and an outbound bandwidth of 1 packet per unit time. Twelve packets arrive at or after T=0, timed so that the router remains busy until Ô¨Ånishing the packets at T=12. (a). What packet arrival schedule leads to the minimum Ô¨Ånal BBRR clock value? (b). What schedule leads to the maximum Ô¨Ånal BBRR clock value? Hint: the rate of the BBRR clock depends only on the number of active subqueues. 12.0. Suppose packets from three subqueues are sent using the quantum algorithm of 23.5.5 DeÔ¨Åcit Round Robin. The packets are listed below in order of arrival for each subqueue, along with their lengths L; the packets are all available at time T=0. The quantum is 1000 bytes. Give the order of transmission. 582 23 Queuing and Scheduling
An Introduction to Computer Networks, Release 2.0.11 Subqueue 1 Subqueue 2 Subqueue 3 P1, L=700 Q1, L=400 R1, L=500 P2, L=700 Q2, L=500 R2, L=600 P3, L=700 Q3, L=1000 R3, L=200 P4, L=700 Q4, L=200 R4, L=900 13.0. At Disneyland, patrons often wait in a queue that winds slowly through one large waiting room, only to feed into another queue in another room. Is this an example of hierarchical queuing, egof one FIFO queue feeding another, without classes? 14.0. Show that a generic hierarchy of FIFO queuing disciplines, described in 23.7.1 Generic Hierarchical Queuing, collapses to a single FIFO queue. 23.10 Exercises 583
An Introduction to Computer Networks, Release 2.0.11 584 23 Queuing and Scheduling
24 TOKEN BUCKET RATE LIMITING The token-bucket algorithm provides an alternative to fair queuing ( 23.5 Fair Queuing ) for providing a trafÔ¨Åc allocation to each of several groups. The main practical difference between fair queuing and token bucket is that if one sender is idle, fair queuing distributes that sender‚Äôs bandwidth among the other senders. Token bucket does not: the bandwidth a sender is allocated is a bandwidth cap. Suppose the outbound bandwidth is 4 packets/ms and we want to allocate to one particular sender, A, a bandwidth of 1 packet/ms. We could use fair queuing and give sender A a bandwidth fraction of 25%, but suppose we do not want A ever to get more bandwidth than 1 packet/ms. We might do this, for example, because A is paying a reduced rate, and any excess available bandwidth is to be divided among the other senders. The catch is that we want the Ô¨Çexibility to allow A‚Äôs packets to arrive at irregular intervals. We could simply wait 1 ms after each of A‚Äôs packets begins transmission, before the next can begin, but this may be too strict. Suppose A has been dutifully submitting packets at 1ms intervals and then the packet that was supposed to arrive at T=6ms instead arrives at T=6.5. If the following packet then arrives on time at T=7, does this mean it should now be held until T=7.5, etc? Or do we allow A to send one late packet at T=6.5 and the next at T=7, on the theory that the average rate is still 1 packet/ms? The latter option is generally what we want, and the solution is to deÔ¨Åne A‚Äôs quota in terms of a tokenbucket speciÔ¨Åcation, which allows for speciÔ¨Åcation of both an average rate and also a burst capacity. The implemented token-bucket speciÔ¨Åcation is often called a token-bucket Ô¨Ålter. If a packet does not meet the token-bucket speciÔ¨Åcation, it is non-compliant; we can do any of the following things: 
- delay the packet until the bucket is ready 
- drop the packet 
- mark the packet as non-compliant The Ô¨Årst option here is often called shaping; the second, more authoritarian option is sometimes known aspolicing. When packet marking is used, the marked packets are sometimes sent at lower priority, and sometimes are dropped preferentially by some downstream router. Note that, of the three approaches here, dropping and marking are the most straightforward to implement. Delaying packets requires creating a place to store them, and an algorithm for resending them at the appropriate time. Policing is generally appropriate only at the point when the trafÔ¨Åc enters the network ( egat the user/ISP interface), where the trafÔ¨Åc generator still has more-or-less full control over the packet spacing. Further downstream, congestion delays may lead to packet bunching, which may lead to violations of the tokenbucket speciÔ¨Åcation that are unfair to blame on the trafÔ¨Åc generator. Another use for token-bucket speciÔ¨Åcations is as a theoretical trafÔ¨Åc description, rather than a rule to be enforced; in this context compliance is a non-issue. A token-bucket Ô¨Ålter can be thought of as a queuing discipline, with an underlying FIFO queue. If noncompliant packets are delayed, it is non-work-conserving. Dropping non-compliant packets can be viewed 585
An Introduction to Computer Networks, Release 2.0.11 as an alternative to tail-drop. The queuing-discipline deÔ¨Ånition in 23.4 Queuing Disciplines does not provide for marking packets, but this is a straightforward extension. 24.1 Token Bucket DeÔ¨Ånition The idea behind a token bucket is that there is a notional bucket somewhere, being Ô¨Ålled at a steady rate with tokens (or, if more divisibility is needed, with Ô¨Çuid); any overÔ¨Çow from the bucket is discarded. To send a packet, we need to be able to take one token from the bucket; if the bucket is empty then the packet is non-compliant and must suffer special treatment as above. If the bucket is full, however, then the sender may send a burst of packets corresponding to the bucket capacity (at which point the bucket will be empty). A common variation is requiring one token per byte rather than per packet, with the Ô¨Åll rate correspondingly scaled; this allows packet size to be taken into account. More precisely, a token-bucket speciÔ¨Åcation TB(r,B max) includes a token Ô¨Åll rate of r tokens/sec, representing the rate at which the bucket Ô¨Ålls with tokens, and also a bucket capacity (or depth) B max>0. The bucket Ô¨Ålls at the rate speciÔ¨Åed, subject to a maximum of B max; we will denote the current capacity by B, or by B(t) if we need to specify the time. In order for a packet of size S (possibly S=1 for counting size in units of whole packets) to be within the speciÔ¨Åcation, the bucket must have at least S tokens; that is, B ¬•S. Otherwise the packet is non-compliant. When the packet is sent, S tokens are removed from the bucket, that is, B=B‚ÄìS. It is possible for the packets of a given Ô¨Çow all to be compliant with a given token-bucket speciÔ¨Åcation at one point ( egone router) in the network but not at another point; this can happen, for example, if more than Bmaxpackets pile up at a downstream router due to momentary congestion. The following graph is a visual representation of a token-bucket constraint. The black and purple curves plotted are of cumulative bits sent as a function of time, that is, bits(t). When bits(t) is horizontal, the sender is idle. 586 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 time tblue line: bits = rtcumulative bits sentred line: TB(r,B) sender can never cross this BmaxB(t) Two token-bucket-compliant senders are shown, one black and one purple. The black sender sends in discrete packets, and the graph is a sequence of steps; the purple sender sends continuously at different rates on different intervals. The blue line represents a sender sending steadily at rate r; the solid red line is the ‚Äúbucket limit‚Äù which a compliant sender may not cross. The purple sender, by crossing below the blue line, cannot go back to the solid red line. In fact the purple line cannot cross the dashed red line after falling ‚Äúbehind‚Äù at point A.ABmax The blue line represents a sender sending linearly at the rate r, with no burstiness. At vertical distance B max above the blue line is the red line. Graphs for compliant senders cannot cross this, because that would entail a burst of more than B maxabove the blue line; we give a more formal argument below. As a sender‚Äôs graph approaches the red line, the sender‚Äôs current bucket contents decreases; the instantaneous bucket contents for the black sender is shown at one point as B(t). The purple sender has fallen below the blue line at one point; as a result, it can never catch up. In fact, after passing through the vertex at point A the purple graph can never cross the dashed red line. A proof is in 24.6.1 Token Bucket Queue Utilization, following some numeric token-bucket examples that illustrate how a token-bucket Ô¨Ålter works. Satellite Token Bucket When I Ô¨Årst got satellite Internet, my service was limited by a token-bucket Ô¨Ålter with rate 56 kbps and bucket 300 megabytes. When the bucket emptied, it took 12 hours to reÔ¨Åll. The idea was that someone could use the Internet intensely but relatively brieÔ¨Çy; satellite access is expensive. Within a year, the provider switched to a Ô¨Çat 300 MB cap per day; the token-bucket rule was apparently not well understood by customers. 24.1 Token Bucket DeÔ¨Ånition 587
An Introduction to Computer Networks, Release 2.0.11 If a packet arrives when there are not enough tokens in the bucket to send it, then as indicated above there are three options. The sender can engage in shaping, making the packet wait until sufÔ¨Åcient tokens accumulate. The sender can engage in policing, dropping the packet. Or the sender can send the packet immediately but mark it as noncompliant. One common strategy is to send noncompliant packets ‚Äì as marked in the third option above ‚Äì with lower priority. Alternatively, marked packets may face a greater chance of being dropped by some downstream router. In ATM networks ( 5.5 Asynchronous Transfer Mode: ATM ) the cell-loss priority bit is often used to mark noncompliant packets. Token-bucket speciÔ¨Åcations supply a framework for making decisions about admission control: a router can decide whether to accept a new connection (or whether to accept the connection‚Äôs quality-of-service request) based on the requested rate and bucket (queue) requirements. Token-bucket speciÔ¨Åcations are the mirror-image equivalent to leaky-bucket speciÔ¨Åcations, in which the Ô¨Çuid leaks out of the leaky bucket at rate r and to send a packet we must add S units without overÔ¨Çowing. The two forms are completely equivalent. So far we have been using token-bucket speciÔ¨Åcations to describe trafÔ¨Åc; egtrafÔ¨Åc arriving at a router. It is also possible to use token buckets to describe the router itself; in this setting, the leaky-bucket formulation may be clearer. The router‚Äôs queue represents the bucket, and the router‚Äôs packet transmissions represent tokens leaking out of the bucket. Arriving packets are added to the bucket; a bucket overÔ¨Çow represents lost packets. We will not pursue this interpretation further. 24.2 Token-Bucket Examples Suppose the token-bucket speciÔ¨Åcation is TB(1/3 packet/ms, 4 packets), and packets arrive at the following times, with the bucket initially full: 0, 0, 0, 2, 3, 6, 9, 12 After all the T=0 packets are processed, the bucket holds 1 token. By the time the fourth packet arrives at T=2, the bucket volume has risen to 1 2/3; it immediately drops to 2/3 when packet 4 is sent. By T=3, the bucket volume has reached 1 and the Ô¨Åfth packet can be sent. The bucket is now empty, but fortunately the remaining packets arrive at 3-ms intervals and can all be sent. In the next set of packet arrival times, again with TB(1/3,4), we have three bursts of four packets each. 0, 0, 0, 0, 12, 12, 12, 12, 24, 24, 24, 24 Each burst empties the bucket, which then takes 12 ms to reÔ¨Åll. All packets are compliant. In the following set of packet arrival times, still with TB(1/3,4), the burst of four packets at T=0 drains the bucket. At T=3 the bucket size has increased back to 1, allowing the packet that arrives then to be sent but also draining the bucket again. 0, 0, 0, 0, 3, 6, 12, 12 At T=6 the same thing happens. From T=6 to T=12 the bucket contents rise from 0 to 2, allowing the two packets arriving at T=12 to be sent. Finally, suppose packets arrive at the following times at our TB(1/3,4) Ô¨Ålter. 588 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 0, 1, 2, 3, 4, 5 Just after T=0 the bucket size is 3; just before T=1 it is 3 1/3. Just after T=1 the bucket size is 2 1/3; just before T=2 it is 2 2/3 Just after T=2 the bucket size is 1 2/3; just before T=3 it is 2 Just after T=3 the bucket size is 1; just before T=4 it is 1 1/3 Just after T=4 the bucket size is 1/3; just before T=5 it is 2/3 At T=5 the bucket size is 2/3 and the arriving packet is noncompliant. We can also represent this in tabular form as follows; note that for the noncompliant packet the bucket is not decremented. packet arrival 01 2 34 5 bucket just before 43 1/3 2 2/3 21 1/3 2/3 bucket just after 32 1/3 1 2/3 11/3 2/3 24.3 Multiple Token Buckets It often makes sense to require that a sender comply with two (or more) separate token-bucket speciÔ¨Åcations. We can think of these being applied to the trafÔ¨Åc sequentially. Often one Ô¨Ålter will specify a peak rate, with a small bucket size, and the other will specify an average rate, with a larger bucket size. Consider, for example, the following pair: 1. TB(1 packet/ms, 1.5 packets) 2. TB(1/5 packet/ms, 6 packets) The Ô¨Årst speciÔ¨Åcation, meant to apply to the peak rate, mandates 1 ms on average between packets, but packets can be only 0.5 ms early without being noncompliant. The second speciÔ¨Åcation, meant to apply over the longer term, states that on average there will be 5 ms between packets, subject to a burst of 6. The following is compliant, assuming both buckets are initially full. 0, 1, 2.5, 3, 4, 5, 6, 10, 15, 20 The Ô¨Årst seven packets arrive at 1 ms intervals (the rate of the Ô¨Årst Ô¨Ålter) except for the packet that arrived at T=2.5 instead of T=2. The sender was allowed to send again at T=3 instead of waiting until T=3.5 because the bucket size in the Ô¨Årst Ô¨Ålter was 1.5 instead of 1.0. Here are the packet arrivals with the current size of each bucket at the time of packet arrival, just before the bucket is decremented. At T=2.0, the Ô¨Ålter2 bucket would be 4.4. arrival: T=0 T=1 T=2.5 T=3 T=4 T=5 T=6 T=10 T=15 T=20 Filter 1: 1.5 1.5 1.5 1.0 1.0 1.0 1.0 1.5 1.5 1.5 Filter 2: 6 5.2 4.5 3.6 2.8 2 1.2 1.0 1.0 1.0 24.3 Multiple Token Buckets 589
An Introduction to Computer Networks, Release 2.0.11 If we move up each packet in time to the Ô¨Årst point when both buckets have reached 1.0, we get the fastest compliant sequence for this pair of Ô¨Ålters. This is the sequence generated by a token-bucket shaper when there is a steady backlog of packets and each is sent as soon as the bucket capacity (or capacities, when applicable) is full enough to allow sending. After T=0, the Ô¨Ålter1 bucket returns to capacity 1.0 at T=0.5. Continuing, the Ô¨Ålter1 bucket allows for additional transmissions at T=1.5, T=2.5, T=3.5, T=4.5 and T=5.5. At this point Ô¨Ålter2 becomes the limiting factor; its bucket is at 0.1 after the T=5.5 packet is sent and does not return to 1.0 until T=10.0. We get the following: arrival: T=0 T=0.5 T=1.5 T=2.5 T=3.5 T=4.5 T=5.5 T=10 T=15 T=20 Filter 1: 1.5 1.0 1.0 1.0 1.0 1.0 1.0 1.5 1.5 1.5 Filter 2: 6 5.1 4.3 3.5 2.7 1.9 1.1 1.0 1.0 1.0 24.4 GCRA Another formulation of the token-bucket speciÔ¨Åcations is the Generic Cell Rate Algorithm, or GCRA; this formulation is frequently used in classiÔ¨Åcation of ATM trafÔ¨Åc. A GCRA speciÔ¨Åcation takes two parameters, a mean packet spacing time T, and an early-arrival allowance ùúè. For each packet we compute a theoretical arrival time, tat, initially zero. A packet may arrive earlier by amount at most ùúè. SpeciÔ¨Åcally, if t is the time of actual arrival, we have two cases: 1. t¬•tat‚Äìùúè: the packet is compliant, and we update tat to max(t,tat) + T 2. t < tat‚Äì ùúè: the packet is too early and is noncompliant; tat is unchanged. A Ô¨Çow satisfying GCRA(T, ùúè) is equivalent to a token-bucket speciÔ¨Åcation with rate 1/T packets/unit time, and bucket size ( ùúè+1)/T; tat represents the time the bucket would next be full. The time to Ô¨Åll an empty bucket is ùúè+1; if the bucket becomes full at time tat then, working backwards, it would contain enough to send one packet at time tat‚Äì ùúè. For trafÔ¨Åc Ô¨Çows with a more-or-less constant rate, ùúèrepresents the time by which one packet can be late without permanently falling behind its regular 1/T rate. The GCRA formulation is sometimes more convenient than the token-bucket formulation, particularly when ùúè<T. 24.4.1 Applications of Token Bucket Unlike fair queuing, token-bucket Ô¨Åltering can be implemented at the downstream end of a link, though possibly with results not quite in agreement with expectations. Let us return to the Ô¨Ånal scenario of 23.6 Applications of Fair Queuing: A B CR ISP6 Mbps While fair queuing cannot be applied at R to enforce equal shares to A, B and C, we canimplement a token-bucket Ô¨Ålter at R that limits each of A, B and C to 2 Mbps. 590 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 There are two drawbacks. First, the Ô¨Ålter is not work-conserving: if A is idle, B and C will still only receive 2 Mbps. Second, in the absence of feedback there is no guarantee that limiting the trafÔ¨Åc at R will eventually result in reduced utilization of the ISP √ù√ëR link. While this is true for TCP trafÔ¨Åc, due to the self-clocking property, it is conceivable that a sender D somewhere is trying to send 8 Mbps of real-time UDP trafÔ¨Åc to A, via ISP and R. Three-quarters of the trafÔ¨Åc would then fail to be compliant, and might be dropped by R, but unless D gets feedback from A that not much of the trafÔ¨Åc is getting through, and that it should reduce its sending rate, the token-bucket Ô¨Ålter at R will not achieve what we want. Most protocols doprovide this kind of feedback, but not all. 24.5 Guaranteeing VoIP Bandwidth As a particular instance of the previous situation, suppose we have an Internet connection from our ISP and want to begin using V oIP for telephony. We would like to reserve something like 64 kbps of bandwidth for one V oIP line (plus room for headers), so that large downloads do not degrade voice quality. We can easily do this for the upstream direction, either with fair or priority queuing; priority queuing will not lead to starvation of other trafÔ¨Åc as the total V oIP trafÔ¨Åc is limited by the number of lines. However, the downstream direction may be more of a problem, if we are unable to enlist the ISP to apply fair or priority queuing at their end. As we argued in 23.6 Applications of Fair Queuing, fair queuing at the downstream end of a congested link has no effect. A queue buildup at the ISP‚Äôs end of the link will mean that incoming V oIP trafÔ¨Åc has to wait in line with trafÔ¨Åc from other downloads, and may never receive the bandwidth it requires. And it is more than likely that the router at the ISP‚Äôs end has a rather large queue, meaning relatively extensive waiting times. We need to limit the total downstream trafÔ¨Åc, but are limited to trafÔ¨Åc manipulations at the downstream end. Token-bucket can provide an answer here. The idea is to limit the aggregate bandwidth of the non-V oIP trafÔ¨Åc entering the site, leaving some room for the V oIP trafÔ¨Åc. We create at the downstream site entrance (node TBF in the diagram below) a token-bucket Ô¨Ålter that applies only to non-V oIP trafÔ¨Åc (this feature is represented by the dashed ‚ÄúV oIP bypass‚Äù path). The Ô¨Ålter‚Äôs rate limit will be the total download bandwidth minus a reservation for V oIP; for example, if we knew that the total bandwidth was 500 bits/ms we might reserve 100 bits/ms, say, for V oIP trafÔ¨Åc by having the token-bucket Ô¨Ålter limit delivery of non-V oIP download trafÔ¨Åc to 400 bits/ms. In the diagram below, the token-bucket Ô¨Ålter is represented conceptually by the large red dot; the short red segment represents the virtual (not physical) ‚Äúbottleneck link‚Äù for the non-V oIP trafÔ¨Åc. ISP host2host1 host3ISP-to-site link siteTBFVoIP bypass Unfortunately, we encounter three problems. The Ô¨Årst is that if no V oIP trafÔ¨Åc is Ô¨Çowing then we probably 24.5 Guaranteeing VoIP Bandwidth 591
An Introduction to Computer Networks, Release 2.0.11 do not want the 400 bits/ms cap on other trafÔ¨Åc; we might arrange this by applying the cap only when the phone is in use, or by setting aside a small enough bandwidth fraction that it does not have a material affect on overall bulk bandwidth. The second problem is the (remote) possibility discussed in the previous example that the sender might keep sending anyway, at 500 bits/ms; our downstream token-bucket Ô¨Ålter can throw away as many bits as it wants but the ISP-to-site link will still be saturated. Third, it is often quite difÔ¨Åcult to determine exactly what the bandwidth of a particular Internet connection is. Especially if, as is often the case, it is shared, or conÔ¨Ågured to change with time, or subject to a large-bucket token-bucket cap by the ISP. Fortunately, typical V oIP bandwidth needs are low enough that one can often muddle through without providing any quality-of-service guarantees at all. This remains, however, a good example of the difÔ¨Åculties often faced by real-time trafÔ¨Åc. 24.6 Limiting Delay Now let‚Äôs repeat the previous example, but instead of trying to guarantee V oIP bandwidth, we will instead attempt to limit the total queuing delay. This may mean that, in the presence of large downloads, V oIP trafÔ¨Åc will get only 50 bits/ms, but there should be little if any queuing at the ISP‚Äôs router. Again, we create a token-bucket Ô¨Ålter at the site entrance, and set it to limit the total rate of incoming trafÔ¨Åc to just below the download link bandwidth. If packets can arrive at 1,000 kbps, we might set the token-bucket rate to 900 kbps, or perhaps even a little higher. This means that the bottleneck link in the downstream direction will now be the ‚Äúvirtual‚Äù link from the downstream token-bucket Ô¨Ålter to the rest of the site, shown in red in the diagram of the previous section. A queue will then build up at this token-bucket Ô¨Ålter, at the red dot in the diagram above. However, at least in the steady state for TCP trafÔ¨Åc, the queue will not build at the ISP‚Äôs end. The next step is to enable CoDel, 21.5.6 CoDel, to manage this queue in front of the token-bucket Ô¨Ålter. CoDel achieves this by dropping trafÔ¨Åc until the queue has an appropriate size. The end result will be that downstream trafÔ¨Åc encounters no queue at the ISP‚Äôs end (again assuming a TCP steady state), and at most a modestly sized CoDel-regulated queue at the site‚Äôs end. Not only have we limited the waiting time for V oIP trafÔ¨Åc, we have limited queuing delays for alltrafÔ¨Åc. In exchange, we are giving up 100% utilization of the downstream link, though if delay is causing signiÔ¨Åcant problems this may be a reasonable tradeoff. It also means CoDel will be throwing away trafÔ¨Åc that has already made it to our site, but, in the long run, that may be well worth it. If the ISP limits download bandwidth via a token-bucket Ô¨Ålter with a signiÔ¨Åcant bucket, as many do in order to accommodate burstiness, we can copy their upstream bucket size at our downstream end. 24.6.1 Token Bucket Queue Utilization Suppose trafÔ¨Åc meeting token-bucket speciÔ¨Åcation TB(r,B max) arrives at a router R, with no competition from other trafÔ¨Åc. The bucket Ô¨Åll rate r corresponds to the minimum outbound link bandwidth needed by R to guarantee that the trafÔ¨Åc does not build up; we do not want trafÔ¨Åc on average arriving faster than it can depart. Intuitively, the bucket size B maxcorresponds to the amount of queue space at R that the Ô¨Çow can consume. To make this more precise, we will argue that, if the output rate from R is at least r, then the number of 592 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 untransmitted bits stored at R is never more than B max. To show this more formally, we start by proving the ‚Äúred line lemma‚Äù implicit in the discussion of the graph in24.1 Token Bucket DeÔ¨Ånition above, that the sender can never cross the red line. SpeciÔ¨Åcally, assume the Ô¨Çow satisÔ¨Åes TB(r,B max) and has a full bucket at time t=0. Let bits(t) be the cumulative number of bits sent (packetized or not) by time t. The blue line is the graph bits = rt and the red line is the graph bits = rt + Bmax; we show the following: bits(t) <= rt + B max We Ô¨Årst prove this so long as the graph is above the blue line; that is, bits(t) >= rt. We claim that the righthand side minus the left-hand side above, rt + B maxbits(t), represents the volume B(t) of Ô¨Çuid (or tokens) in the bucket. Equating and rearranging slightly, we need to show B(t) + bits(t) - rt is always equal to B max. This is true at t=0 when bits(t) = rt = 0 and the bucket is full. We next establish that its rate of change is also 0, and so it is constant. While the bucket is not full, B(t) is always being Ô¨Ålled at rate r. Correspondingly, rt is increasing at rate r, so B(t) - rt is not affected by the Ô¨Åll rate. Similarly, B(t) is being reduced at exactly the rate bits(t) is increasing. If we use the packet formulation, then when a packet arrives B(t) is reduced by the packet size and bits(t) increases by exactly the same amount. The Calculus Version For readers familiar with calculus it may help to note dB(t)/dt = r - bits‚Äô(t), at least if we assume, say, a Ô¨Çuid bit-arrival model where bits(t) is differentiable. That is, the bucket volume B(t) increases at rate r and also decreases at a rate equal to that of arriving data. Therefore, the rate of change of B(t) + bits(t) - rt is just r - bits‚Äô(t) + bits‚Äô(t) - r = 0. This does not quite apply when bits(t) falls below the blue line. However, we have nothing to prove then. If bits(t) has a later interval above the blue line, starting at time t 1, we can reapply the argument above re-starting the clock and the bits counter at t=t 1. In fact, we can argue that whenever the bits(t) graph passes through a point below the blue line, such as point A in the diagram above, then bits(t) cannot in the future climb above the new red line (the dashed red line in the diagram) B maxunits above point A. 24.7 Token Bucket Through One Router We now return to the claim about accumulation at a router R with outbound Ô¨Çow at least r; as before, let bits(t) represent the cumulative amount of data received. As long as bits(t) is above the blue line, the router can continuously transmit at rate r and the net number of bits held within the router is bits(r) - rt. By the argument above, this is bounded by B max. If bits(t) falls below the blue line, the router‚Äôs queue is empty and the router can transmit incoming data at least as fast as it is arriving. While R can never be holding more than B maxbytes, at the instant just before a packet Ô¨Ånishes transmission it can have B maxbytes in the queue, plus the currently transmitting packet still taking up an entire buffer. As a practical matter, then, we may need space equal to B maxplus one packet. 24.7 Token Bucket Through One Router 593
An Introduction to Computer Networks, Release 2.0.11 While a token-bucket speciÔ¨Åcation does not include a delay bound speciÔ¨Åcally, we can compute an upper bound to the queuing delay at a router R as B max/r; this is the time it takes for one full bucket‚Äôs worth of packets to be transmitted. If we have N Ô¨Çows each individually satisfying TB(r,B), then the collective trafÔ¨Åc will satisfy TB(Nr, NB) (see exercise 1.0). However, a bucket size of NB will be needed only when all N individual Ô¨Çows have their bursts ‚Äúgang up‚Äù at a particular instant. Often it is possible to take advantage of theoretical or empirical statistical information and conclude that the collective trafÔ¨Åc ‚Äúmost of the time‚Äù meets a token-bucket speciÔ¨Åcation TB(Nr, B N) for B NsigniÔ¨Åcantly less than NB. 24.8 Token Bucket Through Multiple Routers If we have a single TB(r,B max) Ô¨Çow through N routers, however, the queuing delay is notlarger than for a single router, again assuming no competition. More speciÔ¨Åcally, assume that the trafÔ¨Åc Ô¨Çow arrives at router R1satisfying TB(r,B), and passes in turn through R 1to R N. Each router R ihas an outbound bandwidth at least as large as r. Then the total queuing delay through all N routers remains B max/r. If the packets pile up to the maximum size B max, they only do so once. To prove this we compare the TB sequence of packets with the same sequence of packets sent at a steady rate r through the same series of routers. If the last bit of packet k is the Nth bit since we began, then for the steady stream we send packet k at time N/r. We assume the link rates are all reduced to r. Let t=0 represent the time we start counting bits. For every n, we established above that the nth bit of the TB packet Ô¨Çow can be transmitted at most B max/r seconds ahead of the nth steady-stream bit, which is sent at time n/r. The steady-stream packets do not encounter queuing delays at all, as each router has always Ô¨Ånished the previous one. The TB packets can each arrive no later than the steady-stream packets, as they were sent earlier and they cannot cross. Therefore, the maximum delay faced by any TB packet is B max/r, exactly as for trafÔ¨Åc through a single router. 24.9 Delay Constraints If a trafÔ¨Åc Ô¨Çow arriving at a router R is compliant for token-bucket speciÔ¨Åcation TB(r,B), then as we showed above the amount of R‚Äôs queue space used by the Ô¨Çow will be bounded by B so long as R can devote at least rate r to the Ô¨Çow‚Äôs trafÔ¨Åc. Now let us add a real-time delay constraint: suppose that R is not to be allowed to delay any of the Ô¨Çow‚Äôs packets by more than time D. For the time being, assume that there is no other trafÔ¨Åc at R. We now need to make sure that R has sufÔ¨Åcient bandwidth to forward a bucketful of size B within the time interval D. To send a burst of size B in time D, bandwidth B/D is needed. Therefore, to satisfy the real-time constraint, R needs outbound bandwidth s = max(r, B/D) Example 1: suppose the trafÔ¨Åc speciÔ¨Åcation is TB(1/3, 10), where the rate is in (equal-sized) packets/¬µsec, and D is 40 ¬µsec. Then B/D is 1/4 packets/¬µsec, and the necessary outbound bandwidth s is simply r=1/3. Example 2: now suppose in the previous example that the delay limit D is 20 ¬µsec. In this case, we need s = B/D = 1/2 packets/¬µsec. 594 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 If there isother trafÔ¨Åc, the delay constraint still holds, provided s represents the bandwidth allocated by R to the Ô¨Çow, and the Ô¨Çow‚Äôs packets receive priority service at R, and we Ô¨Årst subtract the largest-packet delay as in 7.3.2 Packet Size and Real-Time TrafÔ¨Åc. Calculations of this sort often play a role in a router‚Äôs decision on whether to accept a reservation for an additional TB(r,B) Ô¨Çow with associated delay constraint. 24.9.1 Hierarchical Token Bucket Token-bucket Ô¨Ålters can also be used to form a hierarchy, as in 23.7.1 Generic Hierarchical Queuing. In this section we will assume that token bucket is used only for shaping; that is, delaying packets until the bucket has sufÔ¨Åciently Ô¨Ålled. As usual, packets will remain in the leaf FIFO queues until they are ready to be transmitted. Central to the hierarchy is the conceptual time each internal token-bucket node releases its next packet; that is, becomes able to inform its parent node (when asked) that it has a packet ready to send, even if the packet physically remains waiting in one of the leaf queues. If a node N is informed by a child node that a packet has been released and N‚Äôs bucket has sufÔ¨Åcient capacity, then N releases the packet in turn to its parent immediately; otherwise N waits until its bucket Ô¨Ålls sufÔ¨Åciently to make the packet compliant. When a packet arrives at a leaf node, it will be progressively released by each node along the path to the root; when it is released by the root node it can be sent. To make token-bucket Ô¨Ålters classful, we will assume that each node may have multiple input subqueues, but treats these as if they were consolidated into a single FIFO subqueue. That is, the node releases packets to its parent in the order they were released to the node by its children. Leaf nodes can mark each packet with its release time at the moment of arrival. Interior nodes may only be able to determine their release times for packets that have been released by their child nodes. It is now straightforward to deÔ¨Åne the peek() operation of 23.7.1 Generic Hierarchical Queuing: a node looks at the set of packets it has released and returns the one with the earliest release time. In a token-bucket hierarchy it makes a sense to say that two child Ô¨Çows have bucket sizes of 200 and 300, respectively, while the combined Ô¨Çow is to be limited to a bucket size of 400. The following diagram illustrates an example of a token-bucket hierarchy. The three token-bucket Ô¨Ålters TB1, TB2 and TB3 have rates in packets/ms and bucket sizes in packets. 24.9 Delay Constraints 595
An Introduction to Computer Networks, Release 2.0.11 TB1 TB(4,70) TB2 TB(2,3) FIFO2TB3 TB(3,6) FIFO3 A B Hierarchical Token Bucket If TB1‚Äôs rate is, as here, less than the sum of its child rates, then as long as its children always have packets ready to send, the children will receive bandwidth in proportion to their token bucket rates. In the example above, TB1‚Äôs rate is 4 packets/ms and yet the sum of the rates of its children is 5 packets/ms. Each child will therefore receive 4/5 its promised rate: TB2 will send at a rate of 2 (4/5) packets/ms while TB3 will send at rate of 3 (4/5) packets/ms. To see this, assume FIFO2 and FIFO3 remain nonempty for a period long enough for their buckets to empty. TB2 and TB3 will then each release packets to TB1 at their respective rates of 2 packets/ms and 3 packets/ms. In the following sequence of release times to TB1, we assume TB3 starts at T=0 and TB2 at T=0.01, to avoid ties. Packets from A released by TB2 are shown in italic: 0,0.01, .33, 0.51, .67, 1.0, 1.01, 1.33, 1.51, 1.67, 2.0, 2.01 They will be dequeued by TB1 at 4 packets/ms, once TB1‚Äôs bucket is empty. In the long run, TB3 has released three packets into this sequence for every two of TB2‚Äôs, so sender B will receive 3/5 of the dequeuings, and thus 3/5 of the 4 packet/ms root bandwidth. We can also have each token-bucket node physically forward released packets to FIFO queues attached to each parent node; we called this an internal-storage hierarchy in 23.7 Hierarchical Queuing. In this particular case, the leaf-storage and internal-storage mechanisms function identically, provided the internal links are inÔ¨Ånitely fast and the internal queues inÔ¨Ånitely large. See exercise 6.0. There is no point in having a node with a bucket larger than the sum of its child buckets and also a rate larger than the sum of its child rates. In the example above, in which the sum of the child rates exceeds the parent rates, A would be able to send at a sustained rate of 2 packets/ms provided B sends at only 2 packets/ms as well; reducing the child rates to 2 (4/5) and 3(4/5) packets/ms respectively is not equivalent. If a node‚Äôs rate is larger than the sum of the child rates, then it will be able to handle the child trafÔ¨Åc without delay once the child buckets have emptied. Before that, though, the parent bucket may be the limiting factor. 24.9.2 Fair Queuing / Token Bucket combinations At Ô¨Årst glance, combining fair queuing with token bucket might seem improbable: the goal of fair queuing is to be work-conserving, allowing the bandwidth assigned to an idle input class to be divided among the active 596 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 input classes, and the goal of token bucket is generally to limit a class to its token-bucket-deÔ¨Åned maximum transmission rate. The usual approach to a hierarchy-based synthesis is to allow the administrator to decide, at each node of the hierarchy, whether or not the node can ‚Äúborrow‚Äù (without repayment) bandwidth from inactive siblings. If it can, the set of siblings with mutual borrowing privileges resembles a fair-queuing scheduler; if not, the node is more like a token-bucket scheduler. 24.10 CBQ CBQ was introduced in [CJ91] and analyzed in [FJ95]. It did not actually use the token-bucket mechanism, but instead implemented shaping by keeping track of the average idle time (more precisely, non-transmitting time) for a given input class. Input classes that tried to send too much were restricted, unless the node was permitted to ‚Äúborrow‚Äù bandwidth from a sibling. When an input class sent less than it was allowed, its average utilization would fall; if a burst arrived then it would take some time for the average to ‚Äúcatch up‚Äù and thus the node could brieÔ¨Çy send faster than its assigned rate. However, the size of the implicit ‚Äúbucket‚Äù could be controlled only indirectly. 24.11 Linux HTB The Linux HTB queuing discipline, part of the TrafÔ¨Åc Control (tc) system, allows the same general functionality of CBQ, but replaces the average-idle calculations with token-bucket Ô¨Ålters. This permits more direct control of burst sizes, and also avoids some technical timing issues that CBQ users had to watch out for. For the sake of efÔ¨Åciency, HTB uses the deÔ¨Åcit round-robin algorithm for fair queuing; as noted in 23.5.5 DeÔ¨Åcit Round Robin, this means less precise control over packet delay. Although the HTB name comes from ‚Äúhierarchical token bucket‚Äù, it is best viewed as primarily an implementation of hierarchical weighted fair queuing ( 23.8 Hierarchical Weighted Fair Queuing ). In fact, HTB doesn‚Äôt actually implement ‚Äúhierarchical token bucket‚Äù; token buckets are for shaping, and HTB does shapingonly at the leaf nodes of the hierarchy. The higher-level, or interior, nodes are present only to create the hierarchy for fair queuing. Each node in the tree has the following attributes: 
- its guaranteed rate, r, corresponding to the token-bucket rate 
- its burst allowance B, corresponding to the bucket size 
- its ceiling rate r ceil; leaf nodes never send faster than this In many cases r ceilmay simply be the output rate of the root node. For interior nodes, if the conÔ¨Ågured ceiling rate is less than the sum of the child rates, the child nodes can still send at their full rates; interior nodes cannot do rate-limiting. A low ceiling rate will, however, affect borrowing, below. The most important attribute of each node is its guaranteed rate. The requested rate at each node should be at least as large as the sum of the child rates. In the following diagram, all rates are in kbps; burst allowances are not shown. We will assume the root guaranteed rate, 600 kbps, is also its ceiling rate. 24.10 CBQ 597
An Introduction to Computer Networks, Release 2.0.11 R 600 N 200M 100N 200 L2 40L1 40L3 80 Linux htb Packets are marked green ,yellow orreddepending on their situation. Red packets are those that must wait; eventually they will turn yellow and then green. Packets are considered green if they are now compliant (perhaps after waiting earlier) for one of the leaf token-bucket nodes; green packets are sent as soon as possible. After L1, L2 and L3 have each emptied their buckets, they will not exhaust node N‚Äôs rate. Similarly, after N and M have emptied their buckets they will use only half of R‚Äôs rate. Nodes are allowed to ‚Äúborrow‚Äù bandwidth ‚Äì without payback ‚Äì from their parent‚Äôs rates; packets beneÔ¨Åting from such borrowed bandwidth are marked yellow, and may also be sent immediately if no green packets are waiting. Borrowing is always in proportion to a node‚Äôs guaranteed rate, in the manner of fair queuing. That is, the guaranteed rates of the child nodes are treated as unnormalized fair-queuing weights; normalized weight fractions are obtained by dividing by their total. N above would have normalized weight fraction 200/(200+100) = 2/3. If L1, L2 and L3 engage in borrowing from N, and each has trafÔ¨Åc to send, then each gets a total bandwidth of 50, 50 and 100 kbps, respectively. If L3 is idle, then L1 and L2 each would get 100 kbps. If N and M borrow in turn from R, they each can send at 400 and 200 kbps respectively, in which case L1, L2 and L3 (again assuming all are active) get 100, 100 and 200 kbps. If M elects not do do any borrowing, because it has nothing to send, then N will get 600 kbps and L1, L2 and L3 will get 150, 150 and 300. If fair-queuing behavior is not desired, we can set r ceil= r so that a node can never send faster than its guaranteed rate. This allows HTB to model the token-bucket-only hierarchy of 24.9.1 Hierarchical Token Bucket. A working example of HTB, with one parent and two child nodes, is constructed in 30.8 Linux TrafÔ¨Åc Control (tc). 24.12 Parekh-Gallager Theorem As a Ô¨Ånal example relating token-bucket speciÔ¨Åcations and fair queuing, we present the Parekh-Gallager Theorem, which provides a precise queuing-delay bound on trafÔ¨Åc that enters a network meeting a token598 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 bucket speciÔ¨Åcation TB(r,B) and which has a guaranteed weighted-fair-queuing fraction through each router along the path. SpeciÔ¨Åcally, let us assume that the trafÔ¨Åc travels from sender A to destination B through N routers R 1.. . RN. The output rate of the ith router R iis ri, of which our Ô¨Çow is guaranteed rate f i¬§ri. Let f = min {f i|i<N}. Suppose the maximum packet size for packets in our Ô¨Çow is S, and the maximum packet size including competing trafÔ¨Åc is S max. Then the total delay encountered by the Ô¨Çow‚Äôs packets is bounded by the sum of the following: 1. propagation delay (total single-bit delay along all N+1 links) 2. B/f 3. The sum from 1 to N of S/f i 4. The sum from 1 to N of S max/ri The second term B/f represents the queuing delay introduced by a single burst of size B; we showed in 24.8 Token Bucket Through Multiple Routers that this delay bound applied regardless of the number of routers. The third term represents the total store-and-forward delay at each router for packets belonging to our Ô¨Çow under GPS; the delay at R iis S/f i. The Ô¨Ånal term represents the degree to which fair-queuing may delay a packet beyond the theoretical GPS time expressed in the third term. If the routers were to use GPS, then the Ô¨Årst three terms above would bound the packet delay; we established in 23.5.4.7 Finishing-Order Bound that router R imay introduce an additional delay above and beyond the GPS delay of at most S max/ri. 24.12.1 Epilog In the previous chapter we saw that if we want to use all the outbound bandwidth, but divide it among several senders, fair queuing was the way to go. But if we want to impose an absolute rather than a relative cap on trafÔ¨Åc, token bucket is appropriate. Sometimes caps have business justiÔ¨Åcations: customers who want more bandwidth should pay more. But sometimes, as in 24.5 Guaranteeing VoIP Bandwidth, token buckets are beneÔ¨Åcial even if we‚Äôre really not interested in caps; they work well on the downstream router where fair queuing does not. It is for real-time trafÔ¨Åc, however, that queuing disciplines such as fair queuing, token bucket and even priority queuing come into their own as fundamental building blocks. These tools allow us to guarantee a bandwidth fraction to V oIP trafÔ¨Åc, or to allow such trafÔ¨Åc to be sent with minimal delay. In the next chapter 25 Quality of Service we will encounter fair queuing and token-bucket speciÔ¨Åcations repeatedly. 24.12.2 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercises marked with a ‚ô¢have solutions or hints at 34.17 Solutions for Queuing and Scheduling. 1.0. If two trafÔ¨Åc streams meet token-bucket speciÔ¨Åcations of TB(r1,b1) and TB(r2,b2) respectively, show their commingled trafÔ¨Åc must meet TB(r1+r2,b1+b2). Hint: imagine a common bucket of size b1+b2, Ô¨Ålled at rate r1 with red tokens and at rate r2 with blue tokens. 24.12 Parekh-Gallager Theorem 599
An Introduction to Computer Networks, Release 2.0.11 2.0. For each sequence of arrival times, indicate which packets are compliant for the given token-bucket speciÔ¨Åcation. If a packet is noncompliant, go on to the next arrival without decrementing the bucket. (a). TB(1/4,5): 0, 0, 0, 2, 3, 4, 5, 7, 9, 11, 15, 18 (b). TB(1/3,6): 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 (c). TB(1/3,6): 0, 2, 4, 6, 8, 10, 12, 14, 16, 18 3.0. Find the fastest sequence (see the end of 24.3 Multiple Token Buckets ) for the following Ô¨Çows. Both start at T=0, and all buckets are initially full. (a). TB(1/4, 4); packets can depart at a minimum of 1 time unit apart. Continue the sequence to at least T=10 (b). TB(1/2, 4) and TB(1/8, 8); multiple packets can depart at the same instant. Continue to at least T=25. 4.0. Give the fastest sequence of packets compliant for all three of the following token-bucket speciÔ¨Åcations. Continue the sequence at least until T=60. 
- TB(1/2, 1) 
- TB(1/6, 4) 
- TB(1/12, 8) Hint: the Ô¨Årst speciÔ¨Åcation means arrival times must always be separated by at least 2. The middle speciÔ¨Åcation should kick in by T=12. 5.0. Show that if a GPS trafÔ¨Åc Ô¨Çow ( 23.5.4 The GPS Model ) satisÔ¨Åes a token-bucket speciÔ¨Åcation TB(r,B), then in any interval of time t 1¬§t¬§t2the amount of trafÔ¨Åc is at most B + r (t2‚Äìt1). Hint: during the interval t1¬§t¬§t2the amount of Ô¨Çuid added to the bucket is exactly r (t2‚Äìt1). 6.0. Show that the token-bucket leaf-storage hierarchy of 24.9.1 Hierarchical Token Bucket produces the same result as an ‚Äúinternal-storage‚Äù hierarchy in which each intermediate token-bucket node contained a real, inÔ¨Ånite-capacity FIFO queue, and each node instantaneously transmitted each packet to the parent‚Äôs FIFO queue as soon as it was released. Show that packets are transmitted by each hierarchy at the same times. Hint: show that each node in the leaf-storage hierarchy ‚Äúreleases‚Äù a packet at the same time the corresponding internal-storage hierarchy forwards the packet upwards. 7.0. The following Linux htb hierarchies are labeled with their guaranteed rates. Is there any difference in terms of the bandwidth allocations that would be received by senders A and B? (a) (b) R R 100 100 / \ / \ / \ / \ L1 L2 L1 L2 60 40 30 20 (continues on next page) 600 24 Token Bucket Rate Limiting
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) | | | | A B A B 8.0. Suppose we know that the real-time trafÔ¨Åc through a given router R uses at most 1 Mbps of the total 10 Mbps bandwidth. Consider the following two ways of giving the real-time trafÔ¨Åc special treatment: i. Using priority queuing, and giving the real-time trafÔ¨Åc higher priority. ii. Using weighted fair queuing, and giving the real-time trafÔ¨Åc a 10% share (a). Show that, if the real-time trafÔ¨Åc meets a token-bucket speciÔ¨Åcation with rate 1 Mbps and negligible bucket size, then the two mechanisms are equivalent, in the sense that if the real-time and non-real-time trafÔ¨Åc Ô¨Çows are sending at fractions ùõºandùõΩ, respectively, of the 10 Mbps outbound rate, with ùõº+ùõΩ=1 (and with ùõº¬§10%), then the two methods above will actually send at the same rates. (b). What differences can be expected if the bucket size is notnegligible? Which approach will favor the real-time fraction? 9.0. In the previous exercise, now suppose we have twoseparate real-time Ô¨Çows, each guaranteed by a token-bucket speciÔ¨Åcation not to exceed 1 Mbps. Is there a material difference between any pair of the following? i. Sending the two real-time Ô¨Çows at priority 1, and the remaining trafÔ¨Åc at priority 2. ii. Sending the Ô¨Årst real-time Ô¨Çow at priority 1, the second at priority 2, and the remaining trafÔ¨Åc at priority 3. iii. Giving each real-time Ô¨Çow a WFQ share of 10%, and the rest a WFQ share of 80% 10.0. Suppose a router uses priority queuing. There is one low-priority and one high-priority input. The outbound bandwidth is r. (a). If the high-priority queue is currently empty, what is the maximum time that an arriving high-priority packet must wait? (b). If the high-priority trafÔ¨Åc follows a token-bucket description TB(rp,B), with rp < r, what is the maximum time an arriving high-priority packet must wait? Hint: use 24.6.1 Token Bucket Queue Utilization. Your answer may include symbolic representations of any necessary additional parameters. 24.12 Parekh-Gallager Theorem 601
An Introduction to Computer Networks, Release 2.0.11 602 24 Token Bucket Rate Limiting
25 QUALITY OF SERVICE So far, the Internet has been presented as a place where all trafÔ¨Åc is sent on a best-effort basis and routers handle all trafÔ¨Åc on an equal footing; indeed, this is often seen as a fundamental aspect of the IP layer. Delays and losses due to congestion are nearly universal. For bulk Ô¨Åle-transfers this is usually quite sufÔ¨Åcient; one way to look at TCP congestive losses, after all, is as part of a mechanism to ensure optimum utilization of the available bandwidth. Sometimes, however, we may want some trafÔ¨Åc to receive a certain minimum level of network services. We may allow some individual senders to negotiate such services in advance, or we may grant preferential service to speciÔ¨Åc protocols (such as V oIP). Such arrangements are known as quality of service (QoS) assurances, and may involve bandwidth, delay, loss rates, or any combination of these. Even bulk senders, for example, might sometimes wish to negotiate ahead of time for a speciÔ¨Åed amount of bandwidth. While any sender might be interested in quality-of-service levels, they are an especially common concern for those sending and receiving real-time trafÔ¨Åc such as voice-over-IP or videoconferencing. Real-time senders are likely to have not only bandwidth constraints, but constraints on delay and on loss rates as well. Furthermore, real-time applications may simply fail ‚Äì at least temporarily ‚Äì if these bandwidth, delay and loss constraints are not met. In any network, large or small, in which bulk trafÔ¨Åc may sometimes create queue backlogs large enough to cause unacceptable delay, quality-of-service assurances must involve the cooperation of the routers. These routers will then use the queuing and scheduling mechanisms of 23 Queuing and Scheduling to set aside bandwidth for designated trafÔ¨Åc. This is a major departure from the classic Internet model of ‚Äústateless‚Äù routers that have no information about speciÔ¨Åc connections or Ô¨Çows, though it is a natural feature of virtualcircuit routing. In this chapter, we introduce some quality-of-service mechanisms for the Internet. We introduce the theory, anyway; some of these mechanisms have not exactly been adopted with warm arms by the ISP industry. Sometimes this is simply the chicken-and-egg problem: ISPs do not like to implement features nobody is using, but often nobody is using them because their ISPs don‚Äôt support them. However, often an ISP‚Äôs problem with a QoS feature comes down costs: routers will have more state to manage and more work to do, and this will require upgrades. There are two speciÔ¨Åc cost issues: 
- it is not clear how to charge endpoints for their QoS requests (as with RSVP), particularly when the endpoints are not direct customers 
- it is not clear how to compare special trafÔ¨Åc like multicast, for the purpose of pricing, with standard unicast trafÔ¨Åc Nonetheless, V oIP at least is here to stay on a medium scale. Streaming video is here to stay on a large scale. Quality-of-service issues are no longer quite being ignored, or, at least, not blithely. Note that a fundamental part of quality-of-service requests on the Internet is sharing among multiple trafÔ¨Åc classes; that is, the transmission of ‚Äúbest-effort‚Äù and various grades of ‚Äúpremium‚Äù trafÔ¨Åc on the same network. One can, after all, lease a SONET line and construct a private, premium-only network; such an approach is, however, expensive. Support for multiple trafÔ¨Åc classes on the same network is sometimes referred to generically as integrated services, a special case of trafÔ¨Åc engineering; see [CSZ92]. It is not to be confused with the speciÔ¨Åc IETF protocol suite of that name ( 25.4 Integrated Services / RSVP ). There 603
An Introduction to Computer Networks, Release 2.0.11 are two separate issues in an integrated network: 
- how to make sure premium trafÔ¨Åc gets the service it requires 
- how to integrate different trafÔ¨Åc classes on the same network The Ô¨Årst issue is addressed largely through the techniques presented in 23 Queuing and Scheduling, and applies as well to networks with only a single service level. The present chapter addresses mostly the second issue. Quality-of-service requests may be made for both TCP and UDP trafÔ¨Åc. For example, an interactive TCP connection might request a minimum-delay path, while a bulk connection might request a maximumbandwidth path and a streaming-prerecorded-video connection might request a speciÔ¨Åc guaranteed bandwidth. However, service quality is a particular concern of real-time trafÔ¨Åc such as voice and interactive video, where delay can be a major difÔ¨Åculty. Such trafÔ¨Åc is more likely to use UDP than TCP, because of the head-of-line blocking problem with the latter. We return to this in 25.3.3 UDP and Real-Time TrafÔ¨Åc. 25.1 Net Neutrality There is a school of thought that says carriers must carry all trafÔ¨Åc on an equal footing, and should be forbidden to charge extra for premium service. This is a strong formulation of the ‚Äúnet neutrality‚Äù principle, and it potentially complicates the implementation of some of the services described here. In principle, it may not matter whether the premium service involves interaction with routers, as is described in some of the mechanisms below, or simply involves improved access to best-effort carriage. There are, however, many weaker formulations of net neutrality; for example, one is that trafÔ¨Åc carriage must be ‚Äúnon-discriminatory‚Äù. That is, a carrier could charge more for premium service so long as it did not single out individual providers for rate throttling. Without taking sides on the net-neutrality debate, there are good reasons for arguing that additional charges for speciÔ¨Åc services from backbone routers are more appropriate than additional charges to avoid rate throttling. 25.2 Where the Wild Queues Are We stated above that to ensure quality-of-service standards, it is necessary to have the participation of routers that have signiÔ¨Åcant queue backlogs. It is not always clear, however, which routers these are. In the late 1990‚Äôs, it was often claimed there was signiÔ¨Åcant congestion at many (or at least some) ‚Äúbackbone‚Äù routers, where the word is in quotes here as a reminder that the term does not have a precise technical meaning. It is worth noting that this was also the era of 56 kbps dialup access for most residential users. In 2003, just a few years later, an analysis of the Internet in [CM03] concluded, ‚Äúthe Internet ‚Äòcloud‚Äô does not contribute heavily to congestion.‚Äù At the time of this writing (2013), it appears that the Internet backbone continues to have excess capacity, and is seldom congested; delays, therefore, are more likely to be encountered more locally. A study in [CBcDHL14], using timestamp pings to each end of suspected links, found that most congestion occurred in high-volume content-delivery networks ( 1.12.2 Content-Distribution Networks ), such as those of NetÔ¨Çix and Google, and their interconnections to ISPs. More concretely, suppose trafÔ¨Åc from A to B goes through routers R1, R2 and R3: 604 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 AR1 R2 R3 B If queuing delays (and losses) occur only at R1 and at R3, then there is no need to involve R2 in any bandwidth-reservation scheme; the same is true of R1 and R3 if the delays occur only at R2. Unfortunately, it is not always easy to determine the location of Internet congestion, and an abundance of bandwidth today may become a dearth tomorrow: bandwidth usage ineluctably grows to consume supply. That said, the early models for quality-of-service requests assumed that all (or at least most) routers would need to participate; it is quite possible that this is ‚Äì practically speaking ‚Äì no longer true. Another possible consequence is that adequate QoS levels might ‚Äì might ‚Äì be attainable with only the participation of one‚Äôs immediate ISP. 25.3 Real-time TrafÔ¨Åc Real-time trafÔ¨Åc is trafÔ¨Åc with some sort of hard or soft delay bound, presumably larger than the one-way no-load propagation delay. Such trafÔ¨Åc can be said to be delay-intolerant. For voice or video trafÔ¨Åc, a packet arriving after the time at which it is to be played back might as well have been lost. Fortunately, voice and video are also loss-tolerant, at least to a degree: a lost voice packet simply results in a momentary voice dropout; a lost video packet might result in replay of the previous video frame. Handling trafÔ¨Åc that is both lossand delayintolerant is very difÔ¨Åcult; we will not consider that case further. Much (but not all) real-time trafÔ¨Åc is also rate-adaptive. For example, the online-video service hulu.com can send at resolutions of 288p, 360p, 480p and 720p, approximately corresponding to bandwidths of 480 kbps, 700 kbps, 1 Mbps, and 2.5 Mbps [2012 data]. Hulu‚Äôs software can dynamically choose the appropriate rate. (Hulu transmissions, where the consequence of delay is a pause in the video replay rather than a loss, are not necessarily ‚Äúreal-time‚Äù; see 25.3.2 Streaming Video below.) As another example of rate-adaptiveness, the voice-grade audio-compression codec Opus (a successor to an earlier codec known as Speex) might normally be used at a 64 kbps rate, but supports a more-or-less continuous range of rates down to 8 kbps. While the lower rates have lower voice quality, they can be used as a fallback in the event that congestion prevents successful use of the 64 kbps rate. Generally speaking, rate-adaptivity notwithstanding, real-time trafÔ¨Åc needs sufÔ¨Åcient management that congestion becomes minimal. Real-time trafÔ¨Åc should not be allowed to arrive at any router, for example, faster than it can depart. The most deÔ¨Ånitive way to achieve this is via some sort of reservation or admissioncontrol mechanism, where new connections will not be accepted unless resources are available. 25.3.1 Playback Buffer Real-time applications cannot avoid delay completely, of course, so the received stream will be delivered to the receiving application (for playback, if it is a voice or video stream) slightly behind the time when it was sent. Applications can intentionally increase this time by creating a playback buffer orjitter buffer; this allows the smoothing over of variations in delay (known as jitter ). For example, suppose a V oIP application sends a packet every 20 ms (a typical rate). If the delay is exactly 50 ms, then packets sent at times T=0, 20, 40, etcwill arrive at times T=50, 70, 90, etc. Now suppose the delay is not so uniform, so packets sent at T=0, 20, 40, 60, 80 arrive at times 50, 65, 120, 125, 130. 25.3 Real-time TrafÔ¨Åc 605
An Introduction to Computer Networks, Release 2.0.11 packet sent expected received (rec‚Äôd ‚Äì expected) 1 0 50 50 0 2 20 70 65 -5 3 40 90 120 30 4 60 110 125 15 5 80 130 130 0 The Ô¨Årst and the last packet have arrived on time, the second is early, and the third and fourth are late by 30 and 15 ms respectively. Setting the playback buffer capacity to 25 ms means that the third packet is not received in time to be played back, and so must be discarded; setting the buffer to a value at least 30 ms means that all the packets are received. 406080100120140 0 20 40 60 80receive time send time The diagram above plots the points of the table. The dashed line represents the expected arrival time for the corresponding send time; packets are late by the amount they are above this line. The solid blue line represents a playback buffer corresponding to 25 ms; packets arriving at points above the line are lost. The higher the line, the more playback delay, but also the more that playback can accommodate late packets For non-interactive voice and video, there is essentially no penalty to making the playback buffer quite long. But for telephony, if one speaker stops and the other starts, they will perceive a gap of length equal to the RTT including playback-buffer delay. For voice, this becomes increasingly annoying if the RTT delay reaches about 200-400 ms. 25.3.2 Streaming Video Streaming video is something of a gray area. On the one hand, it is not really real-time; viewers do not necessarily care how long the playback-buffer delay is as long as it is consistent. Playback-buffer delays of ten seconds to up to a minute are not uncommon. As long as the sender can stay ahead of the playback application, all is well. The real issue is bandwidth; a playback-buffer delay in the tens of seconds is two orders of magnitude larger than the delay bounds that a genuine real-time application might request. 606 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 For very large videos, the sender probably coordinates with the playback application to limit how far ahead it gets; this amounts to using a Ô¨Åxed-size playback buffer. That playback buffer can accommodate some Ô¨Çuctuation in the delivery rate, but in the long run the delivery bandwidth must be at least as large as the playback rate. For smaller videos, egtraditional ten-minute YouTube clips, the sender sends as fast as it can without stopping. If the delivery bandwidth is less than the playback rate then the viewer can hit ‚Äúpause‚Äù and wait until the entire video has been downloaded. On the other hand, viewers do not particularly like pauses, especially during long videos. If a viewer starts a two-hour movie, average network congestion levels may change materially many times during that interval. The viewer would prefer a more-or-less consistent bandwidth, even if competing trafÔ¨Åc ramps up; rateadaptive playback is Ô¨Åne until one has signed up for HD-quality viewing. What the viewer here wants is an average-bandwidth guarantee. This can be supplied by overbuilding the network, by implementing some of the mechanisms of this chapter, or by various intermediate approaches. For example, a large-enough network-content provider N might negotiate with a residential Internet carrier C so that there is sufÔ¨Åcient long-haul bandwidth from N to the end-user viewers. If the problem is backbone congestion, then N might arrange to use BGP‚Äôs MED option ( 15.7.1 MED values and trafÔ¨Åc engineering ) to carry the trafÔ¨Åc as far as possible in its own network; this may also entail the creation of a large number of peering points with C‚Äôs network ( 14.4.1 Internet Exchange Points ). Alternatively, C might be persuaded to set aside one very large trafÔ¨Åc category in its own backbone network (perhaps using 25.7 Differentiated Services ) that is reserved for N‚Äôs trafÔ¨Åc. 25.3.3 UDP and Real-Time TrafÔ¨Åc The main difÔ¨Åculty with using TCP for real-time trafÔ¨Åc is head-of-line blocking: when a loss does occur, the TCP layer will hold any later data until the lost segment times out and is retransmitted. Even if the receiving application is able simply to ignore the lost packet, it is not granted that option. In theory, the TCP timeout interval may be only slightly more than the RTT, which is often well under 100 ms. In practice, however, it is often much larger, due in part to the use of a coarse-grained clock to keep track of timeouts. TCP implementations are actually discouraged from using smaller timeout intervals; the following is from RFC 6298 (2011); RTO stands for Retransmission TimeOut: Whenever RTO is computed, if it is less than 1 second, then the RTO SHOULD be rounded up to 1 second. Such a policy makes TCP unsuitable, except in environments with vanishingly small loss rates, whenever any genuine interactive response is needed; this includes telephony, video telephony, and most forms of video conferencing that involve real-time feedback between participants. (TCP does work well with video streaming.) While it is true that the popular video-telephony package Skype does in fact use TCP, this is not because Skype has Ô¨Ågured a way around this limitation; Skype sessions are not infrequently plagued by congestion-related difÔ¨Åculties. The use of UDP allows an application the option of deciding that lost or late data should simply be skipped over. 25.4 Integrated Services / RSVP Integrated Services, or IntServ, was developed by the IETF in the late 1990‚Äôs as a Ô¨Årst attempt at providing quality-of-service guarantees for selected Internet trafÔ¨Åc. The IntServ model assumed all routers would 25.4 Integrated Services / RSVP 607
An Introduction to Computer Networks, Release 2.0.11 participate, or at least all routers along the connection path, though this is not strictly necessary. Connections were to make reservations using the Resource ReSerVation Protocol RSVP. Note that this is a major retreat from the datagram-routing stateless-router model. Were virtual circuits (5.4 Virtual Circuits ) the better routing model all along? For better or for worse, the marketplace appears to have answered this question with an unambiguous ‚Äúno‚Äù; IntServ has seen very limited adoption in the core Internet. However, many of the ideas of IntServ are implementable on the internet using an appropriate Content Distribution Network. After outlining IntServ itself, we describe this CDN alternative in 25.6.1 A CDN Alternative to IntServ. Under IntServ, routers maintain soft state about a connection, meaning that reservations must be refreshed by the sender at regular intervals ( eg30 seconds); if not, the reservation can be discarded. This also means that reservations can be recovered if the router crashes (though with some small probability of failure). Traditional virtual-circuit switches maintain hard state, so that if a sender stops sending but fails to ‚Äúhang up‚Äù properly then the connection is still maintained (and perhaps charged for), and a router crash means the connection is lost. IntServ has mostly not been supported by the backbone ISP industry. Partly this is because of practical difÔ¨Åculties in Ô¨Åguring out how to charge for reservations; if the charge is zero then everyone might make reservations for every connection. Another issue is that a busy router might have to maintain thousands of reservations; IntServ thus adds a genuine expense to the ISP‚Äôs infrastructure. At the time IntServ was developed, the dominant application envisioned was teleconferencing, in which one speaker‚Äôs audio/video stream would be sent to a large number of receivers. Because of this, IntServ was based on IP multicast, to which we therefore turn next. This decision made IntServ somewhat more complex than would be necessary if point-to-point V oIP were all that was required, and future Internet reservation mechanisms may jettison multicast support (see 25.9 NSIS ). However, multicast and IntServ do share something fundamental in common: both require the participation of intermediate routers; neither can be effectively implemented solely in end systems. 25.5 Global IP Multicast The idea behind IP multicast, following up on 9.3.1 Multicast addresses, is that sender A transmits a stream of packets (real-time or not) to a setof receivers {B1, B2,. .. , Bn}, in such a way that no one packet of the stream is transmitted more than once on any one link. This means that it is up to routers on the way to duplicate packets that need to be forwarded on multiple outbound links. For example, suppose A below wishes to send to B1, B2 and B3 in the following diagram: AR1 R2 R3 R4 B1 B2 B3 Then R1 will receive packet 1 from A and will forward it to both B1 and to R2. R2 will receive packet 1 from R1 and forward it to B2 and R3. R3 will forward only to B3; R4 does not see the trafÔ¨Åc at all. The 608 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 set of paths used by the multicast trafÔ¨Åc, from the sender to all destinations, is known as the multicast tree; these are shown in bold. We should acknowledge upfront that, while IP multicast is potentially a very useful technology, as with IntServ there is not much support for it within the mainstream ISP industry. The central issues are the need for routers to maintain complex new information and the difÔ¨Åculty in Ô¨Åguring out how to charge for this kind of trafÔ¨Åc. At larger scales ISPs normally charge by total trafÔ¨Åc carried; now suppose an ISP‚Äôs portion of a multicast tree is a single path all across the continent, but the tree branches into ten different paths near the point of egress. Should the ISP consider this to be like one unicast connection, or more like ten? Once Upon A Time, an ideal candidate for multicast might have been the large-scale delivery of broadcast television. Ironically, the expansion of the Internet backbone has meant that large-scale video delivery is now achieved with an individual unicast connection for every viewer. This is the model of YouTube.com, NetÔ¨Çix.com and Hulu.com, and almost every other site delivering video. Online education also once upon a time might have been a natural candidate for multicast, and here again separate unicast connections are now entirely affordable. The bottom line for the future of multicast, then, is whether there is an application out there that really needs it. Note that IP multicast is potentially straightforward to implement within a single large (or small) organization. In this setting, though, the organization is free to set its own budget rules for multicast use. Multicast trafÔ¨Åc will consist of UDP packets; there is no provision in the TCP speciÔ¨Åcation for ‚Äúmulticast connections‚Äù. For large groups, acknowledgment by every receiver of the multicast UDP packets is impractical; the returning ACKs could consume more bandwidth than the outbound data. Fortunately, complete acknowledgments are often unnecessary; the archetypal example of multicast trafÔ¨Åc is loss-tolerant voice and video trafÔ¨Åc. The RTP protocol ( 25.11 Real-time Transport Protocol (RTP) ) includes a response mechanism from receivers to senders; the RTP response packets are intended to at least give the sender some idea of the loss rate. Some effort is expended in the RTP protocol (more precisely, in the companion protocol RTCP) to make sure that these response packets, from multiple recipients, do not end up amounting to more trafÔ¨Åc than the data trafÔ¨Åc. In the original Ethernet model for LAN-level multicast, nodes agree on a physical multicast address, and then receivers subscribe to this address, by instructing their network-interface cards to forward on up to the host system all packets with this address as destination. Switches, in turn, were expected to treat packets addressed to multicast addresses the same as broadcast, forwarding on allinterfaces other than the arrival interface. Global broadcast, however, is not an option for the Internet as a whole. Routers must receive speciÔ¨Åc instructions about forwarding packets. Even on large switched Ethernets, newer switches generally try to avoid broadcasting every multicast packet, preferring instead to attempt to Ô¨Ågure out where the subscribers to the multicast group are actually located. In principle, IP multicast routing can be thought of as an extension of IP unicast routing. Under this model, IP forwarding tables would have the usual xudest,next_hop yentries for each unicast destination, andxmdest, set_of_next_hops yentries for each multicast destination. In the diagram above, if G represents the multicast group of receivers {B1,B2,B3}, then R1 would have an entry xG,{B1,R2}y. All that is needed to get multicast routing to work are extensions to distance-vector, link-state and BGP router-update algorithms to accommodate multicast destinations. (We are using G here to denote both the actual multicast group and also the multicast address for the group; we are also for the time being ignoring exactly how a group would be assigned an actual multicast address.) These routing-protocol extensions can be done (and in fact it is quite straightforward in the link-state case, 25.5 Global IP Multicast 609
An Introduction to Computer Networks, Release 2.0.11 as each node can use its local network map to Ô¨Ågure out the optimal multicast tree), but there are some problems. First off, if any Internet host might potentially join any multicast group, then each router must maintain a separate entry for each multicast group; there are no opportunities for consolidation or for hierarchical routing. For that matter, there is no way even to support for multicast the basic unicast-IP separation of addresses into network and host portions that was a crucial part of the continued scalability of IP routing. The Class-D multicast address block contains 228270 million entries, far to many to support a routing-table entry for each. The second problem is that multicast groups, unlike unicast destinations, may be ephemeral; this would place an additional burden on routers trying to keep track of routing to such groups. An example of an ephemeral group would be one used only for a speciÔ¨Åc video-conference speaker. Finally, multicast groups also tend to be of interest only to their members, in that hosts far and wide on the Internet generally do not send to multicast groups to which they do not have a close relationship. In the diagram above, the sender A might not actually be a member of the group {B1,B2,B3}, but there is a strong tie. There may be no reason for R4 to know anything about the multicast group. So we need another way to think about multicast routing. Perhaps the most successful approach has been the subscription model, where senders and receivers join and leave a multicast group dynamically, and there is no route to the group except for those subscribed to it. Routers update the multicast tree on each join/leave event. The optimal multicast tree is determined not only by the receiving group, {B1,B2,B3}, but also by thesender, A; if a different sender wanted to send to the group, a different tree might be constructed. In practice, sender-speciÔ¨Åc trees may be constructed only for senders transmitting large volumes of data; less important senders put up with a modicum of inefÔ¨Åciency. The multicast protocol most suited for these purposes is known as PIM-SM, deÔ¨Åned in RFC 2362. PIM stands for Protocol-Independent Multicast, where ‚Äúprotocol-independent‚Äù means that it is not tied to a speciÔ¨Åc routing protocol such as distance-vector or link-state. SM here stands for Sparse Mode, meaning that the set of members may be widely scattered on the Internet. We acknowledge again that, while PIM-SM is a reasonable starting point for a realistic multicast implementation, it may be difÔ¨Åcult to Ô¨Ånd an ISP that implements it. The Ô¨Årst step for PIM-SM, given a multicast group G as destination, is for the designation of a router to serve as the rendezvous point, RP, for G. If the multicast group is being set up by a particular sender, RP might be a router near that sender. The RP will serve as the default root of the multicast tree, up until such time as sender-speciÔ¨Åc multicast trees are created. Senders will send packets to the RP, which will forward them out the multicast tree to all group members. At the bottom level, the Internet Group Management Protocol, IGMP, is used for hosts to inform one of their local routers (their designated router, or DR) of the groups they wish to join. When such a designated router learns that a host B wishes to join group G, it forwards a join request to the RP. The join request travels from the DR to the RP via the usual IP-unicast path from DR to RP. Suppose that path for the diagram below is xDR,R1,R2,R3,RP y. Then every router in this chain will create (or update) an entry for the group G; each router will record in this entry that trafÔ¨Åc to G will need to be sent to the previous router in the list (starting from DR), and that trafÔ¨Åc from G must come from the next router in the list (ultimately from the RP). 610 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 RP R3 R4 R2 R1 DR B2 R6 S R5 B1 In the above diagram, suppose B1 is Ô¨Årst to join the group. B1‚Äôs designated router is R5, and the join packet is sent R5√ëR2√ëR3√ëRP. R5, R2 and R3 now have entries for G; R2‚Äôs entry, for example, speciÔ¨Åes that packets addressed to G are to be sent to{R5} and must come from R3. These entries are all tagged with x*,Gy, to use RFC 2362‚Äôs notation, where the ‚Äú*‚Äù means ‚Äúany sender‚Äù; we will return to this below when we get to sender-speciÔ¨Åc trees. Now B2 wishes to join, and its designated router DR sends its join request along the path DR√ëR1√ëR2√ëR3√ëRP. R2 updates its entry for G to reÔ¨Çect that packets addressed to G are to be forwarded to the set {R5,R1}. In fact, R2 does not need to forward the join packet to R3 at all. At this point, a sender S can send to the group G by creating a multicast-addressed IP packet and encapsulating it in a unicast IP packet addressed to RP. RP opens the encapsulation and forwards the packet down the tree, represented by the bold links above. Note that the data packets sent by RP to DR will follow the path RP √ëR3√ëR2√ëR1, as set up above, even if the normal unicast path from R3 to R1 were R3 √ëR4√ëR1. The multicast path was based on R1‚Äôs preferred next_hop to RP, which was assumed to be R2. TrafÔ¨Åc here from sender to a speciÔ¨Åc receiver takes the exact reverse of the path that a unicast packet would take from that receiver to the sender; as we saw in 14.4.3 Hierarchical Routing via Providers, it is common for unicast IP trafÔ¨Åc to take a different path each direction. The next step is to introduce source-speciÔ¨Åc trees for high-volume senders. Suppose that sender S above is sending a considerable amount of trafÔ¨Åc to G, and that there is also an R6‚ÄìR2 link (in blue below) that can serve as a shortcut from S to {B1,B2}: 25.5 Global IP Multicast 611
An Introduction to Computer Networks, Release 2.0.11 RP R3 R4 R2 R1 DR B2 R6 S R5 B1 We will still suppose that trafÔ¨Åc from R2 reaches RP via R3 and not R6. However, we would like to allow S to send to G via the more-direct path R6 √ëR2. RP would initiate this through special join messages sent to R6; a message would then be sent from RP to the group G announcing the option of creating a sourcespeciÔ¨Åc tree for S (or, more properly, for S‚Äôs designated router R6). For R1 and R5, there is no change; these routers reach RP and R6 through the same next_hop router R2. However, R2 receives this message and notes that it can reach R6 directly (or, in general, at least via a different path than it uses to reach RP), and so R2 will send a join message to R6. R2 and R6 will now each have general entries for x*,Gybut also a source-speciÔ¨Åc entry xS,Gy, meaning that R6 will forward trafÔ¨Åc addressed to G and coming from S to R2, and R2 will accept it. R6 may still also forward these packets to RP (as RP does belong to group G), but RP might also by then have an xS,Gyentry that says (unless the diagram above is extended) not to forward any further. The tagsx*,GyandxS,Gythus mark two different trees, one rooted at RP and the other rooted at R6. Routers each use an implicit closest-match strategy, using a source-speciÔ¨Åc entry if one is available and the wildcard x*,Gyentry otherwise. As mentioned repeatedly above, the necessary ISP cooperation with all this has not been forthcoming. As a stopgap measure, the multicast backbone or Mbone was created as a modest subset of multicast-aware routers. Most of the routers were actually within Internet leaf-customer domains rather than ISPs, let alone backbone ISPs. To join a multicast group on the Mbone, one Ô¨Årst identiÔ¨Åed the nearest Mbone router and then connected to it using tunneling. The Mbone gradually faded away after the year 2000. We have not discussed at all how a multicast address would be allocated to a speciÔ¨Åc set of hosts wishing to form a multicast group. There are several large blocks of class-D addresses assigned by the IANA. Some of these are assigned to speciÔ¨Åc protocols; for example, the multicast address for the Network Time Protocol is 224.0.1.1 (though you can use NTP quite happily without using multicast). The 232.0.0.0/8 block is reserved for source-speciÔ¨Åc multicast, and the 233.0.0.0/8 block is allocated by the GLOP standard; if a site has a 16-bit Autonomous System number with bytes x and y, then that site automatically gets the multicast block 233.x.y.0/24. A fuller allocation scheme waits for the adoption and development of a broader IP-multicast strategy. This may never happen. 612 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 25.6 RSVP We next turn to the RSVP (ReSerVation) protocol, which forms the core of IntServ. The original model for RSVP was to support multicast, so as to support teleconferencing. For this reason, reservations are requested not by senders but by receivers, as a multicast sender may not even know who all the receivers are. Reservations are also for one direction; bidirectional trafÔ¨Åc needs to make two reservations. Like multicast, RSVP generally requires participation of intermediate routers. Reservations include both a Ô¨Çowspec describing the trafÔ¨Åc Ô¨Çow ( ega unicast or multicast destination) and also a Ô¨Ålterspec describing how to identify the packets of the Ô¨Çow. We will assume that Ô¨Ålterspecs simply deÔ¨Åne unidirectional unicast Ô¨Çows, egby specifying source and destination sockets, but more general Ô¨Ålterspecs are possible. A component of the Ô¨Çowspec is the Tspec, or trafÔ¨Åc spec; this is where the token-bucket speciÔ¨Åcation for the Ô¨Çow appears. Tspecs do not in fact include a bound on total delay; however, the degree ofqueuing delay at each router can be computed from the TB(r,B max) token-bucket parameters as B max/r. The two main messages used by RSVP are PATH packets, which move from sender to receiver, and the subsequent RESV packets, which move from receiver to sender. Initially, the sender (or senders) sends a PATH message to the receiver (or receivers), either via a single unicast connection or to a multicast group. The PATH message contains the sender‚Äôs Tspec, which the receivers need to know to make their reservations. But the PATH messages are not just for the ultimate recipients: every router on the path examines these packets and learns the identity of the next_hop RSVP router in the upstream direction. The PATH messages inform each router along the path of the path state for the sender. As an example, imagine that sender A sends a PATH message to receiver B, using normal unicast delivery. Suppose the route taken is A √ëR1√ëR2√ëR3√ëB. Suppose also that if B simply sends a unicast message to A, however, then the route is the rather different B √ëR3√ëR4√ëR1√ëA. A R1 R3 BR2 R4 Arrows show path of normal unicast traffic between A and B RSVP traffic, however, follows the bold links in both directions As A‚Äôs PATH message heads to B, R2 must record that R1 is the next hop back to A along this particular PATH, and R3 must record that R2 is the next reverse-path hop back to A, and even B needs to note R3 is the next hop back to A (R1 presumably already knows this, as it is directly connected to A). To convey this reverse-path information, each router inserts its own IP address at a speciÔ¨Åc location in the PATH packet, so that the next router to receive the PATH packet will know the reverse-path next hop. All this path state stored at each router includes not only the address of the previous router, but also the sender‚Äôs Tspec. All these path-state records are for this particular PATH only. 25.6 RSVP 613
An Introduction to Computer Networks, Release 2.0.11 The PATH packet, in other words, tells the receiver what the Tspec is, and prepares the routers along the way for future reservations. The actual trafÔ¨Åc from sender A to receiver B will eventually be forwarded along the standard unicast path (or multicast-tree branch) from A to B, just like PATH messages from A to B. Routers will still have to check, though, whether each packet forwarded is part of a reservation or not, in order to give reserved trafÔ¨Åc preferential drop rates and reduced queuing delays, and in order to police the reservation to ensure that the sender is not sending more reserved trafÔ¨Åc than agreed. Reserved trafÔ¨Åc is identiÔ¨Åed by source and destination IP addresses and port numbers. This IP-address-and-port combination can be likened to the VCI of virtual-circuit routing ( 5.4 Virtual Circuits ), though it is constant along the path. After receiving the sender‚Äôs PATH message, each receiver now responds with its RESV message, requesting its reservation. The RESV packets are passed back to the sender not by the default unicast route, but along the reverse path created by the PATH message. In the example above, the RESV packet would travel B√ëR3√ëR2√ëR1√ëA. Each router (and also B) must look at the RESV message and look up the corresponding PATH record in order to Ô¨Ågure out how to pass the reservation message back up the chain. If the RESV message were sent using normal unicast, via B √ëR3√ëR4√ëR1√ëA, then R2 would not see it. Each router seeing the RESV path must also make a decision as to whether it is able to grant the reservation. This is the admission control decision. RSVP-compliant routers will typically set aside some fraction of their total bandwidth for their reservations, and will likely use priority queuing to give preferred service to this fraction. However, as long as this fraction is well under 100%, bulk unreserved trafÔ¨Åc will not be shut out. Fair queuing can also be used. Reservations must be resent every so often ( egevery ~30 seconds) or they will time out and go away; this means that a receiver that is shut down without canceling its reservation will not continue to tie up resources. If the RESV messages are moving up a multicast tree, rather than backwards along a unicast path, then they are likely to reach a router that already has granted a reservation of equal or greater capacity. In the diagram below, router R has granted a reservation for trafÔ¨Åc from A to receiver B1, reached via R‚Äôs interface 1, and now has a similar reservation from receiver B2 reached via R‚Äôs interface 2. A R B2B1 1 2 Assuming R is able to grant B2‚Äôs reservation, it does not have to send the RESV packet upstream any further (at least not as requests for a new reservation); B2‚Äôs reservation can be merged with B1‚Äôs. R simply will receive packets from A and now forward them out both of its interfaces 1 and 2, to the two receivers B1 and B2 respectively. It is not necessary that every router along the path be RSVP-aware. Suppose A sends its PATH messages to B via A√ëR1√ëR2a√ëR2b√ëR3√ëB, where every router is listed but R2a and R2b are part of a non-RSVP ‚Äúcloud‚Äù. Then R2a and R2b will not store any path state, but also will not mark the PATH packets with their IP addresses. So when a PATH packet arrives at R3, it still bears R1‚Äôs IP address, and R3 records R1 as the reverse-path next hop. It is now possible that when R3 sends RESV packets back to R1, they will take a different path R3 √ëR2c√ëR1, but this does not matter as R2a, R2b and R2c are not accepting 614 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 reservations anyway. Best-effort delivery will be used instead for these routers, but at least part of the path will be covered by reservations. As we outlined in 25.2 Where the Wild Queues Are, it is quite possible that we do not need the participation of the various R2‚Äôs to get the quality of service wanted; perhaps only R1 and R3 contribute to delays. In the multicast multiple-sender/multiple-receiver model, not every receiver must make a reservation for all senders; some receivers may have no interest in some senders. Also, if rate-adaptive data transmission protocols are used, some receivers may make a reservation for a sender at a lower rate than that at which the sender is sending. For this to work, some node between sender and receiver must be willing to decode and re-encode the data at a lower rate; the RTP protocol provides some support for this in the form of RTP mixers (25.11.1 RTP Mixers ). This allows different members of the multicast receiver group to receive the same audio/video stream but at different resolutions. From an ISP‚Äôs perspective, the problems with RSVP are that there are likely to be a lotof reservations, and the ISP must Ô¨Ågure out how to decide who gets to reserve what. One model is simply to charge for reservations, but this is complicated when the ISP doing the charging is not the ISP providing service to the receivers involved. Another model is to allow anyone to ask for a reservation, but to maintain a cap on the number of reservations from any one site. These sorts of problems have largely prevented RSVP from being implemented in the Internet backbone. That said, RSVP is apparently quite successful within some corporate intranets, where it can be used to support voice trafÔ¨Åc on the same LANs as data. 25.6.1 A CDN Alternative to IntServ The core components of IntServ are, arguably, 
- IP multicast 
- TrafÔ¨Åc reservations While ‚Äútrue‚Äù IntServ implementations of these may never come to widespread (or even narrow) adoption, for many purposes a content-distribution network (1.12.2 Content-Distribution Networks ) can achieve the same two goals of multicasting and reservations, without requiring any cooperation from the backbone routing infrastructure. CDN-based strategies do not necessarily implement actual reservations, but they do attempt to provide corresponding trafÔ¨Åc guarantees (or almost-guarantees) appropriate to the application. For videoconferencing, these include sufÔ¨Åcient bandwidth and bounded delay. For online gaming, on the other hand, the primary goal is usually extremely small queuing delay, or ‚Äúlag‚Äù. Player-to-player bandwidth requirements are often very modest. As of 2019, there are multiple ‚Äúgaming private networks‚Äù (or ‚Äúoptimized gaming networks‚Äù) available to gamers that operate on the principles described here. Imagine a videoconference presenter at node A, below. Nodes B through G represent video receivers. According to the IntServ strategy, we would create a multicast tree rooted at A, and then A‚Äôs video presentation would be multicast to nodes B through G. The links on this multicast tree ‚Äì consisting of both the red and the dashed links in the image below ‚Äì would each (or mostly each) maintain an appropriate trafÔ¨Åc reservation. 25.6 RSVP 615
An Introduction to Computer Networks, Release 2.0.11 ABC D E F GInternetCDN 12 43 5 But instead of that, imagine that we have a network ‚Äì that is, a CDN ‚Äì consisting of multiple publicly accessible nodes (called points of presence, or PoPs), all connected by high-speed links (virtual or physical). These links between nodes are represented by the red lines in the diagram above. Each of the users A-G then connects to the CDN, using the usual public Internet; these are the dashed lines. These connections will probably be made under the aegis of a videoconferencing provider that has leased the services of the CDN. Each user ‚Äì using software from the provider ‚Äì attempts to connect to the closest, or at least to a reasonably close, access point of the CDN. The CDN will be made aware of A as data source and B-G as subscribers, and will forward A‚Äôs incoming data from 1 to 2, 3 and 4. A‚Äôs data will, in keeping with the idea of multicast, not be forwarded along any link twice. This forwarding will most likely be done within the videoconferencing application layer rather than by IP-layer multicasting. That is, CDN node 1 will receive A‚Äôs data stream and distribute it within the CDN to nodes 2, 3 and 4 as the CDN sees Ô¨Åt. In other words, application-layer multicasting is straightforward to implement in software alone. For real-time trafÔ¨Åc, performance may depend on the implementation of the red intra-CDN links. The gold standard, in terms of support for real-time streaming, is for the red links to be private leased lines ‚Äì and routers ‚Äì over which the CDN has full control. In this case, load and queuing delay can be regulated as necessary, and true reservations (internal to the CDN) can be granted if necessary. Gaming private networks often require private links, because delay on the commodity Internet is difÔ¨Åcult to minimize. The Google internal wide-area network (as of 2017), known as B4, has this sort of architecture; it carries huge volumes of trafÔ¨Åc. See [JKMOPSVWZ13]. At the other end, these red links may simply be paths through the public Internet, making the CDN an ‚Äúoverlay‚Äù network vaguely resembling the original Mbone. In this case the CDN‚Äôs real-time trafÔ¨Åc will compete with other trafÔ¨Åc. This isn‚Äôt necessarily bad, however, if the red links ‚Äì which probably are Internet ‚Äúbackbone‚Äù links ‚Äì have high capacity. Perhaps more importantly, a CDN is a large customer for the ISPs that connect it, and likely has a close business relationship with them. If congestion is a business problem for the CDN, in that it wants to attract videoconferencing providers as clients, it is in a strong position to negotiate appropriate service guarantees and even partial isolation of its real-time trafÔ¨Åc Ô¨Çows. 616 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 NetÔ¨Çix Open Connect The NetÔ¨Çix CDN used to deliver streamed video consists largely of so-called Open Connect Appliances (OCAs), each containing upwards of 100 TB (and colored NetÔ¨Çix red). These are distributed ‚Äì rather widely ‚Äì among ISPs and IXPs. Each OCA gets its daily updates (of multiple TB) delivered over the public Internet. There are no private links. NetÔ¨Çix has no need for them; none of their content is realtime. However the intra-CDN links are implemented, data will Ô¨Çow in our multicast scenario as follows: 
- From source A to CDN node 1, using the Internet 
- Throughout the CDN‚Äôs internal links as necessary 
- From CDN nodes 1, 2, 3 and 4 to subscribers B through G, using each subscriber‚Äôs Internet link Many teleconferencing and gaming services use this sort of architecture. Subscribers can be charged for the privilege of using the network, solving the multicast and reservation pricing issues. In principle (though very seldom in practice) subscribers can be told the network is busy if congestion levels are high, addressing the reservation admission issue. More likely, the provider would handle high-use periods by allowing performance to degrade, while also attempting to provision the network so that it is adequate ‚Äúmost‚Äù of the time. If the CDN is ‚Äúsmall‚Äù ‚Äì perhaps a dozen points of presence ‚Äì then the dashed normal-Internet links from users to the CDN might end up handling most of the trafÔ¨Åc carriage. However, if the dashed links are not the sources of congestion, a small CDN might still produce a large performance beneÔ¨Åt. Furthermore, if the CDN is reasonably large, then the dashed links may shrink to intra-city distances, and the red links will handle the long-haul trafÔ¨Åc. 25.7 Differentiated Services Differentiated Services, or DiffServ, was created as a low-overhead alternative to IntServ; see RFC 2475 for an overall introduction. The idea behind DiffServ is to replace IntServ‚Äôs many reservations with a small handful of service classes: a single regular class (for everyone‚Äôs bulk trafÔ¨Åc) and a few premium classes (for what in IntServ would have been everyone‚Äôs reserved trafÔ¨Åc). For this reason, DiffServ can be described as providing ‚Äúcoarse-grained‚Äù resource allocation versus RSVP‚Äôs ‚ÄúÔ¨Åne-grained‚Äù per-Ô¨Çow allocations. Like IntServ, DiffServ is easiest to implement within a single ISP or Autonomous System; however, DiffServ may be reasonably practical to implement across ISP boundaries as well. A set of routers agreeing on a common DiffServ policy is called a DS domain; a DS domain might consist of a single ISP but might also comprise a larger ‚Äúbackbone‚Äù ISP and some ‚Äúregional‚Äù customer ISPs that have negotiated a single DiffServ policy. Packets are marked for premium service at (and only at) the border routers of each DS domain, as the trafÔ¨Åc in question enters that domain. The DS domain‚Äôs interior routers do no marking and no trafÔ¨Åc policing. At these interior routers, priority queuing is generally used to give premium service to the marked packets. As a simple example of the DiffServ approach, suppose an ISP has a thousand customers. The ith customer negotiates bandwidth r ifor the ‚Äúpremium‚Äù portion of its trafÔ¨Åc; the ISP ensures that the sum of all the ri‚Äôs is less than a maximum designated rate r. The ISP enforces each premium rate cap r iat the point of 25.7 Differentiated Services 617
An Introduction to Computer Networks, Release 2.0.11 customer i‚Äôs connection, and also marks that customer‚Äôs premium trafÔ¨Åc. The ISP can now guarantee each of its customers its individual premium rate simply by ensuring that a bandwidth of at least r is available throughout its network, and by handling marked premium packets on a priority basis. Only one internal service class is needed ‚Äì premium ‚Äì despite a thousand different individual bandwidth guarantees. Packets are marked using the six bits of the DS Ô¨Åeld of the IPv4 header ( 9.1 The IPv4 Header ); these were part of what was originally the IP Type-of-Service bits. DiffServ trafÔ¨Åc classes are known as Per Hop Behaviors, or PHBs; the name comes from the identiÔ¨Åcation of trafÔ¨Åc classes with how each router is to handle packets of that class. The simplest PHB is the default PHB, meaning the packet gets no special processing. The other PHBs all provide for at least some degree of preferred service. The Class Selector PHB is for backwards compatibility with the three-bit precedence subÔ¨Åeld of the old IPv4 Type-of-Service Ô¨Åeld, originally proposed as a three-bit priority Ô¨Åeld. TheExpedited Forwarding (EF) PHB ( 25.7.1 Expedited Forwarding ) is arguably the best service. It is meant for trafÔ¨Åc that has delay constraints in addition to rate constraints. The newer Voice Admit PHB is closely related and has been recommended as the preferred PHB for voice telephony. We will look more closely at this PHB below. Assured Forwarding, or AF ( 25.7.2 Assured Forwarding ), is really four separate PHBs, corresponding to four classes 1 through 4. It is meant for providing (soft) service guarantees that are contingent on the sender‚Äôs staying within a certain rate speciÔ¨Åcation. Each AF class has its own rate speciÔ¨Åcation; these rate speciÔ¨Åcations are entirely at the discretion of the implementing DS domain. AF uses the Ô¨Årst three bits to denote the AF class, and the second three bits to denote the drop precedence. More detail is at 25.7.2 Assured Forwarding. Here are the six-bit patterns for the above PHBs; the AF drop-precedence bits are denoted ‚Äúddd‚Äù. 
- 000 000: default PHB (best-effort delivery) 
- 001 ddd: AF class 1 (the lowest priority) 
- 010 ddd: AF class 2 
- 011 ddd: AF class 3 
- 100 ddd: AF class 4 (the best) 
- 101 110: Expedited Forwarding 
- 101 100: V oice Admit 
- 11x 000: Network control trafÔ¨Åc ( RFC 2597 ) 
- xxx 000: Class Selector (traditional IPv4 Type-of-Service) The goal behind premium PHBs such as EF and AF is for the DS domain to set some rules on admitting premium packets, and hope that their total numbers to any given destination are small enough that high-level service targets can be met. This is not exactly the same as a guarantee, and to a signiÔ¨Åcant degree depends on statistics. The actual speciÔ¨Åcations are written as per-hop behaviors (hence the PHB name); with appropriate admission control these per-hop behaviors will translate into the desired larger-scale behavior. One possible Internet arrangement is that a leaf domain or ISP would support RSVP, but hands trafÔ¨Åc off to some larger-scale carrier that runs DiffServ instead. TrafÔ¨Åc with RSVP reservations would be marked on entry to the second carrier with the appropriate DS class. 618 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 25.7.1 Expedited Forwarding The goal of the EF PHB is to provide low queuing delay to the marked packets; the canonical example is V oIP trafÔ¨Åc, even though the latter now has its own PHB. EF is generally considered to be the best DS service, though this depends on how much EF trafÔ¨Åc is accepted by the DS domain. Each router in a DS domain supporting EF is conÔ¨Ågured with a committed rate, R, for EF trafÔ¨Åc. Different routers can have different committed rates. At any one router, RFC 3246 spells out the rule this way (note that this rule does indeed express a perhopbehavior): Intuitively, the deÔ¨Ånition of EF is simple: the rate at which EF trafÔ¨Åc is served at a given output interface should be at least the conÔ¨Ågured rate R, over a suitably deÔ¨Åned interval, independent of the offered load of non-EF trafÔ¨Åc to that interface. To the EF trafÔ¨Åc, in other words, each output interface should appear to offer bandwidth R, with no competing non-EF trafÔ¨Åc. In general this means that the network should appear to be lightly loaded, though that appearance depends very much on strict control of entering EF trafÔ¨Åc. Normally R will be well below the physical bandwidths of the router‚Äôs interfaces. RFC 3246 goes on to specify how this apparent service should work. Roughly, if EF packets have length L then they should be sent at intervals L/R. If an EF packet arrives when no other EF trafÔ¨Åc is waiting, it can be held in a queue, but it should be sent soon enough so that, when physical transmission has ended, no more than L/R time has elapsed in total. That is, if R and L are such that L/R is 10¬µs, but the physical bandwidth delay in sending is only 2¬µs, then the packet can be held up to 8¬µs for other trafÔ¨Åc. Note that this does notmean that EF trafÔ¨Åc is given strict priority over all other trafÔ¨Åc (though implementation of EF-trafÔ¨Åc processing via priority queuing is a reasonable strategy); however, the sending interface must provide service to the EF queue at intervals of no more than L/R; the EF rate R must be in effect at per-packet time scales. Queuing ten EF packets and then sending the lot of them after time 10L/R is not allowed. Fair queuing can be used instead of priority queuing, but if quantum fair queuing is used then the quantum must be small. An EF router‚Äôs committed rate R means simply that the router has promised to reserve bandwidth R for EF trafÔ¨Åc; if EF trafÔ¨Åc arrives at a router faster than rate R, then a queue of EF packets may build up (though the router may be in a position to use some of its additional bandwidth to avoid this, at least to a degree). Queuing delays for EF trafÔ¨Åc may mean that someone‚Äôs application somewhere fails rather badly, but the router cannot be held to account. As long as the total EF trafÔ¨Åc arriving at a given router is limited to that routers‚Äô EF rate R, then at least that router will be able to offer good service. If the arriving EF trafÔ¨Åc meets a token-bucket speciÔ¨Åcation TB(R,B), then the maximum number of EF packets in the queue will be B and the maximum time an EF packet should be held will be B/R. So far we have been looking at individual routers. A DS domain controls EF trafÔ¨Åc only at its border; how does it arrange things so none of its routers receives EF trafÔ¨Åc at more than its committed rate? One very conservative approach is to limit the total EF trafÔ¨Åc entering the DS domain to the common committed rate R. This will likely mean that individual routers will not see EF trafÔ¨Åc loads anywhere close to R. As a less-conservative, more statistical, approach, suppose a DS domain has four border routers R1, R2, R3 and R4 through which all trafÔ¨Åc must enter and exit, as in the diagram below: 25.7 Differentiated Services 619
An Introduction to Computer Networks, Release 2.0.11 R4R1 R2 R3............ DS domain boundary Suppose in addition the domain knows from experience that exiting EF trafÔ¨Åc generally divides equally between R1-R4, and also that these border routers are the bottlenecks. Then it might allow an EF-trafÔ¨Åc entry rate of R at each router R1-R4, meaning a total entering EF trafÔ¨Åc volume of 4 R. Of course, if on some occasion all the EF trafÔ¨Åc entering through R1, R2 and R3 happened to be addressed so as to exit via R4, then R4 would see an EF rate of 3 R, but hopefully this would not happen often. If an individual ISP wanted to provide end-user DiffServ-based V oIP service, it might mark V oIP packets for EF service as they entered (or might let the customer mark them, subject to the ISP‚Äôs policing). The rate of marked packets would be subject to some ceiling, which might be negotiated with the customer as a certain number of voice lines. These marked V oIP packets would receive EF service as they were routed within the ISP. For calls also terminating within that ISP ‚Äì or switching over to the traditional telephone network at an interface within that ISP ‚Äì this would be all that was necessary, but some calls will likely be to customers of other ISPs. To address this, the original ISP might negotiate with its ISP neighbors for continued preferential service; such service might be at some other DS service class( egAF). Packets would likely need to be remarked as they left the original ISP and entered another. The original ISP may have one larger ISP in particular with which it has a customer-provider relationship. The larger ISP might feel that with its high-volume internal network it has no need to support preferential service, but might still agree to carry along the original ISP‚Äôs EF marking for use by a third ISP down the road. 25.7.2 Assured Forwarding AF, documented in RFC 2597, is simpler than EF, but with little by way of a delay guarantee. The macroscopic goal is to grant speciÔ¨Åc rate assurances to certain trafÔ¨Åc classes, egthe trafÔ¨Åc from a certain set of customers. Distinct trafÔ¨Åc classes are represented by distinct AF classes, which are limited to four. If a contributor to a trafÔ¨Åc class sends more trafÔ¨Åc than that particular contributor is permitted, the excess trafÔ¨Åc is simply marked with a higher drop precedence. Further on in the network, the trafÔ¨Åc may or may not be dropped. The drop-precedence values for the second three bits for the AF classes are as follows: 
- 010: do not drop 
- 100: medium 
- 110 high 620 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 Different priority-queuing levels may used for the different AF classes; this would ensure that all the trafÔ¨Åc needs of AH class 4 are met before AH class 3, and down the line to AH class 1 at the lowest AH priority, just above bulk (default) trafÔ¨Åc. Provided careful limits are placed on how much trafÔ¨Åc is admitted to each class, strict priority queuing need not lead to the starvation of bulk trafÔ¨Åc. Use of priority queuing is not mandatory; another option is fair queuing where the higher-precedence AH classes get a greater fair-queuing-guaranteed fraction of the total bandwidth, at least relative to their trafÔ¨Åc volumes. TrafÔ¨Åc with different drop-precedence values within a single AF class, however, is notassigned to different subqueues (Priority, Fair or otherwise). Doing that would almost certainly lead to signiÔ¨Åcant reordering of the overall trafÔ¨Åc, and reordering tends to be bad for TCP trafÔ¨Åc. If different priority-queuing levels were used here, for example, higher-drop-precedence packets would be delayed until lower-drop-precedence queues were emptied; almost any form of queuing using multiple queues for a different output interface would likely lead to at least some reordering. So long as one TCP connection remains in a single AF class (egbecause all trafÔ¨Åc to or from a given site remains in a single AF class), the packets of that connection should not be reordered. A classic application of AF is for an ISP to be able to grant different performance levels to different customers: gold, silver and bronze (again from the Appendix to RFC 2597 ). Customers would pay an ISP more to carry their trafÔ¨Åc in a higher category. Along with their AF level, each customer would negotiate with the ISP their average rate and also their ‚Äúcommitted‚Äù and ‚Äúexcess‚Äù burst (bucket) capacities. As the customer‚Äôs trafÔ¨Åc entered the network, it would encounter two token-bucket Ô¨Ålters, with rate equal to the agreed-upon rate and with the two different bucket sizes: TB(r,B committed ) and TB(r,B excess ). TrafÔ¨Åc compliant with the Ô¨Årst token-bucket speciÔ¨Åcation would be marked ‚Äúdo not drop‚Äù; trafÔ¨Åc noncompliant for the Ô¨Årst but compliant for the second would be marked ‚Äúmedium‚Äù and trafÔ¨Åc noncompliant for either speciÔ¨Åcation would be marked with a drop precedence of ‚Äúhigh‚Äù. (The use of B excess here corresponds to the sum of ‚Äúcommitted burst size‚Äù and ‚Äúexcess burst size‚Äù in the Appendix to RFC 2597 .) Customers would thus be free to send faster than their agreed-upon rate, subject to the excess trafÔ¨Åc being marked with a lower drop precedence. This process means that an ISP can have many different Gold customers, each with widely varying rate agreements, all lumped together in the same AF-4 class. The individual customer rates, and even the sum of the rates, may have only a tenuous relationship to the actual internal capacity of the ISP, although it is not in the ISP‚Äôs long-term interest to oversubscribe any AF level. If the ISP keeps the AF class 4 trafÔ¨Åc sparse enough, it might outperform EF trafÔ¨Åc in terms of delay. The EF PHB rules, however, explicitly address delay issues while the AF rules do not. 25.8 RED with In and Out Differentiated Services Assured Forwarding (AF) Ô¨Åts nicely with RIO routers: RED with In and Out (or In, Middle, and Out); see 21.5.4 RED. In RIO routers, each drop-precedence value (the ‚ÄúIn‚Äù, ‚ÄúMiddle‚Äù and ‚ÄúOut‚Äù here) is subject to a different drop threshold. Packets with higher drop-precedence values would experience RED signaling losses at correspondingly lower degrees of RED queue utilization. This means that TCP connections with a signiÔ¨Åcant fraction of higher-drop-precedence values would encounter REDinduced losses sooner ‚Äì and would thus reduce their congestion window sooner ‚Äì than connections that stayed within their committed rate. Because each AF class is sent at a single priority, TCP connections within a single AF class should not experience problems( egreordering) other than these RED signaling losses. All this has the effect of gently encouraging customers to stay within their committed trafÔ¨Åc limits. 25.8 RED with In and Out 621
An Introduction to Computer Networks, Release 2.0.11 RIO is likely to have a meaningful effect only on TCP connections; real-time UDP trafÔ¨Åc may be affected slightly or not at all by RED signaling-type losses. 25.9 NSIS The practical problem with RSVP is the need for routers to participate. One approach to gaining ISP cooperation might be a lighter-weight version of RSVP, though that is speculative; Differentiated Services was supposed to be just that and it too has not been widely adopted within the commercial Internet backbone. That said, work has been underway for quite some time now on a replacement protocol suite. One candidate is Next Steps In Signaling, or NSIS, documented in RFC 4080 andRFC 5974. NSIS breaks the RSVP design into two separate layers: the signal transport layer, charged with Ô¨Åguring out how to reach the intermediate routers, and the higher signaling application layer, charged with requesting actual reservations. One advantage of this two-layer approach is that NSIS can be adapted for other kinds of signaling, although most NSIS signaling can be expected to be related to a speciÔ¨Åc network Ô¨Çow. For example, NSIS can be used to tell NAT routers to open up access to a given inside port, in order to allow a V oIP (or other) connection to proceed. Generally speaking, NSIS also abandons the multicast-centric approach of RSVP. Signaling trafÔ¨Åc travels hop-by-hop from one NSIS Element, or NE, to the next NE on the path. In cases when the signaling trafÔ¨Åc follows the same path as the data (the ‚Äúpath-coupled‚Äù case), the signaling packet would likely be addressed to the ultimate destination, but recognized by each NE router on the path. NE routers would then add something to the packet, and perhaps update their own internal state. Nonparticipating (non-NE) routers would simply forward the signaling packet, like any other IP packet, further towards its ultimate destination. 25.10 Comcast Congestion-Management System The large ISP Comcast introduced a DiffServ-like scheme to reduce internal congestion; this was eventually documented in RFC 6057. The effect of the scheme is to reduce bandwidth for those subscribers who are using the largest amount of bandwidth; this reduction is done only when the ISP‚Äôs local network is experiencing signiÔ¨Åcant congestion. The goal is to avoid saturation of the ISP-level backbone, thus perhaps improving fairness for other users. By throttling bulk trafÔ¨Åc throughout its network before maximum capacities are reached, this strategy has the potential to improve the performance of real-time protocols without IntServor DiffServ-type mechanisms. During non-congested operation, all user trafÔ¨Åc is tagged within Comcast‚Äôs network as Priority Best Effort (PBE). This is akin to a medium drop precedence for a particular DiffServ Assured Forwarding class (25.7.2 Assured Forwarding ), though RFC 6057 does not claim that DiffServ is actually used. When congestion occurs, trafÔ¨Åc of some high-volume users may be tagged instead as Best Effort (BE), meaning that it has a lower priority and a higher drop precedence. The mechanism Ô¨Årst deÔ¨Ånes a Near-Congestion State for ports on routers at a certain level of Comcast‚Äôs network. These routers are known as Cable Modem Termination Systems, or CMTS‚Äôs. Each CMTS node serves about 5,000 subscribers. Near-Congestion State is declared when the port utilization exceeding a 622 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 speciÔ¨Åc percentage of its maximum for a speciÔ¨Åc amount of time; typically the utilization threshold is 7080% and the time interval is 15 minutes. One CMTS port typically serves 200-300 customers. Previous experience had established CMTS ports as the likely locus for congestion. When a particular CMTS port enters the Near-Congestion State, the port‚Äôs trafÔ¨Åc is monitored on a persubscriber basis to see if any subscribers are in an Extended High Consumption State, or EHCS, again triggered when a subscriber exceeds a given percentage of their subscriber rate for a given number of minutes. The threshold for entering EHCS state is typically 70% of the subscriber rate ‚Äì either the upstream rate or the downstream rate ‚Äì for 15 minutes. When the switch port is in Near-Congestion State andthe subscriber‚Äôs trafÔ¨Åc is in EHCS, then all that subscriber‚Äôs trafÔ¨Åc is marked Best Effort (BE) rather than PBE, corresponding to a higher AF drop precedence. Downstream trafÔ¨Åc is marked (at the CMTS or even higher up in the network) if it is the user‚Äôs downstream utilization that is high; upstream trafÔ¨Åc is marked at the user‚Äôs connection to the network if upstream utilization is high. Other routers may then drop BE-marked trafÔ¨Åc preferentially, versus PBE-marked trafÔ¨Åc. This will occur only at routers that are actively experiencing congestion, perhaps using a mechanism like RIO ( 25.8 RED with In and Out ), though RIO was originally intended only for TCP trafÔ¨Åc. A subscriber leaves the EHCS state when the subscriber‚Äôs bandwidth drops below 50% of the subscription rate for 15 minutes, where presumably the 50% rate is measured over some very short timescale. Note that this means that a user with a TCP sawtooth ranging from 30% to 60% of the maximum might remain in the EHCS state indeÔ¨Ånitely. Also note that allthe subscriber‚Äôs trafÔ¨Åc will be marked as BE trafÔ¨Åc, not just the overage. The intent is to provide a mild disincentive for sustained utilization within 70% of the subscriber maximum rate. Token bucket speciÔ¨Åcations are not used, except possibly over very small timescales to deÔ¨Åne utilizations of 50% and 70%. The (larger-scale) token-bucket alternative might be to create a token-bucket speciÔ¨Åcation for each customer TB(r,B) where r is some fraction of the subscription rate and B is a bucket of modest size. All compliant trafÔ¨Åc would then be marked PBE and noncompliant trafÔ¨Åc would be marked BE. Such a mechanism might behave quite differently, as only trafÔ¨Åc actually over the ceiling would be marked. 25.11 Real-time Transport Protocol (RTP) RTP is a convenient framework for carrying real-time trafÔ¨Åc. It runs on top of UDP, largely to avoid headof-line blocking ( 16.1 User Datagram Protocol ‚Äì UDP ), and offers several advantages over raw UDP. It does not, however, involve interactions with the intervening routers, and therefore cannot offer any service guarantees. RTP headers include a sequence number, an application-provided timestamp representing when the attached data was recorded, and basic mechanisms for handling trafÔ¨Åc that is a merger of several original sources (‚Äúcontributing sources‚Äù) of related content. RTP also includes support for multicast transmission, egfor the voice and video streams that make up a teleconference. Indeed, teleconferencing is arguably the application for which RTP was developed, although it is also widely used for point-to-point applications such as V oIP. Perhaps the most striking feature of RTP is the absence of frequent acknowledgments. There is indeed a provision for receiver responses, but they are often sent only at several-second intervals, and are frequently omitted entirely. These responses use the companion RTCP protocol, and the ‚Äúreceiver report‚Äù format. 25.11 Real-time Transport Protocol (RTP) 623
An Introduction to Computer Networks, Release 2.0.11 RTCP receiver-report packets are often thought of not as acknowledgments but as a source of statistics about how the RTP Ô¨Çow is being delivered; in particular, the RTCP packets contain a ratio of how many sender packets arrived since the previous RTCP report, the highest sequence number received, and a measure of the degree of jitter. For multicast, the infrequent acknowledgments make sense. If every receiver of a multicast group sent acknowledgments totaling 3% of the received-content bandwidth (about the TCP ratio), and if there were 100 receivers in the group (not large for a teleconference), then the total acknowledgment trafÔ¨Åc arriving at the sender would be triple the content trafÔ¨Åc. There are, in fact, mechanisms in place to limit the total RTCP bandwidth to no more than 5% of the outbound content stream; if a multicast stream has thousands of receivers then those receivers will each respond relatively infrequently ( egat intervals of many seconds, if not many minutes). Even in one-to-one transmission settings, though, RTP may not send many acknowledgments. Many V oIP systems are conÔ¨Ågured not to send RTCP responses at all by default; when these areenabled, the rate has a typical minimum of one RTCP response per second. In one second, a V oIP sender may transmit 50 packets. One explanation for this is that when a voice call is encountering signiÔ¨Åcant congestion, the participants are expected to hang up, rather than keep the line open. For two-way voice calls, symmetric RTP is often used ( RFC 4961 ). This means that each party uses the same port for sending and receiving. This is done only for convenience and to handle NAT routers; the two separate directions are completely independent and do notserve as acknowledgments for one another, as would be the case for bidirectional TCP trafÔ¨Åc. Indeed, one can block one direction of a V oIP symmetricRTP stream and the call continues on indeÔ¨Ånitely, transmitting voice only in the other direction. When the block is removed, the blocked voice Ô¨Çow simply resumes. 25.11.1 RTP Mixers Mixers are network nodes that take one or more RTP source streams and perform any combination of the following operations 
- consolidation of multiple streams into one 
- translation to a different audio or video format 
- re-encoding to a lower-bandwidth format As an example of consolidation, video of several panelists might be consolidated into a single video frame, in which the current speaker has a larger window. At the audio level, a mixer might combine the streams from several individual microphones. If all parties are at the same location this kind of consolidation can be performed by the primary sender, but mixers may be useful if conference participants are geographically dispersed. As for translation, some participants may prefer to receive in a different format. Perhaps the most important task for mixers, however, is rate-adaptation. With a unicast connection, if congestion is experienced then the sender itself can adapt to it by switching the encoding. This is not appropriate for multicast, however; if one receiver is experiencing congestion it is not unlikely that other receivers may be doing just Ô¨Åne. Therefore, if a multicast receiver is having trouble receiving at a high data rate, it is up to it to Ô¨Ånd a mixer that offers a lower-rate encoding, and switch its subscription/reservation to that mixer instead. Mixers offering a range of rates might be set up near the sender (or at least logically near); they might also be geographically distributed to be nearer to clusters of likely receivers. One host might provide several mixers offering a range of bandwidth options. 624 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 One thing mixers do notdo is mix audio and video together; separate audio and video streams should be carried by separate RTP sessions. RTP packets carry timestamps to allow a receiver to synchronize audio and video playback, so mixing is not necessary. But more importantly, mixing of audio and video makes it difÔ¨Åcult to re-encode to a different format, difÔ¨Åcult for a receiver to subscribe only to the audio, and difÔ¨Åcult for the RTCP reports to indicate which original stream was encountering more losses. An individual sending point in RTP, complete with timing information, is called a synchronization source or SSRC. At the point of origin, each camera and microphone might be an SSRC. SSRCs are identiÔ¨Åed by a 32-bit identiÔ¨Åer. Actual SSRC identiÔ¨Åers are to be chosen pseudorandomly (and there is a provision in the protocol for making sure two different sources do not choose the same SSRC identiÔ¨Åer), but for many practical purposes the SSRC can be thought of as a host identiÔ¨Åer, like an IP address. When a mixer combines multiple SSRCs into one stream (or even when a mixer takes as input only a single SSRC), the mixer becomes the new synchronization source and creates a new SSRC identiÔ¨Åer for all its output packets; the mixer also generates a new series of synchronization timestamps. However, the SSRC identiÔ¨Åers for the streams that the mixer used as input are attached to all RTP packets from the mixer; each of these is now known as a contributing source or CSRC. 25.11.2 RTP Packet Format The basic RTP header has the following format, from RFC 3550: PX CC M Payload Type Sequence Number Timestamp Synchronization Source (SSRC) Identifier Contributing Source (CSRC) Identifiers. .. .Ver TheVerÔ¨Åeld holds the version, currently 2. The Pbit indicates that the RTP packet includes at least one padding byte at the end; the Ô¨Ånal padding byte contains the exact count. TheX bit is set if an extension header follows the basic header; we do not consider this further. TheCC Ô¨Åeld contains the number of contributing source (CSRC) identiÔ¨Åers (below). The total header size, in 32-bit words, is 3+CC. TheM bit is set to allow the marking, for example, of the Ô¨Årst packet of each new video frame. Of course, the actual video encoding will also contain this information. ThePayload Type Ô¨Åeld allows (or, more precisely, originally allowed) for speciÔ¨Åcation of the audio/visual encoding mechanism ( eg¬µ-law/G.711), as described in RFC 3551. Of course, there are more than 27possible encodings now, and so these are typically speciÔ¨Åed via some other mechanism, egusing an extension header or the separate Session Description Protocol ( RFC 4566 ) or as part of the RTP stream‚Äôs ‚Äúannouncement‚Äù. RFC 3551 put it this way: During the early stages of RTP development, it was necessary to use statically assigned payload types because no other mechanism had been speciÔ¨Åed to bind encodings to payload 25.11 Real-time Transport Protocol (RTP) 625
An Introduction to Computer Networks, Release 2.0.11 types.. .. Now, mechanisms for deÔ¨Åning dynamic payload type bindings have been speciÔ¨Åed in the Session Description Protocol (SDP) and in other protocols.. .. The sequence-number Ô¨Åeld allows for detection of lost packets and for correct reassembly of reordered packets. Typically, if packets are lost then the receiver is expected to manage without them; there is no timeout/retransmission mechanism in RTP. Thetimestamp is for synchronizing playback. The timestamp should be sufÔ¨Åciently Ô¨Åne-grained to support not only smooth playback but also the measurement of jitter, that is, the degree of variation in packet arrival. Each encoding mechanism chooses its own timestamp granularity. For most telephone-grade voice encodings, for example, the timestamp clock increments at the canonical sampling rate of 8,000 times a second, corresponding to one DS0 channel ( 6.2 Time-Division Multiplexing ).RFC 3551 suggests a timestamp clock rate of 90,000 times a second for most video encodings. Many V oIP applications that use RTP send 20 ms of voice per packet, meaning that the timestamp is incremented by 160 for each packet. The actual amount of data needed to send 20 ms of voice can vary from 160 bytes down to 20 bytes, depending on the encoding used, but the timestamp clock always increments at the 8,000/sec, 160/packet rate. The SSRC identiÔ¨Åer identiÔ¨Åes the primary data source (the ‚Äúsynchronization source‚Äù) of the stream. In most cases this is either the identiÔ¨Åer for the originating camera/microphone, or the identiÔ¨Åer for the mixer that repackaged the stream. If the stream was processed by a mixer, the SSRC Ô¨Åeld identiÔ¨Åes the mixer, and the SSRC identiÔ¨Åers of the original sources are now listed in the CSRC (‚Äúcontributing sources‚Äù) section of the header. If there was no mixer involved, the CSRC section is empty. 25.11.3 RTP Control Protocol The RTP Control Protocol, or RTCP, provides a mechanism for exchange of a variety of extra information between RTP participants. RTCP packets may be Sender Reports (SR packets) or Receiver Reports (RR packets); we are mostly interested in the latter. RTCP receiver reports are the only form of acknowledgments for RTP transmission. These are, however, unlike any other form of acknowledgment we have considered; for one thing, there is generally no question of retransmitting any lost data. RTCP RR packets should perhaps be thought of as statistical summaries regarding delivery rather than acknowledgments per se; in some cases they may in effect simply relay to senders the information ‚Äúmulticast is not working today‚Äù. RTCP packets contain a list of several SSRCs; for each SSRC, the message includes 
- the fraction of RTP packets that were lost in the most recent interval 
- the cumulative number of packets lost since the session began 
- the highest sequence number received 
- a measure of the interarrival jitter (below) 
- for RR packets, information about the most recent RTCP SR packet Jitter is measured as the mean deviation in actual arrival times, versus theoretical arrival times, and is measured in units of the RTP timestamp clock (above). The actual formula is as follows. For each packet P i, 626 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 we can record the actual time of arrival in RTP timestamp units as R iand we can save the RTP timestamp included in the packet as S i. We Ô¨Årst calculate the ith deviation as follows: Dev i= (R iRi-1) - (S iSi-1) For evenly spaced packets, the deviation will be zero. For V oIP packets, S iSi-1is likely exactly 160; R iRi-1is the actual arrival interval in eighths of milliseconds. We then calculate the cumulative jitter as follows, where ùõº= 15/16: Ji=ùõºJi-1+ (1-ùõº)Dev i In a multicast setting, if an RTP receiver is experiencing excessive losses, its most practical option is probably to Ô¨Ånd a mixer offering a lower-bandwidth encoding, or that is otherwise more likely to provide lowcongestion service. RTCP packets are not needed for this. In a unicast setting, an RTP sender can ‚Äì at least theoretically ‚Äì use the information provided by RTCP RRs to adapt its transmission rate downwards, to better make use of the available bandwidth. A sender might also use TFRC ( 21.3.1 TFRC ) to calculate a TCP-friendly sending rate; there are Internet drafts for this ([GP11]), but as yet no RFC. TFRC also allows for a gradual adjustment in rate to accommodate the new conditions. Of course, TFRC can only be used to adjust the sending rate if the sending rate is in fact adaptive, at the application level. This, of course, requires a rate-adaptive encoding. RFC 3550 speciÔ¨Åes a mechanism to make sure that RTCP packets do not consume more than 5% of the total RTP bandwidth, and, of that 5%, RTCP RR packets do not account for more than 75%. This is achieved by each receiver learning from RTCP SR packets how many other receivers there are, and what the total RTP bandwidth is. From this, and from some related information, each RTP receiver can calculate an acceptable RTCP reporting interval. This interval can easily be in the hundreds of seconds. Mixers are allowed to consolidate RTCP information from their direct subscribers, and forward it on to the originating sources. 25.11.4 RTP and VoIP V oice-over-IP data is often carried by RTP, although V oIP calls are initially set up using the Session Initiation Protocol, SIP ( RFC 3261 ), or some other setup protocol such as H.323. As part of the call set-up process, the SIP nodes organizing the call exchange IP addresses and port numbers for the actual endpoints. Each endpoint then is informed of the address and port for the other endpoint, and the endpoints more-or-less immediately begin sending one another RTP packets. If an endpoint is behind a NAT Ô¨Årewall, its associated SIP server may have to continue forwarding packets. V oIP calls typically send voice data in packets containing 20ms of audio input. If the standard DS0 G.711 encoding is used, 8 bytes are needed for each millisecond of voice and so each packet has 160ms of data. The G.711 encoding includes both ¬µ-law and A-law encoders; these are both variations on the idea of having the 8-bit samples be proportional to the logarithm of the actual sound intensity. Other voice encoders provide a greater degree of compression. For example, when G.729 voice encoding is used for V oIP then each packet will carry 20 ms of voice in 20 bytes of data. Each such packet will also, typically, have 54 bytes of header (14 bytes of Ethernet header, 20 bytes of IP header, 8 bytes of UDP header 25.11 Real-time Transport Protocol (RTP) 627
An Introduction to Computer Networks, Release 2.0.11 and 12 bytes of RTP header). Despite a payload efÔ¨Åciency of only 20/74 27%, this is generally considered acceptable, if not ideal. V oIP connections often do not make much use of the data in the RTCP packets; it is not uncommon, in fact, for RTCP packets not even to be sent. The general presumption is that if the voice quality is not adequate, the users involved should simply hang up. The most common voice encoders ( egG.711 and, for that matter, G.729) are not rate-adaptive, and so if a call is encountering congestion there is nothing that can be done. That said, RTCP packets may provide useful information regarding jitter experienced by the call. Furthermore, as we mentioned above in 25.3 Real-time TrafÔ¨Åc, some voice codecs ‚Äì such as Opus and Speex ‚Äì do support rate-adaptiveness. 25.12 Multi-Protocol Label Switching (MPLS) Currently, MPLS is a way of tagging certain classes of trafÔ¨Åc, and perhaps routing them altogether differently from other classes of trafÔ¨Åc. While IPv4 has always allowed routing based on tagged QoS levels, this feature was seldom used, and a common assumption for both Integrated and Differentiated Services was that the priority trafÔ¨Åc essentially took the same route as everything else. MPLS allows priority trafÔ¨Åc to take an entirely different route. MPLS started out as a way of routing IP and non-IP trafÔ¨Åc together, and later became a way to avoid the then-inefÔ¨Åcient lookup of an IP address at every router. It has, however, now evolved into a way to support the following: 
- creation of explicit routes for IP trafÔ¨Åc, either in bulk or by designated class. 
- large-scale virtual private networks (VPNs) We are mostly interested in MPLS for the Ô¨Årst of these; as such, it provides an alternative way for ISPs to handle real-time trafÔ¨Åc. The explicit routes are deÔ¨Åned using virtual circuits ( 5.4 Virtual Circuits ). In effect, virtual-circuit tags are added to each packet, on entrance to the routing domain, allowing packets to be routed along a predetermined path. The virtual circuit paths need not follow the same route that normal IP routing would use, though note that link-state routing protocols such as OSPF already allow different routes for different classes of service. We note also that the MPLS-labeled trafÔ¨Åc might very well use the same internal routers as bulk trafÔ¨Åc; priority or fair queuing would then still be needed at those routers to make sure the MPLS trafÔ¨Åc gets the desired level of service. However, the use of MPLS at least makes the classiÔ¨Åcation problem easier: internal routers need only look at the tag to determine the priority or fair-queuing class, and deep-packet inspection can be avoided. MPLS would also allow the option that a high-priority Ô¨Çow would travel on a special path through its own set of routers that do notalso service low-priority trafÔ¨Åc. Generally MPLS is used only within one routing domain or administrative system; that is, within the scope of one ISP. TrafÔ¨Åc enters and leaves looking like ordinary IP trafÔ¨Åc, and the use of MPLS internally is completely invisible. This local scope of MPLS, however, has meant that it has seen relatively widespread adoption, at least compared to RSVP and IP multicast: no coordination with other ISPs is necessary. To implement MPLS, we start with a set of participating routers, called label-switching routers or LSRs. (The LSRs can comprise an entire ISP, or just a subset.) Edge routers partition (or classify ) trafÔ¨Åc into large Ô¨Çow classes; one distinct Ô¨Çow (which might, for example, correspond to all V oIP trafÔ¨Åc) is called a 628 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 forwarding equivalence class or FEC. Different FECs can have different quality-of-service targets. Bulk trafÔ¨Åc not receiving special MPLS treatment is not considered to be part of any FEC. A one-way virtual circuit is then created for each FEC. An MPLS header is prepended to each IP packet, using for the VCI a label value related to the FEC. The MPLS label is a 32-bit Ô¨Åeld, but only the Ô¨Årst 20 bits are part of the VCI itself. The last 12 bits may carry supplemental connection information, for example ATM virtual-channel identiÔ¨Åers and virtual-path identiÔ¨Åers ( 5.5 Asynchronous Transfer Mode: ATM ). It is likely that some trafÔ¨Åc (perhaps even a majority) does not get put into any FEC; such trafÔ¨Åc is then delivered via the normal IP-routing mechanism. MPLS-aware routers then add to their forwarding tables an MPLS table that consists of xlabel in, interface in, label out, interface outyquadruples, just as in any virtual-circuit routing. A packet arriving on interface interface inwith label label inis forwarded on interface interface outafter the label is altered to label out. Routers can also build their MPLS tables incrementally, although if this is done then the MPLS-routed trafÔ¨Åc will follow the same path as the IP-routed trafÔ¨Åc. For example, downstream router R1 might connect to two customers 200.0.1/24 and 200.0.2/24. R1 might assign these customers labels 37 and 38 respectively. R3 R2 R1200.0.1.0/24 200.0.2.0/24 R1 might then tell its upstream neighbors ( egR2 above) that any arriving trafÔ¨Åc for either of these customers should be labeled with the corresponding label. R2 now becomes the ‚Äúingress router‚Äù for the MPLS domain consisting of R1 and R2. R2 can push this further upstream ( egto R3) by selecting its own labels, eg17 and 18, and asking R3 to label 200.0.1/24 trafÔ¨Åc with label 17 and 200.0.2/24 with 18. R2 would then rewrite VCIs 17 and 18 with 37 and 38, respectively, before forwarding on to R1, as usual in virtual-circuit routing. R2 might not be able to continue with labels 37 and 38 because it might already be using those for inbound trafÔ¨Åc from somewhere else. At this point R2 would have an MPLS virtual-circuit forwarding table like the following: interface inlabel ininterface out label out R3 17 R1 37 R3 18 R1 38 One advantage here of MPLS is that labels live in a Ô¨Çat address space and thus are easy and simple to look up,egwith a big array of 65,000 entries for 16-bit labels. MPLS can be adapted to multicast, in which case there might be two or more label out, interface outcombinations for a single input. Sometimes, packets that already have one MPLS label might have a second (or more) label ‚Äúpushed‚Äù on the front, as the packet enters some designated ‚Äúsubdomain‚Äù of the original routing domain. When MPLS is used throughout a domain, ingress routers attach the initial label; egress routers strip it off. A label information base, or LIB, is maintained at each node to hold any necessary packet-handling 25.12 Multi-Protocol Label Switching (MPLS) 629
An Introduction to Computer Networks, Release 2.0.11 information ( egqueue priority). The LIB is indexed by the labels, and thus involves a simpler lookup than examination of the IP header itself. MPLS has a natural Ô¨Åt with Differentiated Services ( 25.7 Differentiated Services ): the ingress routers could assign the DS class and then attach an MPLS label; the interior routers would then need to examine only the MPLS label. Priority trafÔ¨Åc could be routed along different paths from bulk trafÔ¨Åc. MPLS also allows an ISP to create multiple, mutually isolated VPNs; all that is needed to ensure isolation is that there are no virtual circuits ‚Äúcrossing over‚Äù from one VPN to another. If the ISP has multi-site customers A and B, then virtual circuits are created connecting each pair of A‚Äôs sites and each pair of B‚Äôs sites. A and B each probably have at least one gateway to the whole Internet, but A and B can communicate with each other only through those gateways. MPLS sometimes interacts somewhat oddly with traceroute ( 10.4.1 Traceroute and Time Exceeded ). If a packet‚Äôs TTL reaches 0 at an MPLS router, the router will usually generate the appropriate ICMP Time Exceeded message, but then attach to it the same MPLS label that had been on the original packet. As a result, the ICMP message will continue on to the downstream egress router before being sent back to the original sender. If nothing else, this results in an unusually large round-trip-time report. To the traceroute initiator it will appear as if all the internal routers in the MPLS domain are the same distance from the initiator as the egress router. 25.13 Epilog Quality-of-service guarantees for real-time and other classes of trafÔ¨Åc have been an area of active research on the Internet for over 20 years, but have not yet come into the mainstream. The tools, however, are there. Protocols requiring global ISP coordination ‚Äì such as RSVP and IP Multicast ‚Äì may come slowly if at all, but other protocols such as Differentiated Services and MPLS can be effective when rolled out within a single ISP. Still, after twenty years it is not unreasonable to ask whether integrated networks are in fact the correct approach. One school of thought says that real-time applications (such as V oIP) are only just beginning to come into the mainstream, and integrated networks are sure to follow, or else that video streaming will take over the niche once intended for real-time trafÔ¨Åc. Perhaps IntServ was just ahead of its time. But the other perspective is that the marketplace has had plenty of opportunity to make up its mind and has answered with a resounding ‚Äúno‚Äù, and it is time to move on. Perhaps having separate networks for bulk trafÔ¨Åc and for voice is not unreasonable or inefÔ¨Åcient after all. Or perhaps the Internet will evolve into a network where alltrafÔ¨Åc is handled by real-time mechanisms. Time, as usual, may tell, but not, perhaps, quickly. 25.14 Exercises 1.0. Suppose someone proposes TCP over multicast, in which each router collects the ACKs returning from the group members reached through it, and consolidates them into a single ACK. This now means that, like the multicast trafÔ¨Åc itself, no ACK is duplicated on any single link. What problems do you foresee with this proposal? (Hint: who will send retransmissions? How long will packets need to be buffered for potential retransmission?) 630 25 Quality of Service
An Introduction to Computer Networks, Release 2.0.11 2.0. In the following network, suppose trafÔ¨Åc from RP to R3-R5 is always routed Ô¨Årst right and then down, while trafÔ¨Åc from R3‚ÄìR5 to RP is always routed Ô¨Årst left and then up. What is the multicast tree for the group G = {B1,B2,B3}? RP R1 R2 R3 R4 R5 B1 B2 B3 3.0. What should an RSVP router do if it sees a PATH packet but there is no subsequent RESV packet? 4.0. In 25.7.1 Expedited Forwarding there is an example of an EF router with committed rate R for packets with length L. If R and L are such that L/R is 10¬µs, but the physical bandwidth delay in sending is only 2¬µs, then the packet can be held up to 8¬µs for other trafÔ¨Åc. How large a bulk-trafÔ¨Åc packet can this router send, in between EF packets? Your answer will involve L. 5.0. Suppose, in the diagram in 25.7.1 Expedited Forwarding, EF was used for voice telephony and at some point calls entering through R1, R2 and R3 were indeed all directed to R4. (a). How might the problem be perceived by users? (b). How might the ISP respond to reduce the problem? Note that the ISP has no control over who calls whom. 25.14 Exercises 631
An Introduction to Computer Networks, Release 2.0.11 632 25 Quality of Service
26 NETWORK MANAGEMENT AND SNMP Network management, broadly construed, consists of all the administrative actions taken to keep a network running efÔ¨Åciently. This may include a number of non-technical considerations, egstafÔ¨Ång the help desk and negotiating contracts with vendors, but we will restrict attention exclusively to the technical aspects of network management. The ISO and the International Telecommunications Union have deÔ¨Åned a formal model for telecommunications and network management. The original model deÔ¨Åned Ô¨Åve areas of concern, and was sometimes known as FCAPS after the Ô¨Årst letter of each area: 
- fault 
- conÔ¨Åguration 
- accounting 
- performance 
- security Most non-ISP organizations have little interest in network accounting (the A in FCAPS is often replaced with ‚Äúadministration‚Äù for that reason, but that is a rather vague category). Network security is arguably its own subject entirely. As for the others, we can identify some important subcategories: fault: - device management: monitoring of all switches, routers, servers and other network hardware to make sure they are running properly. 
- server management: monitoring of the network‚Äôs application layer, that is, all network-based software services; these include login authentication, email, web servers, business applications and Ô¨Åle servers. 
- link management: monitoring of long-haul links to ensure they are working. conÔ¨Åguration: - network architecture: the overall design, including topology, switching vs routing and subnet layout. 
- conÔ¨Åguration management: arranging for the consistent conÔ¨Åguration of large numbers of network devices. 
- change management: how does a site roll out new changes, from new IP addresses to software updates and patches? performance: - trafÔ¨Åc management: using the techniques of 23 Queuing and Scheduling to allocate bandwidth shares (and perhaps bucket sizes) among varying internal or external clients or customers. 
- service-level management: making sure that agreed-upon service targets ‚Äì egbandwidth ‚Äì are met (depending on the focus, this could also be placed in the fault category). 633
An Introduction to Computer Networks, Release 2.0.11 While all these aspects are important, technical network management narrowly construed often devolves to an emphasis on fault management and its companion, reliability management: making sure nothing goes wrong, and, when it does, Ô¨Åxing it promptly. It is through fault management that some network providers achieve the elusive availability goal of 99.999% uptime. SNMP versus Management While SNMP is a very important toolfor network management, it isjust a tool. Network management is the process of making decisions to achieve the goals outlined above, subject to resource constraints. SNMP simply provides some input for those decisions. By far the most common device-monitoring protocol, and the primary focus for this chapter, is the Simple Network Management Protocol orSNMP (26.2 SNMP Basics ). This protocol allows a device to report information about its current operational state; for example, a switch or router may report the conÔ¨Åguration of each interface and the total numbers of bytes and packets sent via each interface. In this chapter we describe the basics of SNMP, and, in particular, SNMP version 1. In the following chapter we describe version 2 and version 3; the latter is where reasonable security is Ô¨Ånally implemented. Implicit in any device-monitoring strategy is initial device discovery: the process by which the monitor learns of new devices. The ping protocol ( 10.4 Internet Control Message Protocol ) is common here, though there are other options as well; for example, it is possible to probe sequential IPv4 addresses on the UDP port used for SNMP ‚Äì usually 161. As was the case with router conÔ¨Åguration ( 13 Routing-Update Algorithms ), manual entry is simply not a realistic alternative. SNMP and the Application Layer SNMP can be studied entirely from a network-management perspective, but it also makes an excellent self-contained case study of the application layer. Like essentially all applications, SNMP deÔ¨Ånes rules for client and server roles and for the format of requests and responses. SNMP also contains its own authentication mechanisms ( 26.11 SNMPv1 communities and security and27.3 SNMPv3 ), generally unrelated to any operating-system-based login authentication. It is a practical necessity, for networks of even modest size, to automate the job of checking whether everything is working properly. Waiting for complaints is not an option. Such a monitoring system is known as a network management system orNMS; there are a wide range of both proprietary and open-source NMS‚Äôs available. At its most basic, an NMS consists of a library of scripts to discover new network devices and then to poll each device (possibly but not necessarily using SNMP) at regular intervals. Generally the data received is recorded and analyzed, and alarms are sounded if a failure is detected. When SNMP was Ô¨Årst established, there was a common belief that it would soon be replaced by the OSI‚Äôs Common Management Interface Protocol. CMIP is deÔ¨Åned in the International Telecommunication Union‚Äôs X.711 protocol and companion protocols. CMIP uses the same ASN.1 syntax as SNMP, but has a richer operations set. It remains the network management protocol of choice for OSI networks, and was once upon a time believed to be destined for adoption in the TCP/IP world as well. But it was not to be. TCP/IP-based network-equipment vendors never supported CMIP widely, and so any network management system had to support SNMP as well. With SNMP support essentially universal, there 634 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 was never a need for a site to bother with CMIP. CMIP, unlike SNMP, is supported over TCP, using the CMIP Over Tcp, or CMOT, protocol. One advantage of using TCP is the elimination of uncertainty as to whether a request or a reply was received. CMIP does still exist, and is sometimes used for niche monitoring applications, but overcoming the lack of hardware support makes widespread adoption very difÔ¨Åcult. 26.1 Network Architecture Before turning to SNMP in depth, we offer a few references to other parts of this book relating to network architecture. At the LAN and Internetwork layers local to a site, perhaps the main issues are redundancy, bandwidth and cost. Cabling between buildings, in particular, needs to provide redundancy. See 3.1 Spanning Tree Algorithm and Redundancy and3.2 Virtual LAN (VLAN) for some considerations at the Ethernet level, and 9.6.3 Subnets versus Switching. Next, a site must determine what sort of connection to the Internet it will have. ISP contracts vary greatly in terms of bandwidth, burst bandwidth, and agreed-upon responses in the event of an outage. Some aspects of service-level speciÔ¨Åcation appear in 24 Token Bucket Rate Limiting and25.7.2 Assured Forwarding. Organizations with geographically dispersed internal networks ‚Äì ISPs and larger corporations ‚Äì must decide how their internal sites should be connected. Should they communicate over the public Internet? Should a VPN be used ( 5.1 Virtual Private Networks )? Or should private lines (such as SONET, 6.2.2 SONET, or carrier Ethernet, 5.2 Carrier Ethernet ) be leased between sites? If private lines are used, link monitoring becomes essential. One increasingly important architectural decision at the application layer is the extent to which network software services are outsourced to the cloud, and run on remote servers managed by third parties. 26.2 SNMP Basics SNMP is far and away the most popular protocol for supporting network device monitoring. At its most basic level, SNMP allows polling of individual designated device attributes, such as the system name or the number of packets received via interface eth0. Attributes may, however, be organized into records, sets and tables. Tables may be indexed contiguously, like an array ‚Äì eginterface[1], interface[2], interface[3], etc‚Äì or sparsely ‚Äì eginterface[1], interface[32767]. While the simplest routers and switches may have quite limited provisions for SNMP, virtually all ‚Äúserious‚Äù networking hardware provides extensive support, for both standard sets of ‚Äúbasic‚Äù device attributes and for proprietary attributes as well. Devices with signiÔ¨Åcant SNMP support are sometimes referred to as managed devices. An SNMP node that replies to requests for information is known as an SNMP agent. The network node doing the SNMP querying is known as the manager; it may be part of an NMS or ‚Äì more simply ‚Äì be a standalone tool known as an SNMP browser or MIB browser (where MIB stands for Management Information Base, below). While most MIB browsers are understood to have a graphical user interface, there are also command-line tools to make SNMP queries, such as the snmpget andsnmpwalk commands of 26.1 Network Architecture 635
An Introduction to Computer Networks, Release 2.0.11 the Net-SNMP project at net-snmp.org. These can be invoked by scripting languages to build a simple if rudimentary NMS. SNMP runs exclusively over UDP. The choice of UDP was made to avoid the connection overhead of what was envisioned to be a simple request-reply protocol; if a manager polls 1,000 devices once a minute, that is 2,000 packets in all over UDP but might easily be 8,000 packets with TCP and the necessary SYN/FIN packets. This may be especially signiÔ¨Åcant when the network is congested to near the point of failure. The use of UDP does raise two problems: lost packets and having more data than will Ô¨Åt in one packet. For the simplest case of manager-initiated data requests, a manager can handle packet loss by polling a device until a response is received. If a response (or even a request) is too big, the usual strategy is to use IP-layer fragmentation ( 9.4 Fragmentation and11.5.4 IPv6 Fragment Header ). This is not ideal, but as most SNMP data stays within local and organizational networks it is at least workable. Another consequence of the use of UDP is that every SNMP device needs an IP address. For devices acting at the IP layer and above, this is not an issue, but an Ethernet switch or hub has no need of an IP address for its basic function. Devices like switches and hubs that are to use SNMP must be provided with a suitable IP interface and address. Sometimes, for security, these IP addresses for SNMP management are placed on a private, more-or-less hidden subnet. SNMP also supports the writing of attributes to devices, thus implementing a remote-conÔ¨Åguration mechanism. As writing to devices has rather more security implications than reading from them, this mechanism must be used with care. Even for read-only SNMP, however, security is an important issue; see 26.11 SNMPv1 communities and security and27.3 SNMPv3. Writing to agents may be done either to conÔ¨Ågure the network behavior of the device ‚Äì egto bring an interface up or down, or to set an IP address ‚Äì or speciÔ¨Åcally to conÔ¨Ågure the SNMP behavior of the agent. Finally, SNMP supports asynchronous notiÔ¨Åcation through its traps mechanism: a device can be conÔ¨Ågured to report an error immediately rather than wait until asked. While traps are quite important at many sites, we will largely ignore them here. A crucial problem is that, because UDP is the underlying transport mechanism, there is no delivery guarantee of the trap messages. SNMP is sometimes used for server monitoring as well as device monitoring; alternatively, simple scripts can initiate connections to services and discern at least something about their readiness. It is one thing, however, to verify that an email server is listening on TCP port 25 and responds with the correct RFC 5321 (originally RFC 821 ) EHLO message; it is another to verify that messages are accepted and then actually delivered. 26.2.1 SNMP versions SNMP has three ofÔ¨Åcial versions, SNMPv1, SNMPv2 and SNMPv3. SNMPv1 made its Ô¨Årst appearance in 1988 in a collection of RFCs starting with RFC 1065 (updated in RFC 1155 ); the current deÔ¨Ånition for ‚Äúcore‚Äù attribute reporting was released as RFC 1213 in 1991. We will return to this below in 26.10 MIB-2. SNMPv2 was introduced in 1993 with RFC 1441. Loosely speaking, SNMPv2 expanded on the basic information, starting with RFC 1442 (currently RFC 2578 ), and also introduced improved techniques for managing tables. SNMPv2 also included a proposed security mechanism, but it was largely rejected by the marketplace. Ultimately, a version of SNMPv2 that used the SNMPv1 ‚Äúcommunity‚Äù security mechanism ( 26.11 SNMPv1 636 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 communities and security ) was introduced; see RFC 1901. This ‚Äúcommunity‚Äù-security version became known as SNMPv2 c. SNMPv3 then Ô¨Ånally delivered a model for reasonably effective security. The ‚ÄúUser-based Security Model‚Äù orUSM was Ô¨Årst proposed in 1998 in RFC 2264; the Ô¨Ånal 2002 version is in RFC 3414. 26.3 SNMP Naming and OIDs A central part of the SNMP protocol is how to name each device attribute in a consistent manner. The naming scheme chosen was the Object IdentiÔ¨Åer, orOID, hierarchy. Starting in 1985, the International Standards Organization (ISO) and the standardization sector of the International Telecommunications Union (ITU-T, then known as CCITT) developed the Object IdentiÔ¨Åer scheme for naming anything in the world that needed naming. Names, or OIDs, consist of strings of non-negative integers. In textual representation these component integers are often written separated by periods, eg1.3.6.1; the notation { 1 3 6 1 } is also used. The scheme is standardized in ITU-T X.660. All OIDs used by SNMP begin with the preÔ¨Åx 1.3.6.1. For example, the sysName attribute corresponds to OID 1.3.6.1.2.1.1.5. The preÔ¨Åx 1.3.6.1.2.1 is known as mib-2, and the one-step-longer preÔ¨Åx 1.3.6.1.2.1.1 issystem. Occasionally we will abuse notation and act as if names referred to single additional levels rather than full preÔ¨Åxes, egthe latter two names in mib-2.system.sysName. The basic SNMP read operation is Get() (sent via the GetRequest protocol message), which takes an OID as parameter. The agent receiving the Get request will, if authentication checks out and if the OID corresponds to a valid attribute, return a pair consisting of the OID and the attribute value. We will return to this in 26.8 SNMP Operations, and see how multiple attribute values can be requested in a single operation. OIDs form a tree or hierarchy; the immediate child nodes of, say, 1.3.6.1 of length 4 are all nodes 1.3.6.1.N of length 5. The root node, with this understanding, is anonymous; OIDs are sometimes rendered with a leading ‚Äú.‚Äù to emphasize this: .1.3.6.1. Alternatively, the numbers can be thought of as labels on the arcs of the tree rather than the nodes. There is no intrinsic way to distinguish internal OID nodes ‚Äì preÔ¨Åxes ‚Äì from leaf OID nodes that correspond to actual named objects. Context is essential here. It is common to give the numeric labels at any speciÔ¨Åc level human-readable string equivalents. The three nodes immediately below the root are, with their standard string equivalents 
- itu-t(0) 
- iso(1) 
- joint-iso-itu-t(2) The string equivalents can be thought of as external data; when OIDs are encoded only the numeric values are included. OID naming has been adopted in several non-SNMP contexts as well. ISO and ITU-T both use OIDs to identify ofÔ¨Åcial standards documents. The X.509 standard uses OID naming in encryption certiÔ¨Åcates; seeRFC 5280. The X.500 directory standard also uses OIDs; the related RFC 4524 deÔ¨Ånes several OID classes preÔ¨Åxed by 0.9.2342.19200300.100.1. As a non-computing example, Health Level Seven names US healthcare information beginning with the OID preÔ¨Åx 2.16.840.1.113883. 26.3 SNMP Naming and OIDs 637
An Introduction to Computer Networks, Release 2.0.11 As mentioned above, SNMP uses OIDs beginning with 1.3.6.1; in the OID tree these levels correspond to 
- iso(1) 
- org(3): organizations 
- dod(6): the US Department of Defense, the original sponsor of the Internet 
- internet(1), apparently the only sublevel of DOD Here is a portion of the OID tree showing a few other assignments in the iso subtree, and highlighting the 1.3.6.1 preÔ¨Åx above. More entries are available from oid-info.com (in 2022, try the ‚ÄúTree display‚Äù button). itu-t(0) iso(1) standard(0) registration-authority(1) member-body(2) identiÔ¨Åed-organization(3) nist(5) dod(6) icd-ecma(12) (European Computer Manufacturers Association) oiw(14) (OSE Implementers Workshop) edifactboard(15) ewos(16) (European Workshop on Open Systems) osf(22) (Open Software Foundation) nordunet(23) nato(26) icao(27) (International Civil Aviation Organization). .. amazon(187) (the author has no idea if Amazon currently uses this) joint-iso-itu-t(2) This SNMP 1.3.6.1 preÔ¨Åx is spelled out in RFC 1155 via the syntax internet OBJECT IDENTIFIER ::= { iso org(3) dod(6) 1 } which means that the string internet has the type OBJECT IDENTIFIER and its actual value is the list to the right of ::=: 1.3.6.1. The use of iso here represents the OID preÔ¨Åx .1, conceptually different from just the level iso(1). The formal syntax here is ASN.1, as deÔ¨Åned by the ITU-T standard X.680. We will expand on this below in 26.6 ASN.1 Syntax and SNMP, though the presentation will mostly be informal; further details can be found at http://www.itu.int/en/ITU-T/asn1/Pages/introduction.aspx. After the above, RFC 1155 then deÔ¨Ånes the OID preÔ¨Åxes for management information, mgmt, and for private-vendor use, private, as mgmt OBJECT IDENTIFIER ::= { internet 2 } private OBJECT IDENTIFIER ::= { internet 4 } 638 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 Again,internet represents the newly deÔ¨Åned preÔ¨Åx 1.3.6.1. Most general-purpose SNMP OIDs begin with themgmt preÔ¨Åx. The private preÔ¨Åx is for OIDs representing vendor-speciÔ¨Åc information; for example, the preÔ¨Åx 1.3.6.1.4.1. 9has been delegated to Cisco Systems, Inc. RFC 1155 states that both the mgmt and private preÔ¨Åxes are delegated by the IAB to the IANA. Responsibility for assigning names that begin with a given OID preÔ¨Åx can easily be delegated. The Internet Activities Board, for example, is in charge of 1.3.6.1. (According to RFC 1155, this delegation by the Department of Defense to the IAB was never made ofÔ¨Åcially; the IAB just began using it.) 26.4 MIBs A MIB, or Management Information Base, is a set of deÔ¨Ånitions that associate OIDs with speciÔ¨Åc attributes, and, along the way, associates each numeric level of the OIDs with a symbolic name. For example, below is the set of so-called System deÔ¨Ånitions in the core RFC 1213 MIB known as MIB-2 ( 26.10 MIB2). Themib-2 andsystem preÔ¨Åxes are Ô¨Årst deÔ¨Åned as mib-2 OBJECT IDENTIFIER ::= { mgmt 1 } system OBJECT IDENTIFIER ::= { mib-2 1 } that is, 1.3.6.1.2.1 and 1.3.6.1.2.1.1 respectively. The system deÔ¨Ånitions, which represent actual attributes that can be retrieved, are as follows sysDescr { system 1 } sysObjectID { system 2 } sysUpTime { system 3 } sysContact { system 4 } sysName { system 5 } sysLocation { system 6 } sysServices { system 7 } Most of these attributes represent string values that need to be administratively deÔ¨Åned; we will return to these in 26.10 MIB-2. The MIB is notthe actual attributes themselves; the set of all xOID,valueypairs stored by an SNMP manager ‚Äì perhaps the result of a single set of queries, perhaps the result of a sequence of queries over time ‚Äì is sometimes known as the management database, or MDB, though that term is not as universal. Colloquially, a MIB can be either the abstract set of OID deÔ¨Ånitions or a particular ASCII Ô¨Åle that deÔ¨Ånes the set. The latter must be run through a MIB compiler to be used by other software; users of a MIB browser generally Ô¨Ånd the information much more useful if the appropriate MIB Ô¨Åle is loaded before making SNMP queries. Some RFCs can be read directly by the MIB compiler, which knows to edit out the non-MIB discussion. The Net-SNMP package contains SNMP agents for Linux and Macintosh computers (Microsoft has their own SNMP-agent software), and command-line tools for making SNMP queries; these latter tools are also available for Windows. The sysName value, for example, can be retrieved with the snmpget command snmpget -v1 -c public localhost 1.3.6.1.2.1.1.5.0 26.4 MIBs 639
An Introduction to Computer Networks, Release 2.0.11 The OID here corresponds to { system 5 0 }; the Ô¨Ånal 0 represents a SNMP convention that indicates this is a scalar value and is not part of a table. See 26.9 MIB Browsing for further examples. MIBs serve as both documentation and data. As documentation, they tell the implementer of a given SNMP agent what information is to be returned for each OID request. As data, they serve to translate numeric OIDs into human-readable data. For example, if the above snmpget command understands the appropriate MIB (egbecause the MIB Ô¨Åle is in the right place), we can type snmpget -v1 -c public localhost sysName.0 It is not uncommon for an installation to involve several hundred different (possibly overlapping) MIBs, each deÔ¨Åning a different portion of the OID tree. This is particularly true of the private subtree 1.3.6.1.4.1, for which one might have a separate MIB Ô¨Åle for each manufacturer and device. 26.5 SNMPv1 Data Types SNMP uses OIDs as the indexes for retrieval of attributes. The attributes themselves can have the following types, as of RFC 1155: Datatype Description INTEGER A 32-bit integer OCTET STRING Either a text string or a network address Counter A non-negative INTEGER that increases from 0, and can wrap around. Gauge An INTEGER that can rise and fall but that never wraps around TimeTicks An INTEGER used to measure time in 1/100ths of a second OBJECT IDENTIFIER An OID appearing as data IpAddress An IPv4 address, as an OCTET STRING of Ô¨Åxed length 4 NetworkAddress An IpAddress, in theory allowing for other kinds of addresses later Opaque Any other data, ultimately an OCTET STRING The type of the sysName attribute, for example, is OCTET STRING. When SNMP data values are returned by an agent, they are tagged with their type. Thus, the receiver can determine the type actually transmitted, and so can decode the data even if it does not have the applicable MIB installed. See 26.12 SNMP and ASN.1 Encoding for further details. When the INTEGER type is used to represent an enumerated type, the value of zero must not be used; an example is the status type up(1) ,down(2) ,testing(3). 26.6 ASN.1 Syntax and SNMP We have already seen the ASN.1 deÔ¨Ånition of some OBJECT IDENTIFIERs in 26.3 SNMP Naming and OIDs. The attribute corresponding to any single OID is always a scalar type, egone of the above. However, SNMP also uses some composite type deÔ¨Ånitions: 640 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 SEQUENCE { Ô¨Åeldname1 type1 ,Ô¨Åeldname2 type2, .. . }: This deÔ¨Ånes a record type. SEQUENCE OF type: This deÔ¨Ånes a homogeneous list ‚Äì an array or table ‚Äì of objects of type type. In most cases, type represents a SEQUENCE type, and so the list is a list of records, or table in the database sense. Note the importance of the keyword OF here to distinguish records from lists. The OBJECT-TYPE macro is heavily used in MIB deÔ¨Ånitions. It has the format objnameOBJECT-TYPE SYNTAX type ACCESS read-write orread-only orwrite-only ornot-accessible STATUS mandatory oroptional orobsolete DESCRIPTION descriptive string INDEX OID used for table indexing ::= OID assigned to objname Theobjname, sometimes called the OBJECT DESCRIPTOR, is intended to be globally unique. The value forSYNTAX must be a valid ASN.1 syntax speciÔ¨Åcation; see 26.10 MIB-2 for examples. The values forACCESS andSTATUS are, in each case, one of the literal strings shown above. The value for DESCRIPTION is optional. Many of the objects so deÔ¨Åned will represent tables or other higher-level structures; in this case the Object ID of the last line will represent an internal node of the OID tree rather than a leaf node, and the ACCESS will benot-accessible. It is not that the table is actually inaccessible, but rather that the attributes must be retrieved one by one rather than collectively. Here is a concrete simple example; other examples appear in the following section. The OID value of ifEntry here is 1.3.6.1.2.1.2.2.1. ifInOctets OBJECT-TYPE SYNTAX Counter ACCESS read-only STATUS mandatory DESCRIPTION "The total number of octets received on the interface, including framing characters. " ::= { ifEntry 10 } The data type deÔ¨Ånitions and the above ASN.1 syntax used for are collectively known as the structure of management information, or SMI. The SMI in effect deÔ¨Ånes all the types and structures needed by the various MIBs. For SNMPv1, the SMI is deÔ¨Åned in RFC 1155. 26.6 ASN.1 Syntax and SNMP 641
An Introduction to Computer Networks, Release 2.0.11 26.7 SNMP Tables We have seen in 26.4 MIBs how SNMP organizes the OID names for the system group, which consists of scalar values. Each agent attribute is assigned a permanent OID name; eg{ system 5 0 } is the full OID for thesysName attribute. OIDs for scalar (non-table) attributes always include a Ô¨Ånal ‚Äú.0‚Äù. Most agent data, however, comes in the form of tables, that is, lists of records. In SNMP, they are sometimes referred to as conceptual tables, as the SNMP agent offering the table is often not responsible for its storage or physical organization. In this sense, SNMP tables resemble database ‚Äúviews‚Äù. These tables can be accessed by linear search or by primary key lookup; the key can be a single attribute or a set of attributes and is known as the index. SNMP does not support secondary keys on tables, though it would in principle be possible for an agent to present the same data in two tables, each with a different index (a related example of this trick appears in 27.2.1.5 Matrix ). SNMP tables are almost always small enough that the agent keeps all the records in main memory; databasestyle concerns about disk-storage organization and indexing are not an issue. As in databases, an SNMP table index acts as a constraint, disallowing two distinct rows with identical index values. When an SNMP table mirrors a ‚Äúreal‚Äù table ( egthe IP forwarding table), problems sometimes occur when the SNMP index is too ‚Äúsmall‚Äù ‚Äì has too few attributes ‚Äì and so the SNMP table may not be able to accurately represent the original table. See 27.1.12.2 IP-Forward MIB for an example. In the world of databases, a key with too many attributes may fail to capture an important constraint; it may also be less efÔ¨Åcient. Neither is a concern with SNMP, with the exception of tables that support row creation, which we may take to be a special case. SNMP table indexes may thus sometimes include attributes that from a database perspective would likely be omitted from the key; again, see 27.1.12.2 IP-Forward MIB for examples. We will look here at three examples of SNMPv1 tables, each part of MIB-2. The goal right now is to present how the tabular structure is mapped onto the underlying OID-tree structure; we will return to the syntactic deÔ¨Ånition of tables in 26.10.2 Table deÔ¨Ånitions and the interfaces Group and to the semantics of each table in26.10 MIB-2. SNMPv2 has made some modest improvements to how tables are deÔ¨Åned. The Ô¨Årst example will be the SNMP interface table ,ifTable. For each network interface ( egloopback, Ethernet 0, Ethernet 1, Wireless 0, Point-to-Point 0, etc) a collection of attributes including the device name, MTU, bitrate, physical address, and the number of bytes and packets received and sent. The index here (as we shall see later) is an abstractly assigned interface number, known as ifIndex. Here is some sample data, notshowing all columns that the actual MIB-2 table includes, and not (yet) showing any OIDs. The header of the index column, ifIndex, is in italic; the value is a single integer. The eth0 interface has a higher number for packets than bytes (octets) because the 32-bit inOctets value has wrapped around. ifIndex name MTU bitrate inOctets inPackets 1 lo 16436 10,000,000 171 3 2 eth0 1500 100,000,000 37155014 1833455677 3 eth1 1500 100,000,000 0 0 4 ppp0 1420 0 2906687015 2821825 642 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 Routes Although the data here is mostly a mash-up of records from different sources, the 10.38.0.0 route below was a VPN route using the ppp0 tunnel above. Otherwise, workstation forwarding tables often have just two entries, for the local subnet and the default route. The SNMP routing table ,ipRouteTable, based on the IP forwarding table of 1.10 IP - Internet Protocol, has a column representing the destination network, a variety of status and information columns (egfor RouteAge), and a NextHop column; the index is the destination-network attribute. Again, here is some sample data with the index column again in italic. The index here ‚Äì an IP address ‚Äì is a compound object: a list of four integers. We return to this below. dest mask metric next_hop type 0.0.0.0 0.0.0.0 1 192.168.1.1 indirect(4) 10.0.0.0 255.255.255.0 0 0.0.0.0 direct(3) 10.38.0.0 255.255.0.0 1 192.168.4.1 indirect(4) (Thetype column indicates whether the destination is local or nonlocal; the host is on subnet 10.0.0.0/24 and so the middle entry involves local delivery. The use of indirect(4) anddirect(3) is an example of an SNMP enumerated type.) The indexing of the interfaces table is (usually) dense: the values for ifIndex are typically consecutive numbers. The routing table, on the other hand, has a sparse index: the index values are likely nowhere near one another. TheipRouteTable table is obsolete; among other issues, the CIDR preÔ¨Åx lengths are not part of the index. Seeip-forward mib for a history of replacements. The TCP Connections table tcpConnTable lists every TCP connection together with its connection state as in 17.8 TCP state diagram; one can obtain this on most Linux, Windows or Macintosh systems with the command netstat -a. The index in this case consists of the four attributes of the connectiondeÔ¨Åning socketpair: the local address, the local port, the remote address and the remote port. In this table, the only attribute not part of the index is an integer representing the connection state (again represented by an SNMP enumerated type). localAddr localPort remoteAddr remotePort state 10.0.0.3 31895 147.126.1.209 993 established(5) 10.0.0.3 40113 74.125.225.98 80 timeWait(11) 10.0.0.3 20459 10.38.2.42 22 established(5) SNMP has adopted conventions for how tabular data such as the above is to be encoded in the OID tree. The Ô¨Årst step in this strategy is to deÔ¨Åne, statically, an OID preÔ¨Åx for the subtree representing the table; in this section we will denote this by T(later, in 26.10.2 Table deÔ¨Ånitions and the interfaces Group, we will see thatToften represents two OID levels of the form Table.Entry, or T.E). Each attribute of the table (that is, each column) is then assigned a non-leaf OID by appending successive integers to the table preÔ¨Åx, starting at 1. For example, the root preÔ¨Åx for the interfaces table is T= 1.3.6.1.2.1.2.2.1, known as ifEntry. The columns shown in the fraction of the interfaces table above have the following OIDs: 26.7 SNMP Tables 643
An Introduction to Computer Networks, Release 2.0.11 OID table column { ifEntry 1 } orT.1 The interface number, ifIndex { ifEntry 2 } orT.2 The interface name or description { ifEntry 4 } orT.4 The interface MTU { ifEntry 5 } orT.5 The interface bitrate { ifEntry 10 } orT.10 The number of inbound octets (bytes) { ifEntry 11 } orT.11 The number of inbound unicast packets The second step is to establish a convention for converting the index attributes to a list of integers that can be used as an OID sufÔ¨Åx identifying a particular row. This index OID sufÔ¨Åx is then appended to the attribute OID preÔ¨Åx (identifying the column) to obtain the full OID (with no trailing .0 this time) that represents the name of the table data value. For the case of the interfaces table, the interface number is used as a single-level OID sufÔ¨Åx. The eth1 value forinOctets thus has OID name T.10.3: 10 is the number assigned to the inOctets column and 3 is theifIndex value for the eth1 row. Written in full, this is 1.3.6.1.2.1.2.2.1. 10.3. Note this numbering has the form T.col.row, the transposition of the more common programming-language row-Ô¨Årst convention T[row][col]. For the routing and TCP tables presented here, IPv4 addresses are written in dotted-decimal notation, eg 10.38.0.0, and then converted to four levels of OID sufÔ¨Åx, eg.10.38.0.0. (Note that 10.38.0.0 has the same textual representation as an IPv4 address and as a four-level OID sufÔ¨Åx (though a leading ‚Äò.‚Äô has been added here to the latter), but logically they are entirely different things). This OID-sufÔ¨Åx encoding may seem obvious, but this is deceptive, and in 27.1.13 TCP-MIB we will see a newer convention in which the IPv4 address 10.38.0.0 would be encoded as the OID sufÔ¨Åx. 1.4.10.38.0.0, where the 1denotes an IPv4 address (2 for IPv6) and the 4denotes the address length as an OCTET STRING. The present rule, for the .10.38.0.0 four-level encoding, comes from RFC 2578, ¬ß7.7; theIpAddress type (for IPv4 only) is deÔ¨Åned as IMPLICIT OCTET STRING (SIZE (4)) (inRFC 1155 ), of predetermined length. An IP address might have been encoded as a single OID level using the address as a single unsigned 32-bit integer, but this option was not chosen. In the routing table above, the nextHop column is assigned the number 7, so the nextHop for 10.38.0.0 thus has the OID T.7.10.38.0.0. The OID sufÔ¨Åx encoding used for indexing has nothing to do with the ASN.1 BER encoding used for data values, 26.12 SNMP and ASN.1 Encoding. For the TCP connections table, the two IP addresses involved each translate to four-level OID chunks, as in the routing table, and the two port numbers each translate to one-level chunks; the resultant OID sufÔ¨Åx for the state of the Ô¨Årst connection in the table above would be .10.0.0.3. 31895 .147.126.1.209. 993 where, for clarity, the port-number levels are shown in italic. The state column is assigned the identiÔ¨Åer 1, so this would all be appended to T.1 (it is common, but, as we see here, not universal, for column-number assignment to begin with the index columns). Here is a column-oriented diagram of the interfaces-table fragment from above. As we have been doing, the table preÔ¨Åx is denoted Tand nodes are labeled T.col.row. The topmost T and the Ô¨Årst row (with the T.col 644 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 OIDs) are internal nodes; these are drawn with the heavier, blue boxes. The lower four rows, with black boxes, are leaf nodes. T.1 ifIndexT.2 ifDescT.3 ifMTUT.4 ifSpeedT.10 ifInOctetsT.11 ifInUcastPktsT T.1.1 1 T.1.2 2 T.1.3 3 T.1.4 4T.2.1 lo0T.3.1 16436T.4.1 10,000,000T.10.1 171T.11.1 3 T.2.2 eth0T.3.2 1500T.4.2 100,000,000T.10.2 37155014T.11.2 18334556773 T.2.3 eth1T.3.3 1500T.4.3 100,000,000T.10.3 0T.11.3 0 T.2.4 ppp0T.3.4 1420T.4.4 0T.10.4 2906687015T.11.4 2821825 The diagram above emphasizes the arrangement into columns. The actual OID tree structure, however, is as in the diagram below; all the leaf nodes in one column above are actually sibling nodes. 26.7 SNMP Tables 645
An Introduction to Computer Networks, Release 2.0.11 T.1 ifIndexT.2 ifDescT.3 ifMTUT T.1.1 1T.1.2 2T.1.3 3T.2.1 lo0T.2.2 eth0T.2.3 eth1T.3.1 116436T.3.2 1500T.3.3 1500... In all three tables here, the index columns are part of the table data. This is unnecessary; all the index columns are of necessity encoded in the full OID of any table entry. In SNMPv2 it is required (though not necessarily enforced) to in effect omit the index columns from the table as presented by the agent (though they are still declared); see 27.1.4 SNMPv2 Indexes. We also note that, in all three tables here (and in most SNMP tables that serve purely as sources of information), new rows cannot be added via the SNMP manager. Some SNMP tables, however, do support row creation; see 27.2 Table Row Creation. 26.8 SNMP Operations As mentioned earlier, the fundamental read operation is Get(); the protocol message itself is known as GetRequest, and also contains the authentication information (that is, the community string for SNMPv1 and SNMPv2c). A request-id is included so multiple outstanding GetRequest s aren‚Äôt mixed up. TheGetRequest message can contain a listof OIDs (actually encoded as a list of xOID,nullypairs called aVarBind list). A single-OID get simply entails a VarBind list of length 1. The agent receiving the GetRequest validates the authentication and then, if successful, looks up each OID in the received list to Ô¨Ånd the corresponding value. The Ô¨Ålled-in VarBind list of allxOID,valueypairs is returned. If any value cannot be found, egbecause the OID represented an interior node of the OID tree instead of an actual value, or because the OID lay outside the portion of the OID tree that the request was authorized to access, then in SNMPv1 novalues are returned. Only if every OID in the list corresponds to a valid value is a list of all the xOID,valueypairs returned. SNMPv2 later relaxed this rule to allow the return of whatever individual requests were successful. If the exact OID is not known ‚Äì which is often the case for tabular data ‚Äì the GetNext() operation can be used. It works like Get(), except that for each OID oidin the request list the agent Ô¨Ånds the next leaf 646 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 OID strictly following oidin lexicographical order, which we will call oid1. The agent then includes the pair xoid1,valueyin its response, where value is the value corresponding to oid1. Note that in the GetNext() case oid1is not previously known to the manager. Note also that SNMP always returns, with each value, the corresponding OID, which for tabular data encodes the full index for the value. TheGetNext() operation allows a manager to walk through any subtree of the OID tree. If the root of the subtree is preÔ¨Åx T, then the Ô¨Årst call is to GetNext( T). This returns xoid1,value 1y. The next call to GetNext has parameter oid 1; the agent returns xoid2,value 2y. The manager continues with the series of GetNext calls until, Ô¨Ånally, the subtree is exhausted and the agent returns either an error or else (more likely) anxoidN,value Nyfor which oid Nis no longer an extension of the original preÔ¨Åx p. As an example, let us start with the preÔ¨Åx 1.3.6.1.2.1.1, the start of the system group. The Ô¨Årst call to GetNext() will return the pair x1.3.6.1.2.1.1. 1.0, sysDescr_value y The OID 1.3.6.1.2.1.1. 1.0is the Ô¨Årst leaf node below the interior node 1.3.6.1.2.1.1. The second call to GetNext will take OID 1.3.6.1.2.1.1. 1.0as parameter and GetNext() will return x1.3.6.1.2.1.1. 2.0, sysObjectID_value y Here, the OID returned is the next leaf node strictly following 1.3.6.1.2.1.1. 1.0. The process will continue on through {system 3 0}, {system 4 0}, etc, until an OID is found that is notpart of the system group. As we have seen, this will likely be the Ô¨Årst entry of the interfaces group, ifNumber.0, with OID 1.3.6.1.2.1. 2.1.0. The action of GetNext() is particularly useful when retrieving a table, such as the interfaces table presented in the preceding section. In that example the index values are consecutive integers, and so an SNMP manager could likely guess them. However, guessing the index values for the other two tables ‚Äì the IP forwarding table and the TCP connections table ‚Äì would be well nigh impossible. And without the index values, we do not have a complete OID. Consider again the partial-column version of the interfaces table as diagrammed in this format: 26.8 SNMP Operations 647
An Introduction to Computer Networks, Release 2.0.11 T.1 ifIndexT.2 ifDescT.3 ifMTUT.4 ifSpeedT.10 ifInOctetsT.11 ifInUcastPktsT T.1.1 1 T.1.2 2 T.1.3 3 T.1.4 4T.2.1 lo0T.3.1 16436T.4.1 10,000,000T.10.1 171T.11.1 3 T.2.2 eth0T.3.2 1500T.4.2 100,000,000T.10.2 37155014T.11.2 18334556773 T.2.3 eth1T.3.3 1500T.4.3 100,000,000T.10.3 0T.11.3 0 T.2.4 ppp0T.3.4 1420T.4.4 0T.10.4 2906687015T.11.4 2821825 Let us initially call GetNext( T). The next leaf node is the upper left black box, with OID T.1.1. The call to GetNext( T) returns the pair xT.1.1,1y. We now continue with a call to GetNext( T.1.1); this returns xT.1.2,2y and represents the black box immediately below the Ô¨Årst one. The next two calls to GetNext() return, successively,xT.1.3,3yandxT.1.4,4y. Now we call GetNext( T.1.4). The next leaf node following is the Ô¨Årst leaf node in the second column, T.2.1; the value returned is xT.2.1,‚Äùlo‚Äùy. The next three calls to GetNext, each time supplied with parameter the value returned by the previous GetNext(), return the next three values making up the second, ifDesc column of the table. At the bottom of the second column, the call to GetNext( T.2.4) returns the xOID,valueypair that is the Ô¨Årst leaf node of the third column, xT.3.1,16436y. At the bottom of the third column, GetNext() jumps to the top of the fourth column, and so on. In this manner GetNext() iterates through the entire table, one column at a time. When we Ô¨Ånally get to the last leaf node of the table, shown here as the lower-right T.11.4 (though the actual ifTable has additional columns). The call to GetNext( T.11.4) returns something outside the table. It will 648 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 either return an error ‚Äì in the event that there is no next OID that the manager is authorized to receive ‚Äì or the next leaf node up and to the right of T. (In the normal MIB-2 collection, this is would be the Ô¨Årst entry of theatTable table.) Either way, the manager receiving the data can tell that its request for GetNext( T.11.4) has gone past the end of table T, and so Thas been completely traversed. Here is a diagram of the above sequence of GetNext() operations involved in traversing our partial interfaces table; it shows the GetNext( T.11.4) at the table‚Äôs lower right returning the beyond-the-table xOID,valueypairxA,avaluey: ‚ü®T.1.1, 1‚ü©GetNext(T) ‚ü®T.1.2, 2‚ü©GetNext(T.1.1) ‚ü®T.1.4, 4‚ü©... ‚ü®T.2.1, lo‚ü©GetNext(T.1.4) ‚ü®T.2.2, eth0‚ü© ...GetNext(T.2.1) ‚ü®A, avalue‚ü©GetNext(T.11.4) A is the next OID above and to the right of T, and avalue is its value. Diagram of traversing the (partial) interfaces table with GetNext() Some manager utilities performing this kind of table retrieval ‚Äì often called a ‚Äúwalk‚Äù ‚Äì will present the data in the order retrieved, one column at a time, and some will (perhaps as an option) format the data visually 26.8 SNMP Operations 649
An Introduction to Computer Networks, Release 2.0.11 to look more like a table. See Net-SNMP‚Äôs snmpwalk andsnmptable in26.9 MIB Browsing. 26.8.1 Multi-attribute Get() A singleGet()/GetNext() request can in fact include a list of attributes to be retrieved. This provides an efÔ¨Åcient way to request entire rows of a table. A manager in general does not know all the index values of a table, but likely does know all the column OIDs (as in the blue full-length row above). The manager can speed up the table-retrieval process by asking for entire rows at a time. The Ô¨Årst such GetNext() might be GetNext( T.1,T.2,T.3,T.4,T.10,T.11) These are the OIDs of the blue full-length row; these are all non-leaf OIDs. The result is the list of pairs xT.1.1,1y,xT.2.1,‚Äùlo‚Äùy,xT.3.1,16436y,xT.4.1,10000000y,xT.10.1,171y,xT.11.1,3y That is, the table‚Äôs entire Ô¨Årst row is returned (where we are assuming again that the only columns are 1, 2, 3, 4, 10 and 11). The next call to GetNext() would use the OIDs returned above: GetNext( T.1.1, T.2.1, T.3.1, T.4.1, T.10.1, T.11.1) This would then return the table‚Äôs entire second row, and so on. When we got to the very last row, the call GetNext( T.1.4, T.2.4, T.3.4, T.4.4, T.10.4, T.11.4) would return (assuming now that the columns shown are the only columns that exist) xT.2.1,‚Äùlo‚Äùy,xT.3.1,16436y,xT.4.1,10000000y,xT.10.1,171y,xT.11.1,3y,xA,avaluey where Ais the next leaf OID above and to the right of T(assuming no error). At this point the manager knows, as in the single-attribute-get case, that the entire table has been retrieved. It does not matter whether the manager uses this multi-attribute form of GetNext() to return all the columns of the relevant table, or only the subset of columns in which the manager is interested. We will see in 27.1.3 SNMPv2 GetBulk() that SNMPv2 introduced an operation GetBulk() that can return multiple rows of multiple columns in a single operation. Tables are not necessarily static. New dynamic interfaces may be added, and new routes and TCP connections are added all the time. A manager reading a table using the single-attribute form of GetNext() may Ô¨Ånd that only the latter part of a new row is retrieved, or, alternatively, that the Ô¨Årst part of a deleted row is retrieved. The multi-attribute GetNext() offers a little more protection from this. 26.8.2 Set() SNMP also allows a Set() operation. Not all attributes are writable, of course, and a manager must have write authority for those that arewritable. And there is no SetNext(); the exact OID of each value must be supplied. In the system group, the sysName attribute is writable. It has OID 1.3.6.1.2.1.1.5.0; to update this we would invoke Set(1.3.6.1.2.1.1.5.0, ‚Äúnewsysname‚Äù) 650 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 LikeGet() ,Set() can be invoked on multiple attributes. Suppose table Thas three columns T.1,T.2 andT.3, and two rows: T.1T.2 T.3 10 eth0 3.14159 11 eth1 2.71828 We can then change the second eth1 row tox11,ppp1,0.577216 ywith Set( ( T.2.11,ppp1), ( T.3.11,0.577216) ) We cannot change the indexing this way, but see 27.2 Table Row Creation. The semantics for multi-attribute Set() explicitly require ‚Äì and still require ‚Äì that either all the assignments succeed, or all fail; that is, multi-attribute Set() operations are atomic. (The usual reason for a failure is that the manager issuing the Set() lacked write permission for some item.) SNMPv1 had the same all-ornothing rule for multi-attribute Get(), but that requirement was relaxed in SNMPv2. The all-or-nothing Set() semantics mean that certain updates cannot end up completed only half-way. The multi-attribute Set() semantics do notmean, however, that near-simultaneous assignments by different managers are serialized. Suppose, for example, two separate managers both attempt to update attributes speciÔ¨Åed by OIDs XandY, one to 10 and 100 and the other to 20 and 200. If the updates are scheduled at about the same time, Set(( X,10), ( Y,100)) Set(( X,20), ( Y,200)) then it is possible for the Set()s to be performed in the order ( X,20), ( X,10), ( Y,100), ( Y,200), leaving X= 10 and Y= 200. See 27.1.5 TestAndIncr for a workaround. 26.9 MIB Browsing Tools for individual SNMP reading are widely available, and are very helpful for viewing what is going on. We have already mentioned the Net-SNMP package. The command snmpget issues a single (perhaps multi-attribute) Get() request; the very similar snmpgetnext issues a GetNext request. The command snmpwalk takes an OID representing the root of a subtree, and returns everything in that subtree. Here is an example that assumes the snmp agent on localhost is conÔ¨Ågured to use community string ‚Äútengwar‚Äù (see 26.11 SNMPv1 communities and security ): snmpwalk -v 1 -c tengwar localhost 1.3.6.1.2.1.2.2 The OID here is that of ifTable. Multiple OIDs are permitted for snmpget andsnmpgetnext but not forsnmpwalk. If the appropriate MIB Ô¨Åles are loaded, the above command can also be entered as snmpwalk -v 1 -c tengwar localhost ifTable 26.9 MIB Browsing 651
An Introduction to Computer Networks, Release 2.0.11 This entails putting the RFC1213-MIB Ô¨Åle into a special directory ( eg$HOME/.snmp/mibs), and then either adding a line like mibs +RFC1213-MIB to the snmp.conf Ô¨Åle, or by including the MIB name on the command line: snmpwalk -v 1 -c tengwar -m RFC1213-MIB localhost ifTable Note thatRFC1213-MIB is the identiÔ¨Åer assigned to the MIB Ô¨Åle in its Ô¨Årst line, as below, and notnecessarily the name of the Ô¨Åle containing the MIB. RFC1213-MIB DEFINITIONS ::= BEGIN If the appropriate MIB Ô¨Åle is loaded, the OIDs may be displayed symbolically rather than numerically, but the data presentation will not change: iso.3.6.1.2.1.1.5.0 = STRING: "valhal" ;; without MIB RFC1213-MIB::sysName.0 = STRING: "valhal" ;;withMIB (Actually, if the data is of type Object ID, then without the MIB the OID will be displayed numerically, and with the appropriate MIB all or part of it may be displayed symbolically.) Finally, the Net-SNMP manager package includes snmptable, which is like snmpwalk except that the data is displayed as a table rather than one column at a time. For the snmptable command, the appropriate MIB Ô¨Åle must be installed. Net-SNMP comes with a mib browser with a graphical interface, tkmib. The personal edition of the iReasoning MIB browser is not open-source, but it isfree, and works well on Windows, Macs and Linux; SNMPv3 is not supported. The license does prohibit the publication of benchmark tests without consent. 26.10 MIB-2 We can now turn to the MIB that represents the core of SNMPv1 data, known as MIB-2, and deÔ¨Åned in RFC 1213. The ‚Äú2‚Äù here ‚Äì often represented with a Roman numeral: MIB-II ‚Äì represents the second iteration of the deÔ¨Ånition, but it is still part of SNMPv 1. The predecessor MIB-1 was Ô¨Årst documented in RFC 1066, 1988. As we saw in 26.4 MIBs, the MIB-2 OID preÔ¨Åx is 1.3.6.1.2.1. MIB-2 has since been extended. We look at a few extensions below in 27.1.7 SNMPv2 MIB Changes, 27.1.9 IF-MIB and ifXTable ,27.1.13 TCP-MIB and27.1.12.2 IP-Forward MIB. In general, serious network management should make use of these newer versions. However, MIB-2 is still an excellent place to get started, even if partly obsolete. The original MIB-2 is divided into ten groups, not all of which are in current use: 
- system(1): above 
- interfaces(2): above, in brief 
- at(3): the ARP cache, 10.2 Address Resolution Protocol: ARP 652 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 
- ip(4): including the IP forwarding table used as an example above 
- icmp(5): information about ICMP, 10.4 Internet Control Message Protocol 
- tcp(6): including the TCP connections table used as an example above 
- udp(7): information about UDP activity 
- egp(8): information about the now-obsolete EGP protocol, replaced by BGP 
- transmission(10): never used 
- snmp(11): about the SNMP agent itself Originally group 9 was for CMOT, above, but this entry is commented out in RFC 1213 (though it does not appear at all in MIB-1). Theatgroup is ofÔ¨Åcially deprecated; the same ARP-cache information is available within the ipgroup. The EGP protocol has been entirely replaced by BGP, and management of BGP routers is highly specialized; RFC 4273 contains a BGP MIB but private MIBs are almost universally used here as well. The transmission group was included anticipating the day when ‚ÄúdeÔ¨Ånitions for managed transmission media are deÔ¨Åned‚Äù. This day has not come to pass. The Net-SNMP agent implementation returns nothing for the egp and transmission groups. 26.10.1 The system Group We have already looked at this in 26.4 MIBs and will not say much more, except to note that several of the entries in this group must be manually conÔ¨Ågured. This can be done either through editing of the conÔ¨Åguration Ô¨Åle, egsnmpd.conf, or else (in the case of read-write attributes) by using SNMP itself. ThesysObjectID value represents a vendor-speciÔ¨Åc OID for this particular system‚Äôs SNMP agent. For example, the Net-SNMP package installed on my system returns 1.3.6.1.4.1.8072.3.2.10. The 1.3.6.1.4.1 is the root of the private, vendor-speciÔ¨Åc, OID tree, and 8072 has been assigned to the Net-SNMP project. This often represents, in practice, the way an SNMP manager can Ô¨Ågure out the vendor of an agent, and thus determine what vendor-speciÔ¨Åc information to query. One can use the 8072 discovered here to ask for the subtree 1.3.6.1.4.1.8072, and Ô¨Ånd all sorts of Net-SNMP-speciÔ¨Åc information. Similarly, on one particular Windows XP installation with Microsoft Network Monitoring installed, the sysObjectID value is 1.3.6.1.4.1. 311.1.1.3.1.1; 311 is the OID level assigned to Microsoft in the private subtree 1.3.6.1.4.1. The system group has been expanded in SNMPv2 with an Object Resource table, sysORTable; see 27.1.7 SNMPv2 MIB Changes. 26.10.2 Table deÔ¨Ånitions and the interfaces Group The interfaces group consists of a single INTEGER value ifNumber representing the number of current network interfaces (including ‚Äúdown‚Äù interfaces), and the interfaces table partially introduced earlier. We take the opportunity here to outline exactly how MIB table deÔ¨Ånitions ‚Äì and enumerated types ‚Äì are structured using ASN.1. 26.10 MIB-2 653
An Introduction to Computer Networks, Release 2.0.11 Some of the interfaces-group deÔ¨Ånitions have later been updated. See, for example, RFC 2863, which also redeÔ¨Ånes the group to use the additional features of the SNMPv2 SMI. We will return to an extension of the interfaces group in 27.1.9 IF-MIB and ifXTable. MIB table deÔ¨Ånitions almost always involve a two-level process: an OID is deÔ¨Åned for the table, and then a second OID is deÔ¨Åned for a table entry, that is, for one row of the table. This second OID is usually generated from the Ô¨Årst by appending ‚Äú.1‚Äù, and it is this second OID that represents the table preÔ¨Åx in the sense of 26.7 SNMP Tables, denoted there by T. The actual ASN.1, slightly annotated, is as follows: ifTable OBJECT-TYPE SYNTAX SEQUENCE OF IfEntry -- note UPPER-CASE-I IfEntry ACCESS not-accessible STATUS mandatory DESCRIPTION "A list of interface entries. The number of entries isgiven by the value of ifNumber. " ::= { interfaces 2 } -- that is, 1.3.6.1.2.1.2.2 ifEntry OBJECT-TYPE -- note lower-case-i ifEntry SYNTAX IfEntry -- note UPPER-CASE-I IfEntry ACCESS not-accessible STATUS mandatory DESCRIPTION "An interface entry containing objects at the subnetwork layer andbelow fora particular interface. " INDEX { ifIndex } ::= { ifTable 1 } -- that is, 1.3.6.1.2.1.2.2.1 Both these entries are not-accessible as they do not represent leaf nodes. The second declaration above is for the lower-case-i ifEntry; the next deÔ¨Ånition in RFC 1213 is for the UPPER-CASE-I version. The latter represents the complete list of all columns of an ifEntry/IfEntry object, together with their types from (in most cases) 26.5 SNMPv1 Data Types. ThePhysAddress type is deÔ¨Åned in RFC 1213 as a synonym for OCTET STRING. DeÔ¨Ånitions for the OID associated with each column comes later. It isifEntry that represents an actual row, and thus includes an INDEX entry to specify the attribute or attributes that make up the primary key for that row. We now deÔ¨Åne IfEntry: IfEntry ::= SEQUENCE { ifIndex INTEGER, ifDescr DisplayString, ifType INTEGER, ifMtu INTEGER, ifSpeed Gauge, ifPhysAddress PhysAddress, ifAdminStatus INTEGER, ifOperStatus INTEGER, ifLastChange TimeTicks, ifInOctets Counter, (continues on next page) 654 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) ifInUcastPkts Counter, ifInNUcastPkts Counter, ifInDiscards Counter, ifInErrors Counter, ifInUnknownProtos Counter, ifOutOctets Counter, ifOutUcastPkts Counter, ifOutNUcastPkts Counter, ifOutDiscards Counter, ifOutErrors Counter, ifOutQLen Gauge, ifSpecific OBJECT IDENTIFIER } Attributes There can be a certain eye-glazing tedium in some of SNMP‚Äôs lengthy attribute lists, such as the one above. This can be replaced by panic in a hurry, though, when a problem has arisen and the proper attribute to diagnose it doesn‚Äôt seem to be included anywhere. It would have been possible to plug in the above deÔ¨Ånition of IfEntry into the SYNTAX speciÔ¨Åcation of the previous ifEntry, but that is cumbersome. The IfEntry deÔ¨Ånition is not a stand-alone OBJECT-TYPE and does not have its own OID. If all we wanted to do was to implement the interfaces table using the shortest possible OIDs, we would not have created separate ifTable and ifEntry OIDs. This would mean, however, that we could not use theOBJECT-TYPE macro to deÔ¨Åne ifTable, which would have been less consistent (especially as the ASN.1 syntax also determines the packet encoding, as in 26.12 SNMP and ASN.1 Encoding ). Essentially every table preÔ¨Åx in SNMP is deÔ¨Åned using two additional OID levels, as here, rather than one. We now turn to the 22 speciÔ¨Åc interface attributes. Here is the deÔ¨Ånition for the Ô¨Årst, ifIndex; it deÔ¨Ånes column 1 of the interfaces table to be, in effect, ifEntry.1. As we are now talking about a leaf node, once the OID sufÔ¨Åx is appended to represent the index, the ACCESS is no longer not-accessible. ifIndex OBJECT-TYPE SYNTAX INTEGER ACCESS read-only STATUS mandatory DESCRIPTION "A unique value for each interface. Its value ranges between 1 andthe value of ifNumber. The value foreach interface must remain constant at least from one reinitialization of the entity 's network management system to the next reinitialization. " ::= { ifEntry 1 } TheDESCRIPTION clearly indicates that the values for ifIndex, used to specify interfaces, are to be consecutive integers. Compliance with this rule has been an early casualty, for various reasons, and is formally withdrawn in RFC 2863. Some vendors simply number physical interfaces non-consecutively. In 26.10 MIB-2 655
An Introduction to Computer Networks, Release 2.0.11 other cases, there is some underlying issue with consecutive numbering. For example, one of the author‚Äôs systems running Net-SNMP returns ifNumber = 3, and then the following table values: ifIndex ifName 1 lo 2 eth0 549 ppp0 It turns out that ppp0 is a virtual interface corresponding to a VPN tunnel, 5.1 Virtual Private Networks, and the underlying tunnel regularly fails and is then automatically re-instantiated. Each time it does so, the ifIndex is incremented by 1. The rule that interfaces be numbered consecutively was formally deprecated in RFC 2863, an SNMPv2 update of the interface group. This, in turn, makes the ifNumber value rather less useful than it might be; most SNMP tables are not associated with a count attribute and seem to do just Ô¨Åne. Most SNMP data values correspond straightforwardly with attributes deÔ¨Åned by the hardware and the underlying operating system. The ifIndex value does not, at least not necessarily. Generally the agent must maintain at least some state to keep the ifIndex value consistent. In some cases, the ifIndex value may be taken from the relative position of the interface in some internal operating-system table. This is not, however, universally the case, as with the ifIndex value of 549 for ppp0 in the table above. TheifIndex value is widely used throughout SNMP, and is often referenced in other tables. SNMPv2 even deÔ¨Ånes a special type (a TEXTUAL CONVENTION) for it, named InterfaceIndex (27.1.1 SNMPv2 SMI and Data Types ). As we mentioned earlier, there is no reason to include ifIndex as an actual column in the table; the value ofifIndex can always be calculated from the OID of any component of the row. The SNMPv2 approach here ‚Äì basically to deÔ¨Åne ifIndex asnot-accessible ‚Äì is described below in 27.1.4 SNMPv2 Indexes. TheifDescr is a textual description of the interface; it is usually the device name associated with the interface. RFC 1213 states ‚Äúthis string should include the name of the manufacturer, the product name and the version of the hardware interface‚Äù, but this is inconsistent with Linux device-naming conventions. The current rule is that it is merely unique. TheifType attribute is our Ô¨Årst example of how ASN.1 handles enumerated types. The value is a small integer and the hardware type associated with each integer is spelled out as follows. The majority of the networking technologies in this 1991 list have pretty much vanished from the face of the earth. ifType OBJECT-TYPE SYNTAX INTEGER { other(1), -- none of the following regular1822(2), hdh1822(3), ddn-x25(4), rfc877-x25(5), ethernet-csmacd(6), iso88023-csmacd(7), iso88024-tokenBus(8), iso88025-tokenRing(9), (continues on next page) 656 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) iso88026-man(10), starLan(11), proteon-10Mbit(12), proteon-80Mbit(13), hyperchannel(14), fddi(15), lapb(16), sdlc(17), ds1(18), -- T-1 e1(19), -- European equiv. of T-1 basicISDN(20), primaryISDN(21), -- proprietary serial propPointToPointSerial(22), ppp(23), softwareLoopback(24), eon(25), -- CLNP over IP [11] ethernet-3Mbit(26), nsip(27), -- XNS over IP slip(28), -- generic SLIP ultra(29), -- ULTRA technologies ds3(30), -- T-3 sip(31), -- SMDS frame-relay(32) } ACCESS read-only STATUS mandatory DESCRIPTION "The type of interface, distinguished according to the physical/link protocol(s) immediately 'below' the network layer inthe protocol stack. " ::= { ifEntry 3 } A more serious problem is that over two hundred new technologies are unlisted here. To address this, the values forifType have been placed under control of the IANA, as deÔ¨Åned in IANAifType; see RFC 2863 and https://www.iana.org/assignments/ianaiftype-mib/ianaiftype-mib. The IANA can then add new types without formally updating any RFC. For the meaning of ifMtu, the interface MTU, see 9.4 Fragmentation. The 32-bit ifSpeed value will be unusable once speeds exceed 2 Gbps; RFC 2863 deÔ¨Ånes anifHighSpeed object with speed measured in units of Mbps. This entry is part of the ifXTable table, 27.1.9 IF-MIB and ifXTable .RFC 2863 also clariÔ¨Åes that, for virtual interfaces that do not really have a bandwidth, the value to be reported is zero (though in the example earlier the loopback interface lowas reported to have a bandwidth of 10 Mbps). TheifPhysAddress is, on Ethernets, the Ethernet address of the interface. TheifAdminStatus andifOperStatus attributes are enumerated types: up(1), down(2), testing(3). If theifAdminStatus is up(1), and the ifOperStatus disagrees, then there is a likely hardware malfunction. The ifLastChange attribute reÔ¨Çects the last time, in TimeTicks, there was a change in ifOperStatus. The next eleven entries count bytes, packets and errors. The ifInOctets andifOutOctets count bytes received and sent (including framing bytes, 6.1.5 Framing, if applicable). The problem with these 26.10 MIB-2 657
An Introduction to Computer Networks, Release 2.0.11 is that they may wrap around too fast: in 34 seconds at 1 Gbps. The MIB-2 values are still used, but are generally supplemented with 64-bit counters deÔ¨Åned in ifXTable ,27.1.9 IF-MIB and ifXTable. The packet counters are for unicast packets, non-unicast packets, discarded packets, errors, and, for inbound packets only, packets with unknown protocols. The count of non-unicast packets is separated in ifXTable into separate counts of broadcast and multicast packets. The interface queue length is available in ifOutQLen. It takes a considerable amount of trafÔ¨Åc to make this anything other than 0. RFC 1213 says nothing about the timescale for averaging the queue length. The last member of the classic MIB-2 interfaces group is ifSpecific, which has type ObjectID. It was to return an OID that may be queried for additional ifType-speciÔ¨Åc information about the interface. It was formally deprecated in RFC 2233. 26.10.3 The ip Group The ip group contains several scalar attributes and three tables. The Ô¨Årst two attributes are writable: 
- ipForwarding: a Boolean attribute to enable or disable forwarding 
- ipDefaultTTL: the default TTL in outgoing packets ( 9.1 The IPv4 Header ) The next scalar attributes are as follows. Here and later, we summarize these informally. 
- ipInReceives: the number of received IP packets 
- ipInHdrErrors: the number of apparent IP packets received with header errors 
- ipInAddrErrors: the number of IP packets received with bad destination addresses 
- ipForwDatagrams: the number of forwarded IP packets 
- ipInUnknownProtos: the number of received IP packets with unknown higher-level protocol 
- ipInDiscards: the number of received IP packets that had no errors, but which were still discarded due to resource limits 
- ipInDelivers: the number of received IP packets delivered locally 
- ipOutRequests: the number of IP packets originated by this node for delivery elsewhere 
- ipOutDiscards: the number of outbound IP packets discarded due to resource limits 
- ipOutNoRoutes: the number of outbound IP packets for which there was no route in the IP forwarding table 
- ipReasmTimeout: the value, in seconds, of the IP fragment-reassembly timer 
- ipReasmReqds: the number of IP proper fragments received that needed reassembly at this node (forwarded fragments are reassembled at their destination) 
- ipReasmOKs: the number of fragmented IP packets successfully reassembled 
- ipReasmFails: the number of fragmented IP packets notsuccessfully reassembled 658 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 
- ipFragFails: primarily, packets that needed fragmentation but their Dont Fragment bit ( 9.4 Fragmentation ) was set 
- ipFragCreates: the number of IP fragments created by this node 
- ipRoutingDiscards: the number of packets that should have been forwarded, but were discarded due to resource limits The Ô¨Årst table is the ipAddrTable, a list of all IP addresses assigned to this node together with interfaces and netmasks. The second table is the ipRouteTable, that is, the forwarding table. We looked brieÔ¨Çy at this in 26.7 SNMP Tables. Note that the index consists solely of the destination IP address ipRouteDest, which is not sufÔ¨Åcient for CIDR-based forwarding in which the forwarding-table key is the xdest,netmasky pair. TrafÔ¨Åc to 10.38.0.0/16 might be routed differently than trafÔ¨Åc to 10.38.0.0/24! This table contains four attributes ipRouteMetric1 throughipRouteMetric4 .RFC 1213 states that unused metrics should be set to -1, but many agents simply omit such columns entirely. Agents may similarly omit ipRouteAge. The third table is ipNetToMediaTable, which is the ARP-cache table and which replaces a similar nowdeprecated table in the at group. The ipNetToMedia table adds one column, ipNetToMediaType, not present in the at-group table; this column indicates whether an physical-to-IP-address mapping is invalid(2), dynamic(3) or static(4). Most ARP entries are dynamic. For an updated version of the ip group, see 27.1.12.1 IP-MIB and27.1.12.2 IP-Forward MIB. 26.10.4 The icmp Group The icmp group consists of 26 counters for various ICMP events. ICMP Echo Request (ping request) packets have their own counter. 26.10.5 The tcp Group The tcp group includes the following scalars: 
- tcpRtoAlgorithm: the method for calculating the retransmission timeout, normally vanj(4) for the Jacobson-Karels algorithm in 18.12 TCP Timeout and Retransmission. - tcpRtoMin: the minimum TCP retransmission timeout, in milliseconds 
- tcpRtoMax: the maximum TCP retransmission timeout, again in ms 
- tcpMaxConn: if the system imposes a static cap on the number of allowed TCP connections, that is returned. Most systems have no static cap, though, and return -1. 
- tcpActiveOpens: the number of TCP connections opened from this node; speciÔ¨Åcally, the number of TCP state transitions CLOSED √ëSYN_SENT 
- tcpPassiveOpens: literally, the number of TCP state transitions LISTEN √ëSYN_RECD 
- tcpAttemptFails: the number of TCP connections that went to CLOSED without ever being ESTABLISHED 26.10 MIB-2 659
An Introduction to Computer Networks, Release 2.0.11 
- tcpEstabResets: the number of TCP connections that went from ESTABLISHED or CLOSE_WAIT directly to CLOSED, via RST packets 
- tcpCurrEstab: the number of TCP connections currently in either state ESTABLISHED or state CLOSE_WAIT 
- tcpInSegs: the count of received TCP packets, including errors and duplicates (though duplicate reception is relatively rare) 
- tcpOutSegs: the count of sent TCP packets, notincluding retransmissions 
- tcpRetransSegs: the count of TCP packets with at least one retransmitted byte; the packet may also contain new data 
- tcpInErrs: the number of TCP packets received with errors, including checksum errors 
- tcpOutRsts: the number of RST packets sent Perhaps surprisingly, there is no counter provided for total number of TCP bytes sent or received. There is also no entry for the congestion-management strategy, egReno v NewReno v TCP Cubic ( 22 Newer TCP Implementations ). The tcp group also includes the tcpConnTable table, which lists, for each connection (identiÔ¨Åed by xlocalAddress,localPort,remoteAddress,remotePort y) the connection state. We looked at this earlier in 26.7 SNMP Tables. The table is noteworthy in that four of its Ô¨Åve columns are part of the INDEX. A consequence is that to extract all the information from the table, a manager need only retrieve the tcpConnState column: the other four attributes will all be encoded in the OID index that is returned with eachtcpConnState value. We will look at the newer SNMPv2 replacement in 27.1.13 TCP-MIB. 26.10.6 The udp Group The udp group contains the following scalars: 
- udpInDatagrams: a count of the number of UDP packets received and deliverable to a socket 
- udpNoPorts: a count of UDP packets received but undeliverable because the port was not open 
- udpInErrors: a count of the number of UDP packets undeliverable due to any other errors 
- udpOutDatagrams: a count of the number of UDP packets sent The udp group also contains the table udpTable. This table lists those UDP ports that the node has open. The table rows have the form xlocalAddr,localPort y, with both columns included in the index. If a UDP socket accepts connections from anywhere, the localAddr will appear as 0.0.0.0. 26.10.7 The snmp Group The SNMP group consists of 28 SNMP status and error attributes, numbered from 1 to 30 with 7 and 23 not used. 660 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 26.11 SNMPv1 communities and security An SNMPv1 manager authenticates itself to an agent by providing a string known as a community string that is a combination of both userid and password. That is, the single string identiÔ¨Åes the manager (or manager group) and at the same time authenticates the manager. The community string identiÔ¨Åes a manager as a member of a designated ‚Äúcommunity‚Äù, in the conventional use of the word, of managers. Of course, at many sites there will be only one manager that interacts with any given agent. Community strings are sent unencrypted, and so are vulnerable to snifÔ¨Ång. Even snifÔ¨Ång is not always necessary; far and away the most popular value ‚Äì the default for many agents ‚Äì is the string ‚Äúpublic‚Äù. While the community string can be made obscure, and changed frequently, these are not enough; the only appropriate security practice today is to use SNMPv3 authentication ( 27.3 SNMPv3 ). If a manager sends an incorrect community string, then in SNMPv1 and SNMPv2c there is no reply. SNMPv3 relaxed this rule somewhat, 27.3.3 SNMPv3 Engines. A single agent can support multiple community strings. Each community has an associated subset of the MIB (a view) that it allows. For example, an agent can be conÔ¨Ågured so that the community string ‚Äúsystem‚Äù allows the manager access to the system group, the community string ‚Äútengwar‚Äù allows access to the entire MIB-2 group, and the community string ‚Äúgaladriel‚Äù allows access to the preceding plus the private group(s). Using Net-SNMP (the most common agent on Linux and Macintosh machines) this would be achieved by the following entries in the snmpd.conf Ô¨Åle: rocommunity system default 1.3.6.1.2.1.1 rocommunity tengwar default 1.3.6.1.2.1 rocommunity galadriel default -V mib2+private view mib2+private included 1.3.6.1.2.1 view mib2+private included 1.3.6.1.4.1 In order that the galadriel community could contain two disconnected OID subtrees, it was necessary to make use of the View-based Access Control Model (V ACM). Community galadriel has access to the OIDtree view named ‚Äúmib2+private‚Äù; this view is in turn deÔ¨Åned in the last two lines above. For our purposes here, V ACM can be seen as an implementation mechanism for specifying what portion of the OID tree is accessible to a given community. V ACM allows read and write permissions to be granted to speciÔ¨Åc OID trees, and also to be excluded from speciÔ¨Åc subtrees ( egtable columns). Using the ‚Äúmask‚Äù mechanism, access can even be granted to speciÔ¨Åc rows of a table. We return to V ACM for SNMPv3 very brieÔ¨Çy in 27.3.9 VACM for SNMPv3. On Microsoft operating systems, an SNMP agent is generally included but must be enabled, egfrom ‚ÄúWindows components‚Äù or ‚ÄúPrograms and Features‚Äù. After that, the agent must still be conÔ¨Ågured. This is done by accessing ‚ÄúServices‚Äù ( egthrough Control Panel √ëAdministrative Tools or by launching services.msc), selecting ‚ÄúSNMP service‚Äù, and clicking on ‚ÄúProperties‚Äù. This applet only allows setting the community name and read vs write permissions; specifying collections of visible OID subtrees (views) is not supported here (though it may be via SNMP itself). The community mechanism canoffer a reasonable degree of security, if community names are changed frequently and if eavesdropping is not a concern. Perhaps the real problem with community-based security is that just how much information can leak out if an attacker knows the community string is not always well 26.11 SNMPv1 communities and security 661
An Introduction to Computer Networks, Release 2.0.11 understood. SNMP access can reveal a site‚Äôs complete network and host structure, including VPNs, subnets, TCP connections, host-to-host trust relationships, and much more. Most sites block the SNMP port 161 at their Ô¨Årewall; some even go so far as to run SNMP only on a ‚Äúhidden‚Äù network largely invisible even within the organization. The view model for OID-tree access is formalized in RFC 3415 as part of SNMPv3; it is called the Viewbased Access Control Model or V ACM. It allows the creation of named views. The vacmAccessTable spells out the viewing rights assigned to a given V ACM group, which, in a rough sense, corresponds to an SNMPv1/SNMPv2c community. V ACM supports, in addition to views consisting of disjoint unions of OID subtrees, table views that limit access to a speciÔ¨Åc set of columns. 26.12 SNMP and ASN.1 Encoding When SNMP data is entered into a packet, it is encoded according to the ASN.1 Basic Encoding Rules, or BER. This is a hierarchical syntax-driven binary encoding strategy in which each atomic value (INTEGER, OCTET STRING, OBJECT IDENTIFIER, etc) istagged with its type, and each compound structure is also tagged. This means that the receiver can understand a complex packet format without prior knowledge of its structure. The BER rules are part of the ITU standard X.690. (ASN.1 also supports several other encoding-rule formats, including the human-readable XML Encoding Rules, but SNMP uses BER.) The general representation of an object is a ‚Äútype-length-value‚Äù triple of the following form; the type tag makes the data self-identifying. Length Value TypeTagP/C bitClass bits BER encoding The Ô¨Årst two bits of the type Ô¨Åeld, theclass bits, identify the context. Universal types such as INTEGER and OBJECT IDENTIFIER have class bits 00; application-speciÔ¨Åc types such as Counter32 and TimeTicks have class bits 01. The third bit is 0 for primitive types and 1 for constructed types such as STRUCTURE. The rest of the Ô¨Årst byte is the type tag. If a second byte is needed, the tag bits of the Ô¨Årst byte are 11111. 26.12.1 Primitive Types The standard ASN.1 universal-type primitive tag values used by SNMP are as follows: 00000 NULL 00010 INTEGER 00100 OCTET STRING 00110 OBJECT IDENTIFIER SNMP also deÔ¨Ånes several application-speciÔ¨Åc tags (the following are from RFC 2578 ): 662 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 00000 IpAddress 00001 Counter32 00010 Gauge32 00011 TimeTicks 00100 Opaque 00110 Counter64 The second Ô¨Åeld of the type-length-value structure is the length of the value portion, in bytes. Most lengths of primitive types will Ô¨Åt into a single byte. If a data item is longer than 127 bytes (true for many composite types), the multi-byte integer encoding below is used. The actual data is then encoded into the value Ô¨Åeld. For nonnegative integers, the integer is converted to a twos-complement bitstring and then encoded in as few bytes as possible, provided there is at least one leading 0-bit to represent the sign. Similarly, negative numbers must have at least one leading 1-bit to represent the sign. For example, 127 can be encoded as a length=1 INTEGER with value byte 0111 1111, while 128 must be encoded as a length=2 integer with value bytes 0000 0000 and 1000 0000. The second value byte represents 128, but the Ô¨Årst byte represents the sign; without the Ô¨Årst byte the INTEGER represented by a length of 1 and a value byte of 1000 0000 is ‚Äì128. Similarly, to encode decimal 10,000,000 (0x989680), fourvalue bytes are needed as otherwise the leading bit would be 1 and the number would be interpreted as negative. For OCTET STRINGs, the string bytes are placed in the value portion and the number of bytes is placed in thelength portion. Object IDs are generally encoded using one byte per level; there are two exceptions. First, the initial two levels x.y of the OID are encoded using a single level with value 40 x + y; all SNMP OIDs begin with 1.3 and so the Ô¨Årst byte is 43 (0x2b). Second, if a level is greater than 127, it is encoded as multiple bytes. The Ô¨Årst bit of the last byte is 0; the Ô¨Årst bit of each of the preceding bytes is 1. The seven remaining bits of each byte contain the bits of the OID level. For example, 1.3.6.1.2.1. 128.9 would have a value encoding of the following bytes in hexadecimal: 2b 06 01 02 01 81 00 09 where 128is represented as the two seven-bit blocks 0000001 0000000 and the Ô¨Årst block is preÔ¨Åxed by 1 and the second block by 0. Note that the encoding of 128 in an OID is quite different from the encoding of 128 as an INTEGER. 26.12.2 Composite Types Now that we have the encodings of the primitive types we can build structures. For composite types the P/C bit will be set to 1. The only universal composite type we will consider is 10000 SEQUENCE and SEQUENCE OF There are also several application-speciÔ¨Åc composite types; the Ô¨Årst three bits of the tag Ô¨Åeld for these will be 101. 26.12 SNMP and ASN.1 Encoding 663
An Introduction to Computer Networks, Release 2.0.11 00000 Get-Request 00001 Get-Next-Request 00010 Get-Response 00011 Set-Request 00100 Trap AVarBind pair is encoded in SNMPv1 as SEQUENCE { name OBJECT IDENTIFIER, value ObjectSyntax } ObjectSyntax is speciÔ¨Åed as a CHOICE that can contain any of the tagged SNMP primitive types above; the CHOICE syntax adds no bytes so the value is simply encoded as above. The VarBind pairx1.3.6.1.2.1.1.3.0, 8650000 y‚Äì the OID is sysUpTime and the value is 24 hours ‚Äì would then be represented in hexadecimal bytes as below; the hexadecimal representation of 8650000 is 0x83d600. 30 10 06 08 Encoding of 0x83d600 type = 02, length = 04Encoding of 1.3.6.1.2.1.1.3.0 type = 06, length = 08 Encoding of SEQUENCE of two items; type = 0x30 = 0011 0000, length = 16 = 0x1002 04 00 83 d6 00 2b 06 01 02 01 01 03 00 The Ô¨Årst byte of 0x30 marks this as a SEQUENCE; recall that the type byte for a SEQUENCE has a P/C bit of 1 and Ô¨Åve low-order bits of 10000, for a numeric value of 48 decimal or 0x30. TheVarBindList is deÔ¨Åned to be a SEQUENCE OF VarBind. If we have only the single VarBind above, the resultant enclosing VarBindList would be as follows; the length Ô¨Åeld is 0x12 = 18. 30 12 30 10 06 08 2b 06 01 02 01 01 03 00 02 04 00 83 d6 00 The BER encoding rules do not stop with the VarBindList. A slightly simpliÔ¨Åed ASN.1 speciÔ¨Åcation for an entire SNMPv1 Get-Request packet portion (or ‚Äúprotocol data unit‚Äù) is SEQUENCE { request-id INTEGER, error-status INTEGER, error-index INTEGER, variable-bindings VarBindList } The encoding of the whole packet would also be by the above BER rules. The BER encoding mechanism represents a very different approach from the Ô¨Åxed-Ô¨Åeld layout of, say, the IP and TCP headers ( 9.1 The IPv4 Header and17.2 TCP Header ). The latter approach is generally quite a bit more compact, as only four bytes are needed for a larger integer versus six under BER, and no bytes are used for SEQUENCE speciÔ¨Åcations. The biggest advantage of the SNMP BER approach, however, is that all objects, from entire packets down to individual values, are self-describing. Given the variety of types used by SNMP, the fact that many are of variable length, and the fact that value ‚Äúreaders‚Äù such as MIB 664 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 browsers often operate without having all the type-specifying MIBs loaded, this self-describing feature is quite useful. 26.13 Network Management Systems It is the job of Network Management Systems (NMS) to automate the process of network monitoring. As a rule, this includes the regular collection of SNMP data. An NMS must 
- Discover new devices when they are added 
- Identify the capabilities of the devices, to determine what sort of SNMP queries to send them 
- Regularly pollthe devices with GET requests, at appropriate intervals 
- Maintain a history of network behavior Device discovery may, for example, be implemented via ping (10.4 Internet Control Message Protocol ). Capability determination is often made using the sysObjectID value of 26.10.1 The system Group. Most NMSes are able to Ô¨Ågure out the appropriate manufacturer-speciÔ¨Åc queries under the private preÔ¨Åx, in addition to the appropriate mib-2 queries. It is worth noting that, while discovery and capability determination can in principle be handled by manual conÔ¨Åguration, in practice this fails badly for all but the smallest networks. Automatic detection is a must. The NMS may literally be the SNMP manager for devices (in the sense of 26.2 SNMP Basics ), sending GET requests directly. Alternatively, it may delegate that role to ‚Äúsub-managers‚Äù. NMSes may also use non-SNMP mechanisms. This is quite common for monitoring the status of links and of servers, for which SNMP offers fewer options. NMSes may also use non-SNMP alternatives even for situations that are relatively well-supported by SNMP. However, SNMP-based systems come with reasonable if imperfect strategies for device discovery and capability determination, and automating these issues for non-SNMP systems requires considerable thought and effort. 26.14 SNMP Alternatives SNMP is popular in large part because nearly every serious piece of networking hardware supports it. But there is room for improvement. One of the most serious issues is the use of UDP for transport; some responses may get lost. A second issue is the limitations of polling. Polling intervals of over 10 seconds are common, because of concerns that more frequent polling will lead to excessive trafÔ¨Åc. But a long polling interval makes it impossible to capture sub-second trafÔ¨Åc bursts. These problems are exacerbated by the lack of timestamps from the SNMP agent; it is never quite clear just when an agent response was collected. Several alternatives have been proposed. One promising category is streaming telemetry, and the Google Network Management Interface ( gNMI, [LS18]) implementation in particular. gNMI uses TCP-based gRPC (16.5.5 gRPC ) as its transport, meaning that message delivery is reliable. This, in turn, means that devices can be conÔ¨Ågured to report only changes; this approach can fail badly over unreliable transport. While gNMI does support GET and SET requests, similar to SNMP, its primary mechanism is the SUBSCRIBE request. Subscriptions can be of type POLL, meaning the agent sends the requested data at regular intervals. However, the STREAM subscription is often more useful, as it supports the sending of data updates only when the data value changes. The update response is sent each time the data changes, immediately upon 26.13 Network Management Systems 665
An Introduction to Computer Networks, Release 2.0.11 detection of the data change. While this is not particularly useful for, say, packet counts, it is invaluable for reports of interfaces going down, or, where available, for reports of trafÔ¨Åc exceeding a preset threshold. While gNMI is supported by many devices, adoption is not nearly as complete as for SNMP. Therefore, gNMI installations often make use of at least some translation proxies: devices that speak gNMI to the NMS (or gNMI ‚Äúcollector‚Äù), but which speak SNMP to one or more nearby SNMP agents. Proxies are close to the agents, which means that SNMP packet loss rates are very low, and very frequent SNMP polling does not lead to network congestion. This proxy approach also helps with the difÔ¨Åcult problems of device discovery and capability detection. Information about gNMI can be found at github.com/openconÔ¨Åg/gnmi; the gNMI speciÔ¨Åcation is at github.com/openconÔ¨Åg/reference/blob/master/rpc/gnmi/gnmi-speciÔ¨Åcation.md. 26.15 Exercises 1.0. Consider the table below ( 26.7 SNMP Tables ). The Ô¨Årst column is the index. index count veggie 1 401 kale 3 523 kohlrabi 57 607 m√¢che 92 727 okra Give the OID for each data value, assuming this table were encoded in SNMP. Assume the columns are assigned OID levels 1, 2, and 3 in order (including the index column), and the root of the subtree (the tableEntry OID) is represented by T. Note that rows are not numbered consecutively. 2.0. Suppose interface data appears in a table ( 26.7 SNMP Tables ) as follows; missing entries are simply not present. The four columns are numbered 1, 2, 3 and 4. Only physical interfaces have an ifSpeed; tunnel interfaces built on top of a physical interface identify the latter in the physIface column ifIndex(1) ifName(2) ifSpeed(3) physIface(4) 1 loopback 1 2 eth0 10000000 37 ppp0 2 101 ipv6tun 2 Suppose we traverse this table using GetNext() sequentially. Give the order of values returned, and the OID for each. As above, let Trepresent the root OID, so leaf OIDs are of the form T.col.row. The Ô¨Årst two values, with OIDs, are thus as follows: value OID 1 T.1.1 2 T.1.2 3.0 Consider the following multi-attribute GetNext() as presented in 26.8.1 Multi-attribute Get(): 666 26 Network Management and SNMP
An Introduction to Computer Networks, Release 2.0.11 GetNext( T.1.4, T.2.4, T.3.4, T.4.4, T.10.4, T.11.4) The answer given in the text explicitly assumed that the only columns existing were those shown: 1, 2, 3, 4, 10 and 11. What would be the result if all columns of ifTable were present? Assume, as before, that row 4 is the Ô¨Ånal row. 4.0. Suppose you want an SNMP table identTable to holdxidNum,userName ypairs, where idnum is to be an INTEGER and username an OCTET STRING. The INDEX is idnum. Give ASN.1 deÔ¨Ånitions for the following: 
- identTable 
- identEntry 
- IdentEntry 
- idNum 
- userName Follow the SNMPv1 convention of including the index column as regular table data. 26.15 Exercises 667
An Introduction to Computer Networks, Release 2.0.11 668 26 Network Management and SNMP
27 SNMP VERSIONS 2 AND 3 In this chapter we describe SNMPv2, and then SNMPv3. Mixed in between, we present some useful additional MIBs. 27.1 SNMPv2 SNMPv2 introduced multiple evolutionary changes: to the SMI, to various MIBs, and to the basic protocol operation. Many new MIBs were added. SNMPv2 also contained a proposal for improved security, but this was not widely adopted. Eventually most of the SNMP community settled on SNMPv2 c, the version of SNMPv2 that stayed with the community-based security model. Most of the speciÔ¨Åcation of SNMPv2c is in RFC 1901 through RFC 1909. Generally SNMPv1 and SNMPv2c agents and managers can interoperate quite easily. Essentially all SNMPv2 agents also support SNMPv1 queries, and answer according to the version of the request received. There is slightly greater confusion between SNMPv1 and SNMPv2 MIB Ô¨Åles, but almost all browsers and managers support both. 27.1.1 SNMPv2 SMI and Data Types SNMPv2 introduced the OBJECT-IDENTITY macro, which acts like the OBJECT-TYPE macro except that it leaves out the SYNTAX clause; it serves as a way to deÔ¨Åne OIDs separately from syntax. In theOBJECT-TYPE macro, the ACCESS attribute is renamed MAX-ACCESS, and a new access option read-create (for reading plus row creation) is added. SNMPv2 also provided new 64-bit versions of some of the basic types, and added some other types. These deÔ¨Ånitions are in RFC 2578, originally RFC 1442. A very practical problem with SNMPv1 is that, for example, the 32-bit inOctets counter can wrap around in 34 seconds at 1 Gbps. The INTEGER type remains, now declared synonymous with Integer32. The SNMPv1 Counter and Gauge types have been replaced with Counter32 and Gauge32, and 64-bit versions of both ‚Äì Counter64 and Gauge64 ‚Äì were added (Gauge64 was added somewhat later, in RFC 2856 ). A BITS type, for representing individual bits in a word, was also added. The OBJECT IDENTIFIER type was formally limited to 128 levels. SNMPv2 also introduced the TEXTUAL-CONVENTION macro, which is an alternative name for a type (or a primary name for an enumerated type) together with a description of what the type is actually to represent. Examples include OwnerString, which is an OCTET STRING intended to describe a management station and perhaps its human operator, EntryStatus which is an enumerated type meant to be used for row additions ( 27.2.1 RMON ), andInterfaceIndex, which is meant to be the number of an interface appearing in the MIB-2 ifTable ,26.10.2 Table deÔ¨Ånitions and the interfaces Group, but used in another table. For casual MIB reading, textual conventions can simply be thought of as types. 669
An Introduction to Computer Networks, Release 2.0.11 27.1.2 SNMPv2 Get Semantics When an SNMPv1 agent receives a list of OIDs as part of a Get() request, and one or more of them is unavailable, then an error is returned. Under SNMPv2, the agent returns a list containing the appropriate value for each valid OID. For request OIDs for which the agent is not able to return an actual value, the special valuenoSuchInstance (for missing table entries with legal column speciÔ¨Åcations) or noSuchObject (for other missing values) is substituted. TheSet() semantics remain unchanged: an SNMPv2 agent does not update any of the OID values in the request unless it is able to update all of them. 27.1.3 SNMPv2 GetBulk() SNMPv2 introduced the GetBulk operation as an extension of GetNext. A manager includes an integer N in its request and the agent then iterates the action of GetNext() N times. All N results (which can each represent an entire row) can then be returned in a single operation. For example, suppose a table Thas Ô¨Åve rows indexed by 11-15, and three columns with OIDs T.1 through T.3. The OIDs for the table are as follows: T.1.11 T.2.11 T.3.11 T.1.12 T.2.12 T.3.12 T.1.13 T.2.13 T.3.13 T.1.14 T.2.14 T.3.14 T.1.15 T.2.15 T.3.15 Then a GetBulk request for ( T.1,T.2,T.3) with a repetition count of 3 will return the Ô¨Årst three rows, with the following OIDs: T.1.11 T.2.11 T.3.11 T.1.12 T.2.12 T.3.12 T.1.13 T.2.13 T.3.13 To continue, the next such request would include OIDs ( T.1.13, T.2.13, T.3.13) and the result (again assuming a count of 3) would be of values with these OIDs: T.1.14 T.2.14 T.3.14 T.1.15 T.2.15 T.3.15 T.2.11 T.3.11 A where Ais the next leaf OID above and to the right of T. Note the third row of the second request: the Ô¨Årst leaf OID following T.1.15 ‚Äì the last row of column 1 ‚Äì is T.2.11, that is, the Ô¨Årst row of column 2. Similarly, T.3.11 follows T.2.15. As T.3.15 is the last leaf OID in the table, its leaf-OID successor ( A) is outside the table. The GetBulk request format actually partitions the list of requested OIDs into two parts: those OIDs that are to be requested only once and those for which the request is repeated. Two additional parameters besides 670 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 the OID list are included: non-repeaters indicating the number of OIDs to be requested only once and max-repetitions indicating the number of times the remaining OIDs are retrieved. As with GetNext, it is possible that a request for rows of the table will return xOID,valueypairs outside the table, that is, following the table in the OID-tree order. If the total number of OIDs in the request is N ¬•non-repeaters, then the return packet will contain a list ofxOID,valueyvariable bindings of up to length non-repeaters + (N ‚Äìnon-repeaters )max-repetitions GetBulk has considerable potential to return more data than there is room for, so an agent may return fewer repetitions as it sees Ô¨Åt. 27.1.4 SNMPv2 Indexes As we saw in 26.7 SNMP Tables, in SNMPv1 the index columns were included in the table. Because every attribute returned by an agent comes paired with the attribute‚Äôs OID, and because the OID of a table item encodes the row and thus the index value, index columns are unnecessary except in rare cases (one such case is when alltable columns are part of the index, as in udpTable ). SNMPv2 deals with this by requiring the usual INDEX clause in the ASN.1 tableEntry deÔ¨Ånition, but then when the tableIndex attribute is deÔ¨Åned with OBJECT-TYPE it is assigned a MAX-ACCESS of not-accessible. Here is an example from the sysORTable of 27.1.7 SNMPv2 MIB Changes; it is the second ‚Äúnot-accessible‚Äù that represents the change. sysOREntry OBJECT-TYPE SYNTAX SysOREntry MAX-ACCESS not-accessible STATUS current DESCRIPTION "An entry (conceptual row) in the sysORTable." INDEX { sysORIndex } ::= { sysORTable 1 } ... sysORIndex OBJECT-TYPE SYNTAX INTEGER (1..2147483647) MAX-ACCESS not-accessible STATUS current DESCRIPTION ... ::= { sysOREntry 1 } TheINDEX attribute appears in the declaration of sysOREntry in the usual way. But it is now classed as anauxiliary object, and access to sysORIndex isnot-accessible; in SNMPv1 it would have been read-only and thus an ordinary column. When direct access to index attributes is suppressed this way, the index data is still available in the accompanying OID, but it is no longer tagged by type as in 26.12 SNMP and ASN.1 Encoding. ThesysORIndex value above, for example, would appear as a single OID level, but the manager would have to use the MIB to determine that it was meant as an INTEGER and not a TimeTicks or Counter. An IPv4 address used 27.1 SNMPv2 671
An Introduction to Computer Networks, Release 2.0.11 as an index would appear in the OID as four levels, readily recognizable as an IPv4 address provided the manager knew where to look. When STRING values appear in the index, the lack of an index column can be a particular nuisance; for example, the only indication of the username ‚Äúalice‚Äù in the usmUserTable of 27.3.9.1 The usmUserTable might be the OID fragment 97.108.105.99.101, representing the ASCII codes for the letters a.l.i.c.e. Generally, an SNMPv2 agent will send back the noSuchObject special value (see 27.1.2 SNMPv2 Get Semantics ) when asked for a not-accessible auxiliary object. 27.1.5 TestAndIncr SNMPv2 introduced the TestAndIncr textual convention, which introduces something of an aberration to the usual semantics of Set(). The underlying type is INTEGER, in the range 0..231‚Äì1, andGet() works the usual way. However, if TIis the OID name of a TestAndIncr object, then Set( TI,val) never sets the value of TItoval. Instead, if the value of the object is already equal to val, then theSet() succeeds and the value of the object is incremented, that is, set to val+ 1. If the value of the object is not equal to val, an error occurs and no change is made. ATestAndIncr object acts like a kind of semaphore, though not exactly as there is no way to decrement the object (though the value does wrap around from 231-1 back to 0). The goal here is to provide a voluntary mechanism to enforce serialization when more than one manager may be writing to the same set of attributes. As we saw at the example at the end of 26.8.2 Set(), such serialization is not automatic. But now let us revisit that example using TestAndIncr. Recall that two managers are updating attributes with OIDs XandY; this time, however, they agree also to include a TestAndIncr object with OID TI. Then serialization is assured as long as each manager executes each multi-attribute Set() in the following form, where val:= Get(TI) means that the manager uses Get() to retrieve the value of TIand stores it in its own local variable val. val:= Get( TI) Set(( TI,val), (X,xval), (Y,yval)) To see this, suppose manager B‚Äôs Get( TI) occurs after manager A‚Äôs Set( TI,val) has incremented val. Then manager B‚Äôs Set() operations occur even later, after A‚Äôs Set() has completed. The alternative is that both managers Get() the same value val. Let A be the manager who Ô¨Årst succeeds with Set( TI,val). Now the other manager ‚Äì B ‚Äì will have its Set( TI,val) fail, as the value stored at TIis now val+1. Thus allof B‚Äôs Set() operations will fail. A consequence here is that manager B will have to try again, probably immediately. However, the likelihood of conÔ¨Çict is low, and B can expect to succeed soon. Usually only one TestAndIncr object needs to be provided for an entire table, not one per row. For an actual example, see 27.3.9.1 The usmUserTable. 27.1.6 Table Augmentation SNMPv2 introduced a mechanism for extending an existing table by adding, in effect, more columns, known asaugmentation. In the new table-entry deÔ¨Ånition, instead of an INDEX clause there is an AUGMENTS 672 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 clause; the argument to which is a table-entry name for the existing table to be extended. The new tableentry row will be indexed by whatever attributes indexed the table-entry in the AUGMENTS clause. For example, the ifXTable augments the MIB-2 ifTable, meaning in essence that the ifXTable is automatically indexed by ifTable ‚Äôs index,ifIndex. Here is the ifXEntry deÔ¨Ånition that establishes that: ifXEntry OBJECT-TYPE SYNTAX IfXEntry MAX-ACCESS not-accessible STATUS current DESCRIPTION "An entry containing additional management information applicable to a particular interface. " AUGMENTS { ifEntry } ::= { ifXTable 1 } The intent here is that every ifTable row is now extended to include the nineteen additional values deÔ¨Åned forIfXEntry; that is, there is a one-to-one correspondence between rows if ifTable andifXTable. If, on the other hand, the new table extends only a few rows of the original table, ieis asparse extension, then the new table entry should have an INDEX clause that repeats that of the original table. An example is the EtherLike-MIB of 27.1.10 ETHERLIKE-MIB, in which the dot3StatsTable extends the MIB-2ifTable by providing additional information for those interfaces that behave like Ethernets. The dot3StatsEntry deÔ¨Ånition is dot3StatsEntry OBJECT-TYPE SYNTAX Dot3StatsEntry MAX-ACCESS not-accessible STATUS current DESCRIPTION "Statistics for a particular interface to an ethernet-like √£√ëmedium." INDEX { dot3StatsIndex } ::= { dot3StatsTable 1 } The INDEX is dots3StatsIndex, which is then deÔ¨Åned as follows; note the statement in the DESCRIPTION (and the REFERENCE) that the dot3StatsIndex is to correspond to ifIndex. dot3StatsIndex OBJECT-TYPE SYNTAX InterfaceIndex MAX-ACCESS read-only STATUS current DESCRIPTION "An index value that uniquely identifies an interface to an ethernet-like medium. The interface identified by a particular value of this index isthe same interface asidentified by the same value of ifIndex. " REFERENCE "RFC 2863, ifIndex" ::= { dot3StatsEntry 1 } Finally, it is possible that a new table has a many-to-one, or dense, correspondence to the rows (or a subset of the rows) of an existing table. In this case, the new table will have an INDEX clause that will include the index attributes of the original table, and one or more additional attributes. An example is the EtherLikeMIBdot3CollTable, which keeps, for each interface, a set of histogram buckets giving, for each N, a 27.1 SNMPv2 673
An Introduction to Computer Networks, Release 2.0.11 count of the number of packets that experienced exactly N collisions before successful transmission. The dot3CollEntry deÔ¨Ånition is as follows: dot3CollEntry OBJECT-TYPE SYNTAX Dot3CollEntry MAX-ACCESS not-accessible STATUS current DESCRIPTION ... INDEX { ifIndex, dot3CollCount } ::= { dot3CollTable 1 } TheifIndex entry in the INDEX represents the original table, as before except that here there is also a second, new INDEX attribute, dot3CollCount. 27.1.7 SNMPv2 MIB Changes The core MIB for SNMPv2 ‚Äì essentially the SNMPv2 version of MIB-2 ‚Äì is RFC 3418, originally RFC 1450. Some changes appear under the MIB-2 OID preÔ¨Åx: 1.3.6.1.2.1. SNMPv2 also deÔ¨Ånes a preÔ¨Åx 1.3.6.1.6 speciÔ¨Åcally for SNMPv2 information. This MIB adds the Object Resource table, sysORTable, to the system group. It also slightly modiÔ¨Åes the mib-2.snmp group and provides a new snmp group under the SNMPv2 1.3.6.1.6 preÔ¨Åx. 27.1.8 sysORTable The original system group contained the attribute sysObjectID that identiÔ¨Åes the agent and at the same time suggests a private OID tree that could provide additional information about the agent ( 26.10.1 The system Group ). ThesysORTable, 1.3.6.1.2.1.1.9 or mib-2.system.9, is an attempt to extend this. It consists of a list of OIDs that can be queried for further agent information; each OID also has an associated description string and asysORUpTime value indicating the time that OID was added. For example, my system lists the following (where snmpModules = 1.3.6.1.6.3 and mib-2 = 1.3.6.1.2.1): snmpModules.11.3.1.1 Message Processing MIB snmpModules.15.2.1.1 User-based security MIB snmpModules.10.3.1.1 SNMP management architecture snmpModules.1 SNMPv2 information mib-2.49 TCP mib-2.4 IP mib-2.50 UDP snmpModules.16.2.2.1 V ACM snmpModules.13.3.1.3 SNMP notiÔ¨Åcation mib-2.92 SNMP notiÔ¨Åcation logging 674 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 Each of the above OID preÔ¨Åxes can theoretically then be accessed for further information. Unfortunately, on my system several of them are not conÔ¨Ågured, and a query returns nothing, but sysORTable does not know that. 27.1.9 IF-MIB and ifXTable RFC 2863 has addressed the problems with ifSpeed and the byte counters by introducing a new table, ifXTable (formerlyifExtnsTable with OID mib-2.31.1.2. As such, it is still under the aegis of the MIB-2 OID, but outside the traditional interfaces group. It does use the SNMPv2 SMI. TheifXTable table includes deÔ¨Ånitions for ‚Äúhigh capacity‚Äù 64-bit counters for in and out octets, in and out unicast packets, in and out multicast packets and in and out broadcast packets. It also includes the ifHighSpeed attribute for interface speed, still a 32-bit quantity but now measured in units of 1 Mbps. The full deÔ¨Ånition of an ifXTable row is as follows: IfXEntry ::= SEQUENCE { ifName DisplayString, ifInMulticastPkts Counter32, ifInBroadcastPkts Counter32, ifOutMulticastPkts Counter32, ifOutBroadcastPkts Counter32, ifHCInOctets Counter64, ifHCInUcastPkts Counter64, ifHCInMulticastPkts Counter64, ifHCInBroadcastPkts Counter64, ifHCOutOctets Counter64, ifHCOutUcastPkts Counter64, ifHCOutMulticastPkts Counter64, ifHCOutBroadcastPkts Counter64, ifLinkUpDownTrapEnable INTEGER, ifHighSpeed Gauge32, ifPromiscuousMode TruthValue, ifConnectorPresent TruthValue, ifAlias DisplayString, ifCounterDiscontinuityTime TimeStamp } The original MIB-2 interfaces group counted multicast and broadcast packets together, egin ifInNUcastPkts. 27.1.10 ETHERLIKE-MIB RFC 3635 (originally RFC 1650 ) deÔ¨Ånes a MIB for ‚ÄúEthernet-like‚Äù interfaces. The primary goal is to enable the collection of statistics on collisions and other Ethernet-speciÔ¨Åc behaviors. Several new tables are deÔ¨Åned. The tabledot3StatsTable contains additional per-interface attributes; the name refers to the IEEE designation for Ethernet of 802.3. The table represents a sparse extension of the original ifTable, in the sense of 27.1.6 Table Augmentation (where this table was used as the example). 27.1 SNMPv2 675
An Introduction to Computer Networks, Release 2.0.11 The rows of the table mostly consist of counters for various errors and other noteworthy conditions: 
- Alignment errors: the number of bits in the frame is not divisible by 8 
- CRC checksum failure 
- Frames that experienced exactly one collision 
- Frames that experienced more than one collision 
- Signal quality errors. SQE is speciÔ¨Åc to 10 Mbps Ethernet 
- Deferred transmissions; when the station tried to send, the line was not idle 
- Late collisions: the only way a collision can occur after the slot time is passed is if the physical Ethernet is too big or if collision-detection is failing. See 2.1.5 The Slot Time and Collisions 
- Excessive collisions: the frame experienced 16 collisions and the sender gave up 
- Other hardware errors ( dot3StatsInternalMacTransmitErrors and dot3StatsInternalMacReceiveErrors ) 
- Carrier sense errors (‚Äúcarrier sense‚Äù refers to the collision-detection mechanism; there is no actual carrier) 
- Frames longer than 1500 octets 
- For Ethernets that encode data as symbols ( eg100 Mbps Ethernet‚Äôs 4B/5B), frames arriving with a corrupted symbol There is also an attribute to describe the Ethernet chipset. Thedot3HCStatsTable is like thedot3StatsTable except its counters are 64 bits instead of 32 bits. The tabledot3CollTable lists, for each N, how many packets experienced exactly N collisions before successful transmission or discard. Ethernet generally allows a maximum N of 16. The table is indexed by the pairxifIndex ,Ny. Thedot3StatsTable contains this information for N=0,1,16. The tabledot3ControlTable contains information on those Ethernet-like interfaces that also support the so-called MAC Control sublayer; the dot3PauseTable is similar. 27.1.11 BRIDGE-MIB RFC 1286 (now RFC 4188 ) deÔ¨Åned basic management information for Ethernet switches; this is often complemented by vendor-speciÔ¨Åc MIBs. This MIB contains basic information about the switch, such as the number of ports. The dot1dStp section covers information about the spanning-tree protocol ( 3.1 Spanning Tree Algorithm and Redundancy ). Perhaps the most interesting member of this MIB is the dot1dTpFdb table, which contains the forwarding table: the map from MAC addresses to ports created by the Ethernet learning algorithm ( 2.4.1 Ethernet Learning Algorithm ). The name dot1dTpFdb is an abbreviation of the IEEE 802.1D standard, Transparent bridging protocol, Forwarding database. By using this table, a network manager can trace the origin of a packet with a given MAC address. If a switch S1 has seen the packet, the dot1dTpFdb table reveals the switch port by which the packet arrived. If that 676 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 port leads to another switch, S2, the process is repeated; if the port represents a non-switch port, then the packet originated wherever the other end of that port‚Äôs cable terminates. Typically, this allows determination of the ofÔ¨Åce or apartment unit or hotel room where a device was connected. This tracing is quite hard to evade, short of breaking into the wiring closet or compromising the switch itself. This table can also be used to alert network managers whenever a different device is plugged into an ofÔ¨Åce Ethernet jack, although here MAC-address spooÔ¨Ång (changing the new device‚Äôs MAC address to match the Ô¨Årst device‚Äôs MAC address) can be used to avoid detection. This strategy for identifying a device‚Äôs physical location does not work quite as well for Wi-Fi, though it can be used to identify the access point. Wi-Fi, however, generally requires the presentation of credentials; this gives an alternative approach for tracking users. The MIB does notcontain information about whether internal switch queues have had to drop packets, though some vendor-speciÔ¨Åc MIBs can be used instead. 27.1.12 IP-MIB and IP-Forward MIB The MIB-2 ip group is really about two separate things: strictly local information about IP packets and delivery, and the forwarding table, which affects a much larger collection of nodes. All nodes have local IP information, but only routers do signiÔ¨Åcant forwarding. It did not take long for these two subgroups to separate. The IP-Forward MIB began in RFC 1354, just over a year after the original MIB-2 in RFC 1312; this created a new home for forwarding-table information. By the time the Ô¨Årst SNMPv2 MIB for the ip group came out in RFC 2011, theipRouteTable once part of that group was declared obsolete. 27.1.12.1 IP-MIB The current version of the IP-MIB is RFC 4293. It allows enabling or disabling of forwarding (ipForwarding, setting the default TTL ( ipDefaultTTL ), and, for IPv4 only (as it is not a conÔ¨Ågurable parameter for IPv6) the fragment-reassembly timeout ( ipReasmTimeout ). When IP addresses appear in these tables, the deÔ¨Åning entry is almost always prefaced by an object of type InetAddressType, which can take values ipv4(1) and ipv6(2) (and a few other values for special cases). Two tables are devoted to IP statistics: the ipSystemStatsTable for system-wide IP information and theipIfStatsTable for interface-speciÔ¨Åc information. Each table includes the IP address type (that is the IP version) in its index, meaning that separate statistics are kept for IPv4 and IPv6 trafÔ¨Åc. The system table is indexed by the IP address type alone; the interface table is indexed by that and the interface number (the MIB-2 ifIndex ). TheipIfStatsTable contains, for each interface, counts of packets (and octets) in and out for broadcast, multicast and unicast, counts of forwarded, reassembled and fragmented packets, counts of packets with any of several kinds of errors, and related additional counts. An entry for the counter refresh rate is also provided. When appropriate, 64-bit counters are provided; usually the equivalent 32-bit counter is also provided. The ipSystemStatsTable provides all-interface summaries of these same counts. The tablesipv4InterfaceTable andipv6InterfaceTable represent information about what IP addresses are assigned to each interface. These are indexed by the interface index alone, meaning that the 27.1 SNMPv2 677
An Introduction to Computer Networks, Release 2.0.11 tables cannot accurately represent an interface with more than one IP address of the same type. Additional IP-address information is contained in the ipAddressPrefixTable primarily for IPv6 and theipAddressTable for both IPv4 and IPv6. These tables contain information about what IP addresses are assigned to what interfaces and where these addresses came from ( egDHCP, 10.3 Dynamic Host ConÔ¨Åguration Protocol (DHCP), or PreÔ¨Åx Discovery, 11.6.2 PreÔ¨Åx Discovery ). Indexed by the IP address itself (and also the ipAddressAddrType ), these tables thus support the possibility that one interface has multiple IP addresses (this is particularly common for IPv6). TheipNetToPhysicalTable represents the map from local IP addresses to physical LAN addresses, as created by either ARP for IPv4 or Neighbor Discovery for IPv6. In addition to the interface, the IP address and the physical address, tle table also contains a timestamp indicating when a given entry was last updated or refreshed, an indication of whether the address mapping is dynamic, static or invalid, and, Ô¨Ånally, an attributeipNetToPhysicalState. The values for this last are reachable(1) ,stale(2) for expired reachability, delay(3) andprobe(4) relating to active updates of the reachability, invalid(5), unknown(6) andincomplete(7) indicating that ARP is in progress. See 10.2.1 ARP Finer Points. There is also a simple version of the forwarding table known as the Default Router Table. This contains a list of ‚Äúdefault‚Äù, or, more accurately, ‚Äúinitially conÔ¨Ågured‚Äù routes. While this does represent a genuine forwarding table, it is intended for nodes that do not act as routers and do not engage in routing-update protocols. The table represents a list of ‚Äúdefault‚Äù routes by IP address and interface, and also contains route-lifetime and route-preference values. Theipv6RouterAdvertTable is used for specifying timers and other attributes of IPv6 router advertisements, 11.6.1 Router Discovery. Finally, the IP-MIB contains two tables for ICMP statistics‚Äù icmpStatsTable andicmpMsgTable. The latter keeps track, for example, of how many pings ( ICMP Echo ) and other ICMP messages were sent out; see 10.4 Internet Control Message Protocol. 27.1.12.2 IP-Forward MIB Information speciÔ¨Åc to a host‚Äôs IP-forwarding capability was Ô¨Årst split out from the MIB-2 ip group in RFC 1354; it was updated to SNMPv2 in RFC 2096 and the current version is RFC 4292. The original MIB-2 ip group left off at OID mib-2.ip.23; the new IP-Forward MIB begins at mib-2.ip.24. There have been three iterations of an SNMP-viewable IP forwarding table since the original RFC 1213 ip group‚ÄôsipRouteTable at mib-2.ip.21. Here are all four: 
- ipRouteTable 
- ipForwardTable 
- ipCidrRouteTable 
- inetCidrRouteTable Each new version has formally deprecated its predecessors. The Ô¨Årst replacement was ipForwardTable, described in RFC 1354. It deÔ¨Ånes the OID ipForward to be mib-2.ip.24; the new table is at ipForward .2. This table added several routing attributes, but perhaps more importantly changed the indexing. The index for ipRouteTable was the IP destination network ipRouteDest, alone. The new table‚Äôs index also includes a quality-of-service attribute 678 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 ipForwardPolicy, usually representing the IPv4 Type of Service Ô¨Åeld (now usually known as the DS Ô¨Åeld, 9.1 The IPv4 Header ). This inclusion allows the ipForwardTable to accurately represent routing based onxdest,QoSy, as discussed in 13 Routing-Update Algorithms. Such routing is sometimes called ‚Äúmultipath‚Äù routing, because it allows multiple paths to a given destination based on different QoS values. However, the mask length is notincluded in the index, making ipForwardTable inadequate for representing CIDR routing. The index also includes the next_hop, which for the actual forwarding table does not make sense ‚Äì the next_hop is what one is looking up, given the destination ‚Äì but which works Ô¨Åne for SNMP. See the comments about SNMP indexes with more attributes than expected in 26.7 SNMP Tables. The index even includes an attribute ipForwardProto that represents the routing-update protocol that is the source of the table entry: icmp(4) ,rip(8) (a common distance-vector implementation), is-is(9) andospf(13) (two link-state implementations) and bgp(14). In addition to the next_hop, this table also includes attributes for ipForwardType (eglocal vs remote), ipForwardAge (the time since the last update), ipForwardInfo ,ipForwardNextHopAS, and several routing metrics. The purpose of ipForwardInfo is to provide an OID that can be accessed to provide additional information speciÔ¨Åc to the routing-update algorithm in use. The ipForwardNextHopAS allows the speciÔ¨Åcation of the next_hop Autonomous System number, usually relevant only when BGP (15 Border Gateway Protocol (BGP) ) is involved. (If the AS number is not relevant, it is set to zero.) The second iteration of the SNMP-viewable IP forwarding table is ipCidrRouteTable, appearing in RFC 2096 and located at ipForward .4 (and returning to the practice of calling it a ‚Äúroute‚Äù rather than a ‚Äúforward‚Äù table). This table adds the address mask, ipCidrRouteMask, to the index, Ô¨Ånally allowing distinct routes to 10.38.0.0/16 and 10.38.0.0/24. The quality-of-service Ô¨Åeld ipCidrRouteTos remains in the index (as does the destination), and is now Ô¨Årmly identiÔ¨Åed with the IPv4 Type of Service (DS) Ô¨Åeld. The routing-update algorithm was dropped from the index. This table also adds an attribute ipCidrRouteStatus of typeRowStatus and used for the creation and deletion of entire rows (that is, forwarding table entries) under the control of SNMP. We will return to this process in 27.2 Table Row Creation. The third (and still current) version of the IP forwarding table is inetCidrRouteTable, introduced in RFC 4292 and located at ipForward .7. The main change introduced by this table is the extension to support IPv6: the IP-address columns ( egfor destination and next_hop) have companion columns for the address type:ipv4(1) andipv6(2). See 27.1.13 TCP-MIB for further details. The next_hop attribute (now two columns, with the addition of the address type) is still part of the index. The address mask used inipCidrRouteTable is now updated to be a preÔ¨Åx length, inetCidrRoutePfxLen. The quality-of-service Ô¨Åeld inetCidrRoutePolicy is an Object ID, declared to be ‚Äúan opaque object without any deÔ¨Åned semantics‚Äù; that is, it is at the implementer‚Äôs discretion. The IPv4 ToS/DS Ô¨Åeld evolved in IPv6 to the TrafÔ¨Åc Class Ô¨Åeld, 11.1 The IPv6 Header. Finally, routes are no longer required to list a single associated interface. The table makes use of the InterfaceIndexOrZero textual convention of RFC 2863, covering just this situation. The size of modern IP forwarding tables ( ~ one million entries, 15.5 BGP Table Size ) dwarfs the size of essentially any other SNMP table. Regularly downloading the entire table with SNMP may use considerable bandwidth. Also, the source table may have changed by the time the SNMP retrieval is Ô¨Ånished, leading to an inconsistent SNMP snapshot. 27.1 SNMPv2 679
An Introduction to Computer Networks, Release 2.0.11 27.1.13 TCP-MIB RFC 4022 contains some extensions to MIB-2‚Äôs tcp group. The SNMPv2-based MIB embedded in RFC 4022 repeats the MIB-2 tcp group and then adds new features within the mib-2.tcp (1.3.6.1.2.1.6) tree. RFC 1213 stopped at mib-2.tcp.15; RFC 4022 deÔ¨Ånes new objects starting at mib-2.tcp.17. The newtcpConnectionTable is deÔ¨Åned at mib-2.tcp.19, versus the original tcpConnTable at mib-2.tcp.13. The newer table supports IPv6; like the inetCidrRouteTable above, each IP-address column now also has a companion address-type column, and addresses themselves are represented as OCTET STRINGs preceded by a length byte. This follows the rules for InetAddressType and InetAddress ofRFC 4001, and the OID-sufÔ¨Åx-encoding rules of RFC 2851 ¬ß4.1. Because the length of anInetAddress isn‚Äôt known in advance, the length must be included. For IPv4 users used to the earlier tcpConnTable, this means that there are extra 1.4.‚Äôs preÔ¨Åxing the IPv4 addresses in the index, as in this example of host 10.0.0.5 connecting from port 54321 to web server 147.126.1.230: tcpConnectionState. 1.4.10.0.0.5.54321. 1.4.147.126.1.230.80 The new table also includes a column representing the process ID of the process that has open the local end of the connection. This table adheres to the SNMPv2 convention that index columns are not included in the data ‚Äì the attributes are marked not-accessible. There are only two accessible columns, tcpConnectionState and tcpConnectionProcess. TCP-MIB does notmake available per-interface TCP statistics, egthe number of TCP bytes sent by eth0. Nor does it make available per-connection statistics such as packet-loss and retransmission counts or total bytes transmitted each way. 27.2 Table Row Creation SNMPv2 also reÔ¨Åned the mechanisms by which a manager can add a row to an agent‚Äôs table. In principle, adding a row to a table is done with the Set() operation. Imagine a table Twith three columns indexT, fruitT andprimeT corresponding to T.1,T.2 and T.3. Imagine also that, right now, the table has three rows withindexT values 10, 11 and 12: indexT fruitT primeT T.1 T.2 T.3 10 apple 37 11 blueberry 59 12 cantaloupe 67 Then a new row x13,durian,101ymight be added with the following multi-attribute Set() operation. The entries of the new row will have OIDs T.1.13, T.2.13 and T.3.13, and all we have to do to create the row is assign to these. Note that we are assigning to OIDs that, in the agent‚Äôs current database, do not yet exist. Set(( T.1.13,13), ( T.2.13,durian), ( T.3.13,101)) 680 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 Of course, this raises some questions. Will the agent actually allow this? What happens if these Set() operations are performed individually rather than as a group? Is there any way to delete this newly added row? And, more seriously, what happens if some other manager tries to insert at the same time the row x13,feijoa,103y? We now turn to the speciÔ¨Åc row-creation mechanism of RMON. 27.2.1 RMON RMON, for Remote MONitoring, was an early attempt (Ô¨Årst appearing in RFC 1271 eight months after MIB-2‚Äôs RFC 1213 ) at having an SNMP agent take on some monitoring responsibilities of its own. The current version is in RFC 2819. The original RMON, now often called RMON1, only implemented LANlayer monitoring, but this was later extended to the IP and Transport layers in RMON2, RFC 4502. We will here consider only RMON1. RMON implements only passive monitoring; there is no capability for the remote agent to send out its own SNMP queries, or even pings (though see 27.2.3 PING-MIB ). Monitoring is implemented by putting the designated interface into promiscuous mode ( 2.1 10-Mbps Classic Ethernet ) and capturing all trafÔ¨Åc. In modern fully-switched Ethernets, hosts simply do not see trafÔ¨Åc not actually addressed to them, and so RMON would need to be implemented on a switch or router to be of much practical use. An agent‚Äôs RMON activity is controlled by an SNMP manager through the insertion of new rows in various control tables. The mechanism for doing this is our primary concern here. RMON statistics are divided into ten groups, of which we will consider only the following: 
- statistics: counts of errors and counts of packets in size ranges 0-64, 65-127, 128-255, 255-511, 512-1023 and 1024-1518 octets. 
- history: The statistics group data, taken at regular intervals 
- hosts: The Ethernet senders and receivers seen by an interface 
- host top N: The top-N senders or receivers 
- matrix: Information on trafÔ¨Åc by xsender,receiver ypair 27.2.1.1 Statistics (and use of EntryStatus) TheetherStatsTable is, by default, empty, and is indexed by etherStatsIndex which is an opaque INTEGER. The column etherStatsDataSource represents the OID of a speciÔ¨Åc interface number as deÔ¨Åned by ifTable; for example, the interface with ifNumber = 6 would be represented by mib-2.2.2.1.1.6. One column, etherStatsStatus, has typeEntryStatus as follows: valid(1), createRequest(2), underCreation(3), invalid(4) The attributes etherStatsDataSource andetherStatsStatus were initially read-write, which was later changed to status read-create as they can only be written to as part of the creation 27.2 Table Row Creation 681
An Introduction to Computer Networks, Release 2.0.11 of a new row. There is one more read-create attribute,etherStatsOwner, which is a managersupplied string identifying that particular manager (perhaps by IP address and hostname and, if a human is involved, appropriate additional identiÔ¨Åcation). To enable statistics collection, the SNMP manager creates a row in the agent‚Äôs etherStatsTable by setting the three attributes Status ,DataSource andOwner (where we have left off the attribute-name preÔ¨ÅxetherStats for readability). Once this is done and the Status is set tovalid, the agent begins collecting data about the desired interface. The manager can read the data as it desires, by accessing that particular row. Data collection ends when the manager sets the etherStatsStatus for the row to invalid, though it is up to the agent whether the row is actually deleted at that point. TheetherStatsIndex column is not actually writable, but the manager must still select a value for the index. One approach is to read the entire table and identify the Ô¨Årst unused index value. Other managers, however, may be creating new rows in the agent at the same moment, and so a row index that was available moments before may now be unavailable. We return below to how such conÔ¨Çicts are prevented. One common strategy for reducing the chance of row-creation collisions is to choose a value for the index at random. It is often possible for a manager to create the desired row with a single Set() operation. However, RFC 2819 requires that the the Status attribute in a request for creation of a new row must be createRequest, and a second Set() operation is therefore always required to transition to valid. IfTis the tree OID etherStatsEntry and 157 is the manager‚Äôs chosen index, and the manager wants to monitorifIndex = 6, it could send the following as a single operation. Set(( T.Status.137,createRequest), ( T.DataSource.137,mib-2.2.2.1.1.6), ( T.Owner.137, owner )) at which point the agent will create the row with status underCreation. The value createRequest is used only in manager requests and never appears in actual rows on the agent. The manager will then follow with Set(T.Status.137,valid) If the manager wants or needs to create the entry piecemeal, it can do so as follows: Set(T.Status.137, createRequest) Set(T.DataSource.137, mib-2.2.2.1.1.6) Set(T.Owner.137, owner ). .. Set(T.Status.137, valid) Immediately following the Ô¨Årst Set( T.Status.137, createRequest) the agent will again create the row and mark it asunderCreation, but this time the new row will be missing several columns. TheStatus attribute must be speciÔ¨Åed in the very Ô¨Årst Set() operation for the row. Existing rows may not have their Status set tocreateRequest. The primary legal state transitions are as follows: 682 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 non Existentunder Creationvalid invalidcreateRequest Changed mindNormal row completion Normal termination for more changes, if allowed Transition diagram for EntryStatus. When a row is requested with createRequest, it is actually created as underCreation. In some cases, a row can be edited by the manager by changing the status from valid back to underCreation. However, many speciÔ¨Åc row-creation implementations require that no changes can be made after a row is marked valid. To change an existing row in such a case, the row should be marked invalid and a new row created. These transition rules for createRequest prevent two managers from simultaneously creating rows with the same index. Suppose, for example, that managers M Aand M Beach attempt to create a new row 137 by executing Set(T.Status.137, createRequest) If M A‚Äôs Set() request is the Ô¨Årst to be acted upon, row 137‚Äôs Status becomesunderCreation. Later, when M B‚Äôs Set() request is processed, the row will no longer be nonexistent. So, from the diagram above, MB‚Äôs arrivingcreateRequest is invalid: there is no createRequest arc from any node other than nonexistent. M B‚Äôs Set() operation above, and therefore any of M B‚Äôs other Set() operations, will fail. This strategy is meant to prevent only accidental row-creation conÔ¨Çict; it will not prevent M Bfrom hijacking MA‚Äôs row, or from marking it as invalid (and thus effectively deleting it). But this is generally understood as an inconsequential risk; legitimate managers should be trustable. Any authorized SNMP manager can read all the etherStatsTable records; there is no requirement that only the creator of a row can read that row. It is quite possible for multiple rows to be created all referring to the same interface, egby multiple managers. Although it is not forbidden, there is no reason for one manager to create two rows for the same interface. The four-state EntryStatus type appeared in the original SNMPv1 RFC 1271. Updates to RMON to SNMPv2 have left EntryStatus alone, but SNMPv2 has also introduced the six-state RowStatus type which is more likely to be used by new MIBs. We will turn to this in 27.2.2 SNMPv2 RowStatus. 27.2.1.2 History The history group allows for collection of a series of samples of data from the Statistics group, above, at regular intervals. The group consists of two tables, historyControlTable where the manager creates rows to manage the process, and etherHistoryTable containing the actual data. ThehistoryControlTable contains entries for (again omitting the historyControl preÔ¨Åx from attribute names) DataSource, representing the interface to be examined, and the Status andOwner 27.2 Table Row Creation 683
An Introduction to Computer Networks, Release 2.0.11 attributes as above. As with the etherStatsTable, when the manager creates a new row it speciÔ¨Åes a value for the index, with full name historyControlIndex, again perhaps chosen at random. The manager must also specify two additional attributes: Interval representing the time in seconds between data-collection events, and BucketsRequested representing the requested maximum number of interface records to be kept by the agent. When the agent reaches the maximum, each new record replaces the oldest previous record. The control table contains one agent-created attribute: BucketsGranted indicating the actual maximum number of interface records to be kept. Once the control table row is marked valid, the agent starts accumulating one statistics record for the speciÔ¨Åed interface every Interval seconds. These records are numbered consecutively; the number of a given record is its SampleIndex. The agent keeps only BucketsRequested records, so once SampleIndex reaches that value then, whenever record N is added, record N ‚Äì BucketsRequested is deleted. All this data collected by the agent is stored in etherHistoryTable, which contains columns for all the statistics in etherStatsTable except for the counts of packets in the various size ranges. The table is indexed by the pair of values Index corresponding to etherControlIndex and the record number SampleIndex. As such, this table is the disjoint union of multiple independent series of consecutively numbered records, one series for each Index value in the control table, that is, one series for each manager request. As an example, suppose the manager asked for history statistics to be kept for a given interface at a rate of once a minute, the BucketsGranted is 70, and the control-index value for this request is 491. After one hour, the table contains records with sample indexes 1-60; the full row-index values are x491,1ythrough x491,60y. After one hour and eleven minutes, record x491,71yreplaces record x491,1y. After two hours, records with sample indexes of 51-120 are available. The manager might return once an hour and retrieve the most recent 60 records. A manager might also create twocontrol-table records, one holding 25 records taken at 1-hour intervals and another holding 60 records taken at 1-minute intervals. If all is well, that manager might download the Ô¨Årst table once a day, and entirely ignore the second table. The manager always has available these 1-minute records for the last hour, though, and can access them as needed if a problem arises (perhaps signaled by something else entirely). A manager can easily retrieve only its own rows from the etherHistoryTable. Let Tbe the root of the etherHistoryTable, which has columns 1-15. Suppose again a manager has created its controlTable row with a value for historyControlIndex of 491; the manager can then retrieve the Ô¨Årst of its data rows with the following; note that each OID contains the column number and part of the row index. GetNext( T.1.491, T.2.491,. .. , T.15.491) If the history-table rows associated with 491 have sample-index values ranging from 37 to 66, the above GetNext() will return the row indexed by x491,37y; that is, the values paired with the following OIDs: T.1.491.37, T.2.491.37,. .. , T.15.491.37 Subsequent GetNext()s will return the subsequent rows associated with control entry 491: row x491,38y, rowx491,39y,etc. If theetherHistoryTable had been indexed in the reverse order, with the sample index Ô¨Årst and the historyControlIndex second, a substantial linear search would be necessary to locate the Ô¨Årst row with a given value for the control index. 684 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 27.2.1.3 Hosts The host group allows an agent to keep track of what other hosts ‚Äì in RMON1 identiÔ¨Åed by their Ethernet address ‚Äì are currently active. Like the historyControlTable, thehostControlTable allows a manager to specify DataSource ,Status andOwner; the manager also speciÔ¨Åes TableSize. Once the control-table entry is valid, the agent starts recording hosts in the hostTable, which is indexed by the control-table index and the host Ethernet address. The agent also records each new host‚Äôs CreationOrder value, an integer record number starting at 1. ThehostTable also maintains counters for the following per-host attributes; these are updated whenever the agent sees another packet to or from that host. We will revisit these in the hostTopNTable, following. 
- InPackets 
- OutPackets 
- InOctets 
- OutOctets 
- OutErrors 
- OutBroadcastPkts 
- OutMulticastPkts When the number of host entries for a particular control-table index value exceeds TableSize ‚Äì that is, the new host‚Äôs CreationOrder would exceed TableSize, old entries are removed and all hosts in the table are given updated CreationOrder values. That is, if the table of size three contains entries for hosts A, B and C with creationOrder values of 1, 2 and 3, and host D comes along, then A will be deleted and B, C and D will be given creationOrder values of 1, 2 and 3 respectively. This is quite different from the record-number assignments in the etherHistoryTable, where the SampleIndex record numbers are immutable. A consequence of this is that the CreationOrder values are always contiguous integers starting at 1. Entries are deleted based on order of creation, not order of last update. The hostTable does not even have an attribute representing the time a given host‚Äôs entry (or entries) was last updated. As a convenience, the data in hostTable is also made available in hostTimeTable, but there indexed by the control-table index and the CreationOrder time. This makes for potentially faster lookup; for example, the manager always knows that the record with CreationOrder = 1 is the oldest record. More importantly, this alternative index allows the manager to download the most recent entries in a single step. Whenever a host is deleted because the table is full, the CreationOrder values assigned to other hosts all change, and so the indexing to hostTimeTable changes. Thus, a manager downloading rows from hostTimeTable one at a time must be prepared for the possibility that what had earlier been row 3 is now row 2, and that a host might be duplicated or skipped. To help managers deal with this, the control table has an entry LastDeleteTime representing the time ‚Äì in TimeTicks since startup ‚Äì of the last deletion and thusCreationOrder renumbering. If a manager sees that this value changes, it can, for example, start the data request over from the beginning. 27.2 Table Row Creation 685
An Introduction to Computer Networks, Release 2.0.11 27.2.1.4 Host Top N ThehostTopN table is a report prepared by the agent about the top N entries (where N is manager-supplied) in thehostTable, over an interval of time. The manager speciÔ¨Åes in the control table the Status and Owner attributes, the HostIndex control-table index value from hostControlTable, and also the Duration (in seconds) and the RateBase indicating which of the following hostTable statistics is to be used in the ranking: 
- InPackets 
- OutPackets 
- InOctets 
- OutOctets 
- OutErrors 
- OutBroadcastPkts 
- OutMulticastPkts The value of N is in the attribute RequestedSize; the agent may reduce this and communicates any change (or lack of it) through the attribute GrantedSize. Once the control-table row becomes valid, the agent then starts maintaining counters for all the entries in the part of hostTable indexed by HostIndex, and at the end of the Duration sorts the data and places its top-N results in the hostTopN table. If, during the interval, some hosts were removed from the hostTable because the table was full, the results may be inaccurate. All result statistics are inaccessible until theDuration has elapsed and the particular hostTopN report has run its course, at which point the results become read-only. 27.2.1.5 Matrix The matrix group allows an agent to collect information on trafÔ¨Åc Ô¨Çow indexed by the source and destination addresses. The manager begins the process by supplying attributes for Status ,Owner, the usualIndex, the interface DataSource, and the maximum table size. Once the row is valid, the agent begins collecting, for every xsource,destination ypair, counts of the number of packets, octets and errors. The record for xA,Bycounts these things sent by A; the corresponding record forxB,Aycounts the reverse direction. The actual matrixSDTable is indexed by the manager-supplied Index value and the source and destination Ethernet addresses in that order. A companion table (or view) is also maintained called matrixDSTable, that lists the same information but indexed by destination Ô¨Årst and then source. This view is not present to supply information about the reverse direction; that would be obtained by reversing source and destination in the matrixSDTable. Rather, the matrixDSTable allows a manager to extract all information about a single destination D in a single SNMP tree-walk operation of the preÔ¨Åx matrixDSTable .Index .D. This is similar to the indexing discussion at the end of 27.2.1.2 History. See exercise 7.0. 686 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 27.2.2 SNMPv2 RowStatus RMON made do with four values for its EntryStatus attribute:createRequest ,underCreation, valid andinvalid. SNMPv2 still supports EntryStatus, but the preferred form is the six-value RowStatus ,RFC 2579, with options 
- active: likevalid 
- notReady: the agent knows that the row is not complete 
- notInService: the row is complete, but has not yet been activated 
- createAndGo: likecreateRequest followed by valid 
- createAndWait: likecreateRequest 
- destroy: likeinvalid ThecreateAndGo option allows a manager to make a single new-row request to the agent, and, if successful, the agent will immediately set the row status to active. UsingEntryStatus requires two steps: Ô¨Årst to set the new row to underCreation and then to set it to valid. The manager can ask for any status except notReady. Every row created will, however, be marked at the agent with one of notReady ,notInService oractive. As withEntryStatus, the manager must still choose an index value. Pseudorandom selection remains appropriate in many cases, but the agent may also supply, if the MIB Ô¨Åle supports it, a recommendedNextIndex attribute. Here is a simpliÔ¨Åed RowStatus state-transition diagram. Not all links to destroy, or from a node back to itself, are shown. See RFC 2579, p 9, for more details. non Existentnot Readynot In Serviceactive destroycreateAndWait partial attributes Changed mindrequest active Normal termination for more changes, if allowed Simplified transition diagram for RowStatuscreateAndGo additional Set(), agent knows all req'd attrs now specified createAndWait all required attributesadd'l Set(), partial attributes After the manager issues a createAndWait, the agent Ô¨Ålls in the attributes provided by the manager, and any other default attributes it has available. The manager, if desired, can now Get() the entire row, and Ô¨Ånd out what values are still missing. These values will be reported as noSuchInstance. Alternatively, the manager may simply know that it has more attributes to Set(). 27.2 Table Row Creation 687
An Introduction to Computer Networks, Release 2.0.11 If the agent knows the row is missing attributes necessary for activation, it will set the RowStatus to notReady, otherwise to notInService. AnotInService row can, of course, still have undeÔ¨Åned read-only attributes that the agent will later set after activation. A notInService row can also still have attributes the manager intends to set before activation, but the agent has given default values in the interim. Once the manager has set all the attributes required for activation, it sets the RowStatus attribute to active, and agent activity begins. As withEntryStatus, two managers cannot simultaneously create the same row. If they were to try to do so, the second would be attempting to set RowStatus tocreateAndWait orcreateAndGo for a row entry that already existed, but from the diagram such a transition is not allowed. 27.2.3 PING-MIB The idea behind the ping MIB is to allow a manager to ask an agent to repeatedly ping a target, 10.4 Internet Control Message Protocol, and then to report the success rate. We will use here the IETF ping MIB in RFC 2925 /RFC 4560, and ofÔ¨Åcially titled DISMAN-PING-MIB (DISMAN is short for DIStributed MANagement). An earlier ping MIB is CISCO-PING-MIB.my, now deprecated by Cisco but generally still available on the Internet (2022). Both ping MIBs make use of the RowStatus convention of the previous section. The manager can ping the target itself, ifthere are no Ô¨Årewalls in the way, but the result may likely be different from that obtained by having the agent ping the target. The actual MIB supports multiple types of ping besides the usual ICMP Echo Request, but we will consider only the latter. The DISMAN version has control table pingCtlTable and the results appear in pingResultsTable; in the Cisco version the same table is used for both control and results. The results include the minimum, maximum and average RTT, the number of pings sent and the number of responses. The results table also has an attribute OperStatus to indicate whether the test has stopped. ThepingCtlTable contains a wide range of options for the actual ping request, including options to specify the outgoing IP address for multihomed agents. The more familiar options, however, include 
- pingCtlTargetAddress: and address type, as IPv6 is supported 
- pingCtlDataSize: how big each ping packet is 
- pingCtlTimeout: how long before the agent gives up on any one ping 
- pingCtlProbeCount: the number of pings to be sent 
- pingCtlFrequency: the interval between pings The table also includes pingCtlRowStatus of typeRowStatus, above, and pingCtlOwnerIndex of typeSnmpAdminString, which is a fancier way of identifying the manager than the RMON Owner attributes, and which can include some SNMPv3 credentials as desired. The new row is created, RowStatus is set toactive (perhaps through createAndGo ), and the test begins. By setting the control table‚Äôs RowStatus todestroy, a manager can attempt to halt a ping series in progress. 688 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 27.3 SNMPv3 SNMP version 3 added authentication and encryption to SNMPv2c, but made relatively few other changes except to nomenclature. The original deÔ¨Ånitions are in RFC 3410 through RFC 3415 ;RFC 3410 Ô¨Årst appeared as RFC 2570. SNMPv3 introduced the User Security Model, or USM, in which agents allow manager access only if the manager has presented an appropriate key, derived ultimately from a passphrase. The agent‚Äôs response can be either digitally signed or encrypted, as desired. SNMPv3 did make several terminology changes. Any SNMP node ‚Äì either manager or agent ‚Äì is now known as an SNMP entity. An entity is divided into two parts; the Ô¨Årst is the SNMP engine consisting of the message dispatcher, the message processor, and the security and access-control subsystems. The second part consists of the various SNMP applications, consisting, for agents, primarily of the command responder (responding to Get() andGetNext() ,etc). One goal of this architectural division is to separate the applications from the authentication mechanisms. Another goal, however, is to provide a framework in which future new applications can easily be supported. It is the SNMP engine that must implement all the new security provisions. 27.3.1 What Could Possibly Go Wrong? RFC 3414 analyzes the the risks of inadequate SNMP security, and identiÔ¨Åes two primary and two secondary threats. The primary threats are as follows: 
- Unauthorized modiÔ¨Åcation of information in transit: that is, man-in-the-middle attacks in which A‚Äôs message to B is intercepted and modiÔ¨Åed by an intermediate node M. 
- Masquerade: unauthorized operations by a manager (or agent) masquerading as an authorized entity. Use of guessed or cracked passwords would fall into this category. The secondary threats are these: 
- Message Stream ModiÔ¨Åcation: This includes re-ordering messages, and, perhaps most importantly, replay attacks. For example, even if no keys are ever discovered, an attacker might learn that a certain message causes a certain server to reboot. Retransmitting that message could have serious consequences. 
- Disclosure: reading by an eavesdropper of the management information in transit. All but the last, disclosure, can be addressed by appropriate digital signatures and timestamps; encryption is only needed when disclosure is an active concern. Because disclosure is not always a signiÔ¨Åcant concern, and perhaps because when RFC 2574 was written in 1999 the unlicensed export of encryption technology from the United States was illegal, SNMPv3 deÔ¨Ånes two separate new levels of security: 
- authentication-only, addressing modiÔ¨Åcation, masquerade and replay 
- encryption, addressing the above and disclosure These correspond to three values for the snmpSecurityLevel textual convention (in which ‚Äúpriv‚Äù abbreviates ‚Äúprivacy‚Äù): 
- noAuthNoPriv: no authentication 
- authNoPriv: authentication, but no encryption 27.3 SNMPv3 689
An Introduction to Computer Networks, Release 2.0.11 
- authPriv: encryption of all messages Encryption (privacy) implies authentication; encrypted messages also include the authentication signatures described below. 27.3.2 Cryptographic Fundamentals SNMPv3 authentication is based on cryptographic hash functions: functions which take a data string of arbitrary length and return a Ô¨Åxed-length hash, in such a way that 
- Knowing the hash value sheds no practical light on the original data 
- Given a hash value, there is no feasible way to Ô¨Ånd a message yielding that hash Further details are at 28.6 Secure Hashes. The Internet checksum of 7.4 Error Detection fails as a cryptographic hash, for example, because given a message m and a hash value h1, one can calculate hash(m) = h and then append to m a two-byte string based on h1‚Äìh, yielding a message m1for which hash(m1) = h1. The two cryptographic hash functions originally supported by SNMPv3 in RFC 3414 are MD5, which produces a 128-bit hash, and SHA-1, which produces a 160-bit hash. Since the publication of RFC 3414 in 2002, vulnerabilities in each of these hash functions have been discovered. The SNMP framework in principle allows easy substitution of new hash functions, but RFC 7860, standardizing the use of the SHA-2 family, did not appear even in draft form until 2013. If two parties share a secret key k, the basic hash-based way to sign a message m is to append to it the value hash(m"k), where m"k is the result of appending k to m. An eavesdropper will see m and hash(m"k) but this will provide no information about k, and, similarly, an attacker will, given a message m, not be able to generate the value hash(m"k) without knowing k. Because some hash functions are vulnerable to the so-called ‚Äúlength-extension attack‚Äù when used this way, the actual signature is often slightly more involved (27.3.5 Passwords and Keys and28.6.1 Secure Hashes and Authentication ), but hash(m"k) is a good example of the basic concept. The SNMP encryption mechanism is also based on shared secret keys ( 28.7 Shared-Key Encryption ); that is, public-key encryption is not used. RFC 3414 describes the use of the Data Encryption Standard cipher, or DES, which uses 56-bit keys. Later, RFC 3826 introduced the use of the Advanced Encryption Standard cipher, or AES, which in SNMP users a 128-bit key. (Both DES and AES are discussed brieÔ¨Çy in 28.7.2 Block Ciphers .) DES is, if anything, even more vulnerable than MD5 and SHA-1, due to the limited key length; AES is a much stronger choice. Shared-secret encryption is based, abstractly, on an encrypting function E(p,k) that takes a plaintext message p and a key k and returns the encrypted ciphertext. Similarly, there is a decrypting function D(c,k) that takes an encrypted ciphertext message c and the key k and returns the original plaintext. 27.3.3 SNMPv3 Engines The SNMPv3 engine is the component of an SNMP entity charged with ensuring security. Each SNMPv3 engine ‚Äì manager or agent ‚Äì has an identiÔ¨Åer known as the snmpEngineID. This is a string that, by default, incorporates the node‚Äôs IP or Ethernet address and additional standard information. While it is common 690 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 to leave this default setting unchanged, it is also possible to replace it using a site-based naming scheme, perhaps including the organization name and a locally assigned serial number; see RFC 3411. In any SNMPv3 exchange, one SNMPv3 engine is designated authoritative. For aGet() orSet() request, or any other request that requires a response, it is always the agent that is authoritative. It is the job of the authoritative engine to validate the message received from the other engine. For either authentication-only or encrypting security levels, the nonauthoritative engine must present a pair consisting of the following: 
- userName: a human-readable string identifying a person or manager-computer 
- authParameters: a string holding data speciÔ¨Åc to the authentication/encryption mechanism in use that serves to authenticate the user. This might be thought of as a passphrase, except one goal is never to send passphrases in the clear. The authoritative engine must keep a table of all the userName values it recognizes, and, for each, the appropriate key with which to validate the user. Any SNMPv3 engine also keeps track of two 32-bit attributes 
- snmpEngineTime: the number of seconds since the engine last rebooted 
- snmpEngineBoots: the number of times the engine has rebooted The pair of these, which we will abbreviate as xTime,Bootsy, uniquely identiÔ¨Åes a point in time for an entity (to the nearest second). They are used to prevent replay attacks; two messages sent more than one second apart will never have the same xTime,Bootsytimestamp. Note that keeping track of snmpEngineBoots implies that the entity have some form of persistent storage. When a nonauthoritative engine (for our purposes, the manager) wants to send a request to an authoritative engine (the agent), the Ô¨Årst step is to Ô¨Ånd out the agent‚Äôs snmpEngineID and then its current xTime,Bootsyvalue. This is done through an initial two-step discovery process. The Ô¨Årst request contains an empty varBindList, an empty engineID, a username of the empty string and a security level of noAuthNoPriv. The authoritative side sends a response containing its engineID. The second step is to send a request, again with an empty varBindList but now containing a valid xusername,keyypair. The value forxTime,Bootsyisx0,0y. The authoritative engine now responds with a message including its actual xTime,Bootsy. 27.3.4 Message Authentication After discovering the authoritative engine‚Äôs xTime,Bootsy, the nonauthoritative engine stores the authoritative engine‚Äôs snmpEngineBoots and also the difference between the authoritative engine‚Äôs snmpEngineTime and its own clock time. It can now use this difference to approximate the authoritative engine‚Äôs snmpEngineTime at any point in the future. Every message from the nonauthoritative engine will include its estimate of the authoritative engine‚Äôs current xTime,Bootsyvalue. Relative drift between the two engines‚Äô clocks will eventually mean this estimate fails, but, as we shall see, it can be expected to be close enough for quite a while. The authoritative engine accepts a non-empty request only if all three of the following hold, where Time and Boots are the values submitted by the nonauthoritative engine and snmpEngineTime and 27.3 SNMPv3 691
An Introduction to Computer Networks, Release 2.0.11 snmpEngineBoots are the authoritative engine‚Äôs own values: 
- Boots =snmpEngineBoots 
- snmpEngineBoots < 231‚Äì 1 
- Time and snmpEngineTime differ by less than 150 seconds The maximum allowable clock drift, in other words, is 150 seconds. If the two clocks drift by more than that, the nonauthoritative side must again go through the synchronization process outlined at the end of the previous section. The actual message signing is based on a shared secret key, authKey, negotiated previously between user and agent. Because the key is usually speciÔ¨Åc to the agent, it is sometimes called a local key,kl. 27.3.5 Passwords and Keys Users are identiÔ¨Åed by userName values, and also (usually) have human-readable passwords. These passwords must be converted Ô¨Årst into keys, in such a way that each xuser,agentypair has its own unique key. We will start with a mechanism commonly used for authentication-only security; it is similar for both the MD5 and SHA-1 hashes. We will denote the hash function (either MD5 or SHA-1) by hash(). The Ô¨Årst step is to create a digest based on the password. It would sufÔ¨Åce in principle to set the digest to hash(password). In order to make the password √ëkey conversion process relatively slow, however, so as to hamper brute-force attacks, Appendix A of RFC 3414 recommends repeating the password (logically, at least) to Ô¨Åll a buffer 220bytes in length. The hash() function is then applied to this megabyte string. (For another perspective on intentionally slow password operations, see 28.6.2 Password Hashes .) The next step is to take the engineID of the agent for which the user is generating this particular key and take the hash of the concatenation of the digest, engineID and digest again: kl= hash(digest"engineID"digest) This is now the xuser,agentyshared key, or local key. It must be entered (or computed) on the agent, and stored there. For MD5 it is 16 bytes long; for SHA-1 it is 20 bytes long. This mechanism is, strictly speaking, optional; an agent does not know how manager keys were generated, and thus cannot enforce any particular mechanism when a manager‚Äôs key is later changed as below. Any secure way to generate a unique key klfor each user and each agent would be sufÔ¨Åcient. The mechanism here, though, has the advantage that any manager node can compute a user‚Äôs local key directly from the password and the agent engineID; no keys need be stored by the manager. This allows a manager to use one password for multiple agents; compromise of any one agent and its attendant local key klshould not affect the security of other agents or of the original password. 27.3.6 Message Signing An SNMP message mesg is now signed by a local key klusing the Hash Message Authentication Code, or HMAC ( RFC 2104 ), as follows: 
- klis extended to a 64-byte ekby padding with zeroes. 
- A constant string ipad is formed by repeating the octet 0x36 64 times. 692 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 
- A constant string opad is formed by repeating 0x5c 64 times. 
- Set k1=ekXOR ipad. - Set k2=ekXOR opad. TheauthParameter Ô¨Åeld ‚Äì the actual digital signature ‚Äì is now set to be the Ô¨Årst 12 bytes of hash( k2"hash( k1"mesg )) This is slightly more complicated than the hash( mesg"kl) mechanism suggested in 27.3.2 Cryptographic Fundamentals, in order to make it more resistant to potential vulnerabilities in the hash() function; see 28.6.1 Secure Hashes and Authentication. The receiver can replicate this authParameter calculation and verify that it matches the value transmitted in the packet. 27.3.7 Key Changes Suppose a user wants to change his or her password, and thus the key k. The manager will need to communicate the new key to the agent in such a way that it is not exposed to eavesdroppers. Here is the mechanism, where, again, hash() is either MD5 or SHA-1 as appropriate; we will use N to denote the length in bytes of the result of hash() (either 16 or 20). The original key is here denoted oldkey and the new key is newkey. The manager Ô¨Årst chooses a string random of N bytes chosen as randomly as possible. A second N-byte stringdelta is then calculated as follows: temp =hash(oldkey"random) delta =temp XORnewkey At this point, random anddelta are sent ‚Äì in the clear ‚Äì from the manager to the agent. The agent can userandom andoldkey to compute temp, and thusnewkey =delta XORtemp. An eavesdropper cannot use random to Ô¨Ånd out anything about temp without knowing oldkey, and cannot get anything useful out of delta unless either newkey ortemp is known. The actual process is to combine random anddelta into a single 2N-byte keyChange string, written to one of the key-change columns of usmUserTable ,27.3.9.1 The usmUserTable. Note that if an eavesdropper saves random anddelta, and later discovers the user‚Äôs oldkey, then newkey can be calculated easily. Using the language of 29.2 Forward Secrecy, forward secrecy fails badly. However, if an eavesdropper later discovers newkey, there is no obvious way to Ô¨Ånd oldkey; see exercise 8.0. To improve forward secrecy, and to simplify mass key generation generally, RFC 2786 outlines an experimental mechanism to use DifÔ¨Åe-Hellman-Merkle key exchange ( 28.8 DifÔ¨Åe-Hellman-Merkle Exchange ) to create and change keys, instead of the process above. 27.3.8 Creating Additional Users Suppose we want to add user ‚Äúalice‚Äù to an agent, together with Alice‚Äôs local key kl. The problem is that there is no pre-existing shared key, and so no way to transmit klto the agent via SNMP without risk of 27.3 SNMPv3 693
An Introduction to Computer Networks, Release 2.0.11 eavesdropping. One solution is to require out-of-band account creation, that is, for Alice to log on to the agent via, perhaps, an ssh or direct serial-line connection, and enter there her name and password. This works, but is awkward on a large network. The usual way is to create an account for Alice by cloning some other account on the agent. We will assume that an initial account has been created as in 27.3.5 Passwords and Keys for user ‚Äúmaster‚Äù. Cloning the account just requires speciÔ¨Åcation of the names ‚Äúmaster‚Äù and ‚Äúalice‚Äù; no keys need be sent. The second step is to update the password for the new ‚Äúalice‚Äù account, which is done via the key-change mechanism of the previous section. We look at the details of the cloning procedure in the next section. 27.3.9 VACM for SNMPv3 For SNMPv1 and SNMPv2c, V ACM created associations between community strings and security groups, an then between security groups and permitted views of the OID tree. For SNMPv3, user names replace community strings. There are three primary V ACM tables, deÔ¨Åned in RFC 3415. The Ô¨Årst is vacmSecurityToGroupTable, indexed by the ‚Äúsecurity model‚Äù and the ‚Äúsecurity name‚Äù. For SNMPv3 the model is 3 and the security name is the username; the SNMPv3 rows of the table are thus effectively indexed by the username. It is not unreasonable to have a separate security group for each user, eg‚Äúgrppld‚Äù for user ‚Äúpld‚Äù. The second table is vacmAccessTable, indexed by security group, security model (community or SNMPv3) and security level ( egauthNoPriv ). The table can be used to look up a named view for each of ‚Äúread‚Äù and ‚Äúwrite‚Äù privileges (also ‚Äútrap‚Äù privileges, if desired). (This table is also indexed by ‚ÄúcontextName‚Äù, but often there is only one contextName, the default.) Finally, the named views themselves are stored in vacmViewTreeFamilyTable, indexed by view name and an OID preÔ¨Åx representing an OID subtree. A given view vconsists of all the OID preÔ¨Åxes opfor which xv,opyappears in the table. Some preÔ¨Åxes may have an associated ‚Äúmask‚Äù, typically to allow access to a designated table row, and some preÔ¨Åxes may represent exclusion of that subtree from the view. Typical view names are ‚Äú_all_‚Äù, ‚Äú_none_‚Äù and ‚Äúsystemonly‚Äù; we created a view ‚Äúmib2+private‚Äù in 26.11 SNMPv1 communities and security. 27.3.9.1 The usmUserTable Now it is time to look at the actual table in which agents store user names and their associated authentication credentials, usmUserTable. It has the attributes below, which should all be preÔ¨Åxed by usmUser; the index attributes are the Ô¨Årst two, EngineID andName. We will consider only those attributes related to authentication-only security. 694 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 EngineID usually the agent‚Äôs own engineID Name eg‚Äúalice‚Äù SecurityName usually the same as Name CloneFrom an indication of the row this entry is cloned from AuthProtocol MD5 or SHA-1 AuthKeyChange thexrandom, deltaykeyChange object of 27.3.7 Key Changes OwnAuthKeyChange the same but with different permissions; see below PrivProtocol used for encryption PrivKeyChange used for encryption OwnPrivKeyChange used for encryption Public used for conÔ¨Årming key changes; see below StorageType hopefullypermanent, meaning values persist between reboots Status of typeRowStatus, for row insertion One can think of the table as also having an implicit authKey column, representing the local key corresponding to Name, that is never directly readable or writable. RFC 3414 states Ô¨Çatly ‚Äúthe authKey is not accessible via SNMP.‚Äù However, the agent must still keep the authKey somewhere, tied to the Name, so it can validate a given user based on the Name andauthParameter supplied in a request. BecauseEngineID is usually the agent‚Äôs own EngineID, the table is de facto indexed just by Name. Recall that, as was discussed in 27.1.4 SNMPv2 Indexes, the index attributes, EngineID andName, will not be directly accessible, but will be encoded in the OID associated with every other retrieved attribute. The username ‚Äúalice‚Äù will thus be encoded, using the ASCII string encoding, as 97.108.105.99.101, not necessarily easily readable by human managers. We are now in a position explain the cloning process and the key-change process as they play out with this table. A speciÔ¨Åc semantic rule for this table is that the use of Set() to assign axrandom, deltaykeyChange object toAuthKeyChange orOwnAuthKeyChange causes that user‚Äôs hidden authKey to be updated via the process of 27.3.7 Key Changes. The user cannot conÔ¨Årm directly that this change succeeded, as a read of these keyChange attributes returns the empty string, so the usual recommended strategy is also to write a random value to the Public attribute; because Set() operations are atomic, a change to the latter means the former change succeeded as well. Because it is possible for multiple managers to be updating the table simultaneously, a single TestAndIncr object named usmUserSpinLock may be used to enforce serialization, as in 27.1.5 TestAndIncr. So the full recommended sequence for updating a key is as follows, where keyChange is the xrandom, deltaykeyChange object and index encodes the EngineID and theName: rand =random value chosen by the manager val:= Get(usmUserSpinLock) Set((usmUserSpinLock ,val), (AuthKeyChange .index ,keyChange ), (Public .index, rand )) This is repeated as necessary until Get(Public) returns rand, indicating that the Set() operations all succeeded. 27.3 SNMPv3 695
An Introduction to Computer Networks, Release 2.0.11 The above applies for a key change being done by someone with write privileges to the AuthKeyChange column ofusmUserTable. An alternative to granting ordinary users write access to the AuthKeyChange column is to have them use the column OwnAuthKeyChange instead. Any user may attempt to write to this column, but the write will only succeed if the userName by which the request is authenticated is equal to the Name representing the index for this particular keyChange. In other words, anyone with write access to the OwnAuthKeyChange column can change his or her own key, and only his or her own key. Write access to this column must still be granted, however. As we will see below in 27.3.9.2.1 Cloning in Net-SNMP, this security option is of relatively little practical signiÔ¨Åcance. To clone a new user from an existing one, the Ô¨Årst step is to choose the row to be cloned, represented by the OID of an index value. Call this cloneRow, and let index again encode the EngineID andName: val:= Get(usmUserSpinLock) Set((usmUserSpinLock ,val), (CloneFrom .index ,cloneRow ), (Status .index, createAndWait )) Typically the key change is then executed before changing the Status toactive, though this is not required. 27.3.9.2 Creating Accounts in Net-SNMP To create an initial account, the conÔ¨Åguration Ô¨Åles are used. On the author‚Äôs installation of Net-SNMP version 5.7.x (in 2016), the administratively written conÔ¨Åguration Ô¨Åle is /etc/snmp/snmpd.conf and the systemwritten conÔ¨Åguration Ô¨Åle is /var/lib/snmp/snmpd.conf; the latter contains snmpEngineBoots and other persistent SNMP data. The following line goes in this second Ô¨Åle, with SNMP shut down (some earlier versions of Net-SNMP required that it be placed in /var/net-snmp/snmpd.conf). Its effect is to create an SNMPv3 user named ‚Äúmaster‚Äù with MD5-authentication password ‚Äúsaskatchewan‚Äù. createUser master MD5 saskatchewan When SNMP is then started, the line above is replaced by something like the following (on a single line) in /var/lib/snmp/snmpd.conf: usmUser 1 3 0x80001f8880889cb038b1aca650 "master" "master" NULL \ .1.3.6.1.6.3.10.1.1.2 0x7293f49a82fc950f5c344efd94dbb7db .1.3.6.1.6.3.10.1.2.1 0x 0x The new localized key is 0x7293f49a82fc950f5c344efd94dbb7db; the Ô¨Årst hex string beginning 0x80001 is the engineID. For this new account to be authorized to do anything, we must also add the following permissions entry to /etc/snmp/snmpd.conf: rwuser master The effect of this entry is to create an entry (on SNMP restart) in vacmSecurityToGroupTable associating user ‚Äúmaster‚Äù with its own security group (Net-SNMP names it ‚Äúgrpmaster‚Äù), and then an entry in thevacmAccessTable granting this new group SNMPv3 read and write access to the entire OID tree. 696 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 (In general we could also have used ‚Äúrouser‚Äù, except that we will need ‚Äúmaster‚Äù to be able to create new users.) After permissions (at least read permissions) are enabled, the following snmpget should work snmpget -v 3 -u master -l authNoPriv -a MD5 -A saskatchewan localhost 1.3.6.1.2.1.1.4.0 The hostname is ‚Äúlocalhost‚Äù if this is being run on the same machine that the Net-SNMP agent is running on; it can also of course be run remotely. We can make this a little shorter by editing the Net-SNMP per-user manager conÔ¨Åguration Ô¨Åle $HOME/.snmp/snmp.conf to add the following lines: defSecurityName master defAuthType MD5 defSecurityLevel authNoPriv defAuthPassphrase saskatchewan Now the snmpget command can be shortened to snmpget -v 3 localhost 1.3.6.1.2.1.1.4.0 If we take the password, ‚Äúsaskatchewan‚Äù, and repeat it to 220bytes, the MD5 checksum is 0x3da9dbfc3a78acb675e436746e1f4f8a; this is the ‚Äúdigest‚Äù of 27.3.5 Passwords and Keys. From the /var/lib/snmp/snmpd.conf Ô¨Åle (or from the created usmUser entry above) we Ô¨Ånd the engineID is 0x80001f8880889cb038b1aca650. If we convert these two strings to binary data, concatenate them as digest"engineID"digest, and take the MD5 checksum, we indeed get 7293f49a82fc950f5c344efd94dbb7db which is exactly the key entry in the /var/lib/snmp/snmpd.conf Ô¨Åle. 27.3.9.2.1 Cloning in Net-SNMP We can now clone this account while SNMP is running, using the snmpusm command-line utility. The following line creates an account ‚Äúpld‚Äù cloned from ‚Äúmaster‚Äù, using the master account (and assuming that master is an rwuser and not an rouser); we have abbreviated by auth the full credentials -u master -l authNoPriv -a MD5 -A saskatchewan. We have switched from ‚Äúlocalhost‚Äù to HOST to emphasize that this can be run remotely. snmpusm -v3 auth HOST create pld master We then change the password to ‚Äúramblers‚Äù, still using the authority of user ‚Äúmaster‚Äù; the-Ca option tells snmpusm to change only the authentication key. snmpusm -v3 auth-Ca HOSTpasswd saskatchewan ramblers pld But before new user pld can do anything, we must grant access. We canagain edit /etc/snmp/snmpd.conf on the machine running the Net-SNMP agent, adding the directive rouser pld (granting read-only access this time) and then restarting SNMP. We can also, however, manipulate the V ACM tables of 27.3.9 VACM for SNMPv3 using the Net-SNMP snmpvacm command, which works remotely: snmpvacm -v3 auth HOST createSec2Group 3 pld grppld 27.3 SNMPv3 697
An Introduction to Computer Networks, Release 2.0.11 snmpvacm -v3 auth HOST createAccess grppld 3 2 0 _all_ _none_ _none_ These two commands create entries in the vacmSecurityToGroupTable and the vacmAccessTable respectively. The parameters following grppld in the second command include the USM security model (3), the authNoPriv security level (2), a placeholder (0) for the contextmatch Ô¨Çag which we are not using, and the views _all_ and_none_ predeÔ¨Åned by Net-SNMP. The command here givesgrppld read access to everything, and write access to nothing (the second _none_ denies access for creation of notiÔ¨Åcations such as traps). After all this, user pld can then issue an snmpget using pld‚Äôs own credentials with snmpget -v 3 -u pld -l authNoPriv -a MD5 -A ramblers HOST 1.3.6.1.2.1.1.4.0 If user pld is to be able to change pld‚Äôs password (key), write-access must be granted at least to the usmUserAuthKeyChange column ofusmUserTable. We do this by adding the appropriate OID to the write view for user pld. Views are maintained in the vacmViewTreeFamilyTable as an association between the view name and a list of OIDs. The Ô¨Årst step is to add the usmUserAuthKeyChange column, 1.3.6.1.6.3.15.1.2.2.1.6, to user pld‚Äôs view. The command below can be applied alone, so that the usmUserAuthKeyChange column is the only object to which user pld can write, or as part of a series of createView statements adding a series of OID subtrees to pldwriteview. (An OID can be removed from a view with the deleteView option.) In the following commands we are still using the initial master account for auth. snmpvacm -v3 auth HOST createView pldwriteview 1.3.6.1.6.3.15.1.2.2.1.6 Now we apply this new view, pldwriteview, to pld‚Äôs group, grppld. We must Ô¨Årst clear the previously granted access. snmpvacm -v3 auth HOST deleteAccess grppld 3 2 snmpvacm -v3 auth HOST createAccess grppld 3 2 0 _all_ pldwriteview _none_ The password can now be changed from ‚Äúramblers‚Äù to ‚Äúignatius‚Äù as follows: snmpusm -v3 -u pld -l authNoPriv -a MD5 -A ramblers -Ca HOST passwd ramblers ignatius pld We granted pld access here to the entire usmUserAuthKeyChange column. In principle this might be risky, as it allows user pld to write to the key-changing column for anyuser. One potential approach here is to grant user pld access only to pld‚Äôs own row in usmUserAuthKeyChange; this has the following rather cumbersome OID (in which the last four levels 3.112.108.100 spell out the three-byte string ‚Äúpld‚Äù in ASCII): 1.3.6.1.6.3.15.1.2.2.1.6.17.128.0.31.136.128.22.41.105.105.47.232.188.86.0.0. √£√ë0.0.3.112.108.100 Another approach might be to grant pld write access to the usmUserOwnAuthKeyChange column. The semantics of this column, outlined above in 27.3.9.1 The usmUserTable, are such that key-change requests 698 27 SNMP versions 2 and 3
An Introduction to Computer Networks, Release 2.0.11 are accepted only for the user who signed the request. This would prevent user pld from even attempting to change anyone else‚Äôs password. But the risk of full usmUserAuthKeyChange access is minimal: a request authenticated by user pld can only change the password of another user, say bob, if bob‚Äôs previous password is known to pld, as keychange objects must incorporate the old key/password. But if that password is known, then the password can just as easily be changed by authenticating the request as user bob rather than as user pld. In any event, the Net-SNMP snmpusm command does not support writing to usmUserOwnAuthKeyChange. The command snmpusm does not support making general modiÔ¨Åcations; if a USM entry here is made incorrectly, it may be necessary to delete and re-create it. 27.4 Exercises 1.0. Recall that GetBulk() acts like a repeated GetNext(). Why is there no GetBulk() equivalent ofGet() ? 2.0. What happens if we have a three-row, three-column table and ask, using GetBulk(), for the Ô¨Årst two columns with a repetition count of four? What is retrieved? Assume entries are of the form T.col.row, for col and row each ranging from 1 to 3. 3.0. Suppose, as in exercise 4 of the previous chapter, you want an SNMP table identTable to hold xidNum,userName ypairs, where idnum is to be an INTEGER and username an OCTET STRING. The INDEX is idnum. Give ASN.1 deÔ¨Ånitions for the following: 
- identTable 
- identEntry 
- IdentEntry 
- idNum 
- userName This time, follow the SNMPv2 convention of notincluding the index column in the actual table data, 27.1.4 SNMPv2 Indexes. 4.0. (a) What can an SNMP agent do to detect that a manager has created a row in state underCreation and then crashed, leaving the row abandoned? (b). What can an SNMP agent do to detect that a manager has created a valid row, and has later crashed? 5.0. In the RMON hostTopN table ( 27.2.1.4 Host Top N ), the agent does all the report-building work. Would it be possible for the manager to do this? If not, why not? If so, why do you think RMON provides this table? 6.0. In the RMON matrixSDTable (27.2.1.5 Matrix ), the table constructed collects data from only a single interface on the agent (that interface speciÔ¨Åed by DataSource ). Could we get additional Matrix information by considering other interfaces? Why or why not? 7.0. The RMON matrix group ( 27.2.1.5 Matrix ) provides two tables with the same information, matrixSDTable andmatrixDSTable. 27.4 Exercises 699
An Introduction to Computer Networks, Release 2.0.11 (a). Suppose we want all the table data about a given destination D, and have only the matrixSDTable. Explain why every row of matrixSDTable would need to be examined. (b). Now suppose we repeat the investigation of (a), but this time the manager has previously downloaded the complete list of all hosts on the LAN by using the RMON hosts tables. If N is the number of hosts on the LAN, explain how to Ô¨Ånd all hosts that communicated with D using only N retrieval requests. (c). Explain how to Ô¨Ånd all hosts S that sent packets to D even more quickly using the matrixDSTable. If M¬§N is the number of such S, your answer should involve N+1 retrieval requests. 8.0. In the keychange operation of 27.3.7 Key Changes, suppose the manager simply transmitted delta2 =oldkey XORnewkey to the agent. (a). Suppose an eavesdropper discovers delta2 and also knows a few bits of oldkey. What can the eavesdropper learn about newkey ? Would the same vulnerability apply to the mechanism of 27.3.7 Key Changes ? (b). Suppose an eavesdropper later discovers newkey. Explain how to recover oldkey, and why this does not work when the mechanism of 27.3.7 Key Changes is used. 9.0. List the OID preÔ¨Åxes for which a manager would need to be granted write permission if the manager were to be able to modify all settings in .1.3.6.1.2.1 and .1.3.6.1.4.1, and change their own local key, but nothave access to columns in usmUserTable that would allow modiÔ¨Åcation of other manager accounts. (The V ACM table has an ‚Äúexception‚Äù option to make this easier). 10.0. Suppose a manager has write permission only for the usmUserOwnAuthKeyChange column in usmUserTable, which allows change of only that manager‚Äôs password. However, the manager has full write access to the V ACM tables. Explain how the manager can modify the local keys of other managers. 11.0. Use Wireshark to monitor localhost trafÔ¨Åc while you use the Net-SNMP snmpusm command to change a manager‚Äôs own local key. Does the command use the usmUserAuthKeyChange column or the usmUserOwnAuthKeyChange column? 700 27 SNMP versions 2 and 3
28 SECURITY How do we keep intruders out of our computers? How do we keep them from impersonating us, or from listening in to our conversations or downloading our data? Computer security problems are in the news on almost a daily basis. In this chapter and the next we take a look at just a few of the issues involved in building secure networks. This chapter focuses on general principles and on secret-key approaches to authentication and encryption; the next chapter addresses public-key encryption. For our limited overview here, we will divide attacks into three categories: 1. Attacks that execute the intruder‚Äôs code on the target computer 2. Attacks that extract data from the target, without code injection 3. Eavesdropping on or interfering with computer-to-computer communications The Ô¨Årst category is arguably the most serious; this usually amounts to a complete takeover, though occasionally the attacker‚Äôs code is limited by operating-system privileges. A computer taken over this way is sometimes said to have been ‚Äúowned‚Äù. We discuss these attacks below in 28.1 Code-Execution Intrusion. Perhaps the simplest form of such an attack is through stolen or guessed passwords to a system that offers remote login to command-shell accounts. More technical forms of attack may involve a virus, a buffer overÔ¨Çow ( 28.2 Stack Buffer OverÔ¨Çow and28.3 Heap Buffer OverÔ¨Çow ), a protocol Ô¨Çaw ( 28.1.2 Christmas Day Attack ), or some other software Ô¨Çaw ( 28.1.1 The Morris Worm ). In the second category are intrusions that do not involve remote code execution; a server application may be manipulated to give up data in ways that its designers did not foresee. For example, in 2008 David Kernell gained access to the Yahoo email account of then-vice-presidential candidate Sarah Palin, by guessing or looking up the answers to the forgotten-password security questions for the account. One question was Palin‚Äôs birthdate. Another was ‚Äúwhere did you meet your spouse?‚Äù, which, after some research and trial-and-error, Kernell was able to guess was ‚ÄúWasilla High‚Äù; Palin grew up in and was at one point mayor of Wasilla, Alaska. Much has been made since of the idea that the answers to many security questions can be found on social-networking sites. As a second example in this category, in 2010 Andrew ‚Äúweev‚Äù Auernheimer and Daniel Spitler were charged in the ‚ÄúAT&T iPad hack‚Äù. IPad owners who signed up for network service with AT&T had their iPad‚Äôs ICCID recorded along with their email address. If one of these owners later used the View Account option within the iPad‚Äôs Settings applet, an HTTP GET request was sent to an AT&T server containing the ICC-ID; the server would then return the corresponding email address which was then displayed within the applet. Users would still need to enter a password to actually log in. The same HTTP transaction could be performed in a full-Ô¨Çedged browser (or curl): supply an ICC-ID, and get back the email address. If a randomly selected ICC-ID were presented to the AT&T site that happened to match a real account, that user‚Äôs email address would be returned. ICC-ID strings contain 20+ decimal digits, but the individual-device portion of the identiÔ¨Åer is much smaller. This attack yielded email addresses of 114,000 accounts, though the accounts themselves were not compromised. This attack superÔ¨Åcially resembles the brute-force guessing of a password, except that the success rate was vastly higher. Perhaps one in ten guessed ICC-IDs were valid, while guessing passwords of middling 701
An Introduction to Computer Networks, Release 2.0.11 strength might take on the order of a million more tries. Because of this, Auernheimer‚Äôs defense team described his actions as ‚Äúwalking through an open door‚Äù. Auernheimer was convicted for this ‚Äúintrusion‚Äù in November 2012, but his sentence was set aside on appeal in April 2014. Auernheimer‚Äôs conviction remains controversial as the AT&T site never requested a password in the usual sense, though the site certainly released information not intended by its designers. Finally, the third category here includes any form of eavesdropping. If the password for a login-shell account is obtained this way, a Ô¨Årst-category attack may follow. The usual approach to prevention is the use of encryption. Encryption is closely related to secure authentication; encryption and authentication are addressed below in 28.6 Secure Hashes through 29.5 SSH and TLS. Encryption does not always work as desired. In 2006 intruders made off with 40 million credit-card records from TJX Corp by breaking the WEP Wi-Fi encryption ( 28.7.7 Wi-Fi WEP Encryption Failure ) used by the company, and thus gaining access to account credentials and to Ô¨Åle servers. Albert Gonzalez pleaded guilty to this intrusion in 2009. This was the largest retail credit-card breach until the Target hack of late 2013. 28.1 Code-Execution Intrusion The most serious intrusions are usually those in which a vulnerability allows the attacker to run executable code on the target system. The classic computer virus is broadly of this form, though usually without a network vulnerability: the user is tricked ‚Äì often involving some form of social engineering ‚Äì into running the attacker‚Äôs program on the target computer; the program then makes itself at home more or less permanently. In one form of this attack, the user receives a Ô¨Åle interesting_picture.jpg.exe orIRS_deficiency_notice. pdf.exe. The attack is made slightly easier by the default setting in Windows of not displaying the Ô¨Ånal Ô¨Åle extension .exe. Early viruses had to be completely self-contained, but, for networked targets, once an attacker is able to run some small initial executable then that program can in turn download additional malware. The target can also be further controlled via the network. The reach of an executable-code intrusion may be limited by privileges on the target operating system; if I am operating a browser on my computer as user ‚Äúpld‚Äù and an intruder takes advantage of a Ô¨Çaw in that browser, then the intruder‚Äôs code will also run as ‚Äúpld‚Äù and not as ‚Äúroot‚Äù or ‚ÄúAdministrator‚Äù. This may prevent the intruder from rewriting my kernel, though that is small comfort to me if my Ô¨Åles are encrypted and held for ransom. On servers, it is standard practice to run network services with the minimum privileges practical, though see 28.2.3 Defenses Against Buffer OverÔ¨Çows. Exactly what is ‚Äúexecutable code‚Äù is surprisingly hard to state. Scripting languages usually qualify. In 2000, the ILOVEYOU virus began spreading on Windows systems; users received a Ô¨Åle LOVE-LETTER.TXT. vbs (often with an enticing Subject: line such as ‚Äúlove letter for you‚Äù). The .vbs extension, again not displayed by default, meant that when the Ô¨Åle was opened it was automatically run as a visual basicscript. The ILOVEYOU virus was later attributed to Reonel Ramones and Onel de Guzman of the Philippines, though they were never prosecuted. The year before, the Melissa virus spread as an emailed Microsoft Word attachment; the executable component was a Word macro. 702 28 Security
An Introduction to Computer Networks, Release 2.0.11 Under Windows, a number of conÔ¨Åguration-Ô¨Åle formats are effectively executable; among these are the program-information-Ô¨Åle format .PIF and the screen-saver format .SCR. 28.1.1 The Morris Worm The classic Morris Worm was launched on the infant Internet in 1988 by Robert Tappan Morris. Once one machine was infected, it would launch attacks against other machines, either on the same LAN or far away. The worm used a number of techniques, including taking advantage of implementation Ô¨Çaws via stack buffer overÔ¨Çows ( 28.2 Stack Buffer OverÔ¨Çow ). Two of the worm‚Äôs techniques, however, had nothing to do with code injection. One worm module contained a dictionary of popular passwords that were used to try against various likely system accounts. Another module relied on a different kind of implementation vulnerability: a (broken) diagnostic feature of the sendmail email server. Someone could connect to the sendmail TCP port 25 and send the command WIZ <password>; that person would then get a shell and be able to execute arbitrary commands. It was the intent to require a legitimate sendmail -speciÔ¨Åc password, but an error in sendmail ‚Äôs frozen-conÔ¨Åguration-Ô¨Åle processing meant that an empty password often worked. 28.1.2 Christmas Day Attack The 1994 ‚ÄúChristmas day attack‚Äù ( 18.3.1 ISNs and spooÔ¨Ång ) used a TCP protocol weakness combined with a common computer-trust arrangement to gain privileged login access to several computers at UCSD. Implementations can be Ô¨Åxed immediately, once the problem is understood, but protocol changes require considerable negotiation and review. The so-called ‚Äúrlogin‚Äù trust arrangement meant that computer A might be conÔ¨Ågured to trust requests for remote-command execution from computer B, often on the same subnet. But the ISN-spooÔ¨Ång attack meant that an attacker M could send a command request to A that would appear to come from the trusted peer B, at least until it was too late. The command might be as simple as ‚Äúopen up a shell connection to M‚Äù. At some point the spoofed connection would fail, but by then the harmful command would have been executed. The only Ô¨Åx is to stop using rlogin. (Ironically, the ISN spooÔ¨Ång attack was discovered by Morris but was not used in the Morris worm above; see [RTM85].) Note that, as with the sendmail WIZ attack of the Morris worm, this attack did not involve network delivery of an executable fragment (a ‚Äúshellcode‚Äù). 28.2 Stack Buffer OverÔ¨Çow The stack buffer overÔ¨Çow is perhaps the classic way for an attacker to execute a short piece of machine code on a remote machine, thus compromising it. Such attacks are always due to an implementation Ô¨Çaw. A server application reads attacker-supplied data into a buffer, buf, of length buflen. Due to the Ô¨Çaw, however, the server reads more than buflen bytes of data, and the additional data is written into memory past the end of buf, corrupting memory. In the C language, there is no bounds checking with native arrays, and so such an overÔ¨Çow is not detected at the time it occurs. See [AO96] for early examples. In most memory layouts, the stack grows downwards; that is, a function call creates a new stack frame with a numerically lower address. Array indexing, however, grows upwards: buf[i+1] is at a higher address 28.2 Stack Buffer OverÔ¨Çow 703
An Introduction to Computer Networks, Release 2.0.11 thanbuf[i]. As a consequence, overwriting the buffer allows rewriting the most recent return address on the stack. A common goal for the attacker is to supply an overÔ¨Çowing buffer that does two things: 1. it includes a shellcode - a small snippet of machine code that, when executed, does something bad (traditionally but not necessarily by starting a shell with which the attacker can invoke arbitrary commands). 2. it overwrites the stack return address so that, when the current function exits, control is returned not to the caller but to the supplied shellcode. In the diagram below, the left side shows the normal layout: the current stack frame has a buffer into which the attacker‚Äôs message is read. When the current function exits, control is returned to the address stored in return_address. The right side shows the result after the attacker has written shellcode to the buffer, and, by overÔ¨Çow, has also overwritten the return_address Ô¨Åeld on the stack so that it now points to the shellcode. When the function exits, control will be passed to the code at return_address, that is, to the shellcode rather than to the original caller. The shellcode is here shown below return_address in memory, but it can also be above it. 28.2.1 Return to libc A variant attack is for the attacker to skip the shellcode, but instead to overwrite the stack return address with the address of a known library procedure. The libc library is popular here, making this known as thereturn-to-libc approach. The goal of the attacker is to identify a library procedure that, in the current context of the server process being attacked, will do something that the attacker Ô¨Ånds useful. One version of this attack is to overwrite the stack address with the address of the system() call, and to place on the stack just above this return address a pointer to the string ‚Äú/bin/sh‚Äù (often present in the environment strings of the attacked process). When the current function exits, control branches to system(), which now thinks it has been called with parameter /bin/sh. A shell is launched. 704 28 Security
An Introduction to Computer Networks, Release 2.0.11 Return-to-libc attacks often involve no shellcode at all. The attack of 28.3.2 A JPEG heap vulnerability uses some return-to-libc elements but also does involve injected shellcode. A practical problem with any form of the stack-overÔ¨Çow attack is knowing enough about the memory layout of the target machine so that the attacker can determine exactly where the shellcode is loaded. This is made simpler by standardized versions of Windows in which many library routines are at Ô¨Åxed, well-known addresses. 28.2.2 An Actual Stack-OverÔ¨Çow Example Why the code here? In many accounts of computer vulnerabilities, there is an understandable reluctance to explain the actual mechanics, lest some attacker learn how to exploit the Ô¨Çaw. This secrecy, however, sometimes has the unfortunate side-effect of making vulnerabilities seem almost magical. The goal in presenting this twentyyear-old example in detail is simply to strip away the aura of mystery surrounding many exploits. To put together an actual example, we modify a version of TCP simplex-talk ( 17.6 TCP simplex-talk ) written in the C language. On the server side, all we have to do is read the input with the infamous gets(char *buf), which reads in data up to a newline (or NUL) character into the array buf, with no size restrictions. To be able to use gets() this way, we must arrange for the standard-input stream of the reading process to be the network connection. There is a version of gets() that reads from an arbitrary input stream, namely fgets(char *buf, int bufsize, FILE *stream), butfgets() is not as vulnerable as gets() as it will not read more than bufsize characters into buf. (It is possible, of course, for a programmer to supply a value of bufsize much larger than the actual size of buf[] .) One drawback of using gets() is that the shellcode string must not have any NULL (zero) bytes. Sometimes this takes some subterfuge; in the example below, the shellcode includes the string "/bin/ shNAAAABBBB"; theNand theBBBB will be replaced with zeroes by the injected code itself, after it is read. Alternatively, we can also have the server read its the data with the call recv(int socket, char *buf, int bufsize, int flags) but supply an incorrect (and much too large) value for the parameter bufsize. This approach has the practical advantage of allowing the attacker to supply a buffer with NUL characters (zero bytes) and with additional embedded newline characters. On the client side ‚Äì the attacker‚Äôs side ‚Äì we need to come up with a suitable shellcode and create a too-large buffer containing, in the correct place, the address of the start of the shellcode. Guessing this address used to be easy, in the days when every process always started with the same virtual-memory address. It is now much harder precisely to make this kind of buffer-overÔ¨Çow attack more difÔ¨Åcult; we will cheat and have our server print out an address on startup that we can then supply to the client. An attack like this depends on knowing the target operating system, the target cpu architecture (so as to provide an executable shellcode), the target memory layout, and something about the target server implementation (so the attacker knows what overÔ¨Çow to present, and when). Alas, all but the last of these are 28.2 Stack Buffer OverÔ¨Çow 705
An Introduction to Computer Networks, Release 2.0.11 readily guessable. Once upon a time vulnerabilities in server implementations were discovered by reading source code, but it has long been possible to search for overÔ¨Çow opportunities making use only of the binary executable code. The shellcode presented here assumes that the server system is running 32-bit code. 64-bit Linux systems can be conÔ¨Ågured to run 32-bit code as well, though a simpler alternative is often to create a 32-bit virtual machine to run the server side. It would also be possible to migrate the shellcode to 64-bit format. 28.2.2.1 The server The overÔ¨Çow-vulnerable server, oserver.c, is more-or-less a C implementation of the tcp simplex-talk server of17.6 TCP simplex-talk. For the 32-bit shellcode below to work, the server must be run as a 32-bit program. The server contains an explicit call to bind() which was handled implicitly by the ServerSocket() constructor in the Java version. For each new connection, a new process to handle it is created with fork(); that new process then calls process_connection(). Theprocess_connection() function then reads a line at a time from the client into a buffer pcbuf of 80 bytes. Unfortunately for the server, it may read well more than 80 bytes intopcbuf. For the stack overÔ¨Çow to work, it is essential that the function that reads too much data into the buffer ‚Äì thus corrupting the stack ‚Äì must return. Therefore the protocol has been modiÔ¨Åed so that process_connection() returns if the arriving string begins with ‚Äúquit‚Äù. We must also be careful that the stack overÔ¨Çow does not so badly corrupt any local variables that the code fails afterwards to continue running, even before returning. All local variables in process_connection() are overwritten by the overÔ¨Çow, so we save the socket itself in the global variablegsock. We also call setstdinout(gsock) so that the standard input and standard output within process_connection() is the network connection. This allows the use of the notoriously vulnerablegets(), which reads from the standard input (alternatively, recv() orfgets() with an incorrect value forbufsize may be used). The call to setstdinout() also means that the shell launched by the shellcode will have its standard input and output correctly set up. We could, of course, make the appropriate dup() /fcntl() calls from the shellcode, but that would increase its complexity and size. Because the server‚Äôs standard output is now the TCP connection, it prints incoming strings to the terminal via the standard-error stream. On startup, the server prints an address (that of mbuf[] ) within its stack frame; we will refer to this as mbuf_addr. The attacking client must know this value. No real server is so accommodating as to print its internal addresses, but in the days before address randomization, 28.2.3.2 ASLR, the server‚Äôs stack address was typically easy to guess. Whenever a connection is made, the server here also prints out the distance, in bytes, between the start ofmbuf[] inmain() and the start of pcbuf ‚Äì the buffer that receives the overÔ¨Çow ‚Äì in process_connection(). This latter number, which we will call addr_diff, is constant, and must be compiled into the exploit program (it does change if new variables are added to the server‚Äôs main() or process_connection() ). The actual address of pcbuf[] is thus mbuf_addr ‚Äì addr_diff. This will 706 28 Security
An Introduction to Computer Networks, Release 2.0.11 be the address where the shellcode is located, and so is the address with which we want to overwrite the stack. We return to this below in 28.2.2.3 The exploit, where we introduce a ‚ÄúNOPslide‚Äù so that the attacker does not have to get the address of pcbuf[] exactly right. Linux provides some protection against overÔ¨Çow attacks ( 28.2.3 Defenses Against Buffer OverÔ¨Çows ), so the server must disable these. As mentioned above, one protection, address randomization, we defeat by having the server print a stack address. The server must also be compiled with the -fno-stack-protector option to disable the stack canary of 28.2.3.1 Stack canary, and the-z execstack option to disable making the stack (and other data areas) non-executable, 28.2.3.3 Making the stack non-executable. gcc -fno-stack-protector -z execstack -o oserver oserver.c Even then we are dutifully warned by both the compiler and the linker: warning: 'gets' isdeprecated .... warning: the 'gets'function isdangerous andshould notbe used. In other words, getting this vulnerability still to work in 2014 takes a bit of effort. The server here does work with the simplex-talk client of 17.6 TCP simplex-talk, but with the #USE_GETS option enabled it does not handle a client exit gracefully, unless the client sends ‚Äúquit‚Äù. 28.2.2.2 The shellcode The shellcode must be matched to the operating system and to the cpu architecture; we will assume Linux running on a 32-bit Intel x86. Our goal is to create a shellcode that launches /bin/sh. The approach here is taken from [SH04]. The shellcode must be a string of machine instructions that is completely self-contained; data references cannot depend on the linker for address resolution. Not only must the code be position-independent, but the code together with its data must be position-independent. So-called ‚ÄúIntel syntax‚Äù is used here, in which the destination operand comes Ô¨Årst, egmov eax,37. The nasm assembler is one of many that supports this format. Direct system calls in Linux are made using interrupts, using the int 0x80 opcode and parameter. The x86 architecture has four general-purpose registers eax,ebx,ecx andedx; when invoking a system call via an interrupt, the Ô¨Årst of these holds a code for the particular system routine to be invoked and the others hold up to three parameters. (Below, in 28.3.2 A JPEG heap vulnerability, we will also make use of register edi.) The system call we want to make is execve(char *filename, char *argv[], char *envp[]) We needeax to contain the numeric value 11, the 32-bit-Linux syscall value corresponding to execve (perhaps deÔ¨Åned in /usr/include/i386-linux-gnu/asm/unistd_32.h ). (64-bit Linux uses 59 as the syscall value for execve .) We load this with mov al 11 ;alis a shorthand for the low-order byte of register eax. We Ô¨Årst zero eax by subtracting it from itself. We can also, of course, use mov eax 11, but then the 11 expands into four bytes 0x0b000000, and we want to avoid including NUL bytes in the code. 28.2 Stack Buffer OverÔ¨Çow 707
An Introduction to Computer Networks, Release 2.0.11 We also need ebx to point to the NUL-terminated string /bin/sh. The register ecx should point to an array of pointers [‚Äú/bin/sh‚Äù, 0] in memory (the null-terminated argv[] ), andedx should point to a null word in memory (the null-terminated envp[] ). We include in the shellcode the string ‚Äú/bin/shNAAAABBBB‚Äù, and have the shellcode insert a NUL byte to replace the N and a zero word to replace the ‚ÄúBBBB‚Äù, as the shellcode must contain no NUL bytes at the time it is read in by gets(). The shellcode will also replace the ‚ÄúAAAA‚Äù with the address of ‚Äú/bin/sh‚Äù. We then load ecx with the address of ‚ÄúAAAA‚Äù (now containing the address of ‚Äú/bin/sh‚Äù followed by a zero word) and edx with the address of ‚ÄúBBBB‚Äù (now just a zero word). Loading the address of a string is tricky in the x86 architecture. We want to calculate this address relative to the current instruction pointer IP, but no direct access is provided to IP. The workaround in the code below is to jump to shellstring near the end, but then invoke call start, wherestart: is the label for our main code up at the top. The action of call start is to push the address of the byte following call start onto the stack; this happens to be the address of shellstring:. Back up at start:, thepop ebx pops this address off the stack and leaves it in ebx, where we want it. Our complete shellcode is as follows (the actual code is in shellcode.s): jmp short shellstring start: pop ebx ;get the address of the string in ebx sub eax, eax ;zero eax by subtracting it from itself mov [ebx+7 ], al ;put a NUL byte where the N is in the string mov [ebx+8 ], ebx ;put the address of the string where the AAAA is mov [ebx+12], eax ;put a zero (NULL) word into where the BBBB is mov al, 11 ;execve is syscall 11 lea ecx, [ebx+8] ;load the address of where the AAAA was lea edx, [ebx+12] ;load the address of where the BBBB was, now NULL int 0x80 ;call the kernel. WE HAVE A SHELL! shellstring: call start ;pushes address of string below and jumps to start: db '/bin/shNAAAABBBB' ;the string. AAAA and BBBB get filled in as above We run this through the commands nasm -f elf shellcode.s ld -o shellcode shellcode.o objdump -d shellcode and then, creating a string with the bytes produced, come up with the following: char shellcode[] = "\xeb\x16\x5b\x29\xc0\x88\x43\x07\x89\x5b\x08\x89 " "\x43\x0c\xb0\x0b\x8d\x4b\x08\x8d\x53\x0c\xcd\x80 " "\xe8\xe5\xff\xff\xff /bin/sh/NAAAABBBB"; We can test this (on a 32-bit system) with a simple C program deÔ¨Åning the above and including int main(int argc, char **argv) { void (*func)() = (void ( *)()) shellcode; func(); } 708 28 Security
An Introduction to Computer Networks, Release 2.0.11 We can verify that the resulting shell has not inherited the parent environment with the env andset commands. Additional shellcode examples can be found in [AHLR07]. 28.2.2.3 The exploit Now we can assemble the actual attack. We start with a C implementation of the simplex-talk client, and add a feature so that when the input string is ‚Äúdoit‚Äù, the client 
- sends the oversized buffer containing the shellcode, terminated with a newline to make gets() happy 
- also sends ‚Äúquit‚Äù, terminated with a newline, to force the server‚Äôs process_connection() to return, and thus to transfer control to the code pointed to by the now-overwritten return_address Ô¨Åeld of the stack 
- begins a loop ‚Äì copylines() ‚Äì to copy the local terminal‚Äôs standard input to the network connection (hopefully now with a shell at the other end), and to copy the network connection to the local terminal‚Äôs standard output On startup, the client accepts a command-line parameter representing the address (in hex) of a variable close to the start of the server‚Äôs main() stack frame. When the server starts, it prints this address out; we simply copy that when starting the client. A second command-line parameter is the server hostname. The full client code is in netsploit.c. All that remains is to describe the layout of the malicious oversized buffer, created by buildbadbuf(). We Ô¨Årst compute our guess for the address of the start of the vulnerable buffer pcbuf in the server‚Äôs process_connection() function: we take the address passed in on the command line, which is actually the address of mbuf in the server‚Äôs main(), and add to it the known constant ( pcbuf -mbuf ). This latter value, 147 in the version tested by the author, is stored in netsploit.c‚Äôs BUF_OFFSET. This calculation of the address of the server‚Äôs pcbuf should be spot-on; if we now store our shellcode at the start ofpcbuf and arrange for the server to jump to our calculated address, the shellcode should run. Real life, however, is not always so accommodating, so we introduce a NOP slide: we precede our shellcode with a run of NOP instructions. A jump anywhere into the NOPslide should lead right into the shellcode. In our example here, we make the NOPslide 20 bytes long, and add a fudge factor of between 0 and 20 to our calculated address ( FUDGE is 10 in the actual code). We need the attack buffer to be large enough that it overwrites the stack return-address Ô¨Åeld, but small enough that the server does not incur a segmentation fault when overÔ¨Ålling its buffer. We settle on a BADBUFSIZE of 161 (160 plus 1 byte for a Ô¨Ånal newline character); it should be comparable to but perhaps slightly larger than the BUF_OFFSET value of, in our case, 147). The attack buffer is now 
- 25 bytes of NOPs 
- shellcode (46 bytes in the version above) 
- 90 bytes worth of the repeated calculated address ( baseaddr-BUF_OFFSET+FUDGE in the code 
- 1 byte newline, as the server‚Äôs gets() expects a newline-terminated string 28.2 Stack Buffer OverÔ¨Çow 709
An Introduction to Computer Networks, Release 2.0.11 Here is a diagram like the one above, but labeled for this particular attack. Not all memory regions are drawn to scale, and there are more addresses between the two stack frames than just return address. We double-check the bad buffer to make sure it contains no NUL bytes and no other newline characters. If we wanted a longer NOPslide, we would have to hope there was room above the stack‚Äôs return-address Ô¨Åeld. In that case, the attack buffer would consist of a bunch of repeated address guesses, then the NOPslide, and Ô¨Ånally the shellcode. After the command ‚Äúdoit‚Äù, the netsploit client prompt changes to 1>. We can then type ls, and, if the shellcode has successfully started, we get back a list of Ô¨Åles on the server. As earlier, we can also type env andset to verify that the shell did not inherit its environment from any ‚Äúnormal‚Äù shell. Note that any shell commands that need to write to the stderr stream will fail, as this stream was not set up; this includes any mistyped commands. 28.2.3 Defenses Against Buffer OverÔ¨Çows How to prevent this sort of attack? The most basic approach is to make sure that array bounds are never violated (and also that similar rules for the use of dynamically allocated memory, such as not using a block after it has been freed, are never violated). Some languages enforce this automatically through ‚Äúmemorysafe‚Äù semantics; while this is not a guarantee that programs are safe, it does eliminate an important class of vulnerabilities. In C, memoryand overÔ¨Çow-related bugs can be eliminated through careful programming, but the task is notoriously error-prone. Another basic approach, applicable to almost all remote-code-execution attacks, is to make sure that the server runs with the minimum permissions possible. The server may not have write permission to anything of substance, and may in fact be run in a so-called ‚Äúchroot‚Äù environment in which any access to the bulk of the server‚Äôs Ô¨Ålesystem is disabled. One issue with this approach is that a server process on a unix-derived system that wants to listen on a port less than 1024 needs special privileges to open that port. Traditionally, the process would start with root 710 28 Security
An Introduction to Computer Networks, Release 2.0.11 privileges and, once the socket was opened, would downgrade its privileges with calls to setgid() and setuid(). This is safe in principle, but requires careful attention to the man pages; use of seteuid(), for example, allows the shellcode to recover the original privileges. Linux systems now support assigning to an unprivileged process any of several ‚Äúcapabilities‚Äù (see man capabilities ). The one most relevant here isCAP_NET_BIND_SERVICE, which, if set, allows a process to open privileged ports. Still, to assign these capabilities still requires some privileged intervention when the process is started. 28.2.3.1 Stack canary The underlying system can also provide some defenses. One of these, typically implemented within the compiler, is the stack canary: an additional word on the stack near the return address that is set to a pseudorandom value. A copy of this word is saved elsewhere. When the function calls return (either explicitly or implicitly), this word is checked to make sure the stack copy still agrees with the saved-elsewhere copy; a discrepancy indicates that the stack was overwritten. In thegcc compiler, a stack canary can be enabled with the compiler option -fstack-protector. To compile the stack exploit in 28.2.2.3 The exploit, we needed to add -fno-stack-protector. 28.2.3.2 ASLR Another operating-system-based defense is Address-Space Layout Randomization, or ASLR. In its simplest form, this means that each time the server process is restarted, the stack will have a different starting address. For example, restarting the oserver program above Ô¨Åve times yields addresses (in hex) of bf8d22b7, bf84ed87, bf977d87, bfcc5cb7 and bfa302d7. There are at least four hex digits (16 bits) of entropy here; if the server did not print its stack address it might take 216guesses for the attacker to succeed. Still, 216guesses might take an attacker well under an hour. Worse, the attacker might simply create a stack-buffer-overÔ¨Çow attack with a very long NOPslide; the longer the NOPslide the more room for error in guessing the shellcode address. With a NOPslide of length 210= 1024 bits, guessing the correct stack address will take only 26= 64 tries. (Some implementations squeeze out 19 bits of address-space entropy, meaning that guessing the correct address increases to 29= 512 tries.) For 64-bit systems, however, ASLR is much more effective. Brute-force guessing of the stack address takes a prohibitively long time. ALSR also changes the heap layout and the location of libraries each time the server process is restarted. This is to prevent return-to-libc attacks, 28.2.1 Return to libc. For a concrete example of an attacker‚Äôs use of non-randomized library addresses, see 28.3.2 A JPEG heap vulnerability. On Linux systems, ASLR can be disabled by writing a 0 to /proc/sys/kernel/randomize_va_space; values 1 and 2 correspond to partial and full randomization. Windows systems since Vista (2007) have had ASLR support, though earlier versions of the linker required the developer to request ASLR with the /DYNAMICBASE switch. 28.2.3.3 Making the stack non-executable A more sophisticated idea, if the virtual-memory hardware supports it, is to mark those pages of memory allocated to the stack as non-executable, meaning that if the processor‚Äôs instruction register is loaded with 28.2 Stack Buffer OverÔ¨Çow 711
An Introduction to Computer Networks, Release 2.0.11 an address on those pages (due to branching to stack-based shellcode), a hardware-level exception will immediately occur. This immediately prevents attacks that place a shellcode on the stack, though return-tolibc attacks ( 28.2.1 Return to libc ) are still possible. In the x86 world, AMD introduced the per-page NX bit, for No eXecute, in their x86-64 architecture; CPUs with this architecture began appearing in 2003. Intel followed with its XD, for eXecute Disabled, bit. Essentially all x86-64 CPUs now provide hardware NX/XD support; support on 32-bit CPUs generally requires so-called Physical Address Extension mode. The NX idea applies to all memory pages, not just stack pages. This sometimes causes problems with applications such as just-in-time compilation, where the code page is written and then immediately executed. As a result, it is common to support NX-bit disabling in software. On Linux systems, the compiler option -z execstack disables NX-bit protection; this was used above in 28.2.2.1 The server. Windows has a similar/NXCOMPAT option for requesting NX-bit protection. While a non-executable stack prevents the stack-overÔ¨Çow attack described above, injecting shellcode onto the heap is still potentially possible. The OpenBSD operating system introduced write or execute in 2003; this is abbreviated W^X after the use of ‚Äú^‚Äù as the XOR operator in C. A memory page may be writable or executable, but not both. This is strong protection against virtually all shellcode-injection attacks, but may still be vulnerable to some return-to-libc attacks ( 28.2.1 Return to libc ). See [AHLR07], chapter 14, for some potential attack strategies against systems with non-executable pages. 28.3 Heap Buffer OverÔ¨Çow As with stack overÔ¨Çows, heap overÔ¨Çows all rely on some software Ô¨Çaw that allows data to be written beyond the conÔ¨Ånes of the designated buffer. A buffer on the heap is subject to the same software-failure overÔ¨Çow prospects as a buffer on the stack. An important difference, however, is that buffers on the heap are not in clear proximity to an obvious return address. Despite that difference, heap overÔ¨Çows can also be used to enable remote-code-execution attacks. Perhaps the simplest heap overÔ¨Çow is to take advantage of the fact that some heap pages contain executable code, representing application-loaded library pages. If the page with the overÔ¨Çowable buffer is pointed to byp, and the following page in memory pointed to by qcontains code, then all an attacker has to do is to have the overÔ¨Çow Ô¨Åll the qpage with a long NOPslide and a shellcode. When at some point a call is made to the code pointed to by q, the shellcode is executed instead. A drawback to this attack is that the layout of heap pages like this is seldom known. On the other hand, the fact that heap pages do sometimes legitimately contain executable code means that uniformly marking the heap as non-executable, as is commonly done with the stack, may not be an option. 28.3.1 A Linux heap vulnerability We now describe an actual Linux heap vulnerability from 2003, following [AB03], based on version 2.2.4 of the glibc library. The vulnerable server code is simply the following: char*p = malloc(1024); char*q = malloc(1024); gets(p); // read the attacker 's input (continues on next page) 712 28 Security
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) free(q); // block q islast allocated andfirst freed free(p); As with the stack-overÔ¨Çow example, the gets(p) results in an overÔ¨Çow from block p into block q, overwriting not just the data in block q but also the block headers maintained by malloc(). While there is no guarantee in general that block qwill immediately follow block pin memory, in practice this usually happens unless there has been a great deal of previous allocate/free activity. The vulnerability here relies on some features of the 2003-era (glibc-2.2.4) malloc(). All blocks are either allocated to the user or are free; free blocks are kept on a doubly linked list. We will assume the data portion of each block is always a power of 2; there is a separate free-block list for each block size. When two adjacent blocks are both free, malloc() coalesces them and moves the joined block to the free-block list of the next size up. All blocks are preÔ¨Åxed by a four-byte Ô¨Åeld containing the size, which we will assume here is the actual size 1032 including the header and alignment rather than the user-available size of 1024. As the three low-order bits of the size are always zero, one of these bits is used as a Ô¨Çag to indicate whether the block is allocated or free. Crucially, another bit is used to indicate whether the previous block is allocated or free. In addition to the size-plus-Ô¨Çags Ô¨Åeld, the Ô¨Årst two 32-bit words of a free block are the forward and backward pointers linking the block into the doubly linked free-block list; the size-plus-Ô¨Çag Ô¨Åeld is also replicated as the last word of the block: psize and status user datapsize and status forward pointer backward pointer copy of size Allocated block Free block The forward and backward pointers link the block into the free-block list The strategy of the attacker, in brief, is to overwrite the pblock in such a way that, when block qis freed, malloc() thinks block pis also free, and so attempts to coalesce them, in the process unlinking block p. Butpwas not in fact free, and the forward andbackward pointers manipulated by malloc() as part of the unlinking process are in fact provided by the attacker. As we shall see, this allows writing two attacker-designated 32-bit words to two attacker-designated memory locations; if one of the locations holds a return address and is updated so as to point to attacker-supplied shellcode also in block p, the system has been compromised. The normal operation of free(q), for an arbitrary block q, includes the following: 
- Get the block size ( size ) and Ô¨Çags at address q-4 28.3 Heap Buffer OverÔ¨Çow 713
An Introduction to Computer Networks, Release 2.0.11 
- Check the following block at address p+size to see if it is free; if so, merge (we are not interested in this case) 
- Check the Ô¨Çags to see if the preceding block is free; if so, load its size prev_size from address q-8, the address of the copy of size Ô¨Åeld in the diagram above; from that calculate a pointer to the previous block as p = q - prev_size; then unlink block p(as the coalesced block will go on a different free-block list). For our speciÔ¨Åc block q, however, the attacker can overwrite the Ô¨Ånal size Ô¨Åeld of block p,prev_size above, and can also overwrite the Ô¨Çag at address q-4 indicating whether or not block pis free. The attacker cannotoverwrite the header of block pthat would properly indicate that block pwas still in use, but the free() code did not double-check that. We will suppose that the attacker has overwritten block pto include the following: 
- setting the previous-block-is-free Ô¨Çag in the header of block qto true 
- setting the Ô¨Ånal size Ô¨Åeld of block pto a desired value, badsize 
- placing value ADDR_F at addressq-badsize; this is where free() will believe the previous block‚Äôs forward pointer is located 
- placing the value ADDR_B at addressq-badsize+4; this is where free() will believe the previous block‚Äôs backward pointer is located When thefree(q) operation is now executed, the system will calculate the previous block as at address p1 = q-badsize and attempt to unlink the false ‚Äúblock‚Äù p1. The normal unlink is (p->backward) -> forward = p->forward; (p->forward) -> backward = p->backward; Alas, when unlinking p1the result is *ADDR_B = ADDR_F *(ADDR_F + 4) = ADDR_B where, for the pointer increment in the second line, we take the type of ADDR_F to bechar*orvoid *. At this point the jig is pretty much up. If we take ADDR_B to be the location of a return address on the stack, andADDR_F to be the location of our shellcode, then the shellcode will be executed when the current stack frame returns. Extending this to a working example still requires a fair bit of attention to details; see [AB03]. One important detail is that the data we use to overwrite block pgenerally cannot contain NUL bytes, and yet a small positive number for badsize will have several NUL bytes. One workaround is to have badsize be a small negative number, meaning that the false previous-block pointer p1will actually come afterqin memory. 28.3.2 A JPEG heap vulnerability In 2004 Microsoft released vulnerability notice MS04-028 (and patch), relating to a heap buffer overÔ¨Çow inWindows XP SP1. Microsoft had earlier provided a standard library for displaying JPEG images, known 714 28 Security
An Introduction to Computer Networks, Release 2.0.11 as GDI for Graphics Device Interface. If a specially formed JPEG image were opened via the GDI library, an overÔ¨Çow on the heap would occur that would launch a shellcode. While most browsers had their own, unaffected, JPEG-rendering subroutines, it was often not difÔ¨Åcult to convince users to open a JPEG Ô¨Åle supplied as an email attachment or via an html download link. The problem occurred in the processing of the JPEG ‚Äúcomment‚Äù section, normally invisible to the user. The section begins with the Ô¨Çag 0xFFFE, followed by a two-byte length Ô¨Åeld (in network byte order) that was to include its own two bytes in the total length. During the image-Ô¨Åle loading, 2 was subtracted from the length to get the calculated length of the actual data. If the length Ô¨Åeld had contained 1, for example, the calculated length would be -1, which would be interpreted as the unsigned value 2321. The library, thinking it had that amount of space, would then blithely attempt to write that many bytes of comment into the next heap page, overÔ¨Çowing it. These bytes in fact would come from the image portion of the JPEG Ô¨Åle; the attacker would place here a NOPslide, some shellcode, and, as we shall see, a few other carefully constructed values. As with the Linux heap described above in 28.3.1 A Linux heap vulnerability, blocks on the WinXP heap formed a doubly-linked list, with forward and backward pointers known as flink andblink. As before, the allocator will attempt to unlink a block via blink -> forward = flink; flink -> backward = blink; The Ô¨Årst of these simpliÔ¨Åes to *blink = flink, as the offset to Ô¨Åeld forward is 0; this action allows the attacker to write any word of memory (at address blink ) with any desired value. The JPEG-comment-overÔ¨Çow operation eventually runs out of heap and results in a segmentation fault, at which point the heap manager attempts to allocate more blocks. However, the free list has already been overwritten, and so, as above, this block-allocation attempt instead executes *blink = flink. The attacker‚Äôs conceptual goal is to have flink hold an instruction that branches to the shellcode, and blink hold the address of an instruction that will soon be executed, or, equivalently, to have flink hold the address of the shellcode and blink represent a location soon to be loaded and used as the target of a branch. The catch is that the attacker doesn‚Äôt exactly know where the heap is, so a variant of the return-tolibc approach described in 28.2.1 Return to libc is necessary. The strategy described in the remainder of this section, described in [JA05], is one of several possible approaches. In Windows XP SP1, location 0x77ed73b4 holds a pointer to the entry point of the UndeÔ¨Åned Exception Filter handler; if an otherwise-unhandled program exception occurs, Windows creates an EXCEPTION_POINTERS structure and branches to the address stored here. It is this address, which we will refer to as UEF, the attack will overwrite, by setting blink = UEF. A call to the UndeÔ¨Åned Exception Filter will be triggered by a suitable subsequent program crash. When the exception occurs (typically by the second operation above, flink -> backward = blink ), before the branch to the address loaded from UEF, the EXCEPTION_POINTERS structure is created on the heap, overwriting part of the JPEG comment buffer. The address of this structure is stored in register edi. It turns out that, scattered among some standard libraries, there are half a dozen instructions at known addresses of the form call DWORD [edi+0x74], that is, ‚Äúcall the subroutine at 32-bit address edi + 0x74‚Äù ([AHLR07], p 186). All these call instructions are intended for contexts in which register edi has been set up by immediately preceding instructions to point to something appropriate. In our attacker‚Äôs context, however, edi points to the EXCEPTION_POINTERS structure; 0x74 bytes past edi is part of the attacker‚Äôs overÔ¨Çowed JPEG buffer that is safely past this structure and will not have been overwritten by it. 28.3 Heap Buffer OverÔ¨Çow 715
An Introduction to Computer Networks, Release 2.0.11 One such call instruction happens to be at address 0x77d92a34, inuser32.dll. This address is the value the attacker will put in flink. So now, when the exception comes, control branches to the address stored in UEF. This address now points to the above call DWORD [edi+0x74], so control branches again, this time into the attacker-controlled buffer. At this point, the processor lands on the NOPslide and ends up executing the shellcode (sometimes one more short jump instruction is needed, depending on layout). This attack depends on the fact that a speciÔ¨Åc instruction, call DWORD [edi+0x74], can be found at a speciÔ¨Åc, Ô¨Åxed address, 0x77d92a34. Address-space layout randomization ( 28.2.3.2 ASLR ) would have prevented this; it was introduced by Microsoft in Windows Vista in 2007. 28.3.3 Cross-Site Scripting (XSS) In its simplest form, cross-site scripting involves the attacker posting malicious javascript on a third-party website that allows user-posted content; this javascript is then executed by the target computer when the victim visits that website. The attacker might leave a comment on the website of the following form: I agree withthe previous poster completely <script> do_something_bad() </script> Unless the website in question does careful html Ô¨Åltering of what users upload, any other site visitor who so much as views this comment will have the do_something_bad() script executed by his or her browser. The script might email information about the target user to the attacker, or might attempt to exploit a browser vulnerability on the target system in order to take it over completely. The script and its enclosing tags will not appear in what the victim actually sees on the screen. Thedo_something_bad() code block will usually include javascript function deÔ¨Ånitions as well as function calls. In general, the attacker can achieve the same effect if the victim visits the attacker‚Äôs website. However, the popularity (and apparent safety) of the third-party website is usually important in practice; it is also common for the attack to involve obtaining private information from the victim‚Äôs account on that website. 28.3.4 SQL Injection SQL is the close-to-universal query language for databases; in a SQL-injection attack the attacker places a carefully chosen SQL fragment into a website form, in such a way that it gets executed. Websites typically construct SQL queries based on form data; the attacker‚Äôs goal is to have his or her data treated as additional SQL. This is supposed to be prevented by careful quoting, but quoting often ends up not quite careful enough. A successful attacker has managed to run SQL code on the server that gives him or her unintended privileges or information. As an example, suppose that the form has two Ô¨Åelds, username andpassword. The system then runs the following sub-query that returns an empty set of records if the xusername,password ypair is not found; otherwise the user is considered to be authenticated: select * from PASSWORDS p where p.user = 'username'and p.password = 'password'; 716 28 Security
An Introduction to Computer Networks, Release 2.0.11 The strings username andpassword are taken from the web form and spliced in; note that each is enclosed in single quotation marks supplied by the server. The attacker‚Äôs goal is to supply username/password values so that a nonempty set of records will be returned, thus authenticating the attacker. The following values are successful here, where the quotation marks in username are supplied by the attacker: username :' OR 1=1 OR 'x'='y password :foo The spliced-together query built by the server is now select * from PASSWORDS p where p.user = '' OR 1=1 OR 'x'='y' and p.password = 'foo'; Note that of the eight single-quote marks in the where-clause, four (the Ô¨Årst, sixth, seventh and eighth) came from the server, and four (the second through Ô¨Åfth) came from the attacker. The where-clause here appears to SQL to be the disjunction of three OR clauses (the last of which is 'x'='y' and p.password = 'foo' ). The middle OR clause is 1=1 which is always true. Therefore, the login succeeds. For this attack to work, the attacker must have a pretty good idea what query the server builds from the user input. There are two things working in the attacker‚Äôs favor here: Ô¨Årst, these queries are often relatively standard, and second, the attacker can often discover a great deal from the error messages returned for malformed entries. In many cases, these error messages even reveal the table names used. See also xkcd.com/327. 28.4 Network Intrusion Detection The idea behind network intrusion detection is to monitor one‚Äôs network for signs of attack. Many newer network intrusion-detection systems (NIDS) also attempt to halt the attack, but the importance of simple monitoring and reporting should not be underestimated. Many attacks (such as password guessing, or buffer overÔ¨Çows in the presence of ASLR) take multiple (thousands or millions) of tries to succeed, and a NIDS can give fair warning. There are also host-based intrusion-detection systems (HIDS) that run on and monitor a speciÔ¨Åc host; we will not consider these further. Most NIDS detect intrusions based either on trafÔ¨Åc anomalies or on pattern-based signatures. As an example of the former, a few pings ( 10.4 Internet Control Message Protocol ) might be acceptable but a large number, or a modest number addressed to nonexistent hosts, might be cause for concern. As for signatures, the attack in 28.3.2 A JPEG heap vulnerability can be identiÔ¨Åed by the hex strings 0xFFFE0000 or 0xFFFE0001. What about the attack in 28.2.2.3 The exploit ? Using the shellcode itself as signature tends to be ineffective as shellcode is easy to vary. The NOPslide, too, can be replaced with a wide variety of other instructions that just happen to do nothing in the present context, such as sub eax,eax. One of the most common signatures used by NIDSs for overÔ¨Çow attacks is simply the presence of overly long strings; the false-positive rate is relatively low. In general, however, coming up with sufÔ¨Åciently speciÔ¨Åc signatures can be nontrivial. An attack that keeps changing in relatively trivial ways to avoid signature-based detection is sometimes said to be polymorphic. 28.4 Network Intrusion Detection 717
An Introduction to Computer Networks, Release 2.0.11 28.4.1 Evasion The NIDS will have to reassemble TCP streams (and sometimes sequences of UDP packets) in order to match signatures. This raises the possibility of evasion: the attacker might arrange the packets so that the NIDS reassembles them one way and the target another way. The possibilities for evasion are explored in great detail in [PN98]; see also [SP03]. One simple way to do this is with overlapping TCP segments. What happens if one packet contains bytes 1-6 of a TCP connection as ‚Äúhelp‚Äù and a second packet contains bytes 2-7 as ‚Äúarmful‚Äù? h e l p a r m f u l These can be reassembled as either ‚Äúhelpful‚Äù or ‚Äúharmful‚Äù; the TCP speciÔ¨Åcation does not say which is preferred and different operating systems routinely reassemble these in different ways. If the NIDS reassembles the packets one way, and the target the other, the attack goes undetected. If the attack is spread over multiple packets, there may be many more than two ways that reassembly can occur, increasing the probability that the NIDS and the target will differ. Another possibility is that one of the overlapping segments has a header irregularity (in either the IP or TCP header) that causes it to be rejected by the target but not by the NIDS, or vice-versa. If the packets are h e l p h a r m f u l and both systems normally prefer data from the Ô¨Årst segment received, then both would reassemble as ‚Äúhelpful‚Äù. But if the Ô¨Årst packet is rejected by the target due to a header Ô¨Çaw, then the target receives ‚Äúharmful‚Äù. If the Ô¨Çaw is not recognized by the NIDS, the NIDS does not detect the problem. A very similar problem occurs with IPv4 fragment reassembly, although IPv4 fragments are at this point intrinsically suspicious. One approach to preventing evasion is to conÔ¨Ågure the NIDS with information about how the actual target does its reassembly, so the NIDS can match it. An even safer approach is to have the NIDS reassemble any overlapping packets and then forward the result on to the potential target. 28.5 Cryptographic Goals For the remainder of this chapter we turn to the use of cryptographic techniques in networking, to protect packet contents. Different techniques address different issues; three classic goals are the following: 1. Message conÔ¨Ådentiality: eavesdroppers should not be able to read the contents. 2. Message integrity: the recipient should be able to verify that the message was received correctly, even in the face of a determined adversary along the way. 3. Sender authentication: the recipient should be able to verify the identity of the sender. BrieÔ¨Çy, conÔ¨Ådentiality is addressed through encryption ( 28.7 Shared-Key Encryption and29 PublicKey Encryption ), integrity is addressed through secure hashes ( 28.6 Secure Hashes ), and authentication is addressed through secure hashes and public-key signatures. 718 28 Security
An Introduction to Computer Networks, Release 2.0.11 Encryption by itself does not ensure message integrity. In 28.7.4 Stream Ciphers we give an example using the message ‚ÄúTransfer $ 02000 to Mallory‚Äù. It is encrypted by XORing with the corresponding number of bytes of the keystream ( 28.7 Shared-Key Encryption ), and decrypted by XORing again. If the attacker XORs the two bytes in bold with the byte (‚Äò0‚Äô XOR ‚Äò2‚Äô), the message becomes ‚ÄúTransfer $ 20000 to Mallory‚Äù; if the attacker XORs those bytes in the encrypted message with (‚Äò0‚Äô XOR ‚Äò2‚Äô) then the result will decrypt to the $20,000 transfer. Similarly, integrity does not automatically ensure authentication. Two parties can negotiate a temporary key to guarantee message integrity without ever establishing each other‚Äôs identities as is necessary for authentication. For example, if Alice connects to a website using SSL/TLS, but the site never purchased an SSL certiÔ¨Åcate ( 29.5.2 TLS and29.5.2.1 CertiÔ¨Åcate Authorities ), or Alice does not trust the site‚Äôs certiÔ¨Åcate authority ( 29.5.2.1 CertiÔ¨Åcate Authorities ), then Alice has message integrity for her session, but not authentication. To the above list we might add resistance to message replay. Consider messages such as the following: 1. ‚ÄúI, Alice, authorize the payment to Bob of $1000 from my account‚Äù 2. My facebook.com authentication cookie is Zg8yPCDwbzJ-59Hc-DvHt67qrS Alice does not want the Ô¨Årst message to be executed by her bank more than once, though doing so would violate none of the three rules above. The owner of the second message might be happy to replay it multiple times, but would not want someone else to be able to do so. One straightforward way to prevent replay attacks is to introduce a message timestamp or count. We might also desire resistance to denial-of-service attacks. Such attacks are generally related to implementation failures. Attacks in which SSL users end up negotiating a very early version of the protocol, and thus a much less secure encryption mechanism, are arguably of this type; see the POODLE sidebar at 29.5 SSH and TLS. Finally, one sometimes sees message non-repudiation as a goal. However, in the technical ‚Äì as opposed to legal ‚Äì realm we can never hope to prove Alice herself signed a message; the best we can do is to prove that Alice‚Äôs keywas used to sign the message. At this point non-repudiation is, for our purposes here, quite similar to authentication. See, however, the Ô¨Ånal example of 28.6.1 Secure Hashes and Authentication. 28.5.1 Alice and Bob Cryptographers usually use Alice and Bob, instead of A and B, as the names of the parties sending each other encrypted messages. This tradition dates back at least to [RSA78]. Other parties sometimes include Eve the eavesdropper and Mallory the active attacker (and launcher of man-in-the-middle attacks). (In this article the names Aodh and Bea are introduced, though not speciÔ¨Åcally for cryptography; the Irish name Aodh is pronounced, roughly, as ‚ÄúEh‚Äù.) 28.6 Secure Hashes How can we tell if a Ô¨Åle has been changed? One way is to create a record of the Ô¨Åle‚Äôs checksum, using, for the sake of argument, the Internet checksum of 7.4 Error Detection. If the potential Ô¨Åle corruption is random, this will fail to detect a modiÔ¨Åed Ô¨Åle with probability only 1 in 216. 28.6 Secure Hashes 719
An Introduction to Computer Networks, Release 2.0.11 Alas, if the modiÔ¨Åcation is intentional, it is trivial to modify the Ô¨Åle so that it has the same checksum as the original. If the original Ô¨Åle has checksum c 1, and after the initial modiÔ¨Åcation the checksum is now c 2, then all we have to do to create a Ô¨Åle with checksum c 1is to append the 16-bit quantity c 2‚Äì c1(where the subtraction is done using ones-complement; we need to modify this slightly if the number of bytes in the Ô¨Årst-draft modiÔ¨Åed Ô¨Åle is odd). The CRC family of checksums is almost as easily tricked. The goal of a cryptographically secure hash is to provide a hash function ‚Äì we will not call it a ‚Äúchecksum‚Äù as hashes based on simple sums all share the insecurity above ‚Äì for which this trick is well-nigh impossible. SpeciÔ¨Åcally, we want a hash function such that: 
- Knowing the hash value should shed no practical light on the message 
- Given a hash value, there should be no feasible way to Ô¨Ånd a message yielding that hash A slightly simpler problem than the second one above is to Ô¨Ånd two messages that have the same hash; this is sometimes called the collision problem. When the collision problem for a hash function has been solved, it is (high) time to abandon it as potentially no longer secure. If a single bit of the input is changed, the secure-hash-function output is usually entirely different. Hashes popular in the 1990s were the 128-bit MD5 ( RFC 1321, based on MD4, [RR91]) and the 160-bit SHA-1 (developed by the NSA); SNMPv3 ( 27.3 SNMPv3 ) originally supported both of these ( 27.3.2 Cryptographic Fundamentals ). MD5 collisions (two messages hashing to the same value) were reported in 2004, and collisions where both messages were meaningful were reported in 2005; such collision attacks mean it can no longer be trusted for security purposes. Hash functions currently (2014) believed secure include the SHA-2 family, which includes variants ranging from 224 bits to 512 bits (and known individually as SHA-224 through SHA-512). A common framework for constructing n-bit secure-hash functions is the Merkle-D√•mgard construction ([RM88], [ID89]); it is used by MD5, SHA-1 and SHA-2. The initial n-bit state is speciÔ¨Åed. To this is then applied a set of transformations H i(x,y) that each take an n-bit block x and some additional bits y and return an updated n-bit block. These transformations are usually similar to the rounds functions of a block cipher; see 28.7.2 Block Ciphers for a typical example. In encryption, the parameter y would be used to hold the key, or a substring of the key; in secure-hash functions, the parameter y holds a substring of the input message. The set of transformations is applied repeatedly as the process iterates over the entire input message; the result of the hash is the Ô¨Ånal n-bit state. In the MD5 hash function, the input is processed in blocks of 64 bytes. Each block is divided into sixteen 32-bit words. One such block of input results in 64 iterations from a set of sixteen rounds-functions H i, each applied four times in all. Each 32-bit input word is used as the ‚Äúkey‚Äù parameter to one of the H ifour times. If the total input length is not a multiple of 512 bits, it is padded; the padding includes a length attribute so two messages differing only by the amount of padding should not hash to the same value. While this framework in general is believed secure, and is also used by the SHA-2 family, it does suffer from what is sometimes called the length-extension vulnerability. If h = hash(m), then the value h is simply the Ô¨Ånal state after the above mechanism has processed message m. An attacker knowing only h can then initialize the above algorithm with h, and continue it to Ô¨Ånd the hash h1= hash(m"m1), for an arbitrary message m1concatenated to the end of m, without knowing m. If the original message m was padded to message m p, then the attacker will Ô¨Ånd h1= hash(m p"m1), but that is often enough. This vulnerability must be considered when using secure-hash functions for message authentication, below. 720 28 Security
An Introduction to Computer Networks, Release 2.0.11 The SHA-3 family of hash functions does not use the Merkle-D√•mgard construction and is believed not vulnerable to length-extension attacks. 28.6.1 Secure Hashes and Authentication Secure hash functions can be used to implement message authentication. Suppose the sender and receiver share a secret, pre-arranged ‚Äúkey‚Äù, K, a random string of length comparable to the output of the hash. Then, in principle, the sender can append to the message m the value h = hash(K"m). The receiver, knowing K, can recalculate this value and verify that the h appended to the message is correct. In theory, only someone who knew K could calculate h. This particular hash(K"m) implementation is undermined by the length-extension vulnerability of the previous section. If the hash function exhibits this vulnerability and the sender transmits message m together with hash(K"m), then an attacker can modify this to message m"m1together with hash(K"m"m1),without knowing K. This problem can be defeated by reversing the order and using hash(m"K), but this now introduces potential collision vulnerabilities: if the hash function has the length-extension vulnerability and two messages m 1 and m 2hash to the same value, then so will m 1"K and m 2"K. Taking these vulnerabilities into account, RFC 2104 deÔ¨Ånes the Hash Message Authentication Code, or HMAC, as follows; it can be used with any secure-hash function whether or not it has the length-extension vulnerability. The 64-byte length here comes from the typical input-block length of many secure-hash functions; it may be increased as necessary. (SHA-512, of the SHA-2 family, has a 128-byte input-block length and would be a candidate for such an increase.) 
- The shared key K is extended to 64 bytes by padding with zeroes. 
- A constant string ipad is formed by repeating the octet 0x36 (0011 0110) 64 times. 
- A constant string opad is formed by repeating 0x5c (0101 1100) 64 times. 
- We set K1= K XOR ipad. - We set K2= K XOR opad. Finally, the HMAC value is HMAC = hash( K2"hash( K1"mesg )) The values 0x36 (0011 0110) and 0x5c (0101 1100) are not critical, but the XOR of them has, by intent, a reasonable number of both 0-bits and 1-bits; see [BCK96]. The HMAC algorithm is, somewhat surprisingly, believed to be secure even when the underlying hash function is vulnerable to some kinds of collision attacks; see [MB06] and RFC 6151. That said, a hash function vulnerable to collision attacks may have other vulnerabilities as well, and so HMAC-MD5 should still be phased out. Negotiating the pre-arranged key K can be a signiÔ¨Åcant obstacle, just as it can be for ciphers using prearranged keys ( 28.7 Shared-Key Encryption ). If Alice and Bob meet in person to negotiate the key K, 28.6 Secure Hashes 721
An Introduction to Computer Networks, Release 2.0.11 then Alice can use HMAC for authentication of Bob, as long as K is not compromised: if Alice receives a message signed with K, she knows it must have come from Bob. Sometimes, however, K is negotiated on a per-session basis, without deÔ¨Ånitive ‚Äúpersonal‚Äù authentication of the other party. This is akin to Alice and someone claiming to be ‚ÄúBob‚Äù selecting an encryption key using the DifÔ¨Åe-Hellman-Merkle mechanism ( 28.8 DifÔ¨Åe-Hellman-Merkle Exchange ); such key selection is secure and does not require that Alice or Bob have any prior knowledge of one another. In the HMAC setting, Alice can be conÔ¨Ådent that any HMAC-signed message was sent by the same ‚ÄúBob‚Äù that negotiated the key, and not by a third party (assuming neither side has leaked the key K). This is true even if Alice is not sure ‚ÄúBob‚Äù is the real Bob, or has no idea who ‚ÄúBob‚Äù might be. The signature guarantees the message integrity, but also serves as authentication that the sender is the same entity who sent the previous messages in the series. Finally, if Alice receives a message from Bob signed with HMAC using a pre-arranged secret key K (28.6.1 Secure Hashes and Authentication ), Alice may herself trust the signature, but she cannot prove to anyone else that K was used to sign the message without divulging K. She also cannot prove to anyone else that Bob is the only other party who knows K. Therefore this signature mechanism provides authentication but not non-repudiation. 28.6.2 Password Hashes A site generally does not store user passwords directly; instead, a hash of the password, h(p), is stored. The cryptographic hash functions above, egMD5 and SHA-n, work well as long as the password Ô¨Åle itself is not compromised. However, these functions all execute very quickly ‚Äì by design ‚Äì and so an attacker who succeeds in obtaining the password Ô¨Åle can usually extract passwords simply by calculating the hash of every possible password. There are about 214six-letter English words, and so there are about 238passwords consisting of two such words and three digits. Such passwords are usually considered rather strong, but brute-force calculation of h(p) for 238possible values of p is quite feasible for the hashes considered so far. Checking 108MD5-hashed passwords per second is quite feasible; this means 238= 256230256109 passwords can be checked in under 45 minutes. Use of a GPU increases the speed at least 10-fold. Special hashes have been developed to address this. Two well-known ones are scrypt ([CP09] and this Internet Draft) and bcrypt. A newer entrant is Argon2, while PBKDF2 is an old stalwart. The goal here is to develop a hash that takes ‚Äì ideally ‚Äì the better part of a second to calculate even once, and which cannot easily be sped up by large precomputed tables. Typically these are something like a thousand to a million times slower than conventional hash functions like MD5 and the SHA-2 family; a millionfold-slower hash function is the equivalent of making the passwords longer by 20 random bits (106220). See [ACPRT16] for a theoretical analysis of the effectiveness of scrypt in this context. See also RFC 2898, section 4.2, though the proposal there is only a thousandfold slower. (Good password tables also incorporate a salt, ornonce: a random string s is chosen and appended to the password before hashing; the password record is stored as the pair xs,h(p"s)y. This means that cracking the password for one user will not expose a second user who has chosen exactly the same password, so long as the second user has a different salt s. Salting, however, does not change the scenarios outlined above.) 722 28 Security
An Introduction to Computer Networks, Release 2.0.11 28.6.3 CHAP Secure hashes can also be used to provide secure password-based login authentication in the presence of eavesdropping. SpeciÔ¨Åc implementations include CHAP, the Challenge-Handshake Authentication Protocol ( RFC 1994 ) and Microsoft‚Äôs MS-CHAP (version 1 in RFC 2433 and version 2 in RFC 2759 ). The general idea is that the server sends a random string (the ‚Äúchallenge‚Äù) and the client then creates a response consisting of a secure hash of the challenge string concatenated with the user‚Äôs password (and possibly other information). Assuming the secure hash is actually secure, only someone in possession of the user password could have created this response. By the same token, an eavesdropper cannot Ô¨Ågure out the password from the response. While such protocols can be quite secure in terms of verifying to the server that the client knows the password, the CHAP strategy has a fundamental vulnerability: the server must store the plaintext password rather than a secure hash of it (as in the previous section). An attacker who makes off with the server‚Äôs password Ô¨Åle then has everything; no brute-force password cracking is needed. For this reason, newer authentication protocols often will create an encrypted channel Ô¨Årst ( egusing TLS, 29.5.2 TLS ), and then use that secure channel to exchange credentials. This may involve transmission of the plaintext password, which clearly allows the server to store only hashed passwords. However, as the next example shows, it ispossible to authenticate without having the server store plaintext passwords and without having the plaintext password transmitted at all. 28.6.4 SCRAM SCRAM (Salted Challenge-Response Authentication Mechanism), RFC 5802, is an authentication protocol with the following features: 
- The client password is not transmitted in the clear 
- The server does not store the plaintext client password 
- If the server‚Äôs hashed credentials are compromised, an attacker still cannot authenticate We outline a very stripped-down version of the protocol: password salting has been removed for clarity, and the exchange itself has been greatly simpliÔ¨Åed. The ClientKey is a hashed version of the password; what the server stores is a secure hash of the ClientKey called StoredKey: StoredKey = hash(ClientKey) The exchange begins with the server sending a random nonce string to the client. The client now calculates theClientSignature as follows: ClientSignature = hash(StoredKey, nonce) Only an attacker who has eavesdropped on this exchange can replicate the ClientSignature, but the server can compute it. The client then sends the following to the server: ClientKey XOR ClientSignature Because the server can calculate ClientSignature, it can extract ClientKey, and verify that h(ClientKey) = StoredKey. An attacker who has obtained StoredKey from the server can generate ClientSignature, but cannot generate ClientKey. 28.6 Secure Hashes 723
An Introduction to Computer Networks, Release 2.0.11 However, an attacker who has both exÔ¨Åltrated StoredKey from the server and eavesdropped on a clientserver exchange is able to generate the ClientSignature and thus extract the ClientKey. The attacker can then authenticate at a later time using ClientKey. For this reason, a secure tunnel is still needed for the authentication. Given the presence of such a tunnel, the SCRAM approach appears to offer a relatively modest security improvement over sending the password as plaintext. Note, though, that with SCRAM the server does not see the password at all, so if the client mistakenly connects to the wrong server, the password is not revealed. See below at 28.8.2 Simultaneous Authentication of Equals for another, mutual, form of password authentication. 28.7 Shared-Key Encryption Secure hashes can provide authentication, but to prevent eavesdropping we need encryption. While publickeyencryption ( 29 Public-Key Encryption ) is perhaps more glamorous, the workhorse of the encryption world is the shared-key cipher, orshared-secret orsymmetric cipher, in which each party has possession of a key K, used for both encryption and decryption. Shared-key ciphers are often quite fast; public-key encryption is generally slower by several orders of magnitude. As a result, if two parties without a shared key wish to communicate, they will almost always used public-key encryption only to exchange a key for a shared-key cipher, which will then be used for the actual message encryption. Typical key lengths for shared-key ciphers believed secure range from 128 bits to 256 bits. For most sharedkey ciphers there are no known attacks that are much faster than brute force, and 22561077is quite a large number. Shared-key ciphers can be either block ciphers, encrypting data in units of blocks that might typically be 8 bytes long, or stream ciphers, which generate a pseudorandom keystream. Each byte (or even bit) of the message is then encrypted by (typically) XORing it with the corresponding byte of the keystream. 28.7.1 Session Keys The more messages a key is used to encrypt, the more information a potential eavesdropper may have with which to launch a codebreaking attack. If Alice and Bob have a pre-arranged shared secret key K, is therefore quite common for them to encrypt the bulk of their correspondence with temporary session keys, each one used for a time period that may range from minutes to days. The secret key K might then be used only to exchange new session keys and to forestall man-in-the-middle attacks ( 29.3 Trust and the Man in the Middle ) by signing important messages ( 28.6.1 Secure Hashes and Authentication ). If DifÔ¨Åe-HellmanMerkle key exchange (ref: difÔ¨Åe-hellman ) is used, K might be reserved only for message signing. Sometimes session keys are entirely different keys, chosen randomly; if such a key is discovered, the attacker learns nothing about the secret key K. Other times, the session key is a mash-up of K and some additional information (such as an initialization vector, 28.7.3 Cipher Modes ), that is transmitted in the clear. The session key might then be the concatenation of K and the initialization vector, or the XOR of the two. Either approach means that an eavesdropper who Ô¨Ågures out the session key also has the secret key K, but the use of such session keys still may make any codebreaking attempt much more difÔ¨Åcult. 724 28 Security
An Introduction to Computer Networks, Release 2.0.11 In29.2 Forward Secrecy we consider the reverse problem of how Alice and Bob might keep a session key private even if the secret key is compromised. 28.7.2 Block Ciphers As mentioned, a block cipher encrypts data one block at a time, typically with a key length rather longer than the block size. A typical block cipher proceeds by the iterated application of a sequence of round functions, each updating the block and using some round-dependent substring of the key as auxiliary input. If we start with a block of plaintext P and apply all the round functions in turn, the result is the ciphertext block C = E(P,K) = E K(P). The process can be reversed so that P = D(C,K) = D K(C). A common framework is that of the Feistel network. In such an arrangement, the block is divided into two or more words sized for the machine architecture; an 8-byte block is typically divided into two 32-bit words which we can call L and H for Low and High. A typical round function is now of the following form, where K is the key and F(x,K) takes a word x and the key K and returns a new word: xL,Hy√ù√ëx H, L XOR F(H,K) y Visually, this is often diagrammed as LH XORFK One round here scrambles only half the block, but the other half gets scrambled in the next round (sometimes the operation above is called a half-round for that reason). The total number of rounds is typically somewhere between 10 and 50. Much of the art of designing high-quality block ciphers is to come up with round functions that result in overall very fast execution, without introducing vulnerabilities. The more rounds, the slower. The internal function F, often different for each round, may look at only a designated subset of the bits of the key K. Note that the operation above is invertible ‚Äì that is, can be decrypted ‚Äì regardless of F; given the right-hand side the receiver can compute F(H,K) and thus, by XORing this with L XOR F(H,K), can recover L. This remains true if, as is sometimes the case, the XOR operation is replaced with ordinary addition. Crypto Law The Salsa20 cipher mentioned here is a member of Daniel Bernstein‚Äôs ‚ÄúsnufÔ¨Çe‚Äù family of ciphers based 28.7 Shared-Key Encryption 725
An Introduction to Computer Networks, Release 2.0.11 on secure-hash functions. In the previous century, the US government banned the export of ciphers but not secure-hash functions. They also at one point banned the export (and thus publication) of one of Bernstein‚Äôs papers; he sued. In 1999, the US Court of Appeals for the Ninth Circuit found in his favor. The decision, 176 F.3d 1132, is the only appellate decision of which the author is aware that contains (in the footnotes) not only samples of C code, but also of Lisp. A simple F might return the result of XORing H and a subset of the bits of K. This is usually a little too simple, however. Ordinary modulo-32 addition of H and a subset of K often works well; the interaction of addition and XORing introduces considerable bit-shufÔ¨Çing (or diffusion in the language of cryptography). Other operations used in F(H,K) include Boolean AND and OR operations. 32-bit multiplication introduces considerable bit-shufÔ¨Çing, but is often computationally more expensive. The Salsa20 cipher of [DB08] uses only XOR and addition, for speed. It is not uncommon for the round function also to incorporate ‚Äúbit rotation‚Äù of one or both of L and H; the result of bit-rotation of 1000 1110 two places to the left is 00 11 1010. If a larger blocksize is desired, say 128 bits, but we still want to avail ourselves of the efÔ¨Åciency of 32-bit operations, the block can be divided into xA,B,C,Dy. The round function might then become xA,B,C,Dy√ù√ëx B,C,D, (A XOR F(B,C,D,K)) y As mentioned above, many secure-hash functions use block-cipher round functions that then use successive chunks of the message being hashed in place of the key. In the MD5 algorithm, block A above is transformed into the 32-bit sum of input-message fragment M, a constant K, and G(B,C,D) which can be any of several Boolean combinations of B, C and D. An alternative to the Feistel-network framework for block ciphers is the use of so-called substitutionpermutation networks. The Ô¨Årst block cipher in widespread use was the federally sanctioned Data Encryption Standard, or DES (commonly pronounced ‚Äúdez‚Äù). It was developed at IBM by 1974 and then selected by the US National Bureau of Standards (NBS) as a US standard after some alterations recommended by the National Security Agency (NSA). One of the NSA‚Äôs recommendations was that a key size of 56 bits was sufÔ¨Åcient; this was in an era when the US government was very concerned about the spread of strong encryption. For years, many people assumed the NSA had intentionally introduced other weaknesses in DES to facilitate government eavesdropping, but after forty years no such vulnerability has been discovered and this no longer seems so likely. The suspicion that the NSA had in the 1970‚Äôs the resources to launch brute-force attacks against DES, however, has greater credence. In 1997 an academic team launched a successful brute-force attack on DES. The following year the Electronic Frontier Foundation (EFF) built a hardware DES cracker for about US$250,000 that could break DES in a few days. In an attempt to improve the security of DES, triple-DES or 3DES was introduced. It did not become an ofÔ¨Åcial standard until the late 1990‚Äôs, but a two-key form was proposed in 1978. 3DES involves three applications of DES with keys xK1,K2,K3y; encrypting a plaintext P to ciphertext C is done by C = EK3(DK2(EK1(P))). The middle deciphering option D K2means the algorithm reduces to DES when K1 = K2 = K3; it also reduces exposure to a particular vulnerability known as ‚Äúmeet in the middle‚Äù (no relation to ‚Äúman in the middle‚Äù). In [MH81] it is estimated that 3DES with three distinct keys has a strength roughly equivalent to 2 56 = 112 bits. That same paper also uses the meet-in-the-middle attack to show 726 28 Security
An Introduction to Computer Networks, Release 2.0.11 that straightforward ‚Äúdouble-DES‚Äù encryption C = E K2(EK1(P)) has an effective keystrength of only 56 bits ‚Äì no better than single DES ‚Äì if sufÔ¨Åcient quantities of plaintext and corresponding ciphertext are known. As concerns about the security of DES continued to mount, the US National Institute of Standards and Technology (NIST, the new name for the NBS) began a search for a replacement. The result was the Advanced Encryption Standard, AES, ofÔ¨Åcially approved in 2001. AES works with key lengths of 128, 192 and 256 bits. The algorithm is described in [DR02], and is based on the Rijndael family of ciphers; the name Rijndael (‚Äúrain-dahl‚Äù) is a combination of the authors‚Äô names. Earlier non-DES ciphers include IDEA, the International Data Encryption Algorithm, described in [LM91], andBlowÔ¨Åsh, described in [BS93]. Both use 128-bit keys. The IDEA algorithm was patented; BlowÔ¨Åsh was intentionally not patented. A successor to BlowÔ¨Åsh is TwoÔ¨Åsh. 28.7.3 Cipher Modes The simplest encryption ‚Äúmode‚Äù for a block cipher is to encrypt each input block independently. That is, if Piis the ith plaintext block, then the ith ciphertext block C iis E(P i,K). This is known as electronic codebook orECB mode. ECB is vulnerable to known-plaintext attacks. Suppose the attacker knows that the message is of the following form, where the vertical lines are the block boundaries: Bank Transfer | to MALLORY | Amount $1000 If the attacker also knows the ciphertext for ‚ÄúAmount $100,000‚Äù, and is in a position to rewrite the message (or to inject a new message), then the attacker can combine the Ô¨Årst two blocks above with the third $100,000 block to create a rather different message, without knowing the key. At http://en.wikipedia.org/wiki/Block_ cipher_mode_of_operation there is a remarkable example of the failure of ECB to fail to effectively conceal an encrypted image. As a result, ECB is seldom used. A common alternative is cipher block chaining or CBC mode. In this mode, each plaintext block is XORed with the previous ciphertext block before encrypting: Ci= E(K,C i-1XOR P i) To decrypt, we use Pi= D(K,C i) XOR C i-1 If we stop here, this means that if two messages begin with several identical plaintext blocks, the encrypted messages will also begin with identical blocks. To prevent this, the Ô¨Årst ciphertext block C 0is a random string, known as the initialization vector, or IV. The plaintext is then taken to start with block P 1. The IV is sent in the clear, but contains no private information. CBC works well when encrypting reliable streams of data, such as are delivered by TCP. But CBC becomes more difÔ¨Åcult if some packets might be lost, as may occur with UDP transport. If one cipher block is lost, say C i-1, then we cannot decrypt C i, because we need to XOR the lost block into the Ô¨Ånal mix. We can decrypt C i+1, and subsequent cipher blocks, but if C istarts a packet, then that packet is effectively lost, and so the nondelivery of one packet leads to the effective loss of two. See exercise 5.0. 28.7 Shared-Key Encryption 727
An Introduction to Computer Networks, Release 2.0.11 28.7.4 Stream Ciphers Conceptually, a stream cipher encodes one byte at a time. The cipher generates a pseudorandom keystream. If K iis the ith byte of the keystream, then the ith plaintext byte P iis encrypted as C i= PiXOR K i. A stream cipher can be viewed as a special form of pseudorandom number generator or PRNG, though PRNGs used for numeric methods and simulations are seldom secure enough for cryptography. Simple XORing of the plaintext with the keystream may not seem secure, but if the keystream is in fact truly random then this cipher is unbreakable: to an attacker, all possible plaintexts are equally likely. A truly random keystream means that the entire keystream must be shared ahead of time, and no portion may ever be reused; this cipher is known as the one-time pad. Stream ciphers do not, by themselves, provide authenticity. An attacker can XOR something with the encrypted message to change it. For example, if the message is known to be Transfer $ 02000 to Mallory ^^ then the attacker can XOR the two bytes over the ‚Äú^‚Äù with ‚Äò0‚Äô XOR ‚Äò2‚Äô, changing the character ‚Äò0‚Äô to a ‚Äò2‚Äô and the ‚Äò2‚Äô to a ‚Äò0‚Äô. The attacker does this to the encrypted stream, but the decrypted plaintext stream retains the change. Appending an authentication code such as HMAC, 28.6.1 Secure Hashes and Authentication, prevents this. Stream ciphers are, in theory, well suited to the encryption of data sent a single byte at a time, such as data from a telnet session. The ssh protocol ( 29.5.1 SSH ), however, generally uses block ciphers; when it has to send a single byte it pads the block with random data. 28.7.4.1 RC4 The RC4 stream cipher was quite widely used. It was developed by Ron Rivest at RSA Security in 1987, but never formally published. The code was leaked, however, and so the internal details are widely available. ‚ÄúUnofÔ¨Åcial‚Äù implementations are sometimes called ARC4 or ARCFOUR, the ‚ÄúA‚Äù for ‚ÄúAlleged‚Äù. RC4 was designed for very fast implementation in software; to this end, all operations are on whole bytes. RC4 generates a keystream of pseudorandom bytes, each of which is XORed with the corresponding plaintext byte. The keystream is completely independent of the plaintext. The key length can range from 5 up to 256 bytes. The unofÔ¨Åcial protocol contains no explicit mechanism for incorporating an initialization vector, but an IV is well-nigh essential for stream ciphers; otherwise an identical keystream is generated each time. One simple approach is to create a session key by concatenating the IV and the secret RC4 key; alternatively (and perhaps more safely) the IV and the RC4 key can be XORed together. RC4 was the basis for the ill-fated WEP Wi-Fi encryption, 28.7.7 Wi-Fi WEP Encryption Failure, due in part to WEP‚Äôs requirement that the 3-byte IV precede the 5-byte RC4 key. The Ô¨Çaw there did not affect other applications of RC4, but newer attacks have suggested that RC4 be phased out. Internally, an RC4 implementation maintains the following state variables: An array S[] representing a permutation i √ëS[i] of all bytes two 8-bit integer indexes to S[], i and j 728 28 Security
An Introduction to Computer Networks, Release 2.0.11 S[] is guaranteed to represent a permutation ( ieis guaranteed to be a 1-1 map) because it is initialized to the identity and then all updates are transpositions (involving swapping S[i] and S[j]). Initially, we set S[i] = i for all i, and then introduce 256 transpositions. This is known as the key-scheduling algorithm. In the following, keylength is the length of the key key[] in bytes. J=0; forI=0 to 255: J = J + S[I] + key[I mod keylength]; swap S[I] andS[J] As we will see in 28.7.7 Wi-Fi WEP Encryption Failure, 256 transpositions is apparently not enough. After initialization, IandJare reset to 0. Then, for each keystream byte needed, the following is executed, whereIandJretain their values between calls: I = (I+1) mod 256 J = J + S[I] mod 256 swap S[I] andS[J] returnS[ S[I] + S[J] mod 256 ] ForI= 1, the Ô¨Årst byte returned is S[ S[1] + S[S[1]] ]. 28.7.5 Block-cipher-based stream ciphers It is possible to create a stream cipher out of a block cipher. The Ô¨Årst technique we will consider is counter mode, orCTR mode. The sender and receiver agree on an initial block, B, very similar to an initialization vector. The Ô¨Årst block of the keystream, K 0, is simply the encrypted block E(B,K). The ith keystream block, for i>0, is K i= E(B+i,K), where ‚ÄúB+i‚Äù is the result of treating B as a long number and performing ordinary arithmetic. Going from B+1 to B+(i+1) typically changes only one or two bits before encryption, but no signiÔ¨Åcant vulnerability based on this has ever been found, and this form of counter mode is now widely believed to be as secure as the underlying block cipher. Note that the E(B+i), for different i, can also be computed in parallel; CBC encryption, by comparison, cannot be parallelized. Similarly, if cipher block j never arrives, then E(B+j) need never be computed. A related technique is to generate the keystream by starting with a secret key K and taking a secure hash of each of the successive concatenations K"1, K"2,etc. The drawback to this approach is that many secure-hash functions have not been developed with this use in mind, and unforeseen vulnerabilities may exist. Stream ciphers in general face an error-recovery problem, if the underlying transport is unreliable and so blocks may be lost. However, resynchronization after lost packets becomes trivial if each packet contains the block number i of the Ô¨Årst cipher block of the packet; in some arrangements it is sufÔ¨Åcient to include the packet number. The receiver can then easily compute E(B+i,K), etc, and decrypt the packet. This technique is used in WPA2 encryption used by Wi-Fi. Individual Wi-Fi packets are, of course, frequently lost, but each packet contains a 6-byte packet number that enables decryption in the absence of previous packets. The packet numbers do not help eavesdroppers, as in the absence of losses they can be determined simply by keeping track of the packet count. 28.7 Shared-Key Encryption 729
An Introduction to Computer Networks, Release 2.0.11 The keystream calculated by counter mode is completely independent of the plaintext. This is not always the case. In cipher feedback mode, or CFB, we initialize C 0to the initialization vector, and then deÔ¨Åne the keystream blocks K ifor i>0 as follows: Ki= E K(Ci-1) As usual, C i= P iXOR K ifor i>0, so the right-hand side above is E K(Pi-1XOR K i-1). The evolution of the keystream thus depends on earlier plaintext. However, if a cipher block is lost, the receiver now faces resynchronization problems. 28.7.6 Encryption and Authentication If the ciphertext is modiÔ¨Åed in transit, most decryption algorithms will happily ‚Äúdecrypt‚Äù it anyway, though perhaps to gibberish; encryption provides no implicit validity check. Worse, it is sometimes possible for an attacker to modify the ciphertext so as to produce a meaningful, intentional change in the resultant plaintext. Examples of this appear above in 28.7.4 Stream Ciphers and28.7.3 Cipher Modes; for CBC block ciphers see exercise 5.0. It is therefore common practice to include HMAC authentication signatures ( 28.6.1 Secure Hashes and Authentication ) with each encrypted message; the use of HMAC rather than a simple checksum ensures that an attacker cannot modify messages and have them still appear to be valid. HMAC may not identify the sender as positively as public-key digital signatures, 29.1.1 RSA and Digital Signatures, but it does positively identify the sender as the same entity with whom the receiver negotiated the key. This is all that is needed. Appending an HMAC signature does more than prevent garbled messages; there are attacks that, in the absence of such signatures, allow full decryption of messages (particularly if the attacker can create multiple ciphertexts and Ô¨Ånd out which ones the recipient was able to decrypt successfully). See [SV02] for an example. There are three options for combining encryption with HMAC; for consistency with the literature we replace HMAC with the more general MAC (for Message Authentication Code): 
- MAC-then-encrypt: calculate the MAC signature on the plaintext, append it, and encrypt the whole 
- encrypt-and-MAC: calculate the MAC signature on the plaintext, encrypt the message, and append to that the MAC 
- encrypt-then-MAC: encrypt the plaintext and calculate the MAC of the ciphertext; append the MAC to the ciphertext These are analyzed in [BN00], in which it is proven that encrypt-then-MAC in general satisÔ¨Åes some stronger cryptographic properties than the others, although these properties may hold for the others in special cases. Encrypt-then-MAC means that no decryption is attempted of a forged or spoofed message. 28.7.7 Wi-Fi WEP Encryption Failure The WEP (Wired-Equivalent Privacy) mechanism was the Ô¨Årst Wi-Fi encryption option, introduced in 1999; see4.2.5 Wi-Fi Security. It used RC4 encryption with either a 5-byte or 13-byte secret key; this was always appended to a 3-byte initialization vector that, of necessity, was sent in the clear. The RC4 session key was thus either 8 or 16 bytes; the shorter key was more common. 730 28 Security
An Introduction to Computer Networks, Release 2.0.11 There is nothing inherently wrong with that strategy, but the designers of WEP made some design choices that were, in retrospect, infelicitous: 
- the secret key was appended to the IV (rather than, say, XORed with it) 
- the IV was only three bytes long 
- a new RC4 keystream and new IV was used for each packet The third choice above was perhaps the more serious mistake. One justiÔ¨Åcation for this choice, however, was that otherwise lost packets (which are common in Wi-Fi) would represent gaps in the received stream, and the receiver might have trouble Ô¨Åguring out the size of the gap and thus how to decrypt the later arrivals. A contributing issue is that the Ô¨Årst byte of the plaintext ‚Äì a header byte from the Wi-Fi packet ‚Äì is essentially known. This Ô¨Årst byte is generally from the Logical Link Control header, and contains the same value as the Ethernet Type Ô¨Åeld (see 4.2.4 Access Points ). All IP packets share the same value. Thus, by looking at the Ô¨Årst byte of the encrypted packet, an attacker knows the Ô¨Årst byte of the keystream. A theoretical WEP vulnerability was published in [FMS01] that proved all-too-practical in the real world. We will let K[] denote the 8-byte key, with K[0],K[1],K[2] representing the three-byte initialization vector and K[i] for 3¬§i<8 representing the secret key. The attacker monitors the airwaves for IVs of a particular form; after about 60 of these are collected, the attacker can make an excellent guess at K[3]. Now a slightly different group of IVs is collected, allowing a guess at K[4] and so on. The bytes of the secret key thus fail sequentially. The attack requires about the same (modest) time for each key byte discovered, so a 16-byte total key length does not add much more security versus the 8-byte key. Because the IV is only three bytes, the time needed to collect packets containing the necessary special IV values is modest. We outline the attack on the Ô¨Årst secret byte, K[3]. The IV‚Äôs we are looking for have the form x3,-1,Xy where -1 = 255 for 8-bit bytes; these are sometimes called weak IVs. One IV in 216is of this form, but a more careful analysis (which we omit) can increase the usable fraction of IVs substantially. To see why these IVs are useful, we need to go back to the RC4 key-scheduling algorithm, 28.7.4.1 RC4. We start with an example in which the Ô¨Årst six bytes of the key is K[] = [3,-1,5,4,-14,3] The IV here is x3,-1,5y. Recall that the array S is initialized with S[i] = i; this represents S = S 0. At this point, I and J are also 0. We now run the following loop, introducing one transposition to S per iteration: forI=0 to 255: J = J + S[I] + K[I mod keylength]; swap S[I] andS[J] The Ô¨Årst value of J, whenI= 0, isK[0] which is 3. After the Ô¨Årst transposition, S is as follows, where the swapped values are in bold: 28.7 Shared-Key Encryption 731
An Introduction to Computer Networks, Release 2.0.11 3120456789101112131415161718192001234567891011121314151617181920 For the next iteration, Iis 1 andJis 3+1+(-1) = 3 again. After the swap, S is 3021456789101112131415161718192001234567891011121314151617181920 Next,Iis 2 andJis 3+2+5 = 10. In general, if the IV were x3,-1,Xy,Jwould be 5+X. 3010145678921112131415161718192001234567891011121314151617181920 At this point, we have processed the three-byte IV; the next iteration involves the Ô¨Årst secret byte of K.Iis 3 andJbecomes 10+1+ K[3] = 15: 3010154567892111213141161718192001234567891011121314151617181920 Recall that the Ô¨Årst byte returned in the keystream is S[ S[1] + S[S[1]] ]. At this point, that would beS[0+3] = 15. From this value, 15, and the IV x3,-1,5y, the attacker can calculate, by repeating the process above, that K[3] = 4. If the IV were x3,-1,Xy, then, as noted above, in the I=2 step we would have J= 5+X, and then in the I=3 step we would have J= 6 + X +K[3]. If Y is the value of the Ô¨Årst byte of the keystream, using S[] as of this point, then K[3] = Y ‚Äì X ‚Äì 6 (assuming that X is not -5 or -4, so that S[0] andS[1] are not changed in step 3). Ifnone ofS[0] ,S[1] andS[3]] are changed in the remaining 252 iterations, the value we predict here after three iterations ‚Äì eg15 ‚Äì would be the Ô¨Årst byte of the actual keystream. Treating future selections of the valueJas random, the probability that one iteration does notselectJin the set {0,1,3} ‚Äì and thus leaves these values alone ‚Äì is 253/256. The probability that the remaining iterations do not change these values in S[] is thus about (253/256)2525% A 5% success rate is not terribly impressive, on the face of it. But 5% of the time we identify K[3] correctly, and the other 95% of the time the (incorrect) values we calculate for K[3] are uniformly, randomly distributed in the range 0 to 255. With 60 guesses, we expect about three correct guesses. The other 57 are spread about with, most likely, no more than two guesses for any one value. If we look for the value guessed 732 28 Security
An Introduction to Computer Networks, Release 2.0.11 most often, it is likely to be the true value of K[3]; increasing the number of x3,-1,XyIVs will increase the certainty here. For the sake of completeness, in the next two iterations, Iis 4 and 5 respectively, and Jis 15+4+(-14) = 5 and 5+4+3=12 respectively. The corresponding contents of S[] are 3010155467892111213141161718192001234567891011121314151617181920 3010155126789211413141161718192001234567891011121314151617181920 Now that we know K[3], the next step is to Ô¨Ånd K[4]. Here, we search the trafÔ¨Åc for IVs of the form x4,-1,Xy, but the general strategy above still works. The [FMS01] attack, as originally described, requires on average about 60 weak IVs for each secret key byte, and a weak IV turns up about every 216packets. Each key byte, however, requires a different IV, so any one IV has one chance per key byte to be a weak IV. To guess a key thus requires 60 655364 million packets. But we can do quite a bit better. In [TWP07], the number of packets needed is only about 40,000 on average. At that point the attack is entirely practical. This vulnerability was used in the attack on TJX Corp mentioned in the opening section. 28.8 DifÔ¨Åe-Hellman-Merkle Exchange How do two parties agree on a secret key K? They can meet face-to-face, but that can be expensive or even not possible. We would like to be able to encrypt, for example, online shopping transactions, which often occur at the spur of the moment. DifÔ¨Åe, Hellman and Merkle DifÔ¨Åe and Hellman‚Äôs paper [DH76] appeared in 1976, but they reference Merkle‚Äôs later-published paper [RM78] as ‚Äúsubmitted‚Äù. In [MH04], Hellman suggested ‚Äúif you‚Äôre going to put names on it, it should be called DifÔ¨Åe-Hellman-Merkle key exchange, since it‚Äôs actually based on a concept of Merkle‚Äôs.‚Äù The algorithm was apparently discovered in 1974 by Malcolm Williamson at GCHQ, but Williamson‚Äôs work was classiÔ¨Åed. One approach is to use public-key encryption, below, but that came two years later. The DifÔ¨Åe-HellmanMerkle approach, from [DH76] and [RM78], allows private key exchange without public keys. The goal here is for Alice and Bob to exchange information over a public network so that, at the end, Alice and Bob have determined a common key, and no eavesdropper can determine it without extraordinary computational effort. 28.8 DifÔ¨Åe-Hellman-Merkle Exchange 733
An Introduction to Computer Networks, Release 2.0.11 As a warmup, we begin with a simple example. Alice and Bob each choose a number; say Alice chooses 7 and Bob chooses 9. Alice sends to Bob the value A = 37= 2187, and Bob sends to Alice the value B = 39 = 19683. Alice then computes B7= 397, and Bob computes A7= 379. If they each use 32-bit arithmetic, they have each arrived at 2111105451 (the exact value is 1144561273430837494885949696427). The catch here is that any intruder with a calculator can recover 7 from A and 9 from B, by factoring. But if we make the arithmetic a little more complicated, this becomes extremely difÔ¨Åcult. In the actual protocol, Alice and Bob agree, publicly, on a large prime p (typically several hundred digits) and a small value g, discussed below. Alice then chooses a<p, and sends to Bob the value A = gamod p. Similarly, Bob chooses b<p and sends to Alice the value B = gbmod p. As before, Alice and Bob now compute Bamod p and Abmod p, which both equal gabmod p. This becomes their shared secret, from which a key can be derived. The question is whether an eavesdropper who knows A and B and g can compute a and b. This is the discrete logarithm problem, as in some mod-p sense a = log g(A). It is indeed believed to be computationally intractable, provided that the order of g ‚Äì the smallest k>0 such that gk= 1 modulo p, is large. In DifÔ¨ÅeHellman-Merkle key exchange, g is a so-called primitive root modulo p, meaning that its order is p‚Äì1, the largest possible value. A naive attempt to compute a from A is to try each gifor i<p until one Ô¨Ånds a match; this is much too slow to be practical. There is some work involved in Ô¨Ånding a suitable primitive root g given p, but that can be public, and is usually not tootime-consuming; see the next section. Traditionally, reusing the same p and g was regarded as safe; in fact, popular (p,g) pairs for various lengths of p were published in RFC 2409 andRFC 3526, and were (and still are) widely deployed. However, (p,g) reuse must be done with care; in particular, p must be large enough. Otherwise one may be vulnerable to the logjam attack described in [ABDGHSTVWZ15]. For a Ô¨Åxed prime p, it turns out that relatively fast discrete-logarithm computation ‚Äì that is, Ô¨Ånding a from ga, modulo p ‚Äì is possible if an attacker is able to pre-compute a very large table. For 512-bit primes, the average logarithm-calculation time in [ABDGHSTVWZ15], once the table was constructed, was 90 seconds. Time for calculation of the table itself took almost 105core-hours, though that could be compressed into as little as one week with highly parallel hardware. By comparison, the authors were able to factor 512-bit RSA keys in about 5,000 core-hours; see 29.1.2 Factoring RSA Keys. The authors of [ABDGHSTVWZ15] conjecture that the NSA has built such a table for some well-known (and commonly used) 1024-bit primes. However, for those with fewer computational resources, the logjam attack also includes strategies for forcing common server implementations to downgrade to the use of 512bit primes using TLS implementation vulnerabilities; cfthe POODLE attack (Ô¨Årst sidebar at 29.5 SSH and TLS). The DifÔ¨Åe-Hellman-Merkle exchange is vulnerable to a man-in-the-middle attack, discussed in greater detail in 29.3 Trust and the Man in the Middle. If Mallory intercepts Alice‚Äôs gaand sends Bob his own ga‚Äô, and similarly intercepts Bob‚Äôs gband replaces it with his own gb‚Äô, then the Alice‚ÄìMallory link will negotiate key gab‚Äôand the Mallory-Bob link will negotiate ga‚Äôb. Mallory then remains in the middle, intercepting, decryping, re-encrypting and forwarding each message between Alice and Bob. The usual Ô¨Åx is for Alice and Bob to sign their gaand gb; see 29.2 Forward Secrecy. 734 28 Security
An Introduction to Computer Networks, Release 2.0.11 28.8.1 Fast Arithmetic It is important to note that the basic arithmetic operations here ‚Äì egcalculating gamod p ‚Äì can be done in polynomial time with respect to the number of binary digits in p, sometimes called the bit-length of p. This is not completely obvious, as the naive approach of multipying out g g.. .g, a times, takes O(p) steps when ap, which is exponential with respect to the bit-length of p. The trick is to use repeated squaring: to compute g41, we note 41 = 101001 in binary, that is, 41 = 25+ 23+ 1, and so g41= ((((g2)2)2)2)2((g2)2)2g A simple python3 implementation of this appears at 29.8 RSA Key Examples. It is also worth noting that even Ô¨Ånding large primes is not obviously polynomial in the number of digits. We can, however, use fast probabilistic primality testing, and note that the Prime Number Theorem guarantees that the number of candidates we must test to Ô¨Ånd a prime near a given number n is O(log n). Finally, there is in general no fast algorithm for Ô¨Ånding a primitive root g modulo p ‚Äì a number such that gk = 1 mod p for k=p-1, but not for any smaller k. The straightforward approach involves factoring p-1, and in general factoring of large numbers is believed to be quite hard. However, Ô¨Ånding primitive roots can be done for ‚Äúspecial‚Äù primes, for example, primes p of the form p = 2q+1 for a second prime q. In this case, g<p is a generator if g21 mod p and gq= -1 mod p. Such special primes are straightforward to Ô¨Ånd, usually by searching Ô¨Årst for the prime q and then checking if 2q+1 is also prime. However, this process is often considered too slow to be performed on demand, as part of an interactive exchange; instead, p and g are calculated in advance. 28.8.2 Simultaneous Authentication of Equals The Simultaneous Authentication of Equals protocol, or SAE, is an approach to mutual password authentication: each side has the password, and each side veriÔ¨Åes to the other that it knows the password. Note that an exchange of passwords cannot achieve this. It can be compared to the WPA2 four-way handshake used in Wi-Fi, 4.2.5.1 WPA2 Four-way handshake; in fact, in the WPA3 standard SAE is partly replacing the four-way handshake. The SAE protocol was introduced in [DH08]; a later variant, known as DragonÔ¨Çy, is described in RFC 7664. The version used in WPA3 is deÔ¨Åned in the IEEE standard 802.11-2016. All are essentially the same protocol, with later versions including various technical cryptographic reÔ¨Ånements. A secondary outcome of SAE, beyond mutual authentication, is that each side generates a shared secret that can be used as an encryption key. In this sense, SAE acts as a key-exchange protocol; cf 28.8 DifÔ¨ÅeHellman-Merkle Exchange, and, in fact, parts of SAE strongly resemble DifÔ¨Åe-Hellman-Merkle key exchange. The SAE protocol will be presented here using arithmetic modulo a large prime number, although elliptic curves (sidebar at 29.1 RSA ) can also be used. Each side, Alice and Bob, initially knows a password, likely a string, and each side agrees on a large prime number p. Each side also agrees on a number g (known as the generator) such that the order of g modulo p ‚Äì the smallest k>0 such that gk= 1 modulo p ‚Äì is large. For most purposes a primitive root modulo p would be sufÔ¨Åcient, though RFC 7664 (but not [DH08]) requires that the order of g modulo p be a large prime divisor q of p-1. (If p=2q+1 for prime q, then g can be the square of a primitive root modulo p.) The requirement that q be prime is not used in the analysis here. 28.8 DifÔ¨Åe-Hellman-Merkle Exchange 735
An Introduction to Computer Networks, Release 2.0.11 Each side initially determines, from the string-format password, a numeric value PW < p. This starts with a hash of the password, modulo p, to create a seed value. PW is then calculated as seed(p‚Äì1)/qmod p. This ensures PWqmod p = 1. The authentication starts with each side choosing random values rand andmask, each less than the value of q. Alice‚Äôs values are randA andmaskA, and likewise for Bob. Each side then computes the following: scal = (rand +mask ) mod q elem = PW‚Äìmaskmod p The names here reÔ¨Çect the fact that scal is a scalar value, and elem is an element of the group generated by g, though for mod-p arithmetic this distinction is purely one of convention. The negative exponent in the second formula here refers to the multiplicative inverse mod p; PW‚Äìmaskrepresents the (unique modulo p) number X such that X PWmaskmod p = 1. Let Alice‚Äôs scal andelem be denoted scalA andelemA, and Bob‚Äôs scalB andelemB. The Ô¨Årst step of the protocol is for Alice and Bob to exchange their scal andelem values. One way to view this is that each side‚Äôs value of rand represents that side‚Äôs contribution to the key, and this value is hidden, or ‚Äúmasked‚Äù, with mask. Each side now computes a secret K as follows. Alice computes K = (PWscalBelemB )randAmod p while Bob computes K = (PWscalAelemA )randBmod p We now show these two values of K are in fact equal. Since scal =rand+mask mod q, we can Ô¨Ånd N so scal =rand+mask+Nq in ordinary integer arithmetic. We now start with Alice‚Äôs value of K: K = (PWscalBelemB)randA = (PW(randB+maskB+Nq)PW‚ÄìmaskB)randA = (PWrandB)randA(because PWqmod p = 1) = PWrandBrandA This last value is the same for both Alice and Bob. For comparison with DifÔ¨Åe-Hellman-Merkle key exchange, note that there Alice and Bob both arrive at the same shared secret gab. The second, and Ô¨Ånal, step of the protocol involves an exchange of veriÔ¨Åcation tokens. This is the passwordveriÔ¨Åcation step. Alice sends to Bob a value like the following, where Hash() is a suitable secure-hash function. Hash(K"elemA"scalA"elemB"scalB) Bob sends something similar. Alice and Bob have now each veriÔ¨Åed to the other that they knew the password. If Alice did not really know PW, and used an incorrect value instead, then she will get a different K, and Bob will know when the second-step tokens are exchanged. Perhaps more importantly, an attacker who eavesdrops on the Alice-Bob exchange obtains nothing of use; there is no known ofÔ¨Çine dictionary attack. In this, SAE is much more resistant to eavesdropping than, say, the WPA2 handshake used by Wi-Fi. The second SAE step is protected by a secure-hash function, and, for the Ô¨Årst step, knowing scal andelem convey nothing about PW, in large part due to the difÔ¨Åculty of solving the discrete logarithm problem. 736 28 Security
An Introduction to Computer Networks, Release 2.0.11 The SAE protocol does require that Alice and Bob each keep a copy of the password cleartext, not a copy of the password hash. This seems unavoidable, given that Bob and Alice must each authenticate to one another. 28.9 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0 Modify the netsploit.c program of 28.2.2.3 The exploit so that the NOPslide is about 1024 bytes long, and the NOPslide and shellcode are now above the overwritten return address in memory. 2.0 Disable ASLR on a Linux system by writing the appropriate value to /proc/sys/kernel/ randomize_va_space. Now get the netsploit attack to work without having the attacked server print out a stack address. You are allowed to make a test run of the server beforehand that prints its address. 3.0 Below are a set of four possible TCP-segment reassembly rules. These rules (and more) appear in [SP03]. Recall that, once values for all positions j ¬§i are received, these values are acknowleged and released to the application; there must be some earlier ‚Äúhole‚Äù for segments to be subject to reassembly at all. 1. Always accept the Ô¨Årst value received for each position. 2. Always accept the last value received for each position, up until the value is released to the application. 3. [‚ÄúBSD‚Äù] For each new segment, discard each position from that segment if that position was supplied by an earlier packet with a left edge less than or equal to that of the new packet. Values in any remaining positions replace any previously received values. 4. [‚ÄúLinux‚Äù] Same as the previous rule, except that we only discard positions that were supplied by an earlier packet with a left edge strictly smaller than that of the new packet. For each of the following two sets of packet arrivals, give the reassembled packet. Values in the same column have the same position. Reassembled packets will all begin with ‚Äúdaa‚Äù and ‚Äúebb‚Äù respectively. (a). a a a a a b b b b b b b c c c c d (b). a a a a a b b b b b c c c c c c d d d e 4.0 Suppose a network intrusion-detection system is 10 hops from the attacker, but the target is 11 hops, behind one more router R. Outline a strategy of tweaking the TTL Ô¨Åeld so that the NIDS receives TCP stream ‚Äúhelpful‚Äù and the target receives ‚Äúharmful‚Äù. Segments should either be disjoint or cover exactly the same range of bytes; you may assume that the target accepts the Ô¨Årst segment for any given range of bytes. 28.9 Exercises 737
An Introduction to Computer Networks, Release 2.0.11 5.0. Suppose Alice encrypts blocks P1, P2 and P3 using CBC mode ( 28.7.3 Cipher Modes ). The initialization vector is C0. The encrypt and decrypt operations are E(P) = C and D(C) = P. We have 
- C1 = E(C0 XOR P1) 
- C2 = E(C1 XOR P2) 
- C3 = E(C2 XOR P3) Suppose Mallory intercepts C2 in transit, and replaces it with C2‚Äô = C2 XOR M; C1 and C3 are transmitted normally; that is, Bob receives [C1‚Äô,C2‚Äô,C3‚Äô] where C1‚Äô = C1 and C3‚Äô = C3. Bob now attempts to decrypt; C1‚Äô decrypts normally to P1 while C2‚Äô decrypts to gibberish. Show that C3‚Äô decrypts to P3 XOR M. (This is sometimes used in attacks to Ô¨Çip a speciÔ¨Åc bit or byte, in cases where earlier gibberish does not matter.) 6.0 Suppose Alice uses a block-based stream cipher ( 28.7.5 Block-cipher-based stream ciphers ); block i of the keystream is Ki and Ci = Ki XOR Pi. Alice sends C1, C2 and C3 as in the previous exercise, and Mallory again replaces C2 by C2 XOR M. What are the three plaintext blocks Bob deciphers? 7.0 Show that if p and q are primes with p = 2q + 1, then g is a primitive root mod p if g 1, g21, and gq1. (This exercise requires familiarity with modular arithmetic and primitive roots.) 8.0 Suppose we have a short message m, ega bank PIN number. Alice wants to send message M to Bob that, without revealing m immediately, can be used later to verify that Alice knew m at the time M was sent. During this later veriÔ¨Åcation, Alice may reveal m itself. (a). Suppose Alice simply sends M = hash(m). Explain how Bob can quickly recover m. (b). How can Alice construct M using a secure-hash function, avoiding the problem of (a)? Hint: as part of the later veriÔ¨Åcation, Alice can supply additional information to Bob. 9.0 In the example of 28.7.7 Wi-Fi WEP Encryption Failure, suppose the IV is x4,-1,5yand the Ô¨Årst two bytes of the key are x10,20y. What is the Ô¨Årst keystream byte S[ S[1] + S[S[1]] ] ? 738 28 Security
29 PUBLIC-KEY ENCRYPTION Shared-key encryption is fast and efÔ¨Åcient, and secure temporary keys can be negotiated via the DifÔ¨ÅeHellman-Merkle algorithm ( 28.8 DifÔ¨Åe-Hellman-Merkle Exchange ). But how do you authenticate the other side? How can Alice be sure she has negotiated a key with Bob, rather than with the NSA The signatures of28.6.1 Secure Hashes and Authentication depend on the pre-negotiated key; how does one authenticate new correspondents? Alternatively, suppose Alice wants to send a secure messge to Bob, but circumstances prevent any backand-forth negotiation of a key? This would apply if Alice wanted, for example, to send an encrypted email to Bob. How can Alice encrypt her message? The answer to both issues is public -key encryption, in which communicating entities can all publish the public part of their (public,private) keypair. We explore public-key encryption in this chapter. In public-key encryption, each party has a public encryption key K Eand a private decryption key K D. Alice can publish her K Eto the world, without making it feasible for an eavesdropper to determine K D. If Bob has Alice‚Äôs K E, he can encrypt a message with it and send it to her; only someone in possession of Alice‚Äôs K D can read it. The fundamental idea behind public-key encryption is that knowledge of K Eshould not offer any meaningful information about K D. 29.1 RSA Public-key encryption was outlined in [DH76], but the best-known technique is arguably the RSA algorithm of [RSA78]. (The RSA algorithm was discovered in 1973 by Clifford Cocks at GCHQ, but was classiÔ¨Åed.) RSA is based on the idea that most arithmetic operations on a large number N take time that is polynomial in the number of bits in N, but factoring a large number N requires time that is exponential in the number of bits in N. To construct RSA keys, one Ô¨Årst Ô¨Ånds two very large primes p and q, perhaps 1024 binary digits each. These primes should be chosen at random, by choosing random N‚Äôs in the right range and then testing them for primality until success. Let n = p q. It now follows from Fermat‚Äôs little theorem that, for any integer m, m(p-1)(q-1)= 1 mod n (it sufÔ¨Åces to show m(p-1)(q-1)= 1 mod p and m(p-1)(q-1)= 1 mod q). One then Ô¨Ånds positive integers e and d so that e d = 1 mod (p-1)(q-1); given any e relatively prime to both p-1 and q-1 it is possible to Ô¨Ånd d using the Extended Euclidean Algorithm (a simple Python implementation appears below in 29.8 RSA Key Examples ). From the claim in the previous paragraph, we now know med = (me)d= m mod p. If we take m as a message (that is, as a bit-string of length less than the bit-length of n, rather than as an integer), we encrypt it as c = memod n. We can decrypt c and recover m by calculating cdmod n = med mod n = m. The public key is the pair xn,ey; the private key is d. 739
An Introduction to Computer Networks, Release 2.0.11 Elliptic-curve cryptography One of the concerns with RSA is that a faster factoring algorithm will someday make the encryption useless. A newer alternative is the use of elliptic curves; for example, the set of solutions modulo a large prime p of the equation y2= x3+ ax + b. This set has a natural (but nonobvious) product operation completely unrelated to modulo-p multiplication, and, as with modulo-p arithmetic, Ô¨Ånding n given B = An, using this product operation, appears to be quite difÔ¨Åcult. Several cryptographic protocols based on elliptic curves have been proposed; see Wikipedia. As with DifÔ¨Åe-Hellman-Merkle ( 28.8.1 Fast Arithmetic ), the operations above can all be done in polynomial time with respect to the bit-lengths of p and q. The theory behind the security of RSA is that, if one knows the public key, the only way to Ô¨Ånd d in practice is to factor n. And factoring is a hard problem, with a long history. There are much faster ways to factor than to try every candidate less than?n, but it is still believed to require, in general, close-to-exponential time with respect to the bit-length of n. See 29.1.2 Factoring RSA Keys below. RSA encryption is usually thousands of times slower than comparably secure shared-key ciphers. However, RSA needs only to be used to encrypt a secret key for a shared-key cipher; it is this latter key that actually protects the document. For this reason, if a message is encrypted for multiple recipients, it is usually not much larger than if it were encrypted for only one: the additional space needed for each additional recipient is just the encrypted key for the shared-key cipher. Similarly, for digital-signature purposes Alice doesn‚Äôt have to encrypt the entire message with her private key; it is sufÔ¨Åcient (and much faster) to encrypt a secure hash of the message. If Alice and Bob want to negotiate a session key using public-key encryption, it is sufÔ¨Åcient for Alice to know Bob‚Äôs public key; Alice does not even need a public key. Alice can choose a session key, encrypt it with Bob‚Äôs public key, and send it to Bob. We will look at an actual calculation of an RSA key and then an encrypted message in 29.8 RSA Key Examples. 29.1.1 RSA and Digital Signatures RSA can also be used for strong digital signatures (as can any public-key system in which the roles of the encryption and decryption keys are symmetric, and can be reversed). Suppose, as above, Alice‚Äôs public key is the pairxn,eyand her private key is the exponent d. If Alice wants to sign a message m to Bob, she simply encrypts it using the exponent d from her private key; that is, she computes c = mdmod n. Bob, along with everyone else in the world, can then decrypt the resulting ciphertext c with Alice‚Äôs public exponent e (that is, ce). If this yields a valid message, then Alice (or someone in possession of her key) must have been the sender. Generally, for signature purposes Alice will encrypt not her entire message but simply a secure hash of it, and then send both the message and the RSA-encrypted hash. Bob decrypts the encrypted hash, and the signature checks out if the result matches the hash of the corresponding message. 740 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 The advantage of a public-key signature over an HMAC signature is that the former uses a key with longterm persistence, and can be used to identify the individual who sent the message. The drawback is that HMAC signatures are much faster. It is common for Alice to sign her message to Bob and then encrypt the message plus signature ‚Äì perhaps with Bob‚Äôs public key ‚Äì before sending it all to Bob. Just as Alice probably prefers not to sign undated documents with her handwritten signature, she will likely prefer to apply her digital signature only to documents containing a timestamp. This helps deter replay attacks. 29.1.2 Factoring RSA Keys The security of RSA depends largely on the difÔ¨Åculty in factoring the key modulus n = pq (though it is theoretically possible that an RSA vulnerability exists that does not entail factoring). As of 2015, the factoring algorithm that appears to be the fastest for larger keys is the so-called number-Ô¨Åeld sieve; an early version of this technique was introduced in [JP88]. A heuristic estimate of the time needed to factor a number n via this algorithm is exp(1.923 log(n)1/3log(log(n))2/3), where exp(x) = exand log(x) is the natural logarithm. To a Ô¨Årst approximation this is exp(k L1/3), where L is the bit-length of N. This is sub-exponential (because of the exponent 1/3) but still extremely slow. Side Channels Those placing serious reliance on the relative security levels outlined here should also be aware of the existence of so-called side-channel attacks, egmaking use of electromagnetic leakage from a computer to infer a key. The Ô¨Årst factorization of an RSA modulus with 512 bits occurred in 1999 and was published in [CDLMR00]; this was part of the RSA factoring challenge. It took seven calendar months, using up to 300 processors in parallel. Fifteen years later, [ABDGHSTVWZ15] reported being able to do such a factorization in eight days on a 24-core machine (~5,000 core-hours), or in seven hours using 1800 cores. Some of the speedup is due to software implementation improvements, but most is due to faster hardware. If we normalize the number-Ô¨Åeld-sieve factoring time of a 512-bit n to 1, we get the following table of estimated relative costs for factoring larger n: key length in bits relative factoring time 512 1 1024 8,000,000 1536 81011 2048 81015 3072 31022 4096 81027 From this we might conclude that 1024-bit keys are potentially breakable by a very determined and wellfunded adversary, while the 2048-bit key length appears to be much safer. In 2011 NIST‚Äôs recommended key length for RSA encryption increased to 2048 bits (publication 800-131A, Table 6). This is quite a bit 29.1 RSA 741
An Introduction to Computer Networks, Release 2.0.11 larger than the 128-to-256-bit recommendation for shared-key ciphers, but RSA factoring attacks are much faster than trying all keys by brute force. Note that some parts of the factoring algorithm are highly parallelizable while other parts are less so; relative factoring times when using highly parallel hardware may therefore differ quite a bit. See also 29.8.1 Breaking the key. 29.2 Forward Secrecy Suppose Alice and Bob exchange a shared-key-cipher session key K Susing their RSA keys. Later, their RSA keys are compromised. If the attacker has retained Alice and Bob‚Äôs prior communications, the attacker can go back and decrypt K S, and then use K Sto decrypt the entire session protected by K S. This is not true, however, if Alice and Bob had used DifÔ¨Åe-Hellman-Merkle key exchange. In that case, there is no encryption used in the process of negotiating K S, so no later encryption compromise can reveal KS. This property is called forward secrecy, or, sometimes, perfect forward secrecy (other times, perfect forward secrecy adds the further requirement that the compromise of any other session key negotiated by Alice and Bob does not reveal information about K S). The advantage using public-key encryption along with DifÔ¨Åe-Hellman-Merkle key exchange, then, is that Alice can sign the key K Sshe sends to Bob. Assuming Bob is conÔ¨Ådent he has Alice‚Äôs real public key, a man-in-the-middle attack ( 29.3 Trust and the Man in the Middle ) becomes impossible. It is now common for public-key encryption to be used to sign all the transactions that are part of the DifÔ¨ÅeHellman-Merkle exchange. When this is done, Alice and Bob gain both forward secrecy andprotection from man-in-the-middle attacks. One drawback of forward secrecy, as described here, is that the DifÔ¨Åe-Hellman-Merkle key exchange nominally requires synchronous exchanges. If Alice is encrypting an email message to Bob, synchronous exchanges are probably not an option, as Bob may be ofÔ¨Çine at the time. One approach to forward secrecy in asynchronous communication, used by the X3DH protocol, is for Alice and Bob to generate prekeys. In the key exchange described above in 28.8 DifÔ¨Åe-Hellman-Merkle Exchange, Alice and Bob agree on a prime p and a generator g, and choose random values a and b respectively. They then exchange gamod p and gbmod p. A prekey for Bob is simply a precomputed gb, shared with a server and signed by Bob. To avoid reuse, Bob will typically generate (and share) many prekeys. When Alice wants to send her asynchronous message, she simply asks the server for one of Bob‚Äôs prekeys. With that and her own ga, she can then calculate the session key K Sand encrypt the message. Alice‚Äôs gamust then be sent along with her encrypted message, so Bob can also calculate K Sand decrypt the message. This prekey approach appears to be quite secure, though it does either commit everyone to using the same key-exchange prime p and generator g (the usual approach), or else requires the generation of a set of prekeys for every potential correspondent. Finally, we note that forward secrecy works best with ephemeral data streams, like browser sessions or voice calls, where the data no longer exists anywhere except in the captured, encrypted packets. But when the data consists of messages that have been archived, a compromise of the device on which they are archived is likely to yield them all; even if they are encrypted, a keylogger on the device may yield the necessary local passwords. While brute-force cracking of Alice‚Äôs keys is a possibility, the most common way of obtaining 742 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 those keys is probably through compromise of her computer. If an attacker succeeds at this, then that attacker will likely also have access to any messages Alice has archived on that computer, forward secrecy notwithstanding. 29.3 Trust and the Man in the Middle Suppose Alice wants to send Bob a message. Where does she Ô¨Ånd Bob‚Äôs public key E B? If Alice goes to a public directory that is not completely secure and trustworthy, she may Ô¨Ånd a key that in fact belongs to Mallory instead. Alice may now fall victim to a man-in-the-middle attack, like that at the end of 28.8 DifÔ¨Åe-Hellman-Merkle Exchange: - Alice encrypts Bob‚Äôs message using Mallory‚Äôs public key E Mal, thinking it is Bob‚Äôs 
- Mallory intercepts and decrypts the message, using his own decryption key D Mal 
- Mallory re-encrypts the message with Bob‚Äôs real public key E Bob 
- Bob decrypts the message with D Bob Despite Mallory‚Äôs inability to break RSA directly, he has read (and may even modify) Alice‚Äôs message. Alice can, of course, get Bob‚Äôs key directly from Bob. Of course, if Alice met Bob, the two could also exchange a key for a shared-key cipher. Wi-Fi in the Middle One of the easiest settings at which to launch a man-in-the-middle attack is a public Wi-Fi hotspot, either as the hotspot owner or by setting up a rogue access point to which some customers mistakenly connect. We now come to the mysterious world of trust. Alice might trust Charlie for Bob‚Äôs key, but not Dave. Alice doesn‚Äôt have to meet Charlie to get Bob‚Äôs key; all she needs is 
- a trusted copy of Charlie‚Äôs public key 
- a copy of Bob‚Äôs public key, together with Bob‚Äôs name, signed by Charlie At the same time, Alice might trust Dave but not Charlie for Evan‚Äôs key. And Bob might not trust Charlie or Dave for Alice‚Äôs key. Mathematically, the trust relationship is neither symmetric nor transitive. To handle the possibility that trust might erode with time, signed keys often have an expiration date. At the small scale, key-signing parties are sometimes held in which participants exchange some keys directly and others indirectly through signing. This approach is sometimes known as the web of trust. At the large scale, certiÔ¨Åcate authorities (29.5.2.1 CertiÔ¨Åcate Authorities ) are entities built into the TLS framework ( 29.5.2 TLS ) that verify that a website‚Äôs public key is as claimed; you are implicitly trusting these certiÔ¨Åcate authorities if your browser vendor trusts them. Both the web of trust and certiÔ¨Åcate authorities are examples of ‚Äúpublic-key infrastructure‚Äù or PKI, which is, broadly, any mechanism for reliably tying public keys to their owners. For applications of public-key encryption that manage to avoid the need for PKI, by use of cryptographically generated addresses, see 11.6.4 Security and Neighbor Discovery and the discussion of .onion addresses at the end of 10.1 DNS. 29.3 Trust and the Man in the Middle 743
An Introduction to Computer Networks, Release 2.0.11 29.4 End-to-End Encryption Many communications applications are based on the model of encryption from each end-user to the central server. For example, Alice and Bob might both use https (based on TLS, 29.5.2 TLS ) to encrypt their interactions with their email provider. This means Alice and Bob are now trusting that provider, who decrypts messages from Alice, stores them, and re-encrypts them when delivering them to Bob. This model does protect Alice and Bob from Internet eavesdroppers who have not breached the security of the email provider. However, it also allows government authorities to order the email provider to turn over Alice and Bob‚Äôs correspondence. If Alice and Bob do not wish to trust an intermediary, or their (or someone else‚Äôs) government, they need to implement end-to-end encryption. That is, Alice and Bob must negotiate a key, use that key to encrypt messages between them, and not divulge the key to anyone else. This is quite a bit more work for Alice and Bob, and even more complicated if Alice wishes to use end-to-end encryption with a large number of correspondents. Of course, even with end-to-end encryption Alice may still be compelled by subpoena to turn over her correspondence with Bob, but that is a different matter. Alice‚Äôs private key may also be seized under a search warrant. It is common (though not universal) to protect private keys with a password; this is good practice, but protecting a key having an effective length of 256 bits with a password having an effective length of 32 bits leaves something to be desired. The mechanisms of 28.6.2 Password Hashes provide only limited relief. The mechanism of 29.2 Forward Secrecy may be more useful here, assuming Alice can communicate to Bob that her previous key is now compromised; see also 29.5.2.3 CertiÔ¨Åcate revocation. 29.5 SSH and TLS We now look at two encryption mechanisms in popular use on the Internet. The Ô¨Årst is the Secure Shell, SSH, used to allow login to remote systems, remote command execution, Ô¨Åle transfer and even some forms of VPN tunneling. Public-key-encryption is optional; if it is used, the public keys are generally transported manually. POODLE The 2014 POODLE attack was based in part on a long-known Ô¨Çaw in SSL 3.0. But SSL 3.0 is Ô¨Åfteen years obsolete (though it was not ofÔ¨Åcially deprecated until 2015); the other Ô¨Çaw was a broken versionnegotiation implementation in which the disruption of a few packets by an attacker could force both client and server to settle on SSL 3.0 even when both supported the latest TLS version. See 29.5.2.4 TLS Connection Setup. The second example is the Transport Layer Security protocol, TLS, which is a successor of the Secure Sockets Layer, or SSL. Many people still refer to TLS by the SSL name even though TLS replaced SSL in 1999, though, to be fair, TLS is effectively an update of SSL, and TLS v1.0 could easily have been named SSL v4.0. TLS is used to encrypt web trafÔ¨Åc for the HTTP Secure protocol, https; it is also used to encrypt trafÔ¨Åc for several other applications and canbe used, with appropriate programming, for any application. If Alice wants to contact a server S using either SSH or TLS, at some point she will have to trust a public 744 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 key claimed to be from S. A common approach with SSH is for Alice to accept the key on faith the Ô¨Årst time it is presented, but then to save it for all future veriÔ¨Åcations. Under TLS, the key from S that Alice receives will have been signed by a certiÔ¨Åcate authority ( 29.5.2.1 CertiÔ¨Åcate Authorities ); Alice presumably trusts this certiÔ¨Åcate authority. (SSH does now support certiÔ¨Åcate authorities too, but their use with SSH seems not yet to be common.) Both SSH and TLS eventually end up negotiating a shared-secret session key, which is then used for most of the actual data encryption. 29.5.1 SSH The SSH protocol establishes an encrypted communications channel between two hosts, after establishing the identities of each endpoint. Its primary use is to enable secure remote-command execution with input and output via the secure channel; this includes the remote execution of an interactive shell, which is in effect a telnet-style terminal login with encryption. The companion program scp uses the SSH protocol to implement secure Ô¨Åle transfer. Finally, ssh supports secure port forwarding from one machine (and port) to another; unrelated applications can then connect to one machine and Ô¨Ånd themselves securely talking to another. The current version of the SSH protocol is 2.0, and is deÔ¨Åned in RFC 4251. The authentication and transport sub-protocols are deÔ¨Åned in RFC 4252 andRFC 4253 respectively. One of the Ô¨Årst steps in an SSH connection is for the endpoints to negotiate which secret-key cipher (and mode) to use. Support of the following ciphers is ‚Äúrecommended‚Äù and there is a much longer list of ‚Äúoptional‚Äù ciphers (which include RC4 and BlowÔ¨Åsh); the table below includes those added by RFC 4344: cipher modes nominal keylength 3DES CBC, CTR 168 bits AES CBC, CTR 192 bits AES CTR 128 bits AES CTR 256 bits SSH supports a special name format for including new ciphers for local use. The SSH protocol also allows the endpoints to negotiate a public-key-encryption mechanism, a secure-hash function, and even a key-exchange algorithm although only minor variants of DifÔ¨Åe-Hellman-Merkle key exchange are implemented. If Alice wishes to connect to a server S, the server clearly wants to verify Alice‚Äôs identity, either through a password or some other means. But it is just as important for Alice to verify the identity of S, to prevent man-in-the-middle attacks and the possibility that an attacker masquerading as S is simply collecting Alice‚Äôs password for later use. The SSH protocol is not designed to minimize the number of round-trip packet exchanges, making SSH connection setup quite a bit slower than TLS connection setup ( 29.5.2.4 TLS Connection Setup ). A connection may involve quite a few round trips to get started: the TCP three-way handshake, the protocol version exchange, the ‚ÄúKey Exchange Init‚Äù exchange, the actual DifÔ¨Åe-Hellman-Merkle key exchange, the ‚ÄúNewKeys‚Äù exchange, and the ‚ÄúService Request‚Äù exchange (all but the Ô¨Årst are documented in RFC 4253 ). Still, multi-second delays can usually be reduced through performance tuning; enabling diagnostic output (egwith the-voption) is often helpful. A common delay culprit is a server-side DNS lookup of the client, Ô¨Åxable with a server-side conÔ¨Åguration setting of UseDNS no or the equivalent. 29.5 SSH and TLS 745
An Introduction to Computer Networks, Release 2.0.11 In the following subsections we focus on the ‚Äúcommon‚Äù SSH conÔ¨Åguration and ignore some advanced options. 29.5.1.1 Server Authentication To this end, one of the Ô¨Årst steps in SSH connection negotiation is for the server to send the public half of itshost key to Alice. Alice veriÔ¨Åes this key, which is typically in her known_hosts Ô¨Åle. Alice also asks S to sign something with its host key. If Alice can then decrypt this with the public host key of S, she is conÔ¨Ådent that she is talking to the real S. If this is Alice‚Äôs Ô¨Årst attempt to connect to S, however, she should get a message like the one below: The authenticity of host ‚ÄòS (10.2.5.1)‚Äô can‚Äôt be established. RSA key Ô¨Ångerprint is da:2e:e3:94:84:6b:bf:6d:2f:e4:c3:76:68:72:a5:a0. Are you sure you want to continue connecting (yes/no)? If Alice is cautious, she may contact the administrator of S and verify the key Ô¨Ångerprint. More likely, she will simply choose ‚Äúyes‚Äù, in which case the host key of S will be added to her own known_hosts Ô¨Åle; this latter strategy is sometimes referred to as trust on Ô¨Årst use. If she later re-connects to S after the host key of S has been changed, she will get a rather more dire message, and, under the default conÔ¨Åguration, she will not be allowed to continue until she manually removes the old, now-incorrect, host key for S from her known_hosts Ô¨Åle. See also the ARP-spooÔ¨Ång scenario at 10.2.2 ARP Security, and the comment there about how this applies to SSH. SSH v2.0 now also supports a certiÔ¨Åcate-authority mechanism for verifying server host keys, replacing the known_hosts Ô¨Åle. 29.5.1.2 Key Exchange The next step is for Alice‚Äôs computer and S to negotiate a session key. After the cipher and key-exchange algorithms are negotiated, Alice‚Äôs computer and S use the chosen key-exchange algorithm to agree on a session key for the chosen cipher. The two directions of the connection generally get different session keys. They can even use different encryption algorithms. At this point the channel is encrypted. 29.5.1.3 Client Authentication The server now asks Alice to authenticate herself. If password authentication is used, Alice types in her password and it and her username are sent to the server, over the now-encrypted connection. This does expose the server to brute-force password-guessing attacks, and is not infrequently disabled. Alice may also have set up RSA authentication (other types of public-key authentication are also possible). For this, Alice must create a public/private key pair (often in Ô¨Åles id_rsa.pub andid_rsa ), and the public key must have been previously installed on S. On Unix-based systems it is often installed on S in 746 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 theauthorized_keys Ô¨Åle in the.ssh subdirectory of Alice‚Äôs home directory. If Alice now sends a message to S signed by her private key, S is in a position to verify the signature. If this succeeds, Alice can log in without supplying a password to S. It is common though not universal practice for Alice‚Äôs private-key Ô¨Åle (on her own computer) to be protected by a password. If this is the case, Alice will need to supply that password, but it is used only on Alice‚Äôs end of the connection. (The default password hash is MD5, which may not be a good choice; see 28.6.2 Password Hashes .) Public-key authentication is tried before password authentication. If Alice has created a public key, it is likely to be tried even if she has not copied it to S. If S is set up to require public-key authentication, Alice may not have any direct way to install her public key on S herself, and may need the cooperation of the administrators of S. It is possible that the latter will send Alice a public/private keypair chosen by them speciÔ¨Åcally for S, rather than allowing Alice to choose her own keypair. The standard SSH user conÔ¨Åguration does support different private keys for different servers. In several command-line implementations of ssh, the various stages of authentication can be observed from the client side by using the ssh orslogin command with the -voption. 29.5.1.4 The Session Once an SSH connection has started, a new session key is periodically negotiated. RFC 4253 recommends this after one hour or after 1 GB of data is exchanged. Data is then sent in packets generally with size a multiple of the block size of the cipher. If not enough data is available, egbecause only a single keystroke (byte) is being sent, the packet is padded with random data as needed. Every packet is required to be padded with at least four bytes of random data to thwart attacks based on known plaintext/ciphertext pairs. Included in the encrypted part of the packet is a byte indicating the length of the padding. 29.5.2 TLS Transport Layer Security, or TLS, is an IETF extension of the Secure Socket Layer (SSL) protocol originally developed by Netscape Communications. SSL went through published versions 2.0 and 3.0; the latter was introduced in 1996 and was ofÔ¨Åcially deprecated by RFC 7568 in 2015 (see the POODLE sidebar above at29.5 SSH and TLS ). TLS 1.0 was introduced in 1999, as the IETF took ownership of the speciÔ¨Åcation from Netscape. The current version of TLS is 1.2, speciÔ¨Åed in RFC 5246 (plus updates listed there). A draft of TLS 1.3 is in progress (2018); see draft-ietf-tls-tls13. The original and still primary role for TLS is encrypting web connections and verifying for the client the authenticity of the server. TLS can, however, be embedded in any network application. Unlike SSH, client authentication, while possible, is not common; web servers often have no pre-existing relationship with the client. Also unlike SSH, the public-key mechanisms are all based on established certiÔ¨Åcate authorities, or CAs, whereas the most common way for an SSH server‚Äôs host key to end up on a client is for it to have been accepted by the user during the Ô¨Årst connection. Browsers (and other TLS applications as necessary) have embedded lists of certiÔ¨Åcate authorities, known as trust stores, trusted by the browser vendor. SSH requires no such centralized trust. 29.5 SSH and TLS 747
An Introduction to Computer Networks, Release 2.0.11 If Bob wishes to use TLS on his web server S Bob, he must Ô¨Årst arrange for a certiÔ¨Åcate authority, say CA 1, to sign his certiÔ¨Åcate. A certiÔ¨Åcate contains the full DNS name of S Bob, say bob.int, a public key K Sused by S Bob, and also an expiration date. TLS uses the ITU-T X.509 certiÔ¨Åcate format. The.int domain For Bob to actually have a domain name in the .int top-level domain, as in the example here, Bob‚Äôs organization must be established by international treaty. Now imagine that Alice connect to Bob‚Äôs server S Bobusing TLS. Early in the process S Bobwill send her its signed certiÔ¨Åcate, claiming public key K S. Alice‚Äôs browser will note that the certiÔ¨Åcate is signed by CA 1, and will look up CA 1on its list of trusted certiÔ¨Åcate authorities. If found, the next step is for the browser to use CA 1‚Äôs public key, also on the list, to verify the signature on the certiÔ¨Åcate S Bobsent. If everything checks out, Alice‚Äôs browser now knows that CA 1certiÔ¨Åes bob.int has public key K S. As S Bob has presented this key K S, and is able to verify that it possesses the matching private key, this is proof that SBobis legitimately the server with domain name bob.int. Assuming, of course, that CA 1is correct. As with SSH, once Alice has accepted the public key K Sof S Bob, a secret-key cipher is negotiated and the remainder of the exchange is encrypted. 29.5.2.1 CertiÔ¨Åcate Authorities AcertiÔ¨Åcate authority, or CA, is just an entity in the business of signing certiÔ¨Åcates: messages that include a name S Boband a veriÔ¨Åed public key K S. The purpose of the certiÔ¨Åcate authority is to prevent man-in-themiddle attacks ( 29.3 Trust and the Man in the Middle ); Alice wants to be sure she is not really connected to S Badinstead of to S Bob. What the CA is actually signing is that the particular public key K Sbelongs to domain bob.int, just like Charlie might sign Bob‚Äôs public key on an individual basis. The difference here is that the CA is likely to be a large organization, and the CA‚Äôs public key is likely to be embedded in the network application software somewhere. (Real CA‚Äôs usually have a two-layer key signing arrangement, in which Bob‚Äôs public key would be signed by one of the CA‚Äôs subsidiary keys, but the effect is the same.) A certiÔ¨Åcate speciÔ¨Åc for bob.int will not work for www .bob.int, even though both DNS names might direct to the same server S Bob. If S Bobpresents a certiÔ¨Åcate for bob.int to a browser that has arrived at S Bobvia the url http://www.bob.int, the user should see a warning. It is straightforward, however, for the server either to support two certiÔ¨Åcates, or (if the certiÔ¨Åcate authority supports this) a single certiÔ¨Åcate for www.bob.int with a Subject Alternative Name also entered on the certiÔ¨Åcate for bob.int alone. It is also possible to obtain ‚Äúwildcarded‚Äù certiÔ¨Åcates for *.bob.int (though this does not match bob.int; it also does not match names with two additional levels such as www.foo.bob.int). RFC 6125, ¬ß7.2, recommends against wildcard certiÔ¨Åcates, though they remain widely used. Firefox CertiÔ¨Åcates 748 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 As of this writing, certiÔ¨Åcate authorities for the popular Firefox browser are found in Preferences √ëAdvanced√ëView CertiÔ¨Åcates. It is an interesting list, if only because, for such highly trusted organizations, few people have heard of more than a handful of them. The policy for becoming a Firefox CA is here. Popular browsers all use preset lists of CAs provided by (and presumably trusted by) either the browser vendor or the host operating-system vendor. IfAlice is also willing to trust these CAs, she can feel comfortable using the key she receives to send private messages to Bob. That ‚Äúif‚Äù, however, is sometimes controversial. Just because Alice trusts her browser and operating system ( egnot to contain malware), that does not automatically imply that Alice should trust these vendors‚Äô judgment when it comes to CAs. On the face of it, Bob‚Äôs certiÔ¨Åcate authority CA 1is just signing that domain name bob.int has public key K S. This isn‚Äôt quite the same, however, as attesting that the certiÔ¨Åcate-signing request actually came from Bob. All depends on how thorough CA 1is in checking the identity of its customer, and since those customers typically choose the least expensive CA, there is sometimes an incentive to cut corners. A certiÔ¨Åcate authority‚Äôs failure to verify the identity of the party making the certiÔ¨Åcate-signing request can enable a man-in-the-middle attack ( 29.3 Trust and the Man in the Middle ). Suppose, for example, that Mallory is able to obtain a signed certiÔ¨Åcate from another certiÔ¨Åcate authority CA 2linking bob.int to key Kbadcontrolled by Mallory. If Mallory can now position his server S Badin the middle between Alice and SBob, Alice SBad SBob then Alice can be tricked into connecting to S Badinstead of S Bob. Alice will request a certiÔ¨Åcate, but from SBadinstead of S Bob, and get Mallory‚Äôs from CA 2instead of Bob‚Äôs actual certiÔ¨Åcate from CA 1. Mallory opens a second connection from S Badto S Bob(this is easy, as Bob makes no attempt to verify Alice‚Äôs identity), and forwards information from one connection to the other. As far as TLS is concerned everything checks out. From Alice‚Äôs perspective, Mallory‚Äôs false certiÔ¨Åcate vouches for the key K badof S Bad, CA 2has signed this certiÔ¨Åcate, and CA 2is trusted by Alice‚Äôs browser. There is no ‚Äúsearch‚Äù at any point through the other CAs to see if any of them have any contrary information about S Bob. In fact, there is not necessarily even contact with CA 2, though see 29.5.2.3 CertiÔ¨Åcate revocation below. CertiÔ¨Åcate Errors Sometimes certiÔ¨Åcate signatures fail to verify, either because they are expired or were improperly signed or the certiÔ¨Åcate authority is not recognized. The browser then receives the appropriate TLS error message and communicates the problem to the user. Examples can be found at badssl.com; click the buttons there to see different errors. If the certiÔ¨Åcate authority CA 1were also the domain registrar with whom Bob registered the DNS name bob.int, it would be especially well-positioned to verify that Bob is really the owner of the bob.int domain. But this is not generally the case. Quite often, the CA‚Äôs primary veriÔ¨Åcation method is to send an email to, say, bob@bob.int (or perhaps to the DNS administrative contact listed in the WHOIS database for domain bob.int). If someone responds to this email, it is assumed they must be legitimately part of the bob.int domain. Another authentication strategy is for CA 1to send a special Ô¨Åle to be placed at, say, bob.int/foo/bar/special.html; again, the idea is that only the domain owner would be able to do this. Note, however, that in the man-in-the-middle attack above, it does not matter what CA 1does; what matters is the veriÔ¨Åcation policy used by CA 2. 29.5 SSH and TLS 749
An Introduction to Computer Networks, Release 2.0.11 If Alice is very careful, she may click on the ‚Äúlock‚Äù icon in her browser and see that the certiÔ¨Åcate authority signing her connection to S Badis CA 2rather than CA 1. But if Alice has a secure way of Ô¨Ånding Bob‚Äôs ‚Äúreal‚Äù certiÔ¨Åcate authority, she might as well use it to Ô¨Ånd Bob‚Äôs key K S. As she often does not, this is of limited utility in practice. The second certiÔ¨Åcate authority CA 2might be a legitimate certiÔ¨Åcate authority that has been tricked, coerced or bribed into signing Mallory‚Äôs certiÔ¨Åcate. Alternatively, it might be Mallory‚Äôs own creation, inserted by Mallory into Alice‚Äôs browser through some other vulnerability. SuperÔ¨Åsh The ‚ÄúSuperÔ¨Åsh‚Äù vulnerability of 2015 involved an operating-system module (based on LSP, WFP or, in principle, iptables) that could intercept all browser TLS connections (thus acting as the man in the middle), together with software that could act as a certiÔ¨Åcate authority, generating new certiÔ¨Åcates (like Mallory‚Äôs from CA 2) on the Ô¨Çy. The apparent goal was to inject advertising into TLS-secured connections. Mallory‚Äôs machinations here do require both the man-in-the-middle attack and the bad certiÔ¨Åcate. If Alice is able to establish a direct connection with S Bob, then the latter will send its true key K Ssigned by CA 1. As another attack, Mallory might obtain a certiÔ¨Åcate for b0b.int and hope Alice doesn‚Äôt notice the spelling difference between B0B and BOB. When this is done, Mallory often also sends Alice a disguised link to b0b.int in the hope she will click on it. Unicode domain names make this even easier, as Unicode provides many character pairs that are different but which look identical. CertiÔ¨Åcates tend to come with a number of constraints. First among them is that most certiÔ¨Åcates issued by CAs to end users (either corporations or individuals) are marked as ‚Äúnot a certiÔ¨Åcate authority‚Äù, meaning that the certiÔ¨Åcate cannot be used to sign additional certiÔ¨Åcates. That is, the certiÔ¨Åcate trust relationship is not transitive. If Bob gets a certiÔ¨Åcate for bob.int, he cannot use it to sign a certiÔ¨Åcate for Alice or, for that matter, Google. The second major restriction is on names: a certiÔ¨Åcate is almost always tied to a speciÔ¨Åc DNS name. As described above, multiple names can be accepted via the ‚Äúsubject alternative name‚Äù attribute, or by using a ‚Äúwildcard‚Äù name like *.foo.com. Finally, certiÔ¨Åcates often contain restrictions on how they can be used, in the ‚Äúextended key usage‚Äù attribute. A typical entry is ‚ÄúTLS web server authentication‚Äù, expressed as an OID ( 26.3 SNMP Naming and OIDs ). These restrictions are enforced by the TLS library; it is possible outside that library to use the keys inside certiÔ¨Åcates for whatever you want. Extended-Validation certiÔ¨Åcates were introduced in 2007 as a way of providing greater assurances that the certiÔ¨Åcate issued to bob.int was in fact generated by a request from Bob himself; Mallory should in theory have a much harder time obtaining an EV certiÔ¨Åcate for bob.int from CA 2. Desktop browsers that have secured a TLS connection using an EV certiÔ¨Åcate typically add the name of the domain owner, highlighted in green and/or with a green padlock icon, to the address bar. Financial institutions often use EV certiÔ¨Åcates. So does mozila.org. Of course, if Alice does not know Bob is using an EV certiÔ¨Åcate, she can still be tricked by Mallory as above. Alternatively, perhaps Alice is using a mobile device; most mobile browsers give little if any visual indication of EV certiÔ¨Åcates. Because the majority of browsing today (2018) is done on mobile devices, this severely limits the usefulness of these certiÔ¨Åcates. 750 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 29.5.2.2 CertiÔ¨Åcate pinning Another strategy intended as a more direct prevention of man-in-the-middle attacks is certiÔ¨Åcate pinning. Conceptually, Alice (or her browser) makes a note the Ô¨Årst time she connects to S Bobthat the certiÔ¨Åcate authority is CA 1or that Bob‚Äôs public key is K S, or both. Future connections to S Bobmust match at least one of these credentials. This form of pinning depends on Alice‚Äôs having reached the real S Bobon the Ô¨Årst connection, sometimes called ‚Äútrust on Ô¨Årst use‚Äù. A similar trust-on-Ô¨Årst-use strategy is often (though not always) used with SSH, 29.5.1.1 Server Authentication. In the pinning protocol described in RFC 7469, it is S Bobthat initiates the pinning by including a ‚Äúpin directive‚Äù in its initial HTTP connection. This requests Alice‚Äôs browser to pin the desired certiÔ¨Åcates, though the pinned correspondence between a site and its keys is always maintained at the browser side. The pin directive also speciÔ¨Åes which keys (K Sor CA 1or both) are pinned, and for how long. If S Bobsends a pin directive for K S, then Alice‚Äôs browser remembers K S, and any new certiÔ¨Åcate at S Bob will be a mismatch. If S Bobpins CA 1, then S Bobcan have CA 1issue a new key and it will still pass pinvalidation. If CA 1is later compromised, though, S Bobis not protected against any new rogue certiÔ¨Åcates it issues. (This is generally a minor concern, as CA 1compromise will always mean that newvisitors to S Bob will be vulnerable to man-in-the-middle attacks.) Bob can also obtain, along with K S, a backup certiÔ¨Åcate K S-backup, and have S Bob‚Äôs pin directives pin both of these. Then, if K Sis compromised, K S-backup is ready to go. Hopefully, key compromise is a rare event, so there is a very small chance that both K Sand K S-backup are compromised within the pin‚Äôs lifetime. Key compromise pretty much follows from any server compromise, however, and if intruders break in, K S-backup cannot be put into production until Bob is sure the site is secure. That may take some time. If K Sand K S-backup areboth compromised, and CA 1wasn‚Äôt pinned, S Bobmay simply become inaccessible to returning visitors. Automatic unpinning of revoked certiÔ¨Åcates would help, but certiÔ¨Åcate revocation (see following section) has its own difÔ¨Åculties. The user may have an option to disable pin validation for this particular site, but it‚Äôs not supposed to be as simple as clicking ‚Äúok‚Äù, and in any event if the site is your bank you may be loath to do this. 29.5.2.3 CertiÔ¨Åcate revocation Suppose the key r underlying the certiÔ¨Åcate for bob.int has been compromised, and Mallory has the private key. Then, if Mallory can trick Alice into connecting to S Badinstead of S Bob, the original CA 1will validate the connection. S Badcan prove to Alice that it has the secret key corresponding to K S, and all the certiÔ¨Åcate does is to attest that bob.int has key K S. Mallory‚Äôs deception works even if Bob noticed the compromise and updated S Bob‚Äôs key to K 2; the point is that Mallory still has the original key, and CA 1‚Äôs certiÔ¨Åcate attesting to that original key is still valid. There is a mechanism by which a certiÔ¨Åcate can be revoked. Revocation information, however, must be kept at some central directory; a server can continue to serve up a revoked certiÔ¨Åcate and unless the clients actively check, they will be none the wiser. This is the reason certiÔ¨Åcates have expiration dates. The original revocation mechanism was the global certiÔ¨Åcate revocation list. A newer alternative is the Online CertiÔ¨Åcate Status Protocol, OCSP, described in RFC 6960. If Alice receives a certiÔ¨Åcate signed by CA 1, she can send the serial number of the certiÔ¨Åcate to a designated ‚ÄúOCSP responder‚Äù run by or on behalf of CA 1. If the certiÔ¨Åcate is still valid, the responder site will return a signed conÔ¨Årmation of that. 29.5 SSH and TLS 751
An Introduction to Computer Networks, Release 2.0.11 Of course, an eavesdropper watching Alice‚Äôs trafÔ¨Åc arriving at the OCSP responder ‚Äì and the OCSP responder itself ‚Äì now knows that Alice is visiting bob.int. An eavedropper closer to Alice, however, knows that anyway. More seriously, someone running a man-in-the-middle attack close to Alice can probably intercept and block Alice‚Äôs OCSP request. If Alice receives no response, what should she do? Maybe there is a problem, but maybe the responder site is simply down or the Internet is simply slow. Most browsers that actually do revocation checks assume the latter ‚Äì known as soft fail ‚Äì making revocation checks of dubious value. The alternative of refusing to allow access to the original site ‚Äì hard fail ‚Äì leads to many false positives. As of 2016, there is no generally accepted solution. One minimal approach, in the event of OCSP soft fail, is to use hard fail with EV certiÔ¨Åcates, or at least to downgrade EV certiÔ¨Åcates to ordinary ones upon OCSP non-response. Another approach is OCSP stapling, in which the server site periodically (perhaps daily) requests a signed and dated update from its CA‚Äôs OCSP server indicating that the site‚Äôs certiÔ¨Åcate is still valid. This OCSP report is then ‚Äústapled‚Äù to the ServerHello message (below), if requested by the client. This allows the client to verify that the certiÔ¨Åcate was still valid quite recently. Of course, if the client isthe victim of a man-in-the-middle attack then the (fake) server will not staple an OCSP validity report, and the client must fall back to the regular OCSP lookup process. But this case can be expected to be infrequent, making a hard fail after OCSP non-response a reasonable option. Google‚Äôs Chrome browser implements CRLSets in lieu of checking OCSP servers, described here. This is a list of revoked certiÔ¨Åcates downloaded regularly to the browser. Unfortunately, the full list is much too large, so CRLSets are limited to emergency revocations and certiÔ¨Åcate revocations due to key compromise; even then the list is not complete. 29.5.2.4 TLS Connection Setup The typical TLS client-side user is interested in viewing a web page as quickly as possible, placing a premium on rapid negotiation of the TLS connection. If sites were to load noticeably more slowly when encryption was used, encryption might not be used routinely. As a result, the connection-setup process, known as the TLS handshake protocol, is designed to complete in two RTTs. The goals of the handshake protocol are to agree on the encryption and authentication mechanisms to be used, to provide authentication as necessary, and to negotiate the encryption keys. Here is an outline of a typical exchange, with some options omitted: 752 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 client (Alice) server (Bob) ClientHello 
- preferred TLS version 
- session ID (empty for new connections) 
- list of ciphers acceptable to client 
- ServerName (optional extension) 
- start of key exchange (optional) ServerHello 
- chosen version 
- chosen cipher 
- server key exchange (optional) 
- server certiÔ¨Åcate (separate message) (client now knows the key) optional client certiÔ¨Åcate optional other data (may be encrypted) Finished message Finished message (encrypted) The client initiates the connection by sending its ClientHello message, which contains its preferred TLS/SSL-protocol version number, a session identiÔ¨Åer, some random bytes (a cryptographic nonce), and a list of cipher suites: tuples consisting of a key-exchange algorithm, a shared-key encryption algorithm and a secure-hash algorithm. The list of cipher suites is ranked by the client according to preference. Many large servers host multiple websites through ‚Äúvirtual hosting‚Äù. Each website will have its own certiÔ¨Åcate, and the server needs to be able to know which one to send in its reply. In settings like this, the client includes the server‚Äôs DNS name in a ServerName extension; the Server Name Indication ( SNI) is deÔ¨Åned in RFC 6066. Choosing your cipher In theory, any TLS client can choose which cipher suites it will accept. In practice, for most software applications this is non-trivial. In the Firefox browser, see about:config, under security.ssl3 (yes, ssl3 is in theory not the same as tls). The server then responds with its ServerHello. This contains the Ô¨Ånal protocol-version number, generally the maximum of the version proposed by the client and the highest version supported by the server. TheServerHello also contains the server‚Äôs choice of a cipher suite from the client‚Äôs list, and some more random bytes (the server‚Äôs nonce). The server also sends its certiÔ¨Åcate, if requested (which it almost always is). Exactly which certiÔ¨Åcate is sent may depend on the server name sent by the client in the optional ServerName extension. The certiÔ¨Åcate is considered to be a separate ‚Äúmessage‚Äù at the TLS protocol level, not part 29.5 SSH and TLS 753
An Introduction to Computer Networks, Release 2.0.11 of theServerHello, but everything is sent together. Having received the server‚Äôs certiÔ¨Åcate, the client application‚Äôs next step is to validate this certiÔ¨Åcate, by checking its signature against the client‚Äôs list of trusted certiÔ¨Åcate authorities, the client‚Äôs ‚Äútrust store‚Äù. This step does not involve any network communication. While most operating systems maintain a list of generally trusted certiÔ¨Åcates, the client application can trust all, some or none of this list; the client can also load application-speciÔ¨Åc certiÔ¨Åcates. If the certiÔ¨Åcate does notcheck out, the client is free to continue the connection, perhaps pausing to add the server certiÔ¨Åcate to its trust store (hopefully after user interaction conÔ¨Årming this). The client then responds with its key-exchange response, and its own certiÔ¨Åcate if applicable (which it seldom is). It immediately follows that with its Finished message, the Ô¨Årst message that is encrypted with the just-negotiated cipher suite. The server then replies with itsencryptedFinished message, and encrypted application-layer communication can then begin. Most browsers allow the user to click on the padlock icon for the TLS-secured connection to Ô¨Ånd out what cipher suite was actually agreed upon. A client starting a brand-new connection leaves the Session ID Ô¨Åeld empty in the ClientHello message. However, if a client wishes to resume a previous session, it includes here the Session ID from that previous session. The server must, of course, also recognize that session. Once upon a time, some broken TLS servers failed to respond properly if a client proposed a version number greater than what the server supported; the server would close the connection instead of returning a lower version number. As a result, many clients were programmed to try again with the next-lower version in the event of anyconnection-setup failure. This meant that an attacker could force a client to propose an obsolete version ( egSSL 3.0) simply by interrupting earlier connection-setup attempts, perhaps with a TCP RST packet. A consequence was the POODLE vulnerability mentioned in the sidebar in 29.5 SSH and TLS. An application‚Äôs choice of TLS (versus unencrypted communication) is often signaled by the use of a special port number; for example, the standard http port is 80 while the standard https port is 443. Alternatively, there may be some initial negotiation; for example, in the SMTP email protocol ( RFC 5321 ) it is common for the client to connect to the server on the standard port 25 without encryption, but then to negotiate the use of TLS using the STARTTLS extension to SMTP ( RFC 3207 ). This adds multiple extra RTTs, but for non-interactive protocols this is usually of only minor concern. Occasionally applications do sometimes get confused about whether TLS is in use. Web browsers connecting to port 80 (HTTP) or port 443 (HTTPS) can usually Ô¨Ågure things out correctly, even if the ‚Äúhttp://‚Äù or ‚Äúhttp s://‚Äù part of the URL is missing. However, browsers connecting to nonstandard webservers running on nonstandard ports may receive a very cryptic binary response if TLS was required but the ‚Äúhttps://‚Äù URL preÔ¨Åx was missing. 29.5.2.4.1 Domain Fronting Government censorship of websites and other Internet services ‚Äì such as encrypted messaging ‚Äì is rampant in many parts of the world. We saw above that the ServerName extension is included in many ClientHello messages in order to request the correct certiÔ¨Åcate. This is sent in the clear, and can be used by censors to block selected trafÔ¨Åc. One censorship workaround, known as domain fronting, is to ask for one hostname, say alice.org, in theClientHello ServerName extension, but then request a second, different hostname, badbob.net, 754 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 within the application-layer protocol, after encryption is in place. For example, an HTTP GET request (17.7.1 netcat again ) almost always contains a HOST: header; this is where badbob.net would be speciÔ¨Åed. Of course, this only works if alice.org andbadbob.net are hosted at the same IP address, but large HTTP servers often do handle multiple websites on a single server using a mechanism known as virtual hosting ( 10.1.2 nslookup and dig ). To block badbob.net, a censor must now also block alice.org. The hope is that censors will be unwilling to do that. Domain fronting is even easier when CDNs ( 1.12.2 Content-Distribution Networks ) are involved; virtual hosting is not necessary. A CDN accepts connections to the websites of a large number of customers (often a much larger number than a single server can support using virtual hosting) a each edge server of their network. The edge servers have certiÔ¨Åcates for each of the customers, and so appear legitimate to users. After the connection, the edge server might then answer some requests with cached data, but will forward other requests on to the actual web server. To enable domain fronting in this environment, badbob.net might only need to become a customer of the same CDN as alice.org. CDNs and large hosting providers sometimes do not think highly of domain fronting, particularly as some censors have shown willingness to block critical domains like google.com just to block an unwanted application. Google and Amazon Web Services disabled domain fronting for their CDN customers in 2018. Before the initial connection to the server, the client will usually make a DNS request to obtain the server‚Äôs IP address. These too can be blocked, or monitored, although there are workarounds. 29.5.2.4.2 TLS key exchange Perhaps the most important part of the TLS handshake is to negotiate the encyption keys. The keys themselves are all derived from the TLS master secret. The key derivations are done in deterministic ways ( eg using secure hashes, or a stream-cipher algorithm), and can be done by either side once the master secret is known. In all cases, the client should know the master secret after receiving the ServerHello and related packets. At this point ‚Äì after one RTT ‚Äì the client can begin sending encrypted messages to the server. At this point, the client can even begin sending encrypted data, although it is possible that the client is not yet fully authenticated to the server. Negotiation of the master secret depends on the cipher suite chosen. In the RSA key-exchange method, the client chooses a random pre-master secret. This is sent to the server in the third leg of the exchange, after the client has received the server‚Äôs certiÔ¨Åcate in the ServerHello stage. The pre-master secret is encrypted with the public RSA key from the server‚Äôs certiÔ¨Åcate, thus conveying it securely to the server. Both sides calculate the master secret from the pre-master secret and both sides‚Äô cryptographic nonces, using a secure hash or the pseudorandom keystream of a stream cipher ( 28.7.4 Stream Ciphers ). The inclusion of the server nonce means tha the client does not unilaterally specify the key. IfDifÔ¨Åe-Hellman-Merkle key-exchange is used ( 28.8 DifÔ¨Åe-Hellman-Merkle Exchange ), then the server proposes the prime p and primitive root g during the ServerHello phase, as part of its selection of one of the cipher suites listed in the ClientHello. The server (Bob) also includes gbmod p. The client chooses its exponent a, and sends gamod p to the server. The pre-master secret is gabmod p, which the client, as in the RSA case, knows at the end of the Ô¨Årst RTT. The client‚Äôs gamod p cannot be encrypted using the master key, but everything else sent by the client at that point can be. There is also a version of DifÔ¨Åe-Hellman-Merkle exchange that uses elliptic-curve cryptography. 29.5 SSH and TLS 755
An Introduction to Computer Networks, Release 2.0.11 29.5.2.4.3 TLS version 1.3 Version 1.3 of TLS, RFC 8446, makes numerous improvements to the protocol. Among other things, it prunes the list of acceptable cipher suites of all algorithms no longer considered secure, and also of all keyexchange algorithms that do not support forward secrecy, 29.2 Forward Secrecy. The version-negotiation process has also been improved, to prevent version-downgrade attacks, and the 0-RTT mode (below) enables faster connection startup. Of the two key-exchange methods in the previous section, DifÔ¨Åe-Hellman-Merkle does support forward secrecy while RSA may not, depending on the latter‚Äôs choice of the pre-master secret. The proposal to drop RSA key exchange has caused concern in some enterprise settings, particularly the banking industry, where widespread use is often made of middleboxes that are able to decrypt and monitor internal TLS connections (that is, connections where both endpoints are under control of the same enterprise). This monitoring is done for a variety of reasons, including regulatory compliance and malware detection. To enable such monitoring using TLS v1.2, all TLS connections between the enterprise and the outside Internet terminate at the site‚Äôs border, at a special TLS-proxy appliance. From there, connections to interior systems are re-encrypted, but using a static RSA pre-master secret that is shared with the monitoring equipment. The same applies to any TLS connections that are entirely internal to the enterprise. To enable this kind of monitoring under TLS v1.3, one approach is to have the monitoring occur at the TLS-proxy appliance, though this means that device will have to be considerably bigger, more complicated and more expensive; additionally, it will not help with entirely internal TLS connections. Another approach is to have the TLS proxy, and at least one endpoint of internal TLS connections, turn over all session keys to the monitors. This raises other security concerns. For two proposed amendments to the original TLS v1.3 proposal, see the Internet Drafts draft-rhrd-tls-tls13-visibility and draft-green-tls-static-dh-in-tls13. TLS v1.3 requires that all messages after the ServerHello be encrypted (except possibly one last clientto-server key-exchange message that should be intrinsically secure). The client knows the master secret at this point, and the server will know it as soon as it hears from the client. 29.5.2.4.4 TLS v1.3 0-RTT mode Finally, TLS v1.3 adds a so-called O-RTT mode, in which the client can send encrypted messages from the very beginning, assuming an appropriate master secret has been negotiated during a previous connection. For HTTPS interactions, this is a common case. The 0-RTT mode, in particular, allows the client to send secure data along with its ClientHello. The server can respond with 0-RTT-protected data along with its ServerHello, if it chooses to. After that, everything should be protected with the new session‚Äôs master secret; this is known as 1-RTT protection. To send 0-RTT data, the client must resume the earlier session by including the previous Session ID in its ClientHello message. The server canrefuse 0-RTT data, and demand 1-RTT protection, but usually will not. The advantage of 0-RTT data is that, if the server accepts it, an answer can return to the client in a single RTT. This is particularly signiÔ¨Åcant in QUIC ( 16.1.1 QUIC and18.15.4 QUIC Revisited ), in which the TLS handshake is carried out in parallel with the QUIC connection handshake (a replacement for the TCP three-way handshake). This means that the response comes just one RTT after the very Ô¨Årst message from the client. QUIC requires TLS v1.3 (or later). 756 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 0-RTT protection is not quite as strong ast 1-RTT protection. For one, forward secrecy does not apply. More seriously, a participant receiving 0-RTT data is vulnerable to replay attacks. An attacker cannot modify a previously intercepted 0-RTT message (without breaking the cipher), but can resend it. At a minimum, the recipient of a 0-RTT request should accept it only if it is idempotent: yielding the same results and side-effects whether executed once or twice ( 16.5.2 Sun RPC ). This is generally automatic for simple HTTP GET requests. Idempotency is not itself a security guarantee ‚Äì after all, the request ‚Äúdelete all Ô¨Åles‚Äù is idempotent. The point, however, is that the particular request must, if part of a replay attack, have been executed before, and if it was safe once, it should be safe twice. A TLS server can also suppport other anti-replay mechanisms, such as a database of past requests. Another consequence of mixing 0-RTT and 1-RTT data is that the recipient needs to be able to tell which requests received 0-RTT protection and which received full 1-RTT protection. 29.5.3 A TLS Programming Example In this section we introduce a simple pair of programs, tlsserver.c and tlsclient.c, that communicate via TLS. They somewhat resemble the simplex-talk program in 17.6 TCP simplex-talk, except that neither reads from the command line. Instead, the client sends a message to the server (which may ignore it), and then the server sends a message back. This structure will allow us later to point the client at a ‚Äúreal‚Äù TLS-using (that is, HTTPS-using) web server, instead of tlsserver, have the client send an HTTP GET request (17.7.1 netcat again ), and obtain the web server‚Äôs response. Both programs are written in C, in order to use the OpenSSL library, a descendant of the original Netscape SSL implementation. The openssl package also includes some important command-line utilities for certiÔ¨Åcate creation. While OpenSSL library documentation remains notoriously spare, parts of the library have now been audited, and OpenSSL remains the most widely used implementation of TLS. The code shown here is intended simply as a demonstration; it should not be considered a model of how to implement secure connections. 29.5.3.1 Making a certiÔ¨Åcate The Ô¨Årst step is to create the appropriate application certiÔ¨Åcate, and create our own certiÔ¨Åcate authority to sign it. For each step we will use the openssl command, also used below at 29.8 RSA Key Examples with additional explanation. We start with the certiÔ¨Åcate authority. The Ô¨Årst step is to create the key our certiÔ¨Åcate authority will use; this is what the genrsa option below achieves. We choose a key length of 4096 bits. openssl genrsa -out CAkey.pem 4096 The resultant Ô¨Åle says it is a ‚Äúprivate key‚Äù, but it is in fact a public/private key pair. The next step is to create a self-signed certiÔ¨Åcate. Thereq option asks for a signing ‚Äúrequest‚Äù, the -new option indicates this is a new request, and the -x509 option tells openssl to forget making a ‚Äúrequest‚Äù and instead make a self-signed certiÔ¨Åcate. X.509 is actually a format standard. openssl req -new -x509 -key CAkey.pem -out CAcert.pem -days 10000 29.5 SSH and TLS 757
An Introduction to Computer Networks, Release 2.0.11 The certiÔ¨Åcate here is set to expire in 10,000 days; real published certiÔ¨Åcates are not supposed to be valid for more than about three years. The signing process (selfor not) triggers a series of questions that will form the ‚ÄúDistinguished Name‚Äù of the certiÔ¨Åcate: Country Name (2 letter code): US State orProvince Name (full name): Illinois Locality Name (eg, city):Shabbona Organization Name (eg, company): An Introduction to Computer Networks Organizational Unit Name (eg, section): Security Common Name (e.g. server FQDN orYOUR name): Peter L Dordal Email Address []: If we were requesting a certiÔ¨Åcate for a web server, we would use the hostname as Common Name, but we are not. The output here, CAcert.pem, represents the concatenation of the above information with the public key from CAkey.pem, and then signed by the private key from CAkey.pem. The private key itself, however, isnotpresent in CAcert.pem; this is the Ô¨Åle that the TLS client will receive as its certiÔ¨Åcate authority. We can read the information from the Ô¨Åle as follows: openssl x509 - inCAcert.pem -text This produces the following, somewhat edited for space. We can see that the certiÔ¨Åcate is self-signed because theIssuer: is the same as the Subject: Certificate: Data: Version: 3 (0x2) Serial Number: 14576542390929842281 (0xca4a481320cbe069) Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, ST=Illinois, L=Shabbona, O=An Introduction to Computer √£√ëNetworks, OU=Security, CN=Peter L Dordal Validity Not Before: Jan 10 20:17:55 2017 GMT Not After: May 28 20:17:55 2044 GMT Subject: C=US, ST=Illinois, L=Shabbona, O=An Introduction to Computer √£√ëNetworks, OU=Security, CN=Peter L Dordal Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (4096 bit) Modulus: 00:b0:70:06:7e:38:1d:29:35:a7:ca:40:bf:fd:6e: e5:26:7b:ee:0d:e7:d7:c2:61:8e:42:5f:b9:85:8c: ... d4:12:cf Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Key Identifier: D1:74:15:F5:31:CB:DD:FA:D6:AE:81:7A:40:AA:64:7A:55:96:2E:08 X509v3 Authority Key Identifier: √£√ëkeyid:D1:74:15:F5:31:CB:DD:FA:D6:AE:81:7A:40:AA:64:7A:55:96:2E:08 X509v3 Basic Constraints: (continues on next page) 758 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) CA:TRUE Signature Algorithm: sha256WithRSAEncryption 2a:d2:35:43:c2:5d:1c:5d:e2:88:ed:4e:aa:d2:b5:d6:e9:26: 60:f0:37:ea:29:56:14:62:58:01:78:b0:6f:ee:ab:40:17:36: ... eb:3d:da:79:5c:90:4d:c9 -----BEGIN CERTIFICATE----- MIIF8TCCA9mgAwIBAgIJAMpKSBMgy+BpMA0GCSqGSIb3DQEBCwUAMIGOMQswCQYD VQQGEwJVUzERMA8GA1UECAwISWxsaW5vaXMxETAPBgNVBAcMCFNoYWJib25hMS0w ... ywhRBNEO1XXB7bFrkkv93q4G3Re2zyw2/5BDEn7rPdp5XJBNyQ== -----END CERTIFICATE----- We now create the application key and then the application signature request. This request we will then sign with the above certiÔ¨Åcate CAÔ¨Åle.pem to generate the application certiÔ¨Åcate. openssl genrsa -out appkey.pem 2048 openssl req -new -key appkey.pem -out appreq.pem -days 10000 The certiÔ¨Åcate request here again requires entering a Distinguished Name. This time we enter as follows; the Organization Name must match the CAcert.pem above: Country Name (2 letter code): US State orProvince Name (full name): Illinois Locality Name (eg, city): no fixed abode Organization Name (eg, company): An Introduction to Computer Networks Organizational Unit Name (eg, section): TLS server Common Name (e.g. server FQDN orYOUR name): Odradek Email Address []: We are also asked for a ‚Äúchallenge password‚Äù, but we leave this blank. Now we come to the Ô¨Ånal step: the actual signing. Unlike any of the previous openssl commands, this requires root privileges. It also makes use of the global openssl conÔ¨Åguration Ô¨Åle (/etc/ssl/openssl.cnf), which, among other things, references a Ô¨Åle ‚Äúserial‚Äù to assign the certiÔ¨Åcate a serial number. (We did not have to do any of this to create the self-signed certiÔ¨Åcate.) openssl ca - inappreq.pem -out appcert.pem -days 10000 -cert CAcert.pem - √£√ëkeyfile CAkey.pem We now have our application certiÔ¨Åcate in appcert.pem, which we deliver to the application section, below. If we read it with openssl x509 -in appcert.pem -text, we Ô¨Ånd that the Issuer is that of CAÔ¨Åle.pem, but the Subject is as above, with Common Name = Odradek. 29.5.3.2 TLSserver The complete server is at tlsserver.c, along with the certiÔ¨Åcate and key Ô¨Åles appcert.pem and appkey.pem (both stored with an additional .text sufÔ¨Åx to prevent an accidental click from loading them directly into a browser). To compiler the server under Linux, with the OpenSSL library installed in the usual place, we use 29.5 SSH and TLS 759
An Introduction to Computer Networks, Release 2.0.11 gcc -o tlsserver tlsserver.c -lssl -lcrypto The server should be run in the same directory with its certiÔ¨Åcate and key Ô¨Åles. In the following we go through the important lines of the full program stripped of any error checking. Most OpenSSL library methods return 1 on success and 0 on failure. Different kinds of failure may require different error-message libraries. SSL_library_init(); This does what it sounds like: initializes the SSL subsystem. Loading error strings may also be useful. The next steps commit us to the versions of SSL we agree to accept. method = SSLv23_server_method(); ctx = SSL_CTX_new(method); SSL_CTX_set_options(ctx, SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3 | SSL_OP_NO_TLSv1 √£√ë| SSL_OP_NO_TLSv1_1); Somewhat paradoxically, SSLv23_server_method() accepts allSSL and TLS versions. In the third line above, we then disable everything earlier than TLS version 1.1. In openssl version 1.1.0 (the numbering is unrelated to the TLS versioning), SSLv23_server_method() can be replaced with the more appropriately named TLS_server_method(). The variable ctx represents a TLS context, which is a set of TLS state information. Our server will use the same context for all incoming connections. Next we load the server certiÔ¨Åcate and key Ô¨Åles into our newly created TLS context. Recall that the server side gets our application certiÔ¨Åcate appcert.pem; the client side will get our certiÔ¨Åcate-authority certiÔ¨ÅcateCAcert.pem. int cfile_result = SSL_CTX_use_certificate_file(ctx, "./appcert.pem", SSL_ √£√ëFILETYPE_PEM); int kfile_result = SSL_CTX_use_PrivateKey_file (ctx, "./appkey.pem", SSL_ √£√ëFILETYPE_PEM); The server needs the key Ô¨Åle to sign messages. Next we create an ordinary TCP socket listening on port 4433;createSocket() is deÔ¨Åned at the end of the tlsserver.c Ô¨Åle. sock = createSocket(4433); Finally we get to the main loop. We accept a TCP connection, create a new connection-speciÔ¨Åc SSL object from our context, and tie the new SSL object to the socket. while(1) { int childsock = accept(sock, (struct sockaddr *)&addr, &len); SSL*ssl = SSL_new(ctx); SSL_set_fd(ssl, childsock); SSL_accept(ssl); SSL_write(ssl, reply, strlen(reply)); } 760 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 SSL_accept() is where the handshaking described in 29.5.2.4 TLS Connection Setup takes place. At this point, the server writes its reply message over the now-encrypted channel, and is done. We argued in 1.15 IETF and OSI that TLS can in some ways be regarded as a true Presentation layer. Note, however, that the Application layer here (that is, tlsserver.c) is responsible for accepting childsock, and passing it to TLS through SSL_set_fd(); the Application layer, in other words, does interact directly with TCP. 29.5.3.3 TLSclient The complete client is at tlsclient.c; its certiÔ¨Åcate is CAcert.pem (again with a .text sufÔ¨Åx). Again we go through the sequence of SSL library calls with error-checking removed. As with the server, we start with initialization; this time, we also load error strings: SSL_library_init(); // "SSL_library_init() always returns '1' ERR_load_crypto_strings(); SSL_load_error_strings(); Next we again choose what TLS versions we will allow, this time starting with SSLv23_ client_method(): method = SSLv23_client_method(); ctx = SSL_CTX_new(method); SSL_CTX_set_options(ctx, SSL_OP_NO_SSLv2 | SSL_OP_NO_SSLv3|SSL_OP_NO_ √£√ëTLSv1|SSL_OP_NO_TLSv1_1); If we instead use method = TLSv1_1_client_method(), the connection should fail, this call allows only TLS version 1.1, and the server requires TLS version 1.2 or better. The next step is to load the trust store, that is, the certiÔ¨Åcates from the certiÔ¨Åcate authorities we have elected to trust. If we do nothing, the trust store will be empty. We Ô¨Årst load a standard certiÔ¨Åcate directory (directories are supplied to SSL_CTX_load_verify_locations() as the third parameter and individual Ô¨Åles as the second). CertiÔ¨Åcates in a directory must be named (possibly using symbolic links) by their hash values; see the c_rehash utility. If all we wanted was to be able to trust our own server‚Äôs appcert.pem, we could just load our own certiÔ¨Åcate-authority certiÔ¨Åcate CAcert.pem, but we will need the standard certiÔ¨Åcate directory if we want to point tlsclient at a real HTTPS server rather than at tlsserver. SSL_CTX_load_verify_locations(ctx, NULL, "/etc/ssl/certs" ) Next we load our own certiÔ¨Åcate CAcert.pem, which is then added to the trust store, in addition to the standard certiÔ¨Åcates. We can add multiple individual certiÔ¨Åcates by making multiple calls, or by concatenating all the certiÔ¨Åcates into a single Ô¨Åle. The certiÔ¨Åcates must be separated by their BEGIN CERTIFICATE andEND CERTIFICATE lines. SSL_CTX_load_verify_locations(ctx, "CAcert.pem", NULL); Now we‚Äôre ready to connect to the server. The last line below initiates the TLS handshake, starting with ClientHello. 29.5 SSH and TLS 761
An Introduction to Computer Networks, Release 2.0.11 sock = openConnection(hostname, port); ssl = SSL_new(ctx); SSL_set_fd(ssl, sock); SSL_connect(ssl); Next the client retrieves (and, in our case prints) the certiÔ¨Åcate supplied by the server. If the server is tlsserver, this will normally be appcert.pem, withCN=Odradek. cert = SSL_get_peer_certificate(ssl); certname = X509_get_subject_name(cert); // print certname The printed certname does indeed show CN=Odradek, fromappcert.pem. If we were writing a web browser, this is the point where we would verify that the site hostname matches the CNÔ¨Åeld of the certiÔ¨Åcate. After the client receives the application certiÔ¨Åcate, it must verify its signature, with the call below. This is where the client uses its trust store. ret = SSL_get_verify_result(ssl); If one of the certiÔ¨Åcate authorities in the trust store vouches for the signature on the application certiÔ¨Åcate, the return value above is X509_V_OK, and all is well. If we comment out the loading of CAcert.pem, however, we get ‚Äúunable to get local issuer certiÔ¨Åcate‚Äù. If, with tlsclient still not loading CAcert.pem, we have the server sendCAcert.pem (andCAkey.pem ), instead of its proper certiÔ¨Åcate, we get an error ‚Äúself-signed certiÔ¨Åcate‚Äù. A full list of certiÔ¨Åcate-veriÔ¨Åcation errors is listed with the verify command. If validation fails, the connection is still encrypted, but is vulnerable to a man-in-the-middle attack. Regardless of what happened during validation, our particular tlsclient goes on to write an HTTP GET request to the server, and then read the server‚Äôs response (the companion tlsserver program does not in fact read the GET request). Generally speaking, however, continuing the TLS session after a certiÔ¨Åcate validation failure is a very bad idea. SSL_write(ssl, request, req_len); // ignored by tlsserver program do { bytesread = SSL_read(ssl, buf, sizeof(buf)); fwrite(buf, bytesread, 1, stdout); }while(bytesread > 0); The written request here is ignored by tlsserver; it is an HTTP GET request of the form GET / HTTP/ 1.1\r\nHost: hostname\r\n\r\n. If we point tlsclient at a real webserver, say tlsclient google.com 443 then we should again get an X509_V_OK veriÔ¨Åcation result because we loaded the default certiÔ¨Åcateauthority library. We can also point the built-in openssl client at tlsserver; by default it connects to localhost at port 4433: openssl s_client 762 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 Of course, veriÔ¨Åcation fails. This is because s_client doesn‚Äôt know about our certiÔ¨Åcate authority. We can add it, however, on the command line: openssl s_client -CAfile CAcert.pem Now the veriÔ¨Åcation is successful. 29.6 IPsec The SSH software package was built from the ground up to implement the SSH protocol. All modern web browsers incorporate TLS libraries to enable secure web connections. What can you do if you want to add encryption (or authentication) to a network application that doesn‚Äôt have it built in? Or, alternatively, how can you as a system administrator ensure that everyone‚Äôs trafÔ¨Åc is protected, regardless of what software they are using? IPsec, for ‚ÄúIP security‚Äù, is one answer. It is a general-purpose security protocol which typically behaves as if it were a network sublayer below the IP layer (or, in transport mode, below the Transport layer). In this it is akin to Wi-Fi ( 4.2.5 Wi-Fi Security ), which implements encryption within the LAN layer; in both Wi-Fi and IPsec the encryption is transparent to the communicating applications. In terms of actual implementation it is most often incorporated within the IP layer, but can be implemented as an external network appliance. IPsec can be used to protect anything from individual TCP (or UDP) connections to all trafÔ¨Åc between a pair of routers. It is often used to implement VPN-like access from ‚Äúoutside‚Äù hosts to private subnets behind NAT routers. It is easily adapted to support any encryption or authentication mechanism. IPsec supports two packet formats: the authentication header, AH, for authentication only, and the encapsulating security payload, ESP, below, for either authentication or encryption or both. The ESP format is much more common and is the only one we will consider here. The AH format dates from the days when most export of encryption software from the United States was banned (see the sidebar ‚ÄòCrypto Law‚Äô at 28.7.2 Block Ciphers ), and, in any event, the AH format can be used for authentication only. The ESP packet format is as follows: 29.6 IPsec 763
An Introduction to Computer Networks, Release 2.0.11 32 bits Security Parameters Index (SPI) Sequence number Integrity Check Value (variable length)Payload (variable length) Padding Pad Length Next Header ESP packet layout The SPI identiÔ¨Åes the security association, below. The sequence number is there to prevent replay attacks. Senders must increment it on every transmission, but receivers care only if the received numbers are not strictly increasing; gaps due to lost packets do not matter. The cryptographic algorithm applied to the payload and the integrity-check algorithm are negotiated at connection set-up. The Padding Ô¨Åeld is used Ô¨Årst to bring the Payload length up to a multiple of the applicable encryption blocksize, and then to round up the total to a multiple of four bytes. The Next Header Ô¨Åeld describes the data that is inside the Payload, egTCP or UDP for Transport mode or IP for Tunnel mode. It corresponds to the Protocol Ô¨Åeld of 9.1 The IPv4 Header or the Next Header Ô¨Åeld of 11.1 The IPv6 Header. IPsec has two primary modes: transport andtunnel. In transport mode, the IPsec endpoints are also typically the trafÔ¨Åc endpoints, and only the transport-layer header ( egTCP header) and data are encrypted or protected. In the more-common tunnel mode one of the IPsec endpoints is often a router (or ‚Äúsecurity gateway‚Äù); encryption or protection includes the original IP headers, so that an eavesdropper cannot necessarily identify the actual trafÔ¨Åc endpoints. IPsec is documented in a wide range of RFCs. A good overview of the architectural principles is found in RFC 4301. The ESP packet format is described in RFC 4303. A word of warning: while IPsec does support modern encryption, it also continues to support outdated algorithms as well; users must take care to ensure that the encryption negotiated is sufÔ¨Åcient. IPsec has also attracted, in recent years, rather less attention from the security community than SSH or TLS, and ‚Äúmany eyes make all bugs shallow‚Äù. Or at least some bugs. 29.6.1 Security Associations In order for a given connection or node-to-node path to receive IPsec protection, it is Ô¨Årst necessary to set up a pair of security associations. A security association consists of all necessary encryption/authentication attributes ‚Äì algorithms, keys, rekeying rules, etc‚Äì together with a set of selectors to identify the covered 764 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 trafÔ¨Åc. A given security association covers trafÔ¨Åc in one direction only; bidirectional trafÔ¨Åc requires a separate security association for each direction. For outbound trafÔ¨Åc ‚Äì that is, trafÔ¨Åc going from unprotected (internal) to IPsec-protected status ‚Äì the selector consists of the destination IP address (or set of addresses) and possibly also the source IP address (or set of source addresses) and port or protocol values. Inbound ESP packets carry a 32-bit Security Parameters Index, or SPI, that for unicast trafÔ¨Åc identiÔ¨Åes the security association. However, that security association must still be checked against the packet for an actual match. The destination and source IP addresses need not be the same as the IP addresses of the IPsec endpoints. As an example, consider the following tunnel-mode arrangement, in which traveling host A wants to connect to private subnet 10.1.2.0/24 through security gateway B. IPv4 addresses are shown, but the same arrangement can be created with IPv6. A B 10.1.2.0/24 host security gateway200.4.5.6 100.7.8.9Internet The A-to-B IPsec security association‚Äôs selector will include the entire subnet 10.1.2.0/24 in its set of destination addresses. A packet from A to 10.1.2.3 arriving at A‚Äôs IPsec interface will match this selector, and will be encapsulated and sent (via normal Internet routing) to B at 100.7.8.9. B will de-encapsulate the packet, and then forward it on to 10.1.2.3 using its normal IP forwarding table. B might actually bethe NAT router at its site, with external address 10.7.8.9 and internal subnet 10.1.2.0/24, or B might simply be a publicly visible host at its site that happens to have a route to the private 10.1.2.0/24 subnet. The action of forwarding the encapsulated packet from A to B closely resembles IP forwarding, but isn‚Äôt quite. It is unlikely A will have a true forwarding-table entry for 10.1.2.0/24 at all; it will very likely have only a single default route to its local ISP connection. Delivery of the packet cannot be understood simply by examining A‚Äôs IP forwarding table. A might even have a forwarding-table entry for 10.1.2.0/24 to somewhere else, but the IPsec ‚Äúpseudo-route‚Äù to B‚Äôs 10.1.2.0/24 is still the one taken. This can easily lead to confusion; for complex arrangements with multiple overlapping security associations, this can lead to nontrivial difÔ¨Åculties in Ô¨Åguring out just how a packet is forwarded. A second routing issue exists at B‚Äôs end. Host 10.1.2.3 will see the packet from A arrive with address 200.4.5.6. Its reply back to A will be delivered to A using the tunnel only if the B-site routing infrastructure routes the packet back to B. If B is the NAT router, this will happen as a matter of course, but otherwise some deliberate action may need to be taken to avoid having 10.1.2.3-to-A trafÔ¨Åc take an unsecured route. Additionally, the B-to-A security association needs to list 10.1.2.0/24 in its list of source addresses. The IPsec ‚Äúpseudo-route‚Äù now resembles the policy-based routing of 13.6 Routing on Other Attributes, with routing based on both destination and source addresses. A packet for A arriving at B with source address 10.2.4.3 should nottake the IPsec tunnel. (To add confusion, Linux IPsec pseudo-routes do not actually show up in the Linux policy-based routing tables.) Security associations are created through a software management interface ,egvia the Linux ipsec command and the associated conÔ¨Åguration Ô¨Åle ipsec.conf. It is possible for an application to request creation of the necessary security associations, but it is more common for these to be set up before the IPsec-protected application starts up. A request for the creation of a security association typically triggers the invocation of the Internet Key Exchange, IKE, protocol; the current version 2 is often abbreviated IKEv2. IKEv2 is described in RFC 29.6 IPsec 765
An Introduction to Computer Networks, Release 2.0.11 7296. IKEv2 typically uses public keys to negotiate a session key ( 28.7.1 Session Keys ); IKEv2 may then renegotiate the session key at intervals. In the simplest (and not very secure) case, both sides have been manually conÔ¨Ågured with a session key, and IKEv2 has little to do beyond verifying that the two sides have the same key. NAT traversal of IPsec packets is particularly tricky. For AH packets it is impossible, because the cryptographic authentication code in the packet covers the original IP addresses, as well as the packet transport data. That is not an issue for ESP packets, but even there the incoming packet must match the receivers‚Äôs security-association selector, which it will not if that was negotiated using the sender‚Äôs original IP address. An additional problem is that many NAT routers fail to forward (or fail to forward properly) packets outside of protocols ICMP, UDP and TCP. As a result, IPsec has its very own NAT-traversal mechanism, outlined in RFC 3715 ,RFC 3947 andRFC 3948. IPsec packets are encapsulated in UDP packets, with their original headers. After de-encapsulation at the IPsec receiving end, it is these original headers that are used in the security-association check. Additionally, a keepalive mechanism is deÔ¨Åned in which the IPsec nodes send regular small packets to make sure the NAT mapping for the connection does not time out. 29.7 DNSSEC The DNS Security Extensions, DNSSEC, make it possible for authoritative nameservers to provide authenticated responses to DNS queries, by using digital signatures, below. The primary goal of DNSSEC is to prevent cache poisoning ( 10.1.4 DNS Cache Poisoning ) by allowing resolvers to verify any DNS records received. DNSSEC is documented in RFC 4033 ,RFC 4034 ,RFC 4035 and updates. RFC 6891 outlines a general framework for extensions to DNS, known as EDNS; these extensions include new record types. EDNS in particular deÔ¨Ånes the DNSSEC OK, or DO, Ô¨Çag; this is used to signal that the receiving nameserver should return a DNSSEC-aware response. The basic idea behind DNSSEC is for each zone, including the root zone, to digitally sign all of its RRsets ‚Äì sets of DNS resource records matching a given name ( egcs.luc.edu ) and type ( egA records, AAAA records, or MX records) ‚Äì with a public-key signature. Each zone except the root zone then has its parent zone sign its public key. The root-zone public key must be pre-loaded into the resolver or end-system, much as the root-zone IP addresses must be pre-loaded. This sort of linked sequence of digital-signature veriÔ¨Åcations is often called a chain of trust. The root keys are known as the trust anchors; these can be compared to certiÔ¨Åcate authorities ,29.5.2.1 CertiÔ¨Åcate Authorities. The root zone implemented support for DNSSEC in 2010, after .gov and with the .org zone not far behind; other top-level domains including .com, .net and .edu became DNSSEC-aware by the following year. In order to support all these signed records and keys, DNSSEC introduced several additional DNS record types: 
- RRSIG: a signature for an RRset. 
- DNSKEY: the public half of the keypair used to sign zone records. 
- DS: a ‚Äúdelegation signer‚Äù record in a parent zone containing a signature for a child zone‚Äôs zone-signing key. 766 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 
- NSEC andNSEC3: the next DNS name in sequence, used to afÔ¨Årm securely that a given hostname does notexist. In slightly more detail, each DNSSEC authoritative nameserver creates, for each zone, a Zone Signing Key, or ZSK. For each possible RRset the nameserver creates an RRSIG signature record, signed by the ZSK. The public half of the zone‚Äôs ZSK is available in a record of type DNSKEY, and with DNS name equal to the zone name. Finally, a hash of the public ZSK is signed by the parent ‚Äôs ZSK and made available in the parent zone as a record of type DS, again with name equal to the child zone‚Äôs name. If a DNSSEC-aware query arrives for xname ,typey, the nameserver will return both that RRset and also the RRSIG record for that name and type. For example, if we use the dig tool to query for the Arecord for isc.org, and include the +dnssec Ô¨Çag for DNSSEC-aware results, dig isc.org A +dnssec, we receive (in part, and with formatting applied) isc.org. 52 IN A 149.20.64.69 isc.org. 52 IN RRSIG A 5 2 60 20190328000617 √£√ë20190226000617 19923 isc.org. √£√ëKrBysOlbe4L6sJJOJNbJhfAuNt11q+6A2cQTnr3CXeFwxYJTXdqAkSwg √£√ëQzGHpIrVfOw2dn6GdqXQ6umqU1cnFNtXumdvUp45+XSCoZC6YciR4xNs f8YMR5F66LIcMZewP11ofWOV6/ √£√ëm9rSfR38FRnDkPf3Jg+O2+qvSKQ+Mq lV8= Note that RRSIG records must always be piggybacked onto the original query, as there is no independent way to request ‚ÄúRRSIG records matching type type‚Äù. DNS queries may contain only one type. The RRSIG records are part of the DNS ANSWER section, not the ADDITIONAL section; these records are an essential part of the response. We can obtain the isc.org zone‚Äôs public ZSK by requesting the record with name isc.org and type DNSKEY. We can verify this key by asking for the record with name isc.org and type DS from the .org nameserver. To fetch the A record for www.example.com, a DNSSEC resolver (often called a validating resolver) would (if nothing has been cached) start by asking the root zone for the appropriate NS record, just as with plain DNS ( 10.1.2 nslookup and dig ). The root zone replies with the NS record (and A record) pointing to the.com authoritative nameserver, and, because the DNSSEC OK bit in the request has been set, also includes the corresponding RRSIG record. The resolver can validate the RRSIG signature because it knows by prearrangement the root-zone KSK. The resolver also asks the root nameserver for the DS record for .com. This is a signature for the KSK that the.com nameserver uses, signed by the root key. It will be used in the next step. The resolver now switches to sending its requests to the authoritative nameserver for the .com zone. It asks for the NS record for example.com, and receives the appropriate NS and A records. Because DNSSEC is involved, it also receives the corresponding RRSIG records. The latter are signed with the KSK for. com. The resolver obtains this KSK by requesting the DNSKEY record from the .com nameserver. This KSK can then be validated by using the signed DS record previously obtained from the root zone, and the validated KSK can then in turn be used to validate the RRSIG records. Note that the DNSKEY and DS records for the .com DNS name form a pair, with the latter signing the 29.7 DNSSEC 767
An Introduction to Computer Networks, Release 2.0.11 former (or a hash of the former). The DNSKEY record, though, is obtained from the .com nameserver, while the corresponding DS record is obtained from the parent (root, in this case) nameserver. The resolver also asks the .com nameserver for the DS record for example.com; this will be used in the next step. In the Ô¨Ånal stage, the resolver sends its requests to the example.com namserver. This time it asks for the A record for www.example.com. This address is returned, along with the corresponding RRSIG. The resolver gets the example.com ZSK by requesting the DNSKEY record. The resolver obtained a signature for this ZSK in the previous DS record (obtained from the .com nameserver), and so can validate this example.com ZSK, and so in turn can validate the RRSIG for the address record for www. example.com. The DNS queries involved are summarized in the following diagram. Each request shows the xname ,typey pair. Responses are indicated only for queries that traditional, non-DNSSEC DNS would make, to emphasize RRSIG records are included. These queries are shown with black paths. The DS/DNSKEY requests for the .com zone are shown with blue paths; the DS/DNSKEY requests for the example.com zone are shown in green. validating resolver.com authoritative nameserverroot authoritative nameserver example.com authoritative nameserverrequest: (example.com,NS) response; NS+A+RRSIG request: (.com,DS) request: (example.com,NS) response; NS+A+RRSIG request: (.com,DNSKEY) request: (example.com,DS) request: (www.example.com,A) response: A+RRSIG request: (example.com,DNSKEY) The chain of key validations can be summarized as follows: 
- The known root ZSK validates the .com ZSK, through the DNSKEY/DS pair for the DNS name .com 
- The.com ZSK validates the example.com ZSK, and is itself validated via the DNSKEY/DS pair for the DNS name .com 
- Theexample.com ZSK validates the A record for www.example.com, and is itself validated via 768 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 the DNSKEY/DS pair for the DNS name example.com It is possible that the resolver knows, likely through caching, the IP address of the example.com nameserver. In this case, if the RRSIG and DS credentials were not also cached, the resolver might go through the above process from the bottom up ( example.com to.com to root) to validate the information. Ifexample.com had not yet implemented DNSSEC support, but a subdomain cs.example.com did, then the chain of trust between the subdomain and the root zone would be broken. In this case, a resolver could only use DNSSEC to validate records for cs.example.com if it already had the ZSK for this domain, known, as with the root zones, as the trust anchor. Trust anchors for such isolated zones, or ‚Äúislands of security‚Äù, are often made available through manual conÔ¨Åguration. Any and all of the records above might be cached by the validating resolver, following a previous query. Caching works for DNSSEC just as it works for ordinary DNS. The key-related records, like all DNS records, each have a time-to-live (TTL) value; the resolver must abide by these. The diagram above shows two separate requests to the root nameserver, three to the .com nameserver, and two to the example.com nameserver. That is inefÔ¨Åcient. Fortunately, it is not mandatory: clients can request everything at once. It is also likely that many of these records will be cached (particularly requests to the root and .com nameservers). For improved security, some authoritative nameservers may be conÔ¨Ågured with two keys: a shorter zonesigning key, used to sign the RRSIGs, and then a longer key-signing key, or KSK; the latter is the one signed in the parent-zone DS record. The DNSKEY records return both ZSK and KSK; this is sufÔ¨Åcient to maintain the chain of trust. As an example of a ZSK/KSK pair, let us send the following DNSKEY request to the .com domain (having previously looked up the NS record to get the .com nameserver IP address, 192.12.94.30) dig @192.12.94.30 com DNSKEY +dnssec We get back two keys. The shorter one, as is explained below, is the ZSK and the longer is the KSK (this output has been formatted so the key data lines up neatly). com. 86400 IN DNSKEY 256 3 8 AQO+kWUV3rtj/ √£√ëVi6FLBfxMRcFoz69Go6xVwa99AWzENDi98y9CIJfx6w n9aR0SWsCk/ √£√ëoY+hreX6egC7nyyxQ5bxq52aovlZI34Cn+hpy/YGGO2HS b44AWONsjuZTAfGYLBdaJi2Wg+Z0IVqPw/ √£√ëLp0Ysu9I8orc2KyNIPQGA/ rTgXOw== com. 86400 IN DNSKEY 257 3 8 AQPDzldNmMvZFX4NcNJ0uEnKDg7tmv/ √£√ëF3MyQR0lpBmVcNcsIszxNFxsB √£√ëfKNW9JYCYqpik8366LE7VbIcNRzfp2h9OO8HRl+H+E08zauK8k7evWEm u/6od+2boggPoiEfGNyvNPaSI7FOIroDsnw/ √£√ëtaggzHRX1Z7SOiOiPWPN √£√ëIwSUyWOZ79VmcQ1GLkC6NlYvG3HwYmynQv6oFwGv/KELSw7ZSdrbTQ0H √£√ëXvZbqMUI7BaMskmvgm1G7oKZ1YiF7O9ioVNc0+7ASbqmZN7Z98EGU/Qh 2K/ √£√ëBgUe8Hs0XVcdPKrtyYnoQHd2ynKPcMMlTEih2/2HDHjRPJ2aywIpK (continues on next page) 29.7 DNSSEC 769
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) Nnv4oPo/ The numbers 256 and 257 immediately following the DNSKEY type label represent a 16-bit Ô¨Çag Ô¨Åeld. Both have the bit set in the 256 position, indicating the keys are there for zone-signing generally. The longer key also has the bit set in the 1 position; this is the ‚ÄúSecure Entry Point‚Äù Ô¨Çag and, rather loosely, indicates that this key is the KSK. More speciÔ¨Åcally, the SEP Ô¨Çag is used to mark a key for which a DS record will be created in the parent zone. See RFC 3757 for further details. After the 256/257 is a 3, and then an 8. The 8 means that the keys and signatures use RSA and SHA-256, as speciÔ¨Åed in RFC 5702. The keys are encoded in base64 ( RFC 4648 ). The Ô¨Årst key has 176 encoded bytes (3 lines of 56, plus 8). Decoding it with the Python b64decode function in the base64 library, we get a byte string of length 130. The Ô¨Årst byte is 0x01, with seven leading zero bits; the number of bits from the Ô¨Årst nonzero bit to the end is 1033. The second has 344 bytes, or 2057 bits after stripping the leading 0-bits. In common parlance, these are 1024-bit and 2048-bit keys respectively. A 1024-bit RSA keylength is not terribly secure, but is meant for relatively short-term use (~30 days). The 2048-bit KSK provides excellent security. Each RRSIG record contains a Key Tag Ô¨Åeld, consisting of a 16-bit hash of information about the key. This makes it much easier, when there are multiple DNSKEY records, to help Ô¨Ågure out which one was used to create the RRSIG signature. The fallback, if necessary, is to re-calculate the signature with each available key, and see which one matches the RRSIG. Multiple DNSKEY records occur when separate ZSK and KSK keys have been created, as above. They also occur anytime a key is in the process of being updated, as in the next paragraph. To change the KSK for the example.com nameserver, the Ô¨Årst step is to create the new key, and then to create new RRSIGs for each RRset. A new DNSKEY record is also created. The new KSK public key must then be communicated to the parent zone, in order for the latter to create the corresponding DS record. For a while, the RRSIG and DNSKEY (and DS, at the parent nameserver, if the KSK is being changed) record sets returned will contain records for both KSKs; the receiving resolver can use the Key Tag Ô¨Åeld (above) to Ô¨Ågure out which key goes with which RRSIG. Eventually the old KSK will expire, egwhen its TTL is reached, and the older records can be removed. Signing failure responses for non-existent DNS names is also important, but is a little trickier. First, the RRset in question is empty. An empty set can be signed, but the signed response can now be replayed, perhaps to convince someone later that an existing DNS name is not valid. It would be possible to prevent this by including the non-existent DNS name in the signed response, but that would require that the signing private key be available whenever necessary. One of the goals of the RRset/RRSIG strategy above, however, is to make possible the pre-signing of all record sets, so the actual private key can then be secured ofÔ¨Çine. To get around this, the original strategy for authenticating non-existence was to return a record containing the previous legitimate DNS name, and then an NSEC record listing the subsequent legitimate DNS name, according to alphabetical order. The NSEC record corresponding to a valid DNS host name is the next valid DNS name in sequence. For example, if a query asked for information about nonexistent host foo. example.com, the results returned might be the DNS name erl.example.com and then the NSEC record forerl indicating that the next record following was gateway.example.com. The nameserver can prepare signatures for such records ahead of time for each consecutive pair of DNS names. The NSEC record corresponding to the last valid hostname wraps around to the Ô¨Årst, usually the zone name itself. The drawback to this strategy is that it makes it easy for someone to enumerate all the valid DNS names in a 770 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 zone, by sequential querying. To prevent this, the NSEC approach was updated to NSEC3, which does much the same thing, except that cryptographic hashes of each hostname are returned instead, and the ordering used is that of the hashed values. An attacker can now enumerate the hashed values of each DNS name, but this doesn‚Äôt help discover the actual hostnames without considerable effort. The necessary signatures can, as with the NSEC approach, all be prepared in advance. 29.7.1 Using DNSSEC If you are managing an authoritative nameserver, egfor your own website, enabling DNSSEC takes some deliberate effort. To enable DNSSEC requires conÔ¨Åguring the nameserver software to be DNSSEC-aware, creating the keys, and forwarding the appropriate DS record to the parent zone. For sites that have done this, however, there is no guarantee that the DNSSEC validation beneÔ¨Åts will actually be available to a given user workstation; that depends on the resolver the workstation uses. If it supports DNSSEC, then DNS results from DNSSEC-aware authoritative nameservers will be validated according to the DNSSEC process. Most user workstations are conÔ¨Ågured to use the site resolver provided by the local ISP. Some of these support DNSSEC; many do not. It is possible to switch to a DNSSEC-validating resolver manually, eg a public DNS server, but most users do not do this. Typical workstation ‚Äústub resolvers‚Äù do not perform DNSSEC validation by default; the popular Linux dnsmasq resolver can be conÔ¨Ågured to use DNSSEC by adding the--dnssec command-line Ô¨Çag in the appropriate startup Ô¨Åle. Does dnssec-failed.org exist? If your site DNS resolver performs DNSSEC validation, clicking on the link here to www.dnssecfailed.org will simply fail. To verify the site actually exists without changing resolvers, Ô¨Årst get the NS record fordnssec-failed.org from the.org nameserver; as of 2019 this was dns101.comcast.net at 69.252.250.103. Then dig @69.252.250.103 www.dnssec-failed.org should give you the desired A records (two in 2022: 68.87.109.242 and 69.252.193.191). The simplest way to tell if a resolver supports DNSSEC, andvalidates the DNSSEC responses received is to look up one of several DNS names that have intentionally been misconÔ¨Ågured. One of these is www.dnssecfailed.org, managed (2019) by Comcast. A validating resolver will return the NXDOMAIN (Non-eXistent DOMAIN) error message (or, in other words, zero records in the ANSWER section); clicking on the link should yield a browser error message like ‚ÄúHmm. We‚Äôre having trouble Ô¨Ånding that site.‚Äù A non-DNSSECvalidating resolver will return an A record, and the link should work normally. Some partially DNSSECaware (but non-validating) resolvers still manage to return an address (see below). Another way to gauge the degree of resolver support for DNSSEC is to use the dig command( 10.1.2 nslookup and dig ). In the example below, the dns_server should be the IP address of the resolver; if omitted (along with the @sign) then the default resolver is used. The +dnssec argument sets theDNSSEC OK Ô¨Çag in the query. dig @ dns_server isc.org A +dnssec This command returns something like the following if the resolver does not support DNSSEC at all (this is from the OpenDNS resolver at 208.67.222.222, as of 2019). 29.7 DNSSEC 771
An Introduction to Computer Networks, Release 2.0.11 ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 15612 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;isc.org. IN A ;; ANSWER SECTION: isc.org. 60 IN A 149.20.64.69 There is no RRSIG record, the flags on the second line do notinclude the adÔ¨Çag (for ‚Äúauthenticated data‚Äù), and the EDNS line does not include the doÔ¨Çag (for ‚ÄúDNSSEC OK‚Äù). For resolvers understanding DNSSEC, the following might appear (this is from the Google resolver at 8.8.8.8): ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60302 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION:; EDNS: version: 0, flags: do; udp: 512 ;; QUESTION SECTION: ;isc.org. IN A ;; ANSWER SECTION: isc.org. 59 IN A 149.20.64.69 isc.org. 59 IN RRSIG A 5 2 60 20190403233441 √£√ë20190304233441 28347 isc.org. KlUPLww/FO3rOV3rDiuEH2ttUDSA/ √£√ëPtpCaHTGTSyHMaIGlOU3YoHn8hP FJtCkhoopF/ √£√ëZ6dFXPxAq1LpwtEP9KqU+PxQYGSsZESg1KkEWIaoiE2MB √£√ëhHsGqDnJG2GJhyBfCESIt21QY9Q28+nraTG0OUHiKwE8H2c0/PM2VTXP clQ= Note the RRSIG record, the adÔ¨Çag in the second line, and the doÔ¨Çag in the fourth. Without the +dnssec argument, we should get an ordinary DNS response, without RRSIG, adordo. Occasional intermediate results may also sometimes appear, eghaving the RRSIG record and the doÔ¨Çag but missing the adÔ¨Çag. If we try the same lookup with the domain name www.dnssec-failed.org, some resolvers return no answer (that is, NXDOMAIN) while others return A and RRSIG records. However, we should not see any resolver return with the adÔ¨Çag, as due to an artiÔ¨Åcially induced key error the data cannot in this case be authenticated. 29.7.2 DNS-based Authentication of Named Entities The idea behind DNS-based Authentication of Named Entities, or DANE, is to have DNSSEC sign not just the usual DNS records, but also TLS certiÔ¨Åcates. By doing so, DANE replaces the need for TLS certiÔ¨Åcate authorities ( 29.5.2.1 CertiÔ¨Åcate Authorities ). See RFC 6698 and updates. The basic idea behind DANE is that a domain owner can make their public TLS keys available on their authoritative DNS nameserver. The same chain of trust that authenticates A records from this nameserver will also authenticate the certiÔ¨Åcates. That is, certiÔ¨Åcates are signed by local KSKs, and those in turn are signed by parent KSKs, until the root zone is reached. The long list of certiÔ¨Åcate-authority keys stored in browsers would be reduced to the much shorter list of root-zone DNSSEC keys. 772 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 certiÔ¨Åcates are stored as DNS objects of type TLSA. CertiÔ¨Åcates are speciÔ¨Åc to applications, so the domain name ( egwww.example.com ) is preÔ¨Åxed with the port and transport protocol, each with a leading underscore ‚Äú_‚Äù (443 is the port number for https trafÔ¨Åc): _443._tcp.www.example.com DANE solves a number of problems. First, it makes (or potentially makes) expensive certiÔ¨Åcate authorities unnecessary. Second, certiÔ¨Åcate authorities have to verify that someone asking for a certiÔ¨Åcate for a given domain is actually the owner of that domain; under DANE, this is solved by having the domain‚Äôs authoritative nameserver be responsible for both the domain and the keys. Finally, DANE simpliÔ¨Åes the process of adding new certiÔ¨Åcates, and makes this pretty much a do-it-yourself operation. Once the owners of a domain have created a zone-signing key and had it signed in the parentzone DS record, they can create as many additional certiÔ¨Åcates as they want. The validation library used by clients (whether incorporated into the TLS library or not) would be able to ensure that certiÔ¨Åcates so issued are for the domain in question, or a subdomain, and also verify the port and protocol constraints above. Each such DANE certiÔ¨Åcate would be veriÔ¨Åable through the DNSSEC chain of trust. More-Ô¨Çexible certiÔ¨Åcate creation would eliminate the need for the ‚Äúprivate‚Äù (self-signed) certiÔ¨Åcates often used to validate access to VPNs and email servers. Under DANE, certiÔ¨Åcates could become lightweight objects created on a moment‚Äôs notice. (Of course, no browser currently supports any of this.) While DANE is intended for the creation of TLS (X.509) certiÔ¨Åcates, other forms of veriÔ¨Åable public keys would also be possible. See RFC 6394 for further examples. While compromise of an authoritative nameserver using DANE could lead to false TLS certiÔ¨Åcates, this can also happen with conventional certiÔ¨Åcate authorites, who generally use control of the domain as evidence that the entity requesting the certiÔ¨Åcate actually owns that domain. Compromise of the .com nameserver would allow the distribution of false DANE-based TLS certiÔ¨Åcates for any.com domain. But if that happens we are all in trouble, regardless of DANE. There is one subtle difference, which is that of trust. Under the conventional certiÔ¨Åcate-authority regime, uses can in principle decide which CAs they trust. While this is rarely exercised by browser users, it is available as a potential option. Under DANE, to trust the certiÔ¨Åcate for example.com is to trust the .com and root nameservers; there is no choice. For some, the fact that the .com and root nameservers are under control of the United States government (through IANA and agreements with IANA) is of concern. It is easy enough for other governments to create their own hierarchy, starting with the two-letter country-code domain and an associated trust-anchor key, but it is still a government in charge. However, while certiÔ¨Åcate authorities are private entities, it is not clear to what extent they can or would resist government demands to issue misleading certiÔ¨Åcates. 29.7.3 Why Isn‚Äôt DNSSEC More Popular? SSH and TLS have been adopted eagerly by system and website administrators. Even SNMPv3 ( 27.3 SNMPv3 ) has seen widespread adoption, following early concerns about deployment difÔ¨Åculties. But while many governments (such as the United States) have mandated DNSSEC for their own websites, and while organizations with ties to Internet governance have been early adopters (especially ICANN; see this statement), DNSSEC adoption by major Internet corporations has been vanishingly small. Using the dig tool, 29.7 DNSSEC 773
An Introduction to Computer Networks, Release 2.0.11 it is straightforward to verify that none of the domains in the table below support DNSSEC (none return RRSIG records when A records are requested with the +dnssec Ô¨Çag, as of 2019): apple.com alibaba.com amazon.com ebay.com expedia.com facebook.com google.com microsoft.com netÔ¨Çix.com salesforce.com tencent.com twitter.com uber.com yahoo.com youtube.com. Google has supported DNSSEC at their public resolver, 8.8.8.8, since 2013, but not at their authoritative nameservers. Paypal.com is one of the few large sites that hasadopted DNSSEC. These companies are seriously interested in Internet security. Why do they turn their backs on DNSSEC? The primary answer is that if a site enables DNSSEC, and some third party makes the wrong DNSSECconÔ¨Åguration mistake, then the site becomes unreachable, for at least some users. It goes dark. It is cast into the void. And there is no error message for those users. If a TLS certiÔ¨Åcate is faulty, at least the end user gets a message in their browser; see the sidebar ‚ÄúCertiÔ¨Åcate Errors‚Äù at 29.5.2.1 CertiÔ¨Åcate Authorities. The browser directly manages the TLS connection, and, if something goes wrong, the browser learns what, and can inform the user. The majority of certiÔ¨Åcate errors have to do with misspellings and expirations and updates and other things under control of the domain owner; pretty much the only other major source of TLS errors is the certiÔ¨Åcate authority, and the domain owner can keep an eye on them. But with DNS, most browsers farm out lookups to a system resolver library; that library likely does not return anything to the browser that might distinguish DNSSEC validation errors from NXDOMAIN errors. The site or public resolver that the local system sends its DNS queries to likely also does not return different results for validation errors versus NXDOMAIN. And while an error in DNSSEC validation can occur at the root, it can also occur at any lower level in the DNS hierarchy. It is impossible for domain owners to track potential problems at every possible resolver. A second reason sites choose not to make use of DNSSEC is that it is often perceived as unnecessary. If a user receives a corrupted address through DNS, the problem should be detected through TLS certiÔ¨Åcatesigning failure. There have been remarkably few DNS-based attacks for which DNSSEC would have made the difference in terms of detection. A large-scale series of DNS attacks in 2018 involved changing the IP addresses retrieved via DNS for selected hostnames to those of servers under the attackers‚Äô control, just as with cache-poisoning attacks (10.1.4 DNS Cache Poisoning ). These attacks, however, entailed the compromise of the authoritative nameservers involved; the attacks were achieved through stolen DNS-administrator credentials (and possibly through misfeatures in the DNS-conÔ¨Åguration software within some domain registrars). See this FireEye blog for more details. DNSSEC does notprotect against an attacker who is able to compromise a zone‚Äôs authoritative nameserver. Nameserver credentials (all login credentials, really) must be guarded carefully. The counter argument is that DNSSEC certainly has the potential to block some attacks, and that more security is always better than less. This latter concept is called the defense in depth principle. It is, in the abstract, hard to argue with. In the concrete, on the other hand, it depends. DNSSEC is complex, and introduces its own risks. The defense-in-depth principle has been used to argue for regular password 774 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 changes, but that policy seems to have the paradoxical effect of encouraging users to choose shorter, easierto-remember passwords. The real question is whether DNSSEC adds enough to Internet security to be worth the change. Eventually, as most public DNS servers and ISP-based site resolvers gain experience with DNSSEC, switching over to it at the server side will become safer and safer. It is likely that, eventually, the going-dark risk will be seen as negligible; at that point, DNSSEC starts to make a lot of sense even if the added beneÔ¨Åt is modest. Another concern is that DNSSEC is old, and the world of DNSSEC still contains a great many legacy 1024bit RSA keys. By today‚Äôs standards, those are not considered long enough for long-term use. But DNSSEC does not mandate such short keys. As we saw in an example above, the .com key-signing key was 2048 bits; root keys and other top-level KSKs are similar. There is still the issue that the domain hierarchy is more tightly controlled by governments, but while some individual users may be concerned about this, it is not clear that large Internet companies are. 29.7.4 DNS over HTTPS An entirely different approach to securing DNS ‚Äì or at least to reducing DNS problems ‚Äì has been taken by Mozilla in their Firefox browser. The idea behind DNS over HTTPS ‚Äì DoH ‚Äì is to abandon the use of site resolvers, and have Firefox send DNS queries to one central DNS resolver, secured by TLS. The speciÔ¨Åcs are in RFC 8484. One stated goal is to eliminate DNS cache-poisoning attacks by, in essence, switching Firefox users to one large, highly secure resolver (a ‚Äútrusted recursive resolver‚Äù or TRR ) that gets all its data directly from authoritative nameservers. Another goal is to prevent ISP eavesdropping on (and even interference with) DNS requests sent to local ISP-provided resolvers. Of course, the ISP-eavesdropping concern is simply replaced by the concern that the central DNS resolver is collecting DNS queries. The TRR partner selected by Mozilla is CloudÔ¨Çare, manager of a large CDN. There is an agreement negotiated between Mozilla and CloudÔ¨Çare that personally identiÔ¨Åable data in DNS requests is to be discarded after 24 hours, and never shared with outside parties. Use of DoH by Firefox is controlled by the about:config settingnetwork.trr.mode; a value of 0 prefers the normal system resolver and a value of 5 completely disables DoH. One can also set the resolver itself, egto a non-CloudÔ¨Çare one, though it must be one that understands DNS-over-HTTP queries. The DoH speciÔ¨Åcation spells out how DNS queries are to be mapped onto HTTP GET requests; traditional DNS queries and responses look nothing like HTTP. There is a separate approach, DNS over TLS; it is outlined in RFC 7858. The goal is simply to prevent eavesdropping on DNS trafÔ¨Åc from hosts to their individually chosen resolvers. 29.8 RSA Key Examples In this section we create a short RSA key, using the openssl package, available for Windows, Macs and Linux. We then break it, via factoring. The Ô¨Årst step is to create an RSA key with length 96 bits; this length was chosen for easy factorability. 29.8 RSA Key Examples 775
An Introduction to Computer Networks, Release 2.0.11 openssl genrsa -out key96.pem 96 The resultant Ô¨Åle is as follows: -----BEGIN RSA PRIVATE KEY----- MFICAQACDQCo1hzP6/gTzbNAEHcCAwEAAQINAJzcEHi8aYSO0iizgQIHANkzC28P jwIHAMb/VQH8mQIHALc+9ZqRyQIHAKkfQ43msQIGJxhmMMOs -----END RSA PRIVATE KEY----- This is in the so-called PEM format, which means that the two lines in the middle are the base64 encoding of the ASN.1 encoding ( 26.12 SNMP and ASN.1 Encoding ) of the actual data. Despite the PRIVATE KEY label, the Ô¨Åle in fact contains both private and public keys. SSH private keys, typically generated with the ssh-keygen command, are also in PEM format. We can extract the PEM-Ô¨Åle data with the next command: openssl rsa - inkey96.pem -text The output is the following: Private-Key: (96 bit) modulus: 00:a8:d6:1c:cf:eb:f8:13:cd:b3:40:10:77 publicExponent: 65537 (0x10001) privateExponent: 00:9c:dc:10:78:bc:69:84:8e:d2:28:b3:81 prime1: 00:d9:33:0b:6f:0f:8f prime2: 00:c6:ff:55:01:fc:99 exponent1: 00:b7:3e:f5:9a:91:c9 exponent2: 00:a9:1f:43:8d:e6:b1 coefficient: 27:18:66:30:c3:ac The default OpenSSL encryption exponent, denoted e in 29.1 RSA, is 65537 = 216+1. (The default exponent used to be 3, but see exercise 10.0) We next convert all these hex numbers to decimal; the corresponding notation of 29.1 RSA is in parentheses. modulus (n) 52252327837124407964427358327 privateExponent (d) 48545702997494592199601992577 prime1 (p) 238813258387343 prime2 (q) 218799945153689 exponent1 201481036403145 exponent2 185951742453425 coefÔ¨Åcient 42985747170220 776 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 We now verify some arithmetic, using any tool that supports large integers ( egpython3, used here, or the unixbccommand). First we check n = pq: >>>238813258387343 *218799945153689 52252327837124407964427358327 Next we check that ed = 1 mod (p-1)(q-1): >>>e=65537 >>>d=48545702997494592199601992577 >>>p=238813258387343 >>>q=218799945153689 >>>(p-1)*(q-1) 52252327837123950351223817296 >>>e*d % 52252327837123950351223817296 1 To encrypt a message m, we must use efÔ¨Åcient mod-n calculations; here is an implementation of the repeatedsquaring algorithm (mentioned above in 28.8.1 Fast Arithmetic ) in python3. (This function is built into python aspow(x,e,n) .) defpower(x,e,n): # computes x^e mod n pow = 1 whilee>0: ife%2 == 1: pow = pow *x % n x = x*x % n e = e//2 # // denotes integer division returnpow Let m be the string ‚ÄúRivest‚Äù. In hex this is 0x526976657374; in decimal, 90612911403892. >>>m=0x526976657374 >>>c=power(m,e,n) >>>c 38571433489059199500953769621 >>>power(c,d,n) 90612911403892 What about the last three numbers in the PEM Ô¨Åle, exponent1 ,exponent2 andcoefficient ? These are pre-computed values to speed up decryption. Their values are 
- exponent1 = d mod (p-1) 
- exponent2 = d mod (q-1) 
- coefficient is the solution of coefÔ¨Åcient q = 1 mod p 29.8.1 Breaking the key Finally, let us break this 96-bit key and decrypt the message with ciphertext c above. The hard part is factoring n; we use the Gnu/Linux factor command: 29.8 RSA Key Examples 777
An Introduction to Computer Networks, Release 2.0.11 > factor 52252327837124407964427358327 52252327837124407964427358327: 218799945153689 238813258387343 The factors are indeed the values of p and q, above. Factoring took 2.584 seconds on the author‚Äôs laptop. Of course, 96-bit RSA keys were never secure; recall that the current recommendation is to use 2048-bit keys. The Gnu/Linux factor command uses Pollard‚Äôs rho algorithm, and, while serviceable, is not especially well suited to factoring the product of two large primes. The author was able to factor a 200-bit modulus in just over 5 seconds using the msieve program, one of several large-number-factoring programs available on the Internet. Msieve implements a version of the number-Ô¨Åeld-sieve algorithm mentioned in 29.1.2 Factoring RSA Keys. We are almost done; we now need to Ô¨Ånd the decryption key d, knowing e, p-1 and q-1. For this we need an implementation of the extended Euclidean algorithm; the following Python implementation is taken from WikiBooks: defegcd(a, b): ifa == 0: return(b, 0, 1) else: g, y, x = egcd(b % a, a) return(g, x - (b // a) *y, y) A call toegcd(a,b) returns a triple (g,x,y) where g is the greatest common divisor of a and b, and x and y are solutions to g = ax + by. From 29.1 RSA, we need d to be positive and to satisfy 1 = de + (p-1)(q-1)y. The x value (the second value) returned by egcd(e, (p-1) *(q-1)) satisÔ¨Åes the second part, but it may be negative in which case we need to add (p-1)(q-1) to get a positive value which is congruent mod (p-1)(q-1). This x value is -3706624839629358151621824719; after adding (p-1)(q-1) we get d=48545702997494592199601992577. This is the value of d we started with. If c is the ciphertext, we now calculate m = pow(c,d,n) as before, yielding m=90612911403892, or, in hex, 52:69:76:65:73:74, ‚ÄúRivest‚Äù. 29.9 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. 1.0 Suppose Alice uses RSA to send messages to three friends, Bob, Charlie and Deborah, who have respective public keys (n B,3), (n C,3) and (n D,3); note that all three friends use the same encryption exponent e=3. Assume n B, nCand n Dare relatively prime (if they are not, Alice‚Äôs friends have a much bigger problem!). Alice sends message m to each, encrypted as 
- C B= m3mod n B 
- C C= m3mod n C 
- C D= m3mod n D If Mallory intercepts all three encrypted messages, explain how he can efÔ¨Åciently decrypt m. Hint: the Chinese Remainder Theorem implies that Mallory can Ô¨Ånd C < n BnCnDsuch that 778 29 Public-Key Encryption
An Introduction to Computer Networks, Release 2.0.11 
- C = C Bmod n B 
- C = C Cmod n C 
- C = C Dmod n D (One simple way to avoid this risk is for Alice to include a timestamp and the recipient‚Äôs name in each message, ensuring that she never sends exactly the same message twice. Another way is to choose a larger exponent e.) 2.0 Repeat the key-creation of 29.8 RSA Key Examples using a 110-bit key. Extract the modulus from the key Ô¨Åle, convert it to decimal, and attempt to factor it. Can you do the factoring in under a minute? 3.0 Below are a series of public RSA keys and encrypted messages; the encrypted message is c and the modulus is n=pq. In each case, Ô¨Ånd the original message, using the methods of 29.8.1 Breaking the key; you will have to factor n and then Ô¨Ånd d. For some keys, the Gnu/Linux factor command will be sufÔ¨Åcient; for the larger keys consider msieve or some other fast factorer. Each number below is in decimal. The encryption exponent e is always 65537; the encryption is c = power(message,e,n). Each message is an ASCII string; that is, after the numeric message is converted to a string, the byte values are each in the range 32-127 (in real use, RSA is never applied directly to messages, but rather to session keys). The following Python function may be useful in converting numeric messages to strings: defint2ascii(n): ifn==0: return"" returnint2ascii(n // 256) + chr(n % 256) (a) [64 bits] c=13467824835325843134 n=15733922878520524621 (b) [96 bits] c=8007751471156136764029275727 n=57644199986835279860947893727 (c) [104 bits] c=6642328489179330732282037747645 n=17058317327334907783258193953123 (d) [127 bits] c=95651949760509273124353927897611145475 n=122096824047754908887766043915630626757 Limit for Gnu/Linux factor without the GMP library (e) [185 bits] c=14898070767615435522751082309577192810119252877170446296 n=36881105206579952723396854897336450502002018818436622611 29.9 Exercises 779
An Introduction to Computer Networks, Release 2.0.11 (f) [210 bits] c=1030865591241775131254813948981782525587720826169501849049177362 n=1089313781487492651628882855744766776820791642308868127824231021 (g) [280 bits] c=961792929180423930042975913300802531765775361218349440433358416557620430721970697783 n=1265365011260907658483984328995361695181000868013851878620685943648084220336740539017 (h) [304 bits] c=17860252858059565448950681133486824440185752167054796213786228492658586864179401029486173539 n=26294146550372428669569992924076340150116542153388301312743129088600749884420889043685502979 780 29 Public-Key Encryption
30 MININET Sometimes simulations are not possible or not practical, and network experiments must be run on actual machines. One can always use a set of interconnected virtual machines, but even pared-down virtual machines consume sufÔ¨Åcient resources that it is hard to create a network of more than a handful of nodes. Mininet is a system that supports the creation of lightweight logical nodes that can be connected into networks. These nodes are sometimes called containers, or, more accurately, network namespaces. Virtual-machine technology is not used. These containers consume sufÔ¨Åciently few resources that networks of over a thousand nodes have been created, running on a single laptop. While Mininet was originally developed as a testbed for software-deÔ¨Åned networking ( 3.4 Software-DeÔ¨Åned Networking ), it works just as well for demonstrations and experiments involving traditional networking. A Mininet container is a process (or group of processes) that no longer has access to all the host system‚Äôs ‚Äúnative‚Äù network interfaces, much as a process that has executed the chroot() system call no longer has access to the full Ô¨Ålesystem. Mininet containers then are assigned virtual Ethernet interfaces (see the ip-link man page entries for veth), which are connected to other containers through virtual Ethernet links. The use of veth links ensures that the virtual links behave like Ethernet, though it may be necessary to disable TSO (17.5 TCP OfÔ¨Çoading ) to view Ethernet packets in WireShark as they would appear on the (virtual) wire. Any process started within a Mininet container inherits the container‚Äôs view of network interfaces. For efÔ¨Åciency, Mininet containers all share the same Ô¨Ålesystem by default. This makes setup simple, but sometimes causes problems with applications that expect individualized conÔ¨Åguration Ô¨Åles in speciÔ¨Åed locations. Mininet containers canbe conÔ¨Ågured so that each container has at least one private directory, eg for conÔ¨Åguration Ô¨Åles. See 30.6 Quagga Routing and BGP for an example, though mostly we avoid this feature. Mininet is a form of network emulation, as opposed to simulation. An important advantage of emulation is that all network software, at any layer, is simply run ‚Äúas is‚Äù. In a simulator environment, on the other hand, applications and protocol implementations need to be ported to run within the simulator before they can be used. A drawback of emulation is that as the network gets large and complex the emulation may slow down. In particular, it is not possible to emulate link speeds faster than the underlying hardware can support. (It is also not possible to emulate non-Linux network software.) The Mininet group maintains extensive documentation; three useful starting places are the Overview, the Introduction and the FAQ. The goal of this chapter is to present a series of Mininet examples. As of 2021, these have been upgraded to Python3, following the upgrade of Mininet itself, but Python2 distributions of Mininet (such as that on the Mininet VM, below) are still widespread. Each Mininet Python Ô¨Åle conÔ¨Ågures the network and then starts up the Mininet command-line interface (which is necessary to start commands on the various node containers). The use of self-contained Python Ô¨Åles arguably makes the conÔ¨Ågurations easier to edit, and avoids the complex command-line arguments of many standard Mininet examples. The Python code uses what the Mininet documentation calls the ‚Äúmid-level‚Äù API. The Mininet distribution comes with its own set of examples, in the directory of that name. A few of particular interest are listed below; with the exception of linuxrouter.py, the examples presented here do not use any of these techniques. 
- bind.py: demonstrates how to give each Mininet node its own private directory (otherwise all nodes 781
An Introduction to Computer Networks, Release 2.0.11 share a common Ô¨Ålesystem) 
- controllers.py: demonstrates how to arrange for multiple SDN controllers, with different switches connecting to different controllers 
- limit.py: demonstrates how to set CPU utilization limits (and link bandwidths) 
- linuxrouter.py: creates a node that acts as a router. Any host node can act as a router, though, provided we enable forwarding with sysctl net.ipv4.ip_forward=1 
- miniedit.py: a graphical editor for Mininet networks 
- mobility.py: demonstrates how to move a host from one switch to another 
- nat.py: demonstrates how to connect hosts to the Internet 
- tree1024.py: creates a network with 1024 nodes We will occasionally need supplemental programs as well, egfor sending, monitoring or receiving trafÔ¨Åc. These are meant to be modiÔ¨Åed as necessary to meet circumstances; they contain few command-line option settings. These supplemental programs are also written in Python3. 30.1 Installing Mininet Mininet runs only under the Linux operating system. Windows and Mac users can, however, easily run Mininet in a single Linux virtual machine. Even Linux users may wish to do this, as running Mininet has a nontrivial potential to affect normal operation (a virtual-switch process started by Mininet has, for example, interfered with the suspend feature on the author‚Äôs laptop). The Mininet group maintains a virtual machine with a current Mininet installation (the ‚ÄúMininet VM option‚Äù) at their downloads site. This Mininet VM option is Option 1 listed there. As of 2021, however, this version still used a 2014 version of Linux, and Python2. The download Ô¨Åle is actually a .zip Ô¨Åle, which unzips to a modest .ovf Ô¨Åle deÔ¨Åning the speciÔ¨Åcations of the virtual machine and a much larger (~2 GB) .vmdk Ô¨Åle representing the virtual disk image. Even with the virtual disk fully utilized, the footprint is still just 4 GB. There are several choices for virtual-machine software; two options that are well supported and free (as of 2017) for personal use are VirtualBox and VMware Workstation Player. Those using the Miniet VM option can open the .ovf Ô¨Åle in either (in VirtualBox with the ‚Äúimport appliance‚Äù option). However, it may be easier simply to create a new Linux virtual machine and specify that it is to use an existing virtual disk; then select the downloaded .vmdk Ô¨Åle as that disk. Both the login name and the password for the Mininet VM option is ‚Äúmininet‚Äù. Once logged in, the sudo command can be used to obtain root privileges, which are needed to run Mininet. It is in principle safest to do this on a command-by-command basis; egsudo python switchline.py. It is also possible to keep a terminal window open that is permanently logged in as root, egviasudo bash. The preinstalled Mininet VM does notcome with any graphical-interface desktop. A lightweight option, recommended by the Mininet site, is to install the alternative desktop environment lxde; it is half the size of Ubuntu. Install lxde with 782 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 apt-get install xinit lxde The standard graphical text editor included with lxde is leafpad, though of course others ( eggedit or emacs) can be installed as well. After desktop installation, the command startx may be necessary after login to start the graphical environment (though one can automate this). 30.1.1 Mininet and Python 3 A more up-to-date approach is to create a full Ubuntu virtual machine (Lubuntu is another, lighter, option), using a downloaded disk image from Ubuntu.com. This is Option 2 at http://mininet.org/download. To install Mininet on this virtual machine, one uses the install.sh script from the Mininet Github repository, which is cloned with git clone https://github.com/mininet/mininet See also the instructions at the downloads site above. This approach, like the Mininet VM option, includes the additional packages most commonly used with Mininet, such as openvswitch and Pox. Note that some system defaults have changed since the 2014 Mininet VM. Explicitly setting the link speed and delay, and queue capacity for the bottleneck link, often improves consistency. A standard recommendation for all new Debian-based Linux systems, before installing anything else, is apt-get update apt-get upgrade Most virtual-machine software offers a special package to improve compatibility with the host system. One of the most annoying incompatibilities is the tendency of the virtual machine to grab the mouse and not allow it to be dragged outside the virtual-machine window. (Usually a special keypress releases the mouse; on VirtualBox it is by default the right-hand Control key and on VMWare Player it is Control-Alt .) Installation of the compatibility package (in VirtualBox called Guest Additions) usually requires mounting a CD image, with the command mount /dev/cdrom /media/cdrom The Mininet installation itself, whether from the Mininet VM or cloned from Github, can be upgraded as follows, assuming /home/mininet/mininet holds the cloned github Ô¨Åles (from http://mininet.org/download): cd /home/mininet/mininet git fetch git checkout master # Or a specific version like 2.2.1 git pull util/install.sh # this script accepts some options The simplest environment for beginners is to install a graphical desktop ( eglxde or full Ubuntu) and then work within it. This allows seamless opening of xterm and WireShark as necessary. Enabling copy/paste between the virtual system and the host is also convenient. 30.1 Installing Mininet 783
An Introduction to Computer Networks, Release 2.0.11 The xterm font size is quite small, but can be changed by clicking the right mouse button while holding down the control key. Pasting from the clipboard is done by clicking the middle mouse button. Text on the screen is copied automatically when it is selected, but it may only be copied to the xterm internal clipboard, not the system clipboard. This can be changed by selecting the ‚ÄúSelect to Clipboard‚Äù option on the menu that should pop up with a control-click of the middle mouse button. There are keyboard alternatives available for those without a three-button mouse. However, it is also possible to work entirely without the desktop, by using multiple ssh logins with Xwindows forwarding enabled: ssh -X -l username mininet This does require an X-server on the host system, but these are available even for Windows (see, for example, Cygwin/X). At this point one can open a graphical program on the ssh command line, egwireshark & or gedit mininet-demo.py &, and have the program window display properly (or close to properly). Finally, it is possible to access the Mininet virtual machine solely via ssh terminal sessions, without Xwindows, though one then cannot launch xterm or WireShark. 30.2 A Simple Mininet Example Starting Mininet via the mncommand (as root!), with no command-line arguments, creates a simple network of two hosts and one switch, h1‚Äìs1‚Äìh2, and starts up the Mininet command-line interface (CLI). By convention, Mininet host names begin with ‚Äòh‚Äô and switch names begin with ‚Äòs‚Äô; numbering begins with 1. At this point one can issue various Mininet-CLI commands. The command nodes, for example, yields the following output: available nodes are: c0 h1 h2 s1 The nodec0is the controller for the switch s1. The default controller action her makes s1behave like an Ethernet learning switch ( 2.4.1 Ethernet Learning Algorithm ). The command intfs lists the interfaces for each of the nodes, and links lists the connections, but the most useful command is net, which shows the nodes, the interfaces and the connections: h1 h1-eth0:s1-eth1 h2 h2-eth0:s1-eth2 s1 lo: s1-eth1:h1-eth0 s1-eth2:h2-eth0 From the above, we can see that the network looks like this: h1 s1 h2h1-eth0s1-eth1s1-eth2h2-eth0 784 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 30.2.1 Running Commands on Nodes The next step is to run commands on individual nodes. To do this, we use the Mininet CLI and preÔ¨Åx the command name with the node name: h1 ifconfig h1 ping h2 The Ô¨Årst command here shows that h1 (or, more properly, h1-eth0) has IP address 10.0.0.1. Note that the name ‚Äòh2‚Äô in the second is recognized. The ifconfig command also shows the MAC address of h1-eth0, which may vary but might be something like 62:91:68:bf:97:a0. We will see in the following section how to get more human-readable MAC addresses. There is a special Mininet command pingall that generates pings between each pair of hosts. We can open a full shell window on node h1using the Mininet command below; this works for both host nodes and switch nodes. xterm h1 Note that the xterm runs with root privileges. From within the xterm, the command ping h2 now fails, as hostnameh2is not recognized. We can switch to ping 10.0.0.2, or else add entries to /etc/hosts for the IP addresses of h1andh2: 10.0.0.1 h1 10.0.0.2 h2 As the Mininet system shares its Ô¨Ålesystem with h1andh2, this means that any /etc/host entries will be deÔ¨Åned everywhere within Mininet (though be forewarned that when a different Mininet conÔ¨Åguration assigns different addresses to h1orh2, chaos will ensue). In the examples below, we sometimes use hostnames such as h1,etc, assuming these /etc/hosts entries have been made. From within the xterm on h1we might try logging into h2via ssh:ssh h2 (if h2 is deÔ¨Åned in /etc/hosts as above). But the connection is refused: the ssh server is not running on node h2. We will return to this in the following example. We can also start up WireShark, and have it listen on interface h1-eth0, and see the progress of our pings. To start WireShark on a host node, say h1, we can either enter h1 wireshark & at themininet> prompt, or else launch WireShark from within an xterm window running on h1. With either approach, all the interfaces of h1 will be visible. These methods also work for starting WireShark on a switch node. However, there is a simpler way, that takes advantage of the fact that, unlike for host interfaces, all switch interfaces are by default visible to the top-level Linux system; in the simple example above these are s1-eth1 ands1-eth2. So we can simply start WireShark at the top level, outside of Mininet (though the interfaces we are interested in won‚Äôt generally exist until after Mininet has started). In terms of the Mininet container model, switches do not by default get their own network namespace; they share the ‚Äúroot‚Äù namespace with the host. We can see this by running the following from the Mininet command line in the example above: 30.2 A Simple Mininet Example 785
An Introduction to Computer Networks, Release 2.0.11 s1 ifconfig and comparing the output with that of ifconfig run on the Mininet host, while Mininet is running but using a terminal notassociated with the Mininet process itself. In the example above we see these interfaces: eth0 lo s1 s1-eth1 s1-eth2 We see the same interfaces on the controller node c0, even though the net andintfs commands above showed no interfaces for c0. 30.2.2 Mininet WireShark Demos We can take advantage of this simple h1‚Äìs1‚Äìh2 conÔ¨Åguration to observe trafÔ¨Åc with WireShark on a nearly idle network; by default, the Mininet nodes are not connected to the outside world. After starting Mininet in one terminal window, we start WireShark in another and set it to listening to, say, s1-eth1. To watch a TCP connection we then could start up xterm windows on h1 and h2. We run netcat -l 5432 on h2 and then netcat 10.0.0.2 5432 on h1. At this point we can see the ARP exchange followed by the TCP three-way handshake. If we type a line of text to netcat on h1, we can watch the data packet and the returning ACK in WireShark. We can drill into the data packet to see the message contents. Typing cntl-D (or cntl-C) to netcat on h1 will result in the closing exchange of FIN packets. Often there is no other trafÔ¨Åc at all, though if we wait long enough we will see repeat ARP packets. Wireshark Ô¨Åltering is not generally needed (though in a busier environment we could Ô¨Ålter all the non-TCP, non-port-5432 trafÔ¨Åc with the WireShark Ô¨Ålter option tcp.port == 5432 ). 30.3 Multiple Switches in a Line The next example creates the topology below. All hosts are on the same subnet. s1h1 s2 s3 s4h2 h3 h4 h1-eth0 h2-eth0 h3-eth0 h4-eth0 s4-eth1 s3-eth1 s2-eth1 s1-eth1 s1-eth2 s2-eth3 s3-eth3 s4-eth2 s3-eth2 s2-eth2 The Mininet-CLI command links can be used to determine which switch interface is connected to which neighboring switch interface. The full Python3 program is switchline.py; to run it use 786 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 python3 switchline.py This conÔ¨Ågures the network and starts the Mininet CLI. The default number of host/switch pairs is 4, but this can be changed with the -Ncommand-line parameter, for example python3 switchline.py -N 5. We next describe selected parts of switchline.py. The program starts by building the network topology object,LineTopo, extending the built-in Mininet class Topo, and then call Topo.addHost() to create the host nodes. (We here override __init()__, but overriding build() is actually more common.) class LineTopo ( Topo ): def__init__( self, **kwargs): "Create linear topology" super(LineTopo, self).__init__( **kwargs) h = [] # list of hosts; h[0] will be h1, etc s = [] # list of switches forkey inkwargs: ifkey =='N': N=kwargs[key] # add N hosts h1..hN foriinrange(1,N+1): h.append(self.addHost( 'h'+ str(i))) MethodTopo.addHost() takes a string, such as ‚Äúh2‚Äù, and builds a host object of that name. We immediately append the new host object to the list h[]. Next we do the same to switches, using Topo. addSwitch(): # add N switches s1..sN foriinrange(1,N+1): s.append(self.addSwitch( 's'+ str(i))) Now we build the links, with Topo.addLink. Note that h[0]..h[N-1] represent h1..hN. First we build the host-switch links, and then the switch-switch links. foriinrange(N): # Add links from hi to si self.addLink(h[i], s[i]) foriinrange(N-1): # link switches self.addLink(s[i],s[i+1]) Now we get to the main program. We use argparse to support the -Ncommand-line argument. defmain(**kwargs): parser = argparse.ArgumentParser() parser.add_argument( '-N','--N', type=int) args = parser.parse_args() ifargs.N is None: N = 4 else: N = args.N 30.3 Multiple Switches in a Line 787
An Introduction to Computer Networks, Release 2.0.11 Next we create a LineTopo object, deÔ¨Åned above. We also set the log-level to ‚Äòinfo‚Äô; if we were having problems we would set it to ‚Äòdebug‚Äô. ltopo = LineTopo(N=N) setLogLevel( 'info') Finally we‚Äôre ready to create the Mininet net object, and start it. We‚Äôve speciÔ¨Åed the type of switch here, though at this point that does not really matter. It does matter that we‚Äôre using the DefaultController, as otherwise the switches will not behave automatically as Ethernet learning switches. The autoSetMacs option sets the host MAC addresses to 00:00:00:00:00:01 through 00:00:00:00:00:04 (for N=4), which can be a great convenience when manually examining Ethernet addresses. net = Mininet(topo = ltopo, switch = OVSKernelSwitch, controller = DefaultController, autoSetMacs = True ) net.start() Finally we start the Mininet CLI, and, when that exits, we stop the emulation. CLI( net) net.stop() 30.3.1 ConÔ¨Åguring SSH In the multiple-switch example above, if we want to run sshd on the Mininet nodes, the fragment below starts the daemon /usr/sbin/sshd on each node. This command automatically puts itself in the background; otherwise we would need to add an ‚Äò&‚Äô to the string to run the command in the background. foriinrange(1, N+1): hi = net[ 'h'+ str(i)] hi.cmd('/usr/sbin/sshd' ) Usingsshd requires a small bit of conÔ¨Åguration, if ssh for the root user has not been set up already. We must Ô¨Årst, as root, run ssh-keygen, or, better yet, ssh-keygen -t ed25519 which creates the directory/root/.ssh and then the public and private key Ô¨Åles. For RSA keys (the default), these Ô¨Åles areid_rsa.pub andid_rsa respectively; with the ed25519 option, they are id_ed25519.pub and id_ed25519. An advantage to the elliptic-curve keys (ed25519) is that the public key is much shorter, and typically Ô¨Åts on a single line in a terminal window. Unless the virtual machine is being used for other connections, there should be no need to protect the keys with a password. To enable passwordless login, the second step is to go to the .ssh directory and copy id_rsa.pub (orid_ed25519.pub ) to the (new) Ô¨Åle authorized_keys (if the latter Ô¨Åle already exists, append id_rsa.pub to it). The ssh command can be Ô¨Ånicky. The .ssh directory must not allow any access to other than the owner; that is, the permissions must be rwx------. Theauthorized_keys Ô¨Åle must not be writeable except by the owner. Finally, some Linux distributions may ‚Äúlock‚Äù the root account; it can be unlocked by giving the root account a password, or by editing the Ô¨Åle /etc/shadow and changing the hashed-password Ô¨Åeld from ‚Äò!‚Äô to ‚Äò*‚Äô. 788 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 Because we started sshd on each host, the command ssh 10.0.0.4 on h1 should successfully connect to h4. The Ô¨Årst time a connection is made from h1 to h4 (as root), ssh will ask for conÔ¨Årmation, and then store h4‚Äôs key in /root/.ssh/known_hosts. As this is the same Ô¨Åle for all Mininet nodes, due to the common Ô¨Ålesystem, a subsequent request to connect from h 2to h4 will succeed immediately; h4 has already been authenticated for all nodes. Finally, we note that to have ssh return immediately after starting a persistent process on a remote host, we need something like this, where foo.sh is the command in question: ssh hostname 'nohup foo.sh >/dev/null 2>&1 &' 30.3.2 Running a webserver Now let‚Äôs run a web server on, say, host 10.0.0.4 of the switchline.py example above. Python includes a simple implementation that serves up the Ô¨Åles in the directory in which it is started. After switchline.py is running, start an xterm on host h4, and then change directory to /usr/share/doc (where there are some html Ô¨Åles). Then run the following command (the 8000 is the server port number): python -m SimpleHTTPServer 8000 If this is run in the background somewhere, output should be redirected to /dev/null or else the server will eventually hang. The next step is to start a browser. If a full desktop environment has been installed ( eglxde, 30.1 Installing Mininet ), then a full browser should be available (lxde comes with chromium-browser ). Start an xterm on host h1, and on h1 run the following (the --no-sandbox option is necessary to run chromium as root): chromium-browser --no-sandbox Assuming chromium opens successfully, enter the following URL: 10.0.0.4:8000. If chromium does not start, try wget 10.0.0.4:8000, which stores what it receives as the Ô¨Åle index.html. Either way, you should see a listing of the /usr/share/doc directory. It is possible to browse subdirectories, but only browser-recognized Ô¨Åletypes ( eg.html ) will open directly. A few directories with subdirectories namedhtml areiperf ,iptables andxarchiver; try navigating to these. 30.4 IP Routers in a Line In the next example we build a Mininet example involving a router rather than a switch. A router here is simply a multi-interface Mininet host that has IP forwarding enabled in its Linux kernel. Mininet support for multi-interface hosts is somewhat fragile; interfaces may need to be initialized in a speciÔ¨Åc order, and IP addresses often cannot be assigned at the point when the link is created. In the code presented below we assign IP addresses using calls to Node.cmd() used to invoke the Linux command ifconfig (Mininet containers do not fully support the use of the alternative ip addr command). Our Ô¨Årst router topology has only two hosts, one at each end, and N routers in between; below is the diagram with N=3. All subnets are /24. The program to set this up is routerline.py, here invoked as python routerline.py -N 3. We will use N=3 in most of the examples below. A somewhat simpler version of the program, which sets up the topology speciÔ¨Åcally for N=3, is routerline3.py. 30.4 IP Routers in a Line 789
An Introduction to Computer Networks, Release 2.0.11 r1 r2 r3 h210.0.0.10h110.0.0.2 10.0.1.1 10.0.1.2 10.0.2.1 10.0.2.2 10.0.3.1 10.0.3.10 In both versions of the program, routing entries are created to route trafÔ¨Åc from h1 to h2, but not back again. That is, every router has a route to 10.0.3.0/24, but only r1 knows how to reach 10.0.0.0/24 (to which r1 is directly connected). We can verify the ‚Äúone-way‚Äù connectedness by running WireShark ortcpdump on h2 (perhaps Ô¨Årst starting an xterm on h2), and then running ping 10.0.3.10 on h1 (perhaps using the Mininet command h1 ping h2 ).WireShark ortcpdump should show the arriving ICMP ping packets from h1, and also the arriving ICMP Destination Network Unreachable packets from r3ash2tries to reply (see 10.4 Internet Control Message Protocol ). It turns out that one-way routing is considered to be suspicious; one interpretation is that the packets involved have a source address that shouldn‚Äôt be possible, perhaps spoofed. Linux provides the interface conÔ¨Åguration optionrp_filter ‚Äì reverse-path Ô¨Ålter ‚Äì to block the forwarding of packets for which the router does not have a route back to the packet‚Äôs source. This must be disabled for the one-way example to work; see the notes on the code below, and also 13.6.1 Reverse-Path Filtering. Despite the lack of connectivity, we can reach h2 from h1 via a hop-by-hop sequence of ssh connections (the program enables sshd on each host and router): h1: slogin 10.0.0.2 r1: slogin 10.0.1.2 r2: slogin 10.0.2.2 r3: slogin 10.0.3.10 (that is, h3) To get the one-way routing to work from h1 to h2, we needed to tell r1 and r2 how to reach destination 10.0.3.0/24. This can be done with the following commands (which are executed automatically if we set ENABLE_LEFT_TO_RIGHT_ROUTING = True in the program): r1: ip route add to 10.0.3.0/24 via 10.0.1.2 r2: ip route add to 10.0.3.0/24 via 10.0.2.2 To get full, bidirectional connectivity, we can create the following routes to 10.0.0.0/24: r2: ip route add to 10.0.0.0/24 via 10.0.1.1 r3: ip route add to 10.0.0.0/24 via 10.0.2.1 When building the network topology, the single-interface hosts can have all their attributes set at once (the code below is from routerline3.py: h1 = self.addHost( 'h1', ip='10.0.0.10/24', defaultRoute= 'via 10.0.0.2' ) h2 = self.addHost( 'h2', ip='10.0.3.10/24', defaultRoute= 'via 10.0.3.1' ) The routers are also created with addHost(), but with separate steps: r1 = self.addHost( 'r1') r2 = self.addHost( 'r2') ... (continues on next page) 790 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) self.addLink( h1, r1, intfName1 = 'h1-eth0', intfName2 = 'r1-eth0' ) self.addLink( r1, r2, inftname1 = 'r1-eth1', inftname2 = 'r2-eth0' ) Later on the routers get their IPv4 addresses: r1 = net[ 'r1'] r1.cmd('ifconfig r1-eth0 10.0.0.2/24' ) r1.cmd('ifconfig r1-eth1 10.0.1.1/24' ) r1.cmd('sysctl net.ipv4.ip_forward=1' ) rp_disable(r1) Thesysctl command here enables forwarding in r1. The rp_disable(r1) call disables Linux‚Äôs default refusal to forward packets if the router does not have a route back to the packet‚Äôs source; this is often what is wanted in the real world but notnecessarily in routing demonstrations. It too is ultimately implemented via sysctl commands. 30.5 IP Routers With Simple Distance-Vector Implementation The next step is to automate the discovery of the route from h1 to h2 (and back) by using a simple distancevector routing-update protocol. We present a partial implementation of the Routing Information Protocol, RIP, as deÔ¨Åned in RFC 2453. Alternatively, one can use the (full) RIP implementation that is part of Quagga, 30.6 Quagga Routing and BGP. The distance-vector algorithm is described in 13.1 Distance-Vector Routing-Update Algorithm. In brief, the idea is to add a cost attribute to the forwarding table, so entries have the form xdestination,next_hop,cost y. Routers then send xdestination,cost ylists to their neighbors; these lists are referred to the RIP speciÔ¨Åcation asupdate messages. Routers receiving these messages then process them to Ô¨Ågure out the lowest-cost route to each destination. The format of the update messages is diagrammed below: Addr Family route_tag IP Address Netmask Next_hop Address metric The full RIP speciÔ¨Åcation also includes request messages, but the implementation here omits these. The full speciÔ¨Åcation also includes split horizon, poison reverse and triggered updates ( 13.2.1.1 Split Horizon and 13.2.1.2 Triggered Updates ); we omit these as well. Finally, while we include code for the third next_hop increase case of 13.1.1 Distance-Vector Update Rules, we do not include any test for whether a link is down, so this case is never triggered. The implementation is in the Python3 Ô¨Åle rip.py. Most of the time, the program is waiting to read update messages from other routers. Every UPDATE_INTERVAL seconds the program sends out its own update 30.5 IP Routers With Simple Distance-Vector Implementation 791
An Introduction to Computer Networks, Release 2.0.11 messages. All communication is via UDP packets sent using IP multicast, to the ofÔ¨Åcial RIP multicast address 224.0.0.9. Port 520 is used for both sending and receiving. Rather than creating separate threads for receiving and sending, we conÔ¨Ågure a short (1 second) recv() timeout, and then after each timeout we check whether it is time to send the next update. An update can be up to 1 second late with this approach, but this does not matter. The program maintains a ‚Äúshadow‚Äù copy RTable of the real system forwarding table, with an added cost column. The real table is updated whenever a route in the shadow table changes. In the program, RTable is a dictionary mapping TableKey values (consisting of the IP address and mask) to TableValue objects containing the interface name, the cost, and the next_hop. To run the program, a ‚Äúproduction‚Äù approach would be to use Mininet‚Äôs Node.cmd() to start up rip.py on each router, egviar.cmd('python3 rip.py &') (assuming the Ô¨Åle rip.py is located in the same directory in which Mininet was started). For demonstrations, the program output can be observed if the program is started in an xterm on each router. 30.5.1 Multicast Programming Sending IP multicast involves special considerations that do not arise with TCP or UDP connections. The Ô¨Årst issue is that we are sending to a multicast group ‚Äì 224.0.0.9 ‚Äì but don‚Äôt have any multicast routes (multicast trees, 25.5 Global IP Multicast ) conÔ¨Ågured. What we would likeis to have, at each router, trafÔ¨Åc to 224.0.0.9 forwarded to each of its neighboring routers. However, we do not actually want to conÔ¨Ågure multicast routes; all we want is to reach the immediate neighbors. Setting up a multicast tree presumes we know something about the network topology, and, at the point where RIP comes into play, we do not. The multicast packets we send should in fact notbe forwarded by the neighbors (we will enforce this below by setting TTL); the multicast model here is very local. Even if we did want to conÔ¨Ågure multicast routes, Linux does not provide a standard utility for manual multicast-routing conÔ¨Åguration; see the ip-mroute.8 man page. So what we do instead is to create a socket for each separate router interface, and conÔ¨Ågure the socket so that it forwards its trafÔ¨Åc only out its associated interface. This introduces a complication: we need to get the list of all interfaces, and then, for each interface, get its associated IPv4 addresses with netmasks. (To simplify life a little, we will assume that each interface has only a single IPv4 address.) The function getifaddrdict() returns a dictionary with interface names (strings) as keys and pairs (ipaddr,netmask) as values. If ifaddrs is this dictionary, for example, then ifaddrs['r1-eth0'] might be('10. 0.0.2','255.255.255.0'). We could implement getifaddrdict() straightforwardly using the Python module netifaces, though for demonstration purposes we do it here via low-level system calls. We get the list of interfaces using myInterfaces = os.listdir('/sys/class/net/'). For each interface, we then get its IP address and netmask (in get_ip_info(intf) ) with the following: s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) SIOCGIFADDR = 0x8915 # from /usr/include/linux/sockios.h SIOCGIFNETMASK = 0x891b intfpack = struct.pack( '256s', bytes(intf, 'ascii')) # ifreq, below, is like struct ifreq in /usr/include/linux/if.h ifreq = fcntl.ioctl(s.fileno(), SIOCGIFADDR, intfpack) ipaddrn = ifreq[20:24] # 20 is the offset of the IP addr in ifreq (continues on next page) 792 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) ipaddr = socket.inet_ntoa(ipaddrn) netmaskn = fcntl.ioctl(s.fileno(), SIOCGIFNETMASK, intfpack)[20:24] netmask = socket.inet_ntoa(netmaskn) return(ipaddr, netmask) We need to create the socket here (never connected) in order to call ioctl(). TheSIOCGIFADDR and SIOCGIFNETMASK values come from the C language include Ô¨Åle; the Python3 libraries do not make these constants available but the Python3 fcntl.ioctl() call does pass the values we provide directly to the underlying C ioctl() call. This call returns its result in a C struct ifreq; the ifreq above is a Python version of this. The binary-format IPv4 address (or netmask) is at offset 20. 30.5.1.1 createMcastSockets() We are now in a position, for each interface, to create a UDP socket to be used to send and receive on that interface. Much of the information here comes from the Linux socket.7 andip.7 man pages. The functioncreateMcastSockets(ifaddrs) takes the dictionary above mapping interface names to (ipaddr,netmask) pairs and, for each interface intf, conÔ¨Ågures it as follows. The list of all the newly conÔ¨Ågured sockets is then returned. The Ô¨Årst step is to obtain the interface‚Äôs address and mask, and then convert these to 32-bit integer format as ipaddrn andnetmaskn. We then enter the subnet corresponding to the interface into the shadow routing tableRTable with a cost of 1 (and with a next_hop of None ), via RTable[TableKey(subnetn, netmaskn)] = TableValue(intf, None, 1) Next we create the socket and begin conÔ¨Åguring it, Ô¨Årst by setting its read timeout to a short value. We then set the TTL value used by outbound packets to 1. This goes in the IPv4 header Time To Live Ô¨Åeld ( 9.1 The IPv4 Header ); this means that no downstream routers will ever forward the packet. This is exactly what we want; RIP uses multicast only to send to immediate neighbors. sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 1) We also want to be able to bind the same socket source address, 224.0.0.9 and port 520, to all the sockets we are creating here (the actual bind() call is below): sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) The next call makes the socket receive only packets arriving on the speciÔ¨Åed interface: sock.setsockopt(socket.SOL_SOCKET, socket.SO_BINDTODEVICE, bytes(intf, 'ascii √£√ë')) We add the following to prevent packets sent on the interface from being delivered back to the sender; otherwise multicast delivery may do just that: sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, False) The next call makes the socket send on the speciÔ¨Åed interface. Multicast packets do have IPv4 destination addresses, and normally the kernel chooses the sending interface based on the IP forwarding table. This call 30.5 IP Routers With Simple Distance-Vector Implementation 793
An Introduction to Computer Networks, Release 2.0.11 overrides that, in effect telling the kernel how to route packets sent via this socket. (The kernel may also be able to Ô¨Ågure out how to route the packet from the subsequent call joining the socket to the multicast group.) sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_IF, socket.inet_ √£√ëaton(ipaddr)) Finally we can join the socket to the multicast group represented by 224.0.0.9. We also need the interface‚Äôs IP address, ipaddr. addrpair = socket.inet_aton( '224.0.0.9' )+ socket.inet_aton(ipaddr) sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, addrpair) The last step is to bind the socket to the desired address and port, with sock.bind(('224.0.0.9', 520)). This speciÔ¨Åes the source address of outbound packets; it would fail (given that we are using the same socket address for multiple interfaces) without the SO_REUSEADDR conÔ¨Åguration above. 30.5.2 The RIP Main Loop The rest of the implementation is relatively nontechnical. One nicety is the use of select() to wait for arriving packets on any of the sockets created by createMcastSockets() above; the alternatives might be to poll each socket in turn with a short read timeout or else to create a separate thread for each socket. Theselect() call takes the list of sockets (and a timeout value) and returns a sublist consisting of those sockets that have data ready to read. Almost always, this will be just one of the sockets. We then read the data with s.recvfrom(), recording the source address src which will be used when we, next, call update_tables(). When a socket closes, it must be removed from the select() list, but the sockets here do not close; for more on this, see 30.7.3.2 dualreceive.py. Theupdate_tables() function takes the incoming message (parsed into a list of RipEntry objects viaparse_msg() ) and the IP address from which it arrives, and runs the distance-vector algorithm of 13.1.1 Distance-Vector Update Rules .TKis theTableKey object representing the new destination (as an (addr,netmask) pair). The new destination rule from 13.1.1 Distance-Vector Update Rules is applied when TKis not present in the existing RTable. The lower cost rule is applied when newcost < currentcost, and the third next_hop increase rule is applied when newcost > currentcost but currentnexthop == update_sender. 30.6 Quagga Routing and BGP The Quagga package is a fully functional router package that supports RIP, OSPF, and BGP. We give a simple example here involving BGP. Quagga is a replacement for an earlier package known as ‚Äúzebra‚Äù (the Quagga package is named for a now-extinct subspecies of the zebra). Our goal is to create a network of routers and have them learn the global routing map using BGP. We will give an example here using a linear series of routers; here is the layout for N=3 routers: 794 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 r1 r2r1-eth2 r2-eth110.0.1.1 10.0.1.2 h1h0r1-eth1 r1-eth0 10.0.10.1 h1-eth0 10.0.10.1010.0.0.2r3 h4 h2 r2-eth0 10.0.20.1 h1-eth0 10.0.20.10 h3 r3-eth0 10.0.30.1 h3-eth0 10.0.30.10r2-eth2 r3-eth110.0.2.1 10.0.2.2 10.0.0.1 h0-eth0 r3-eth2 BGP routers r1, r2, r3 and hosts To do this we set up the appropriate conÔ¨Åguration Ô¨Åles for the Quagga daemons (in our case, zebra and bgpd), and then run those daemons on each BGP node. The main Mininet Python3 Ô¨Åle is bgpanycast.py, with some helper functions in bgphelpers.py. Our setup requires two conÔ¨Åguration Ô¨Åles for each host: zebra.conf for general information and bgpd. conf for (mostly) BGP-speciÔ¨Åc information. Although Mininet does support having each node have its own private copy of one directory (for the conÔ¨Åguration Ô¨Åles), we take the approach here of supplying the conÔ¨ÅgÔ¨Åle names on the command line of each quagga daemon; this way, each node gets its own conÔ¨Åguration Ô¨Åle in a unique location. Because these conÔ¨Åguration Ô¨Åles, particularly bgpd.conf, are tedious and error-prone to create, so we introduce Python code to generate them automatically. These automatically generated Ô¨Åles can be preserved, hand-edited and reused, if desired, by appropriately adjusting the code. The existing code deletes the Ô¨Åles on exit. Hand-editing will be necessary if, for example, BGP routing and Ô¨Åltering policies ( 15.4 BGP Filtering and Routing Policies ) are to be applied. Each conÔ¨Åguration Ô¨Åle is created in a subdirectory named for the router in question, preÔ¨Åxed by the path to the directory in which Mininet is started. The directories are created if they do not already exist, and are set to allow wrting by anyone (as the log Ô¨Åles created need to be writable by processes without root privileges). The speciÔ¨Åcation of the appropriate log Ô¨Åle will be via an absolute path name ending in the appropriate directory. Creating the conÔ¨Åguration Ô¨Åles will involve extracting a little more topology information from the Mininet network than we have done previously. Given a Mininet node object representing a router, say r, we can Ô¨Ånd its interfaces in Python with r.intfList(). Given an interface intf, we can Ô¨Ånd the node it is attached to with intf.node, and the link to which it connects with intf.link. Finally, given a link lnk we can Ô¨Ånd the two interfaces it connects with lnk.intf1 andlnk.intf2. This is sufÔ¨Åcient for our needs here. The Ô¨Årst conÔ¨Åguration Ô¨Åle is for quagga itself, called zebra.conf. The zebra.conf Ô¨Åle needs to know the router‚Äôs name, some passwords, the log Ô¨Åle, and the interfaces that participate in the routing. Here, for example, is the zebra.conf Ô¨Åle for router r2 in the Ô¨Ågure below; conÔ¨Åguration-speciÔ¨Åc parameters are in bold: ! zebra conÔ¨Åguration Ô¨Åle ! This Ô¨Åle is automatically generated hostname r2 30.6 Quagga Routing and BGP 795
An Introduction to Computer Networks, Release 2.0.11 password zpassword enable password epassword log Ô¨Åle /home/mininet/loyola/bgp/r2/zebra.log interface r2-eth1 multicast interface r2-eth2 multicast In creating the zebra.conf Ô¨Åle, the most interesting step is the listing of the interfaces that connect to other BGP routers. To do this, we Ô¨Årst create a a dictionary of neighbors, ndict. For a BGP router r, with interfacer-eth (both as string names, not as Mininet objects), the value of ndict[ (r,r-eth) ] is the BGP router (again, as a string) directly connected to rvia the link connected to the r-eth interface. If the neighbor of rconnected via r-eth is an ordinary host or non-BGP router, no entry in ndict is created. To build ndict, we start with a list of all designated BGP routers ( BGPnodelist in the code), as Mininet objects. Given a node n in this list, we Ô¨Årst Ô¨Ånd all of n‚Äôs interfaces ( n.intfList() ). For each interface we Ô¨Ånd the link it connects to ( intf.link ), and then the two endpoint nodes of that link (link.intf1.node andlink.intf2.node ). One of these endpoint nodes is r, of course, and the other is the neighbor node we are looking for. The second conÔ¨Åguration Ô¨Åle is bgpd.conf. This is where all the BGP route preferences can be expressed, though for this demonstration we include none of that. Here is the Ô¨Åle for, again, r2: ! bgpd conÔ¨Åguration Ô¨Åle for router r2 ! This Ô¨Åle is automatically generated hostname r2 password zpassword enable password epassword log Ô¨Åle /home/mininet/loyola/bgp/r2/bgpd.log router bgp 2000 bgp router-id 10.0.20.1 ! These are the networks we announce; conÔ¨Ågured here to be ALL directly connected networks network 10.0.20.0/24 796 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 network 10.0.1.0/24 network 10.0.2.0/24 !These are the neighbors with which we estabish BGP sessions: neighbor 10.0.1.1 remote-as 1000 neighbor 10.0.2.2 remote-as 3000 There is quite a bit more information we must extract about the network topology. The Ô¨Årst item is the Autonomous System (AS) number assigned to the router, 2000 above. These are arbitrary; we create ASdict as a map of BGP-router string names to numbers. For simplicity, the AS number of the nth router is set to 1000*n. The next item is the BGP ‚Äúrouter-id‚Äù value. This is chosen as the largest (in alphabetical sorted order) of the router‚Äôs IPv4 addresses. We will have each router announce every directly connected network. To do this we create addrdict, a dictionary mapping (router,interface) pairs to the IPv4 address (and preÔ¨Åx length) assigned to that interface. The dictionary includes only routers that are BGP routers, but (unlike ifdict above) includes every interface. As with ndict above, for simplicity all entries in this dictionary are strings rather than Mininet objects. Finding the IPv4 address assigned to the Mininet interfaces involves a slight complication, as we have had to set these previously with explicit calls to ifconfig ,egwith r.cmd('ifconfig r-eth1 10.0.10.1/24' ) We need to query the relevant node and interface and Ô¨Ånd the IPv4 address attached to it. To do this, we take advantage of the fact that the Mininet Node.cmd() method returns, as a Python string, the output of the command that was run. The function ipv4addr(node, intf) runs the command ip addr list dev {intf} on the node; this command is widely used to view interface IP addresses. The command output is stored in string s; this string is then parsed to Ô¨Ånd the IPv4 address assignment. This technique allows for any Mininet node properties to be queried by the Mininet run() command. Creation of the network lines in the conÔ¨Åguration Ô¨Åle, one per interface, is now straightforward. The Ô¨Ånal step is the neighbor lines. Here we need to Ô¨Ågure out the neighbor BGP routers, and, for each, Ô¨Ånd its AS-number and an IP address that we can use to connect to it from the original router. There is an issue here: because BGP isthe routing algorithm, we cannot rely on any pre-existing routing information to route from one BGP router to its neighbors. In practice, this means that the BGP neighbors must be directly connected; in the real world, the route between two BGP speakers is typically known to both sides‚Äô interior routing protocol. For our example here, given one router, say r1, and BGP neighbor r2, we must Ô¨Ånd the IP address of r2that is assigned to the r2-end of the link connecting r1andr2. This is straightforward, though, with the two dictionaries above ndict andaddrdict. At this point we can Ô¨Åre up Mininet and, after a short while, conÔ¨Årm that we can ping from h1 to h3, etc, conÔ¨Årming that routing is working. 30.6 Quagga Routing and BGP 797
An Introduction to Computer Networks, Release 2.0.11 30.6.1 The Anycast Part We now try something more speciÔ¨Åc to BGP: anycast routing ( 15.8 BGP and Anycast ). We give host h4 the same IPv4 address as h0, namely 10.0.0.1 (the righthand interface of the rightmost BGP router must also have an IPv4 address on the same subnet, eg10.0.0.2). BGP starts up without errors, and we can now ping 10.0.0.1 from any of h1-hN. But it is not always the same 10.0.0.1. Setting N=5 in the bgpanycast.py Ô¨Åle, we can use traceroute to see the path to 10.0.0.1. If we run ping 10.0.0.1 fromh3, we get the following: traceroute to 10.0.0.1 (10.0.0.1), 30 hops max, 60 byte packets 1 10.0.30.1 (10.0.30.1) 0.095 ms 0.023 ms 0.020 ms ;; 10.0.30.1 √£√ëisr3 2 10.0.2.1 (10.0.2.1) 0.044 ms 0.031 ms 0.027 ms ;; 10.0.2.1 √£√ëisr2 3 10.0.1.1 (10.0.1.1) 0.049 ms 0.038 ms 0.036 ms ;; 10.0.1.1 √£√ëisr1 4 10.0.0.1 (10.0.0.1) 0.058 ms 0.046 ms 0.047 ms If we try from h4, however, we (probably) go the other way (the exact dividing point may vary between runs): traceroute to 10.0.0.1 (10.0.0.1), 30 hops max, 60 byte packets 1 10.0.40.1 (10.0.40.1) 0.084 ms 0.024 ms 0.019 ms ;; 10.0.40.1 √£√ëisr4 2 10.0.4.2 (10.0.4.2) 0.043 ms 0.030 ms 0.079 ms ;; 10.0.4.2 √£√ëisr5 3 10.0.0.1 (10.0.0.1) 0.060 ms 0.040 ms 0.036 ms There is no conÔ¨Çict due to h0 and the rightmost h having the same IPv4 address. The entire network has been partitioned, by BGP, into a portion that believes 10.0.0.1 is to the left, and a disjoint portion that believes 10.0.0.1 is to the right. We can attach servers to each of the 10.0.0.1 and have everyone else reach this destination via whichever route is ‚Äúbetter‚Äù. 30.6.2 Logging in to the routers We can also adjust the conÔ¨Ågurations on the Ô¨Çy, by using the routers‚Äô command-line interface. This is what the two passwords in the conÔ¨Åguration Ô¨Åles are for; the Ô¨Årst password grants read-only access, and the second allows changes. The Quagga command-line interface is quite similar to that used on Cisco routers (and switches), and offers an excellent opportunity to gain familiarity with the Cisco command-line interface for those without ready access to Cisco hardware. The command-line interface has four privilege levels: 
- user level; typical prompt is r2> 
- privileged command level; typical prompt is r2# 
- global conÔ¨Åguration level; typical prompt is r2(conÔ¨Åg)# 
- speciÔ¨Åc conÔ¨Åguration level; typical prompt might be r2(conÔ¨Åg-router) 798 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 To log in to a BGP router, open an xterm window on that node and use the command telnet localhost bgpd (the ‚Äúbgpd‚Äù here represents the TCP port, 2605; the numeric form can also be used). The password requested is ‚Äúzpassword‚Äù, the Ô¨Årst of the two conÔ¨Åguration-Ô¨Åle passwords. At this point, you are at the lowest privilege level. Typically, all available commands are read-only; a list of commands can be obtained from the list command. One useful command is show bgp summary. To get to the next privilege level, type enable, which will ask for the ‚Äúenable‚Äù password as speciÔ¨Åed in the conÔ¨Åguration Ô¨Åle. The example here uses ‚Äúepassword‚Äù. At this level, one command to try is show running-config, which should display bgpd.conf. The next two privilege levels have no separate passwords. Entering configure terminal enters the conÔ¨Åguration level. The Ô¨Ånal step is to specify just what is to be conÔ¨Ågured, egwithrouter bgp 2000 (where 2000 is the AS number). At this Ô¨Ånal privilege level, commands ‚Äì including conÔ¨Åguration-Ô¨Åle commands ‚Äì can be entered. For example, we can advertise a new network with the following, just as if it had been entered in bgpd.conf: network 10.0.50.0/24 If the router running this command does not have a direct connection to the speciÔ¨Åed network, this amounts toBGP hijacking (sidebar at 15.4 BGP Filtering and Routing Policies ). If we set the number of BGP routers to be N=5, for example, set up a connection from h3toh5, and then run the above on r2, the connection will be broken. Router r2has in effect told r3thatitis the way to reach the destination, but once the trafÔ¨Åc arrives at r2it is simply undeliverable. The traceroute command on h3can be used to verify that trafÔ¨Åc from h3to 10.0.50.10 now goes to r2, where it dies. Proper trafÔ¨Åc Ô¨Çow can be restored with the command no network 10.0.50.0/24 onr2. 30.7 TCP Competition The next routing example uses the following topology in order to emulate competition between two TCP connections h1 √ëh3 and h2√ëh3. We introduce Mininet features to set, on the links, an emulated bandwidth and delay, and to set on the router an emulated queue size. Our Ô¨Årst application will be to arrange a competition between TCP Reno ( 19 TCP Reno and Congestion Management ) and TCP Vegas ( 22.6 TCP Vegas ). The Python3 Ô¨Åle for running this Mininet conÔ¨Åguration is competition.py. h1 h2r h38 MBit, 110 ms80 MBit 80 MBit We want the r‚Äìh3 link to be the bottleneck, making this an instance of the singlebell topology ( 20.2.3 Example 3: competition and queue utilization ). To do this, we need to be able to control the bandwidth, delay and queue size on the r‚Äìh3 link. (While this topology is fairly standard for TCP competitions, we really 30.7 TCP Competition 799
An Introduction to Computer Networks, Release 2.0.11 only need the separate sender h2if theh1--r andh2--r links have different propagation delays, or differ in some other attribute. Otherwise we can run both TCPs from h1toh3, which simpliÔ¨Åes some of the conÔ¨Åguration discussed below.) The bottleneck bandwidth shown of 1 KB/ms is not particularly high, but can be adjusted. The bandwidthdelay product in the network shown is thus around 220 KB. We might be interested in TCP competition to determine which TCP is ‚Äúbest‚Äù. Alternatively, we might be interested in order to verify that a new TCP is ‚ÄúReno-fair‚Äù, in the sense of 21.3 TCP Friendliness. Or we might be interested in TCP competition in the context of the TCP high-bandwidth problem ( 21.6 The High-Bandwidth TCP Problem ); the goal might be to show that a proposed new TCP competes fairly with respect to Reno at lower bandwidths, or that it performs well ‚Äì if ‚Äúunfairly‚Äù with respect to Reno ‚Äì at higher bandwidths. See 22.2 High-Bandwidth Desiderata. Finally, we might be interested in comparing two TCP connections with different RTTs, or different starting times. We might also be interested in comparing two TCPs in the context of a speciÔ¨Åc trafÔ¨Åc environment, or packet-loss environment. The experiments below involve two senders and a small amount of randomizing trafÔ¨Åc ( 30.7.3.3 udprandomtelnet.py ), but we might also be interested in competition in an environment in which a large number of pre-existing connections shares the link. Measuring relative TCP performance is fraught with potential misinterpretation. Perhaps the most important question is how long the competition is to run. While there is a place for short-term competitions, long-term performance is usually the more interesting case. TCP Reno, for example, adjusts its cwnd through a sequence of ‚Äúteeth‚Äù ( 19.1.1 The Somewhat-Steady State ), so any long-term competition should run for, at a minimum, many teeth. But how many is enough? A general answer is that the competition should be long enough to yield reasonably consistent results; sometimes that is much longer than naively expected. The Reno tooth size can be estimated from the bottleneck queue capacity plus the transit capacity (or bandwidthdelay product); nominally, if those two quantities sum to M, then cwnd will range between M/2 and M, and the length of a tooth will be M/2 RTTs (see 19.7 TCP and Bottleneck Link Utilization and 21.2 TCP Reno loss rate versus cwnd ). If M=1200 and RTT = 100ms, then M/2 RTTs is 60 seconds. This in turn raises the question of what value we should use for the capacity of the bottleneck-link queue. The classic answer for TCP Reno is to have the bottleneck queue capacity at least as large as the transit capacity, as this ensures that, in the absence of competition, a single connection receives 100% of the bottleneck bandwidth ( 19.7 TCP and Bottleneck Link Utilization ). But having only one connection through a nonleaf link is an unusual situation, and in any event large queues tend to aggravate the bufferbloat problem (21.5.1 Bufferbloat ). The queue capacity will be particularly important when comparing Reno and Vegas (22.6.1 TCP Vegas versus TCP Reno ), as theory predicts that TCP Vegas should do best with small queues. All these variables should be kept at least somewhat in mind in the sequel, which primarily addresses the mechanics of setting up and measuring TCP competitions. 30.7.1 Emulating Bandwidth, Delay and Queue Consider the following diagram, in which we want R to have a FIFO queue (Ô¨Årst-in, Ô¨Årst out; that is, a traditional queue) with maximum capacity of 100, and the R‚ÄìB link to have a bandwidth of 1 byte/microsecond (8 Mbps), and to introduce a one-way R->B delay of 100 ms. 800 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 A S B1 Byte/¬µsec 100 ms delay queue size 100 packets Conceptually, here is what happens to packets arriving at R from A: 
- They are dropped if R‚Äôs FIFO queue is full 
- Packets that are not dropped wait in R‚Äôs FIFO queue for R to send the earlier packets at the 1 byte/¬µsec rate 
- Once packets are sent by R, they wait the 100 ms propagation delay before arriving at B, where they are immediately accepted for processing. To emulate this successfully, we need to apply these three limits ‚Äì FIFO queue, bandwidth and delay. The order here matters; generally, the FIFO queue must immediately precede the bandwidth throttler. Each of these is implemented as an appropriate Linux queuing discipline in the sense of 23.4 Queuing Disciplines. These queuing disciplines would generally all be attached to R‚Äôs interface that faces B, not B‚Äôs interface facing R. Formally, an emulation mechanism is equivalent to the physical conÔ¨Åguration above if for any arbitrary sequence of packets xPi|i<Ny, where P ihas size S iand is sent by A at time T i, we get the same packet arrival times at B, and the same packet losses, as we would in the physical conÔ¨Åguration. While we will avoid here formal equivalence proofs, the idea is that equivalence will occur if the same rules are applied in the same order. Linux has long had queuing disciplines for implementing bandwidth throttling; see, for example, HTB (24.11 Linux HTB ). HTB supports multiple trafÔ¨Åc classes; there is also the simpler Token Bucket Filter, TBF, that applies bandwidth throttling to all trafÔ¨Åc uniformly. Linux also has the NetEm (Network Emulator) queuing discipline, which supports delays and traditional FIFO queues, and which also offers its own mechanism for bandwidth throttling. (NetEm also supports several random-loss models, packet duplication, corruption reordering and variable delay (jitter). We will not use these features here.) There are some subtle niceties with these trafÔ¨Åc emulators, particularly when it comes to bandwidth throttling. Neither HTB nor TBF was quite meant to emulate the physical bandwidth of a single link, for which token buckets ( 24 Token Bucket Rate Limiting ) do not generally apply. A token-bucket Ô¨Ålter will, if the line has been idle, allow a ‚Äúbucketful‚Äù of packets to be sent all at once. We will get around this when using HTB or TBF by choosing small bucket sizes (‚Äúburst‚Äù sizes in HTB/TBF parlance). We cannot choose the burst size toosmall, however, or we run into another issue. Linux systems have a speciÔ¨Åc interrupt frequency, denoted as HZ, representing the number of interrupts per second, and HTB, TBF and NetEm wake up and process packets once per interrupt. On the author‚Äôs 2021 laptop, for example, HZ=250, meaning that interrupts occur every 4 ms (traditionally, HZ was 100; on some systems it can be 1,000). On each interrupt, these queuing disciplines wake up and send a batch of packets; speciÔ¨Åcally, the number of packets that need to be sent in the next 1/HZ time interval in order to maintain the desired bandwidth. HTB and TBF support a user-provided burst parameter, and if this designated burst size is smaller than the packet-batch size, the full cluster cannot be sent and bandwidth falls short. For HZ=250 and packets of 1500 bytes, one packet per CPU interrupt yields a bandwidth of 250 packets/sec, or 3 Mbps. To send at 8 Mbps, we thus need the burst parameter to be at least 3 1500 = 4500 bytes. In general, to 30.7 TCP Competition 801
An Introduction to Computer Networks, Release 2.0.11 send at rate r bits/sec, we need a burst size of r/(12000 HZ), rounded up to a multiple of 1500 bytes (there are 12000 bits in 1500 bytes). An advantage of using NetEm here is that it calculates all this for us, though NetEm still has the HZ-granularity issue. One consequence of the use of token buckets for bandwidth throttling is that, because the bucket size is usually at least as large as one packet, single packets may not be delayed at all. For example, if we increase the size ofping packets with the -soption, we may Ô¨Ånd that the RTT reported by the ping command is unchanged: the individual ping packets are not being delayed at all by the token-bucket Ô¨Ålter. Mininet uses HTB by default for specifying link rate limits, with NetEm used for delay and the FIFO queue. For convenience, we will often go along with the Mininet default. For critical links, though, having NetEm handle all three means we avoid having to deal with putting things in the right order. In the Ô¨Åle competition.py, we use NetEm alone on the bottleneck link r‚Äìh3. To specify for NetEm a rate of 8 Mbps, a delay of 110 ms, and a FIFO-queue size of 25, we can use the following: netem rate 8mbit delay 110ms limit 25 To add this to interface r-eth3, we need to preface it with tc qdisc add root dev r-eth3. 30.7.2 Link Emulation in Mininet To create links in Mininet with bandwidth/delay support, we can simply set Link=TCLink in the Mininet() call inmain(). The TCLink class represents a TrafÔ¨Åc Controlled Link. Next, in the topology section calls to addLink(), we add keyword parameters such as bw=BottleneckBW (where BottleneckBW is a number, eg8 or 0.8, and represents Mbps) and delay=DELAY (where DELAY is a string, such as ‚Äò110ms‚Äô). For example, to create the R‚ÄìB link above we might use (in the build() method) self.addLink( r, b, intfName1 = 'r-eth1', intfName2 = 'b-eth0', bw = 8, delay= √£√ë'110ms') To implement this, Mininet creates a two-layer queuing hierarchy ( 23.7 Hierarchical Queuing ). An HTB qdisc at the root provides the bandwidth throttling, and a NetEm qdisc below provides the delay. The hierarchy is managed by the tc(trafÔ¨Åc control; 30.8 Linux TrafÔ¨Åc Control (tc) ) command, part of the LARTC subsystem. In the A‚ÄìR‚ÄìB topology above, assuming we have delay and bandwidth limits declared for both links, Mininet sets up all four queues this way. Using thetc qdisc show command we can see that the ‚Äúhandle‚Äù of the netem queue is 10:; we can now set the maximum FIFO queue size to, for example, 25 with the following command on r: tc qdisc change dev r-eth3 handle 10: netem limit 25 Alternatively, if we wish to use NetEm for everything, we simply ignore Mininet‚Äôs bandwidth and delay options for a link, and then include the following in the run() section: r.cmd('tc qdisc add root dev r-eth1 netem rate 8mbit delay 110ms limit 25' ) 802 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 30.7.3 Python Utilities for a TCP Competition In order to arrange a TCP competition, we introduce the following tools: 
- sender.py, to open the TCP connection and send bulk data, after requesting a speciÔ¨Åc TCP congestioncontrol mechanism (Reno or Vegas) 
- dualreceive.py, to receive data from two connections and track the results 
- udprandomtelnet.py, to send random additional data to break TCP phase effects. 30.7.3.1 sender.py The Python3 program sender.py is similar to tcp_stalkc.py, except that it transmits a speciÔ¨Åed number of 1KB blocks, and allows speciÔ¨Åcation of the TCP congestion algorithm. This last is done with the following setsockopt() call: s.setsockopt(socket.IPPROTO_TCP, TCP_CONGESTION, cong) wherecong is ‚Äúreno‚Äù or ‚Äúcubic‚Äù or some other available TCP Ô¨Çavor. The list is at /proc/sys/net/ipv4/tcp_allowed_congestion_control, which can be edited to include any entry from /proc/sys/net/ipv4/tcp_available_congestion_control. See also 22.1 Choosing a TCP on Linux. sender.py accepts up to four positional arguments: 1. Number of 1KB blocks to send 2. IP address or hostname 3. Port number 4. Congestion-control algorithm name Typically we will run these as follows, where ports 5430 and 5431 are the two listening ports of dualreceive.py in the following section: 
- onh1:python3 sender.py 5000 10.0.3.10 5430 reno 
- onh2:python3 sender.py 5000 10.0.3.10 5431 vegas Any sender can be used that supports selection of the TCP congestion-control algorithm. 30.7.3.2 dualreceive.py The receiver for sender.py‚Äôs data is dualreceive.py. It listens on two ports, by default 5430 and 5431, and, when both connections have been made, begins reading. The main loop starts with a call to select(), wheresset is the list of all (both) connected sockets: sl,_,_ = select(sset, [], []) The valueslis a sublist of sset consisting of the sockets with data ready to read. It will normally be a list consisting of a single socket, though with so much data arriving it may sometimes contain both. We then calls.recv() for s in sl, and record in either count1 orcount2 the running total of bytes received. 30.7 TCP Competition 803
An Introduction to Computer Networks, Release 2.0.11 If a sender closes a socket, this results in a read of 0 bytes. At this point dualreceive.py must close the socket, at which point it must be removed from sset as it will otherwise always appear in the sllist. We repeatedly set a timer (in printstats() ) to print the values of count1 andcount2 at onesecond intervals, reÔ¨Çecting the cumulative amounts of data received by the connections. (If the variable PRINT_CUMULATIVE is set toFalse, then the values printed are the amounts of data received in the most recent time interval.) If the TCP competition is fair, count1 andcount2 should stay approximately equal. In Python, calling exit() only exits the current thread; the other threads keep running. Dualreceive.py may be given a single command-line parameter matching the number of 1KB blocks sent by each sender; this allows it to exit as soon as the competition is over. If this option is left off, it terminates once it detects no further changes in count1 andcount2. 30.7.3.3 udprandomtelnet.py In31.3.4 Phase Effects we show that, with completely deterministic travel times, two competing TCP connections can have throughputs differing by a factor of as much as 10 simply because of unfortunate synchronizations of transmission times. We must introduce at least some degree of packet-arrival-time randomization in order to obtain meaningful results. In31.3.6 Phase Effects and overhead we used the ns2 overhead attribute for this. This is not availble in real networks, however. The next-best thing is to introduce some random telnet-like trafÔ¨Åc, as in 31.3.7 Phase Effects and telnet trafÔ¨Åc. This is the purpose of udprandomtelnet.py. This program sends UDP packets at random intervals; we use UDP because TCP likes to combine small packets into fewer, larger ones. The lengths of the intervals are exponentially distributed, meaning that to Ô¨Ånd the length of the next interval we choose X randomly between 0 and 1 (with a uniform distribution), and then set the length of the wait interval to a constant times -log(X). The packet sizes are 210 bytes (a very atypical value for real telnet trafÔ¨Åc). Crucially, the average rate of sending is held to a small fraction (by default 1%) of the available bottleneck bandwidth, which is supplied as a constant BottleneckBW. This means the udprandomtelnet trafÔ¨Åc should not interfere signiÔ¨Åcantly with the competing TCP connections (which, of course, have no additional interval whatsoever between packet transmissions, beyond what is dictated by sliding windows). The udprandomtelnet trafÔ¨Åc appears to be quite effective at eliminating TCP phase effects. UDP is used because runs of small TCP packets typically end up being coalesced into one larger TCP packet, which defeats the purpose. Udprandomtelnet.py sends to port 5433 by default. We will usually use netcat (17.7.1 netcat again ) as the receiver, as we are not interested in measuring throughput for this trafÔ¨Åc. This is run with netcat -l -u 5433 >/dev/null In principle, competition results obtained in the presence of running udprandomtelnet.py are valid only in the presence of that particular pattern of random trafÔ¨Åc. In practice, it does sometimes appear that any randomness that eliminates TCP ‚Äúphase effects‚Äù ( 31.3.4 Phase Effects ) is more or less equivalent. 804 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 30.7.4 Monitoring cwnd At the end of the competition, we can look at the dualreceive.py output and determine the overall throughput of each connection, as of the time when the Ô¨Årst connection to send all its data has just Ô¨Ånished. We can also plot throughput at intervals by plotting successive differences of the cumulative-throughput values. However, this does not give us a view of each connection‚Äôs cwnd, which is readily available when modeling competition in a simulator such as ns2 ( 31 Network Simulations: ns-2 ). Essentially all TCPs exhibit some degree of longer-term variation of cwnd, meant to converge in some sense to a steady-state value, or at least to a steady-state average. For example, for TCP Reno this variation is the ‚ÄúTCP sawtooth‚Äù of 19.1.1 The Somewhat-Steady State. Therefore, the graph of cwnd versus time is often very useful in understanding connection behavior. At a minimum, experiments meant to demonstrate long-term behavior must be run long enough to include multiple ‚Äúteeth‚Äù, or whatever the ‚Äútooth‚Äù analog is for the TCP in question. (Sometimes as many as a thousand teeth might be appropriate.) Up through the end of Linux kernel version 4.14.x, a kernel module tcp_probe was available that would reportcwnd values. However, it is no longer supported. This leaves the following two methods: 
- monitor the (approximate) cwnd by eavesdropping on data and ACK packets in Ô¨Çight 
- Use thesscommand (for socket statistics) at the sender, and parse out the cwnd data from the output 30.7.4.1 Monitoring cwnd by eavesdropping The approach here is to monitor the number of packets (or bytes) a connection has in Ô¨Çight; this is the difference between the highest byte sent and the highest byte acknowledged. The highest byte ACKed is one less than the value of the ACK Ô¨Åeld in the most recent ACK packet, and the highest byte sent is one less than the value of the SEQ Ô¨Åeld, plus the packet length, in the most recent DATA packet. To get these ACK and SEQ numbers requires eavesdropping on the network trafÔ¨Åc. We can do this using a packet-capture library such as libpcap. The program wintracker.py uses the Python3 module libpcap (named for the corresponding C library) to monitor packets on the interfaces r-eth1 and r-eth2 of router r. It would be slightly more accurate to monitor on h1-eth0 and h2-eth0, but that entails separate monitoring of two different nodes, and the difference is small as the h1‚Äìr and h2‚Äìr links have negligible delay and no queuing. Wintracker.py must be conÔ¨Ågured to monitor only the two TCP connections that are competing. Note that we cannot use the outbound r-eth3 interface, as packets don‚Äôt show up there until after they are done waiting in r‚Äôs queue. The way libpcap works is that we Ô¨Årst create a packet Ô¨Ålter to identify the packets we want to capture. A Ô¨Ålter that can be used for both connections in the competition.py Mininet example is host 10.0.3.10 andtcp andportrange 5430-5431 The host is, of course, h3; packets are captured if either source host or destination host is h3. Similarly, packets are captured if either the source port or the destination port is either 5430 or 5431. The connection from h1 to h3 is to port 5430 on h3, and the connection from h2 to h3 is to port 5431 on h3. For the h1‚Äìh3 connection, each time a packet arrives heading from h1 toh3 (in the code below we determine this because the destination port dport is 5430), we save in seq1 the TCP header SEQ Ô¨Åeld plus the packet length. Each time a packet is seen heading from h3 to h1 (that is, with source port 5430), we record in ack1 30.7 TCP Competition 805
An Introduction to Computer Networks, Release 2.0.11 the TCP header ACK Ô¨Åeld. The packets themselves are captured as arrays of bytes; we then use the Python dpkt module to decode the IP and TCP headers. The parsepacket(p: bytes) function extracts the TCP source and destination ports, the sequence and acknowledgement numbers, and the TCP data: defparsepacket(p): # p is the captured packet eth = dpkt.ethernet.Ethernet(p) if notisinstance(eth.data, dpkt.ip.IP): return None ip = eth.data if notisinstance(ip.data, dpkt.tcp.TCP): return None tcp = ip.data return(tcp.sport, tcp.dport, tcp.seq, tcp.ack, (tcp.data)) Separate threads are used for each interface ‚Äì that is, each connection ‚Äì as there is no variant of select() available to return the next captured packet of either interface. Both the SEQ and ACK Ô¨Åelds have had ISN h1added to them, but this will cancel out when we subtract. The SEQ and ACK values are subject to 32-bit wraparound. As with dualreceive.py, a timer Ô¨Åres at regular intervals and prints out the differences seq1 - ack1 and seq2 - ack2. This isn‚Äôt completely thread-safe, but it is close enough. There is some noise in the results; this can be minimized by taking the running average of several differences in a row. It may also be necessary to delete the Ô¨Årst few records, where not all of the variables have yet been assigned a measured value. 30.7.4.2 Monitoring cwnd with ss Thesscommand collects a variety of statistics about all TCP connections matching certain criteria. The value ofcwnd is reported in packets, so we multiply this by the value obtained for mss. In our stylized setting here, there is one connection from h1 to h3 and one from h2 to h3. We run ss repeatedly on both h1 and h2; each sees only one connection. If we are comparing two connections with the same starting host, we must arrange to have two ss-probe threads (or processes), one monitoring the connection to port 5430 and one to port 5431. The program ss_cwnd.py invokes ssat appropriate intervals and prints the (time,cwnd) records to the standard output. It takes the remote host and port number as command-line arguments. It should be started ( egby a shell script) as soon as the corresponding TCP connection is started, perhaps via something like this, where $DIR supplies the full pathname: python3 $DIR/sender.py $BLOCKCOUNT h3 5430 reno & python3 $DIR/ss_cwnd.py √£√ëh3 5430 > h1cwnd.out Alternatively, the code for ss_cwnd.py and sender.py might be combined, for better control. Note that the output times are, of necessity, absolute Linux timestamps, as we cannot easily synchronize the two ss_cwnd.py threads ‚Äì running on different Mininet nodes ‚Äì to subtract the common start time, as was done with dualreceive.py. 806 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 30.7.4.3 Synchronizing the start The next issue is to get both senders to start at about the same time. One approach is to use two ssh commands (run from r): ssh h1'nohup python3 $DIR/sender.py 5000 10.0.3.10 5430 reno >/dev/null 2>& √£√ë1 &' ssh h2'nohup python3 $DIR/sender.py 5000 10.0.3.10 5431 vegas >/dev/null 2>& √£√ë1 &' However, ssh commands can take several hundred milliseconds to complete. A faster method is to use netcat to trigger the start. On h1 and h2 we run shell scripts like the one below (separate values for $PORT and $CONG are needed for each of h1 and h2, which is simplest to implement with separate scripts, say h1.sh and h2.sh): netcat -l 2345 python3 sender.py $BLOCKS 10.0.3.10 $PORT $CONG We then start both at very close to the same time with the following on r (not on h3, due to the delay on the r‚Äìh3 link); these commands typically complete in under ten milliseconds. The -q 0 option means that the client-side netcat should quit immediately after the end-of-Ô¨Åle on its input (the alternative is for the connection to remain open in the reverse direction): echo hello | netcat -q 0 h1 2345 & echo hello | netcat -q 0 h2 2345 The full sequence of steps is 
- On h3, start the netcat -l -u 5433 >/dev/null & for receiving the udprandomtelnet.py output. 
- On h1 and h2, start the udprandomtelnet.py senders: python3 udprandomtelnet.py h3 &. - On h3, start dualreceive.py. - On h1 and h2, start the scripts ( egh1.sh and h2.sh) that wait for the signal and start sender.py, possibly followed by ss_cwnd.py. 
- If using cwnd eavesdropping, start wintracker.py on r. 
- On r, send the two start triggers via netcat. This is somewhat cumbersome; it may help to incorporate everything into a hierarchy of shell scripts. 30.7.5 TCP Compeition: Reno vs Vegas In the Reno-Vegas graph at 31.5 TCP Reno versus TCP Vegas, we set the Vegas parameters ùõºandùõΩto 3 and 6 respect.ively. The implementation of TCP Vegas on the Mininet virtual machine does not, however, support changing ùõºandùõΩ, and the default values are more like 1 and 3. To give Vegas a Ô¨Åghting chance, we reduce the queue size at r to 10 in competition.py. Here is the graph, with the packets-in-Ô¨Çight monitoring above and the throughput below: 30.7 TCP Competition 807
An Introduction to Computer Networks, Release 2.0.11 05000001e+061.5e+062e+06 0 50 100 150 200 250 300 350 400 450Reno Vegas TCP Vegas is getting a smaller share of the bandwidth (overall about 40% to TCP Reno‚Äôs 60%), but it is consistently holding its own. It turns out that TCP Vegas is greatly helped by the small queue size; if the queue size is doubled to 20, then Vegas gets a 17% share. In the upper part of the graph, we can see the Reno sawteeth versus the Vegas triangular teeth (sloping down as well as sloping up); compare to the red-and-green graph at 31.5 TCP Reno versus TCP Vegas. The tooth shapes are somewhat mirrored in the throughput graph as well, as throughput is proportional to queue utilization which is proportional to the number of packets in Ô¨Çight. 30.7.6 TCP Competition: Reno vs BBR We can apply the same technique to compare TCP Reno to TCP BBR. This was done to create the graph at 22.16 TCP BBR. The Mininet approach is usable as soon as a TCP BBR module for Linux was released (in source form); to use a simulator, on the other hand, would entail waiting for TCP BBR to be ported to the simulator. One nicety is that it is essential that the fqqueuing discipline be enabled for the TCP BBR sender. If that is h2, for example, then the following Mininet code (perhaps in competition.py ) removes any existing queuing discipline and adds fq: h2.cmd('tc qdisc del dev h2-eth root' ) h2.cmd('tc qdisc add dev h2-eth root fq' ) The purpose of the fqqueuing discipline is to enable pacing; that is, the transmission of packets at regular, very small intervals. 808 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 30.8 Linux TrafÔ¨Åc Control (tc) The Linux tccommand, for trafÔ¨Åc control, allows the attachment of any implemented queuing discipline (23 Queuing and Scheduling ) to any network interface (usually of a router). A hierarchical example appears in 24.11 Linux HTB. The tc command is also used extensively by Mininet to control, for example, link queue capacities. An explicit example, of adding the fqqueuing discipline, appears immediately above. The two examples presented in this section involve ‚Äúsimple‚Äù token-bucket Ô¨Åltering, using tbf, and then ‚Äúclassful‚Äù token-bucket Ô¨Åltering, using htb. We will use the latter example to apply token-bucket Ô¨Åltering only to one class of connections; other connections receive no Ô¨Åltering. The granularity of tc-tbf rate control is limited by the cpu-interrupt timer granularity; typically tbf is able schedules packets every 10 ms. If the transmission rate is 6 MB/s, or about four 1500-byte packets per millisecond, then tbf will schedule 40 packets for transmission every 10 ms. They will, however, most likely be sent as a burst at the start of the 10-ms interval. Some tc schedulers are able to achieve much Ô¨Åner pacing control; egthe ‚Äòfq‚Äô qdisc of 30.7.6 TCP Competition: Reno vs BBR above. The Mininet topology in both cases involves a single router between two hosts, h1‚Äîr‚Äîh2. We will here use the routerline.py example with the option -N 1; the router is then r1with interfaces r1-eth0 connecting toh1andr1-eth1 connecting to h2. The desired topology can also be built using competition.py and then ignoring the third host. To send data we will use sender.py ( 30.7.3.1 sender.py ), though with the default TCP congestion algorithm. To receive data we will use dualreceive.py, though initially with just one connection sending any signiÔ¨Åcant data. We will set the constant PRINT_CUMULATIVE toFalse, sodualreceive.py prints at intervals the number of bytes received during the most recent interval; we will call this modiÔ¨Åed version dualreceive_incr.py. We will also redirect the stderr messages to /dev/null, and start this on h2: python3 dualreceive_incr.py 2>/dev/null We start the main sender on h1with the following, where h2has IPv4 address 10.0.1.10 and 1,000,000 is the number of blocks: python3 sender.py 1000000 10.0.1.10 5430 The dualreceive program will not do any reading until both connections are enabled, so we also need to create a second connection from h1in order to get started; this second connection sends only a single block of data: python3 sender.py 1 10.0.1.10 5431 At this point dualreceive should generate output somewhat like the following (with timestamps in the Ô¨Årst column rounded to the nearest millisecond). The byte-count numbers in the middle column are rather hardware-dependent 1.016 14079000 0 1.106 12702000 0 1.216 14724000 0 1.316 13666448 0 1.406 11877552 0 30.8 Linux TrafÔ¨Åc Control (tc) 809
An Introduction to Computer Networks, Release 2.0.11 This means that, on average, h2is receiving about 13 MB every 100ms, which is about 1.0 Gbps. Now we run the command below on r1to reduce the rate ( tcrequires the abbreviation mbit for megabit/sec; it treats mbps as MegaBytes per second). The token-bucket Ô¨Ålter parameters are rate and burst. The purpose of the limit parameter ‚Äì used by netem and several other qdiscs as well ‚Äì is to specify the maximum queue size for the waiting packets. Its value here is not very signiÔ¨Åcant, but too low a value can lead to packet loss and thus to momentarily plunging bandwidth. Too high a value, on the other hand, can lead to bufferbloat ( 21.5.1 Bufferbloat ). tc qdisc add dev r1-eth1 root tbf rate 40mbit burst 50kb limit 200kb We get output something like this: 1.002 477840 0 1.102 477840 0 1.202 477840 0 1.302 482184 0 1.402 473496 0 477840 bytes per 100 ms is 38.2 Mbps. That is received application data; the extra 5% or so to 40 Mbps corresponds mostly to packet headers (66 bytes out of every 1514, though to see this with WireShark we need to disable TSO, 17.5 TCP OfÔ¨Çoading ). We can also change the rate dynamically: tc qdisc change dev r1-eth1 root tbf rate 20mbit burst 100kb limit 200kb The above use of tbf allows us to throttle (or police) alltrafÔ¨Åc through interface r1-eth1. Suppose we want to police selected trafÔ¨Åc only? Then we can use hierarchical token bucket, or htb. We set up an htb root node, with no limits, and then create two child nodes, one for policed trafÔ¨Åc and one for default trafÔ¨Åc. root htb qdisc, handle 1: root class, 1000 mbit, classid 1:1 policed leaf class, 40 mbit, classid 1:2default leaf class, 1000 mbit, classid 1:10 To create the htb hierarchy we will Ô¨Årst create the root qdisc and associated root class. We need the raw interface rate, here taken to be 1000mbit. Class identiÔ¨Åers are of the form major :minor, where major is the integer root ‚Äúhandle‚Äù and minor is another integer. (We do not absolutely need to create a root class, but only children with a common parent class can share bandwidth.) 810 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 tc qdisc add dev r1-eth1 root handle 1: htb default 10 tcclass add dev r1-eth1 parent 1: classid 1:1 htb rate 1000mbit We now create the two child classes (not qdiscs), one for the rate-limited trafÔ¨Åc and one for default trafÔ¨Åc. The rate-limited class has classid 1:2 here; the default class has classid 1:10. These child classes have the 1:1 class ‚Äì not the 1: qdisc ‚Äì as parent. tcclass add dev r1-eth1 parent 1:1 classid 1:2 htb rate 40mbit tcclass add dev r1-eth1 parent 1:1 classid 1:10 htb rate 1000mbit We still need a classiÔ¨Åer (orÔ¨Ålter ) to assign selected trafÔ¨Åc to class 1:2. Our goal is to police trafÔ¨Åc to port 5430 (by default, dualreceive.py accepts trafÔ¨Åc at ports 5430 and 5431). There are several classiÔ¨Åers available; for example u32(man tc-u32) and bpf(man tc-bpf). The latter is based on the Berkeley Packet Filter virtual machine for packet recognition. However, what we use here ‚Äì mainly because it seems to work most reliably ‚Äì is the iptables fwmark mechanism, used earlier in 13.6 Routing on Other Attributes. Iptables is intended for Ô¨Åltering ‚Äì and sometimes modifying ‚Äì packets; we can associate a fwmark value of 2 to packets bound for TCP port 5430 with the command below (thefwmark value does not become part of the packet; it exists only while the packet remains in the kernel). iptables --append FORWARD --table mangle --protocol tcp --dport 5430 --jump √£√ëMARK --set-mark 2 When this is run on r1, then packets forwarded by r1to TCP port 5430 receive the fwmark upon arrival. The next step is to tell the tcsubsystem that packets with a fwmark value of 2 are to be placed in class 1:2; this is the rate-limited class above. In the following command, flowid may be used as a synonym for classid. tc filter add dev r1-eth1 parent 1:0 protocol ip handle 2 fw classid 1:2 We can view all these settings with tc qdisc show dev r1-eth1 tcclass show dev r1-eth1 tc filter show dev r1-eth1 parent 1:1 iptables --table mangle --list We now verify that all this works. As with tbf, we startdualreceive_incr.py onh2and two senders onh1. This time, both senders send large amounts of data: h2: python3 dualreceive_incr.py 2>/dev/null h1: python3 sender.py 500000 10.0.1.10 5430 h1: python3 sender.py 500000 10.0.1.10 5431 If everything works, then shortly after the second sender starts we should see something like the output below (taken after both TCP connections have their cwnd stabilize). The middle column is the number of received data bytes to the policed port, 5430. 1.000 453224 10425600 1.100 457568 10230120 (continues on next page) 30.8 Linux TrafÔ¨Åc Control (tc) 811
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) 1.200 461912 9934728 1.300 476392 10655832 1.401 438744 10230120 With 66 bytes of TCP/IP headers in every 1514-byte packet, our requested 40 mbit data-rate cap should yield about 478,000 bytes every 0.1 sec. The slight reduction above appears to be related to TCP competition; the full 478,000-byte rate is achieved after the port-5431 connection terminates. 30.9 OpenFlow and the POX Controller In this section we introduce the POX controller for OpenFlow ( 3.4.1 OpenFlow Switches ) switches, allowing exploration of software-deÔ¨Åned networking ( 3.4 Software-DeÔ¨Åned Networking ). In the switchline.py Ethernet-switch example from earlier, the Mininet() call included a parameter controller=DefaultController; this causes each switch to behave like an ordinary Ethernet learning switch. By using Pox to create customized controllers, we can investigate other options for switch operation. Pox is preinstalled on the Mininet virtual machine. Pox is written in Python2. It receives and sends OpenFlow messages, in response to events. Event-related messages, for our purposes here, can be grouped into the following categories: 
- PacketIn: a switch is informing the controller about an arriving packet, usually because the switch does not know how to forward the packet or does not know how to forward the packet without Ô¨Çooding. Often, but not always, PacketIn events will result in the controller providing new forwarding instructions. 
- ConnectionUP: a switch has connected to the controller. This will be the point at which the controller gives the switch its initial packet-handling instructions. 
- LinkEvent: a switch is informing the controller of a link becoming available or becoming unavailable; this includes initial reports of link availability. 
- BarrierEvent: a switch‚Äôs response to an OpenFlow Barrier message, meaning the switch has completed its responses to all messages received before the Barrier and now may begin to respond to messages received after the Barrier. The Pox program comes with several demonstration modules illustrating how controllers can be programmed; these are in the pox/misc and pox/forwarding directories. The starting point for Pox documentation is the Pox wiki (archived copy at poxwiki.pdf), which among other thing includes brief outlines of these programs. We now review a few of these programs; most were written by James McCauley and are licensed under the Apache license. The Pox code data structures are very closely tied to the OpenFlow Switch SpeciÔ¨Åcation, versions of which can be found at the OpenNetworking.org technical library. 30.9.1 hub.py As a Ô¨Årst example of Pox, suppose we take a copy of the switchline.py Ô¨Åle and make the following changes: 812 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 
- change the controller speciÔ¨Åcation, inside the Mininet() call, from controller=DefaultController tocontroller=RemoteController. - add the following lines immediately following the Mininet() call: c = RemoteController( 'c', ip='127.0.0.1', port=6633 ) net.addController(c) This modiÔ¨Åed version is available as switchline_rc.py, ‚Äúrc‚Äù for remote controller. If we now run this modiÔ¨Åed version, then pings fail because the RemoteController, c, does not yet exist; in the absence of a controller, the switches‚Äô default response is to do nothing. We now start Pox, in the directory /home/mininet/pox, as follows; this loads the Ô¨Åle pox/forwarding/hub.py ./pox.py forwarding.hub Ping connectivity should be restored! The switch connects to the controller at IPv4 address 127.0.0.1 (more on this below) and TCP port 6633. At this point the controller is able to tell the switch what to do. Thehub.py example conÔ¨Ågures each switch as a simple hub, Ô¨Çooding each arriving packet out all other interfaces (though for the linear topology of switchline_rc.py, this doesn‚Äôt matter much). The relevant code is here: def_handle_ConnectionUp (event): msg = of.ofp_flow_mod() msg.actions.append(of.ofp_action_output(port = of.OFPP_FLOOD)) event.connection.send(msg) This is the handler for ConnectionUp events; it is invoked when a switch Ô¨Årst reports for duty. As each switch connects to the controller, the hub.py code instructs the switch to forward each arriving packet to the virtual port OFPP_FLOOD, which means to forward out all other ports. Theevent parameter is of class ConnectionUp, a subclass of class Event. It is deÔ¨Åned in pox/ openflow/__init__.py. Most switch-event objects throughout Pox include a connection Ô¨Åeld, which the controller can use to send messages back to the switch, and a dpid Ô¨Åeld, representing the switch identiÔ¨Åcation number. Generally the Mininet switch s1will have a dpid of 1, etc. The code above creates an OpenFlow modify-Ô¨Çow-table message,msg; this is one of several types of controller-to-switch messages that are deÔ¨Åned in the OpenFlow standard. The Ô¨Åeld msg.actions is a list of actions to be taken; to this list we append the action of forwarding on the designated (virtual) port OFPP_FLOOD. Normally we would also append to the list msg.match the matching rules for the packets to be forwarded, but here we want to forward all packets and so no matching is needed. A different ‚Äì though functionally equivalent ‚Äì approach is taken in pox/misc/of_tutorial.py. Here, the response to the ConnectionUp event involves no communication with the switch (though the connection is stored inTutorial.__init__() ). Instead, as the switch reports each arriving packet to the controller, the controller responds by telling the switch to Ô¨Çood the packet out every port (this approach does result in sufÔ¨Åcient unnecessary trafÔ¨Åc that it would not be used in production code). The code (slightly consolidated) looks something like this: 30.9 OpenFlow and the POX Controller 813
An Introduction to Computer Networks, Release 2.0.11 def_handle_PacketIn (self, event): packet = event.parsed # This is the parsed packet data. packet_in = event.ofp # The actual ofp_packet_in message. self.act_like_hub(packet, packet_in) defact_like_hub (self, packet, packet_in): msg = of.ofp_packet_out() msg.data = packet_in action = of.ofp_action_output(port = of.OFPP_ALL) msg.actions.append(action) self.connection.send(msg) The event here is now an instance of class PacketIn. This time the switch sents a packet out message to the switch. The packet andpacket_in objects are two different views of the packet; the Ô¨Årst is parsed and so is generally easier to obtain information from, while the second represents the entire packet as it was received by the switch. It is the latter format that is sent back to the switch in the msg.data Ô¨Åeld. The virtual port OFPP_ALL is equivalent to OFPP_FLOOD. For either hub implementation, if we start WireShark on h2 and then ping from h4 to h1, we will see the pings at h2. This demonstrates, for example, that s2 is behaving like a hub rather than a switch. 30.9.2 l2_pairs.py The next Pox example, l2_pairs.py, implements a real Ethernet learning switch. This is the pairs-based switch implementation discussed in 3.4.2 Learning Switches in OpenFlow. This module acts at the Ethernet address layer (layer 2, the l2part of the name), and Ô¨Çows are speciÔ¨Åed by (src,dst) pairs of addresses. The l2_pairs.py module is started with the Pox command ./pox.py forwarding.l2_pairs. A straightforward implementation of an Ethernet learning switch runs into a problem: the switch needs to contact the controller whenever the packet source address has not been seen before, so the controller can send back to the switch the forwarding rule for how to reach that source address. But the primary lookup in the switch Ô¨Çow table must be by destination address. The approach used here uses a single OpenFlow table, versus the two-table mechanism of 30.9.3 l2_nx.py. However, the learned Ô¨Çow table match entries will all include match rules for both the source and the destination address of the packet, so that a separate entry is necessary for each pair of communicating hosts. The number of Ô¨Çow entries thus scales as O(N2), which presents a scaling problem for very large switches but which we will ignore here. When a switch sees a packet with an unmatched (dst,src) address pair, it forwards it to the controller, which has two cases to consider: 
- If the controller does not know how to reach the destination address from the current switch, it tells the switch to Ô¨Çood the packet. However, the controller also records, for later reference, the packet source address and its arrival interface. 
- If the controller knows that the destination address can be reached from this switch via switch port dst_port, it sends to the switch instructions to create a forwarding entry for (dst,src) √ëdst_port. At the same time, the controller also sends to the switch a reverse forwarding entry for (src,dst), forwarding via the port by which the packet arrived. The controller maintains its partial map from addresses to switch ports in a dictionary table, which takes a (switch,destination) pair as its key and which returns switch port numbers as values. The switch is repre814 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 sented by the event.connection object used to reach the switch, and destination addresses are represented as Pox EthAddr objects. The program handles only PacketIn events. The main steps of the PacketIn handler are as follows. First, when a packet arrives, we put its switch and source into table: table[(event.connection,packet.src)] = event.port The next step is to check to see if there is an entry in table for the destination, by looking up table[(event.connection,packet.dst)]. If there is notan entry, then the packet gets Ô¨Çooded by the same mechanism as in of_tutorial.py above: we create a packet-out message containing the to-be-Ô¨Çooded packet and send it back to the switch. If, on the other hand, the controller Ô¨Ånds that the destination address can be reached via switch port dst_port, it proceeds as follows. We Ô¨Årst create the reverse entry;event.port is the port by which the packet just arrived: msg = of.ofp_flow_mod() msg.match.dl_dst = packet.src # reversed dst and src msg.match.dl_src = packet.dst # reversed dst and src msg.actions.append(of.ofp_action_output(port = event.port)) event.connection.send(msg) This is like the forwarding rule created in hub.py, except that we here are forwarding via the speciÔ¨Åc port event.port rather than the virtual port OFPP_FLOOD, and, perhaps more importantly, we are adding two packet-matching rules to msg.match. The next step is to create a similar matching rule for the src-to-dst Ô¨Çow, andto include the packet to be retransmitted. The modify-Ô¨Çow-table message thus does double duty as a packet-out message as well. msg = of.ofp_flow_mod() msg.data = event.ofp # Forward the incoming packet msg.match.dl_src = packet.src # not reversed this time! msg.match.dl_dst = packet.dst msg.actions.append(of.ofp_action_output(port = dst_port)) event.connection.send(msg) Themsg.match object has quite a few potential matching Ô¨Åelds; the following is taken from the Pox-Wiki: Attribute Meaning in_port Switch port number the packet arrived on dl_src Ethernet source address dl_dst Ethernet destination address dl_type Ethertype / length (e.g. 0x0800 = IPv4) nw_tos IPv4 TOS/DS bits nw_proto IPv4 protocol (e.g., 6 = TCP), or lower 8 bits of ARP opcode nw_src IPv4 source address nw_dst IP destination address tp_src TCP/UDP source port tp_dst TCP/UDP destination port 30.9 OpenFlow and the POX Controller 815
An Introduction to Computer Networks, Release 2.0.11 It is also possible to create a msg.match object that matches all Ô¨Åelds of a given packet. We can watch the forwarding entries created by l2_pairs.py with the Linux program ovs-ofctl. Suppose we startswitchline_rc.py and then the Pox module l2_pairs.py. Next, from within Mininet, we have h1 ping h4 and h2 ping h4. If we now run the command (on the Mininet virtual machine but from a Linux prompt) ovs-ofctl dump-flows s2 we get cookie=0x0,. .. , dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:04 actions=output:3 cookie=0x0,. .. , dl_src=00:00:00:00:00:04,dl_dst=00:00:00:00:00:02 actions=output:1 cookie=0x0,. .. , dl_src=00:00:00:00:00:02,dl_dst=00:00:00:00:00:04 actions=output:3 cookie=0x0,. .. , dl_src=00:00:00:00:00:04,dl_dst=00:00:00:00:00:01 actions=output:2 Because we used the autoSetMacs=True option in the Mininet() call inswitchline_rc.py, the Ethernet addresses assigned to hosts are easy to follow: h1 is 00:00:00:00:00:01, etc. The Ô¨Årst and fourth lines above result from h1 pinging h4; we can see from the output port at the end of each line that s1 must be reachable from s2 via port 2 and s3 via port 3. Similarly, the middle two lines result from h2 pinging h4; h2 lies off s2‚Äôs port 1. These port numbers correspond to the interface numbers shown in the diagram at 30.3 Multiple Switches in a Line. 30.9.3 l2_nx.py Thel2_nx.py example accomplishes the same Ethernet-switch effect as l2_pairs.py, but using only O(N) space. It does, however, use two OpenFlow tables, one for destination addresses and one for source addresses. In the implementation here, source addresses are held in table 0, while destination addresses are held in table 1; this is the reverse of the multiple-table approach outlined in 3.4.2 Learning Switches in OpenFlow. The l2again refers to network layer 2, and the nxrefers to the so-called Nicira extensions to Pox, which enable the use of multiple Ô¨Çow tables. Initially, table 0 is set up so that it tries a match on the source address. If there is no match, the packet is forwarded to the controller, andsent on to table 1. If there is a match, the packet is sent on to table 1 but not to the controller. Table 1 then looks for a match on the destination address. If one is found then the packet is forwarded to the destination, and if there is no match then the packet is Ô¨Çooded. Using two OpenFlow tables in Pox requires the loading of the so-called Nicira extensions (hence the ‚Äúnx‚Äù in the module name here). These require a slightly more complex command line: ./pox.py openflow.nicira --convert-packetinforwarding.l2_nx Nicira will also require, eg,nx.nx_flow_mod() instead ofof.ofp_flow_mod(). The no-match actions for each table are set during the handling of the ConnectionUp events. An action becomes the default action when no msg.match() rules are included, and the priority is low; recall (3.4.1 OpenFlow Switches ) that if a packet matches multiple Ô¨Çow-table entries then the entry with the highest priority wins. The priority is here set to 1; the Pox default priority ‚Äì which will be used (implicitly) 816 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 for later, more-speciÔ¨Åc Ô¨Çow-table entries ‚Äì is 32768. The Ô¨Årst step is to arrange for table 0 to forward to the controller and to table 1. msg = nx.nx_flow_mod() msg.table_id = 0 # not necessary as this is the default msg.priority = 1 # low priority msg.actions.append(of.ofp_action_output(port = of.OFPP_CONTROLLER)) msg.actions.append(nx.nx_action_resubmit.resubmit_table(table = 1)) event.connection.send(msg) Next we tell table 1 to Ô¨Çood packets by default: msg = nx.nx_Ô¨Çow_mod() msg.table_id = 1 msg.priority = 1 msg.actions.append(of.ofp_action_output(port = of.OFPP_FLOOD)) event.connection.send(msg) Now we deÔ¨Åne the PacketIn handler. First comes the table 0 match on the packet source; if there is a match, then the source address has been seen by the controller, and so the packet is no longer forwarded to the controller (it is forwarded to table 1 only). msg = nx.nx_flow_mod() msg.table_id = 0 msg.match.of_eth_src = packet.src # match the source msg.actions.append(nx.nx_action_resubmit.resubmit_table(table = 1)) event.connection.send(msg) Now comes table 1, where we match on the destination address. All we know at this point is that the packet with source address packet.src came from port event.port, and we forward any packets addressed topacket.src via that port: msg = nx.nx_Ô¨Çow_mod() msg.table_id = 1 msg.match.of_eth_dst = packet.src # this rule applies only for packets topacket.src msg.actions.append(of.ofp_action_output(port = event.port)) event.connection.send(msg) Note that there is no network state maintained at the controller; there is no analog here of the table dictionary of l2_pairs.py. Suppose we have a simple network h1‚Äìs1‚Äìh2. When h1 sends to h2, the controller will add to s1‚Äôs table 0 an entry indicating that h1 is a known source address. It will also add to s1‚Äôs table 1 an entry indicating that h1 is reachable via the port on s1‚Äôs left. Similarly, when h2 replies, s1 will have h2 added to its table 0, and then to its table 1. 30.9.4 multitrunk.py The goal of the multitrunk example is to illustrate how different TCP connections between two hosts can be routed via different paths; in this case, via different ‚Äútrunk lines‚Äù. This example and the next are not part of the standard distributions of either Mininet or Pox. Unlike the other examples discussed here, these examples consist of Mininet code to set up a speciÔ¨Åc network topology and a corresponding Pox controller module that is written to work properly only with that topology. Most real networks evolve with time, making such a tight link between topology and controller impractical (though this may sometimes work well 30.9 OpenFlow and the POX Controller 817
An Introduction to Computer Networks, Release 2.0.11 in datacenters). The purpose here, however, is to illustrate speciÔ¨Åc OpenFlow possibilities in a (relatively) simple setting. The multitrunk topology involves multiple ‚Äútrunk lines‚Äù between host h1andh2, as in the following diagram; the trunk lines are the s1‚Äìs3ands2‚Äìs4links. s5s2 s1h1 h2 s6 s3s4 Multitrunk topology, N=1, K=2No packets flooded along this link The Mininet Ô¨Åle is multitrunk12.py and the corresponding Pox module is multitrunkpox.py. The number of trunk lines is K=2 by default, but can be changed by setting the variable K. We will prevent looping of broadcast trafÔ¨Åc by never Ô¨Çooding along the s2‚Äìs4link. TCP trafÔ¨Åc takes either the s1‚Äìs3trunk or the s2‚Äìs4trunk. We will refer to the two directions h1√ëh2 andh2√ëh1of a TCP connection as Ô¨Çows, consistent with the usage in 11.1 The IPv6 Header. Only h1√ëh2Ô¨Çows will have their routing vary; Ô¨Çows h2√ëh1will always take the s1‚Äìs3path. It does not matter if the original connection is opened from h1toh2or fromh2toh1. The Ô¨Årst TCP Ô¨Çow from h1toh2goes vias1‚Äìs3. After that, subsequent connections alternate in roundrobin fashion between s1‚Äìs3ands2‚Äìs4. To achieve this we must, of course, include TCP ports in the OpenFlow forwarding information. All links will have a bandwidth set in Mininet. This involves using the link=TCLink option; TC here stands for TrafÔ¨Åc Control. We do not otherwise make use of the bandwidth limits. TCLinks can also have a queue size set, as in 30.7.5 TCP Compeition: Reno vs Vegas. For ARP and ICMP trafÔ¨Åc, two OpenFlow tables are used as in 30.9.3 l2_nx.py. ThePacketIn messages for ARP and ICMP packets are how switches learn of the MAC addresses of hosts, and also how the controller learns which switch ports are directly connected to hosts. TCP trafÔ¨Åc is handled differently, below. During the initial handling of ConnectionUp messages, switches receive their default packet-handling instructions for ARP and ICMP packets, and a SwitchNode object is created in the controller for each switch. These objects will eventually contain information about what neighbor switch or host is reached by each switch port, but at this point none of that information is yet available. The next step is the handling of LinkEvent messages, which are initiated by the discovery module. This module must be included on the ./pox.py command line in order for this example to work. The discovery module sends each switch, as it connects to the controller, a special discovery packet in the Link Layer Discovery Protocol (LLDP) format; this packet includes the originating switch‚Äôs dpid value and the switch port by which the originating switch sent the packet. When an LLDP packet is received by 818 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 the neighboring switch, that switch forwards it back to the controller, together with the dpid and port for the receiving switch. At this point the controller knows the switches and port numbers at each end of the link. The controller then reports this to our multitrunkpox module via a LinkEvent event. AsLinkEvent messages are processed, the multitrunkpox module learns, for each switch, which ports connect directly to neighboring switches. At the end of the LinkEvent phase, which generally takes several seconds, each switch‚Äôs SwitchNode knows about all directly connected neighbor switches. Nothing is yet known about directly connected neighbor hosts though, as hosts have not yet sent any packets. Once hosts h1andh2exchange a pair of packets, the associated PacketIn events tellmultitrunkpox what switch ports are connected to hosts. Ethernet address learning also takes place. If we execute h1 ping h2, for example, then afterwards the information contained in the SwitchNode graph is complete. Now suppose h1tries to open a TCP connection to h2,egvia ssh. The Ô¨Årst packet is a TCP SYN packet. The switch s5will see this packet and forward it to the controller, where the PacketIn handler will process it. We create a Ô¨Çow for the packet, flow = Flow(psrc, pdst, ipv4.srcip, ipv4.dstip, tcp.srcport, tcp.dstport) and then see if a path has already been assigned to this Ô¨Çow in the dictionary flow_to_path. For the very Ô¨Årst packet this will never be the case. If no path exists, we create one, Ô¨Årst picking a trunk: trunkswitch = picktrunk(flow) path = findpath(flow, trunkswitch) The Ô¨Årst path will be the Python list [h1, s5, s1, s3, s6, h2], where the switches are represented by SwitchNode objects. The supposedly Ô¨Ånal step is to call result = create_path_entries(flow, path) to create the forwarding rules for each switch. With the path as above, the SwitchNode objects know what ports5should use to reach s1,etc. Because the Ô¨Årst TCP SYN packet must have been preceeded by an ARP exchange, and because the ARP exchange willresult ins6learning what port to use to reach h2, thisshould work. But in fact it does not, at least not always. The problem is that Pox creates separate internal threads for the ARP-packet handling and the TCP-packet handling, and the former thread may not yet have installed the location of h2into the appropriate SwitchNode object by the time the latter thread calls create_path_entries() and needs the location of h2. This race condition is unfortunate, but cannot be avoided. As a fallback, if creating a path fails, we Ô¨Çood the TCP packet along the s1‚Äìs3link (even if the chosen trunk is the s2‚Äìs4link) and wait for the next TCP packet to try again. Very soon, s6willknow how to reach h2, and socreate_path_entries() will succeed. If we run everything, create two xterms on h1, and then create two ssh connections to h2, we can see the forwarding entries using ovs-ofctl. Let us run ovs-ofctl dump-flows s5 Restricting attention only to those Ô¨Çow entries with foo=tcp, we get (with a little sorting) 30.9 OpenFlow and the POX Controller 819
An Introduction to Computer Networks, Release 2.0.11 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02,nw_src=10.0.0.1,nw_dst=10.0.0.2, tp_src=59404 ,tp_dst=22 actions=output:1 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02,nw_src=10.0.0.1,nw_dst=10.0.0.2, tp_src=59526 ,tp_dst=22 actions=output:2 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:02,dl_dst=00:00:00:00:00:01,nw_src=10.0.0.2,nw_dst=10.0.0.1,tp_src=22,tp_dst=59404 actions=output:3 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:02,dl_dst=00:00:00:00:00:01,nw_src=10.0.0.2,nw_dst=10.0.0.1,tp_src=22,tp_dst=59526 actions=output:3 The Ô¨Årst two entries represent the h1√ëh2Ô¨Çows. The Ô¨Årst connection has source TCP port 59404 and is routed via the s1‚Äìs3trunk; we can see that the output port from s5is port 1, which is indeed the port that s5 uses to reach s1 (the output of the Mininet links command includes s5-eth1<->s1-eth2 ). Similarly, the output port used at s5by the second connection, with source TCP port 59526, is 2, which is the ports5uses to reach s2. The switch s5reaches host h1via port 3, which can be seen in the last two entries above, which correspond to the reverse h2√ëh1Ô¨Çows. The OpenFlow timeout here is inÔ¨Ånite. This is not a good idea if the system is to be running indeÔ¨Ånitely, with a steady stream of short-term TCP connections. It does, however, make it easier to view connections withovs-ofctl before they disappear. A production implementation would need a Ô¨Ånite timeout, and then would have to ensure that connections that were idle for longer than the timeout interval were properly re-established when they resumed sending. The multitrunk strategy presented here can be compared to Equal-Cost Multi-Path routing, 13.7 ECMP. In both cases, trafÔ¨Åc is divided among multiple paths to improve throughput. Here, individual TCP connections are assigned a trunk by the controller (and can be reassigned at will, perhaps to improve the load balance). In ECMP, it is common to assign TCP connections to paths via a pseudorandom hash, in which case the approach here offers the potential for better control of the distribution of trafÔ¨Åc among the trunk links. In some conÔ¨Ågurations, however, ECMP may route packets over multiple links on a round-robin packet-bypacket basis rather than a connection-by-connection basis; this allows much better load balancing. OpenFlow has low-level support for this approach in the select group mechanism. A Ô¨Çow-table trafÔ¨Åcmatching entry can forward trafÔ¨Åc to a so-called group instead of out via a port. The action of a select group is then to select one of a set of output actions (often on a round-robin basis) and apply that action to the packet. In principle, we could implement this at s5to have successive packets sent to either s1ors2in round-robin fashion. In practice, Pox support for select groups appears to be insufÔ¨Åciently developed at the time of this writing (2017) to make this practical. 30.9.5 loadbalance31.py The next example demonstrates a simple load balancer. The topology is somewhat the reverse of the previous example: there are now three hosts (N=3) at each end, and only one trunk line (K=1) (there are also no leftand right-hand entry/exit switches). The right-hand hosts act as the ‚Äúservers‚Äù, and are renamed t1,t2andt3. 820 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 r st1 h1 h2 h3t2 t310.0.0.1/2410.0.0.1/24 10.0.0.1/2410.0.3.1/2410.0.2.1/2410.0.1.1/24 c 10.0.1.2/24 10.0.3.2/2410.0.2.2/2410.0.0.2/24 The servers all get the same IPv4 address, 10.0.0.1. This would normally lead to chaos, but the servers are not allowed to talk to one another, and the controller ensures that the servers are not even aware of one another. In particular, the controller makes sure that the servers never all simultaneously reply to an ARP ‚Äúwho-has 10.0.0.1‚Äù query from r. The Mininet Ô¨Åle is loadbalance31.py and the corresponding Pox module is loadbalancepox.py. The noderis a router, not a switch, and so its four interfaces are assigned to separate subnets. Each host is on its own subnet, which it shares with r. The router rthen connects to the only switch, s; the connection fromsto the controller cis shown. The idea is that each TCP connection from any of the hi to 10.0.0.1 is connected, via s, to one of the servers ti, but different connections will connect to different servers. In this implementation the server choice is round-robin, so the Ô¨Årst three TCP connections will connect to t1,t2andt3respectively, and the fourth will connect again to t1. The servers t1throught3are conÔ¨Ågured to all have the same IPv4 address 10.0.0.1; there is no address rewriting done to packets arriving from the left. However, as in the preceding example, when the Ô¨Årst packet of each new TCP connection from left to right arrives at s, it is forwarded to cwhich then selects a speciÔ¨Åc tiand creates in sthe appropriate forwarding rule for that connection. As in the previous example, each TCP connection involves two Flow objects, one in each direction, and separate OpenFlow forwarding entries are created for each Ô¨Çow. There is no need for paths; the main work of routing the TCP connections looks like this: server = pickserver(flow) flow_to_server[flow] = server addTCPrule(event.connection, flow, server+1) # ti is at port i+1 addTCPrule(event.connection, flow.reverse(), 1) # port 1 leads to r The biggest technical problem is ARP: normally, rand thetiwould contact one another via ARP to Ô¨Ånd the appropriate LAN addresses, but that will not end well with identical IPv4 addresses. So instead we create ‚Äústatic‚Äù ARP entries. We know (by checking) that the MAC address of r-eth0 is 00:00:00:00:00:04, and so the Mininet Ô¨Åle runs the following command on each of the ti: arp -s 10.0.0.2 00:00:00:00:00:04 This creates a static ARP entry on each of the ti, which leaves them knowing the MAC address for their default router 10.0.0.2. As a result, none of them issues an ARP query to Ô¨Ånd r. The other direction is 30.9 OpenFlow and the POX Controller 821
An Introduction to Computer Networks, Release 2.0.11 similar, except that r(which is not really in on the load-balancing plot) must think 10.0.0.1 has a single MAC address. Therefore, we give each of the tithe same MAC address (which would normally lead to even more chaos than giving them all the same IPv4 address); that address is 00:00:00:00:01:ff. We then install a permanent ARP entry on rwith arp -s 10.0.0.1 00:00:00:00:01:ff Now, when h1, say, sends a TCP packet to 10.0.0.1, rforwards it to MAC address 00:00:00:00:01:ff, and thensforwards it to whichever of t1..t3it has been instructed by the controller cto forward it to. The packet arrives at tiwith the correct IPv4 address (10.0.0.1) and correct MAC address (00:00:00:00:01:ff), and so is accepted. Replies are similar: tisends torat MAC address 00:00:00:00:00:04. As part of the ConnectionUp processing, we set up rules so that ICMP packets from the left are always routed tot1. This way we have a single responder to ping requests. It is entirely possible that some important ICMP message ‚Äì egFragmentation required but DF flag set ‚Äì will be lost as a result. If we run the programs and create xterm windows for h1, h2 and h3 and, from each, connect to 10.0.0.1 via ssh, we can tell that we‚Äôve reached t1,t2ort3respectively by running ifconfig. The Ethernet interface on t1is named t 1-eth0, and similarly for t2andt3. (Finding another way to distinguish the tiis not easy.) An even simpler way to see the connection rotation is to run h1 ssh 10.0.0.1 ifconfig at themininet> prompt several times in succession, and note the successive interface names. If we create three connections and then run ovs-ofctl dump-flows s and look at tcp entries with destination address 10.0.0.1, we get this: cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:04,dl_dst=00:00:00:00:01:ff, nw_src=10.0.1.1 ,nw_dst=10.0.0.1,tp_src=35110,tp_dst=22 actions= output:2 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:04,dl_dst=00:00:00:00:01:ff, nw_src=10.0.2.1 ,nw_dst=10.0.0.1,tp_src=44014,tp_dst=22 actions= output:3 cookie=0x0,. .. , tcp,dl_src=00:00:00:00:00:04,dl_dst=00:00:00:00:01:ff, nw_src=10.0.3.1 ,nw_dst=10.0.0.1,tp_src=55598,tp_dst=22 actions= output:4 The three different Ô¨Çows take output ports 2, 3 and 4 on s, corresponding to t1,t2andt3. 30.9.6 l2_multi.py This Ô¨Ånal Pox controller example takes an arbitrary Mininet network, learns the topology, and then sets up OpenFlow rules so that all trafÔ¨Åc is forwarded by the shortest path, as measured by hopcount. OpenFlow packet-forwarding rules are set up on demand, when trafÔ¨Åc between two hosts is Ô¨Årst seen. This module is compatible with topologies with loops, provided the spanning_tree module is also loaded. We start with the spanning_tree module. This uses the openflow.discovery module, as in 30.9.4 multitrunk.py, to build a map of all the connections, and then runs the spanning-tree algorithm of 3.1 Spanning Tree Algorithm and Redundancy. The result is a list of switch ports on which Ô¨Çooding should not occur; Ô¨Çooding is then disabled by setting the OpenFlow NO_FLOOD attribute on these ports. We can see the ports of a switch sthat have been disabled via NO_FLOOD by usingovs-ofctl show s. 822 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 One nicety is that the spanning_tree module is never quite certain when the network is complete. Therefore, it recalculates the spanning tree after every LinkEvent. We can see the spanning_tree module in action if we create a Mininet network of four switches in a loop, as in exercise 9.0 below, and then run the following: ./pox.py openflow.discovery openflow.spanning_tree forwarding.l2_pairs If we runovs-ofctl show for each switch, we get something like the following: s1: (s1-eth2):. .. NO_FLOOD s2: (s2-eth2):. .. NO_FLOOD We can verify with the Mininet links command that s1-eth2 and s2-eth2 are connected interfaces. We can verify with tcpdump -i s1-eth2 that no packets are endlessly circulating. We can also verify, with ovs-ofctl dump-flows, that thes1‚Äìs2link is not used at all, not even fors1‚Äìs2trafÔ¨Åc. This is not surprising; the l2_pairs learning strategy learns ultimately learns source addresses from Ô¨Çooded ARP packets, which are not sent along the s1‚Äìs2link. Ifs1hears nothing from s2, it will never learn to send anything to s2. Thel2_multi module, on the other hand, creates a full map of all network links (separate from the map created by the spanning_tree module), and then calculates the best route between each pair of hosts. To calculate the routes, l2_multi uses the Floyd-Warshall algorithm (outlined below), which is a form of the distance-vector algorithm optimized for when a full network map is available. (The shortest-path algorithm of 13.5.1 Shortest-Path-First Algorithm might be a faster choice.) To avoid having to rebuild the forwarding map on each LinkEvent, l2_multi does not create any routes until it sees the Ô¨Årst packet (not counting LLDP packets). By that point, usually the network is stable. If we run the example above using the Mininet rectangle topology, we again Ô¨Ånd that the spanning tree has disabled Ô¨Çooding on the s1‚Äìs2link. However, if we have h1pingh2, we see that h1√ëh2trafÔ¨Åc does take thes1‚Äìs2link. Here is part of the result of ovs-ofctl dump-flows s1: cookie=0x0,. .. , priority=65535,icmp,in_port=1,.. . ,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02,nw_src=10.0.0.1,nw_dst=10.0.0.2,.. ., icmp_type=8. .. actions= output:2 cookie=0x0,. .. , priority=65535,icmp,in_port=1,.. . 0,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02,nw_src=10.0.0.1,nw_dst=10.0.0.2,.. ., icmp_type=0. .. actions= output:2 Note thatl2_multi creates separate Ô¨Çow-table rules not only for ARP and ICMP, but also for ping (icmp_type=8) and ping reply (icmp_type=0). Such Ô¨Åne-grained matching rules are a matter of preference. Here is a brief outline of the Floyd-Warshall algorithm. We assume that the switches are numbered {1,.. . ,N}. The outer loop has the form for k<=N:; at the start of stage k, we assume that we‚Äôve found the best path between every i and j for which every intermediate switch on the path is less than k. For many (i,j) pairs, there may be no such path. At stage k, we examine, with an inner loop, all pairs (i,j). We look to see if there is a path from i to k and a second path from k to j. If there is, we concatenate the i-to-k and k-to-j paths to create a new i-to-j path, which we will call P. If there was no previous i-to-j path, then we add P. If there was a previous i-to-j path Q that is longer than P, we replace Q with P. At the end of the k=N stage, all paths have been discovered. 30.9 OpenFlow and the POX Controller 823
An Introduction to Computer Networks, Release 2.0.11 30.10 Exercises Exercises may be given fractional (Ô¨Çoating point) numbers, to allow for interpolation of new exercises. Exercise 2.5 is distinct, for example, from exercises 2.0 and 3.0. Exercises marked with a ‚ô¢have solutions or hints at 34.18 Solutions for Mininet. 1.0. In the RIP implementation of 30.5 IP Routers With Simple Distance-Vector Implementation, add Split Horizon ( 13.2.1.1 Split Horizon ). 2.0. In the RIP implementation of 30.5 IP Routers With Simple Distance-Vector Implementation, add support for link failures (the third rule of 13.1.1 Distance-Vector Update Rules ) 3.0. Explain why, in the example of 30.9.3 l2_nx.py, table 0 and table 1 will always have the same entries. 4.0. Suppose we try to eliminate the source addresses from the l2_pairs implementation. 
- by default, all switches report all packets to the controller, and the controller then tells the switch to Ô¨Çood the packet. 
- if a packet from ha to hb arrives at switch S, and S reports the packet to the controller, and the controller knows how to reach hb from S, then the controller installs forwarding rules into S for destination hb. The controller then tells S to re-forward the packet. In the future, S will not report packets to hb to the controller. 
- when S reports to the controller a packet from ha to hb, then the controller notes that ha is reachable via the port on S by which the packet arrived. Why does this not work? Hint: consider the switchline example ( 30.3 Multiple Switches in a Line ), with h1 sending to h4, h4 sending to h1, h3 sending to h1, and Ô¨Ånally h1 sending to h3. 5.0. Suppose we make the following change to the above strategy: 
- if a packet from ha to hb arrives at switch S, and S reports the packet to the controller, and the controller knows how to reach both ha and hb from S, then the controller installs forwarding rules into S for destinations ha and hb. The controller then tells S to re-forward the packet. In the future, S will not report packets to ha or hb to the controller. Show that this still does not work for the switchline example. 6.0. Suppose we try to implement an Ethernet switch as follows: 
- the default switch action for an unmatched packet is to Ô¨Çood it and send it to the controller. 
- if a packet from ha to hb arrives at switch S, and S reports the packet to the controller, and the controller knows how to reach both ha and hb from S, then the controller installs forwarding rules into S for destinations ha and hb. In the future, S will not report packets with these destinations to the controller. 
- Unlike in exercise 4.0, the controller then tells S to Ô¨Çood the packet from ha to hb, even though it could be forwarded directly. TrafÔ¨Åc is sent in the network below: 824 30 Mininet
An Introduction to Computer Networks, Release 2.0.11 h1 h2 h3 s1s2s3 (a)‚ô¢. Show that, if the trafÔ¨Åc is as follows: h1 pings h2, h3 pings h1, then all three switches learn where h3 is. (b). Show that, if the trafÔ¨Åc is as follows: h1 pings h2, h1 pings h3, then none of the switches learn where h3 is. Recall that each ping for a new destination starts with a broadcast ARP. Broadcast packets are always sent to the controller, as there is no destination match. 7.0. In 30.9.5 loadbalance31.py, we could have conÔ¨Ågured the tito have default router 10.0.0.3, say, and then created the appropriate static ARP entry for 10.0.0.3: ip route add to default via 10.0.0.3 dev ti-eth0 arp -s 10.0.0.3 00:00:00:00:00:04 Everything still works, even though the tithink their router is at 10.0.0.3 and it is actually at 10.0.0.2. Explain why. (Hint: how is the router IPv4 address actually used by the ti?) 8.0. As discussed in the text, a race condition can arise in the example of 30.9.4 multitrunk.py, where at the time the Ô¨Årst TCP packet the controller still does not know where h2is, even though it should learn that after processing the Ô¨Årst ARP packet. Explain why a similar race condition cannot occur in 30.9.5 loadbalance31.py. 9.0. Create a Mininet network with four hosts and four switches as below: h1s1 s2h2 h4s4 s3h3 The switches should use an external controller. Now let Pox be that controller, with ./pox.py openflow.discovery openflow.spanning_tree l2_pairs.py 10.0. Create the topology below with Mininet. Run the l2_multi Pox module as controller, with the openflow.spanning_tree option, and identify the spanning tree created. Also identify the path taken by icmp trafÔ¨Åc from h1toh2. 30.10 Exercises 825
An Introduction to Computer Networks, Release 2.0.11 h1 s1 s2 s3 s4 s5 s6 s7 s8 s9 h2 Between the idea And the reality Between the motion And the act Falls the Shadow ‚Äì TS Eliot, The Hollow Men Try to leave out the part that readers tend to skip. ‚Äì Elmore Leonard, 10 Rules for Writing 826 30 Mininet
31 NETWORK SIMULATIONS: NS-2 In previous chapters, especially 20 Dynamics of TCP, we have at times made simplifying assumptions about TCP Reno trafÔ¨Åc. In the present chapter we will look at actual TCP behavior, through simulation, enabling us to explore the accuracy of some of these assumptions. The primary goal is to provide comparison between idealized TCP behavior and the often-messier real behavior; a secondary goal is perhaps to shed light on some of the issues involved in simulation. For example, in the discussion in 31.3 Two TCP Senders Competing of competition between TCP Reno connections with different RTTs, we address several technical issues that affect the relevance of simulation results. Parts of this chapter may serve as a primer on using the ns-2 simulator, though a primer focused on the goal of illuminating some of the basic operation and theory of TCP through experimentation. However, some of the outcomes described may be of interest even to those not planning on designing their own simulations. Simulation is frequently used in the literature when comparing different TCP Ô¨Çavors for effective throughput (for example, the graphs excerpted in 22.15 TCP CUBIC were created through simulation). An alternative to simulation, network emulation, involves running actual system networking software in a multi-node environment created through containerization or virtualization; we present this in 30 Mininet. An important advantage of simulation over emulation, however, is that as emulations get large and complex they also get bogged down, and it can be hard to distinguish results from artifacts. We begin this chapter by looking at a single connection and analyzing how well the TCP sawtooth utilizes the bottleneck link. We then turn to competition between two TCP senders. The primary competition example here is between TCP Reno connections with different RTTs. This example allows us to explore the synchronized-loss hypothesis ( 20.3.4 Synchronized-Loss Hypothesis ) and to introduce phase effects, transient queue overÔ¨Çows, and other unanticipated TCP behavior. We also introduce some elements of designing simulation experiments. The second example compares TCP Reno and TCP Vegas. We close with a straightforward example of a wireless simulation. 31.1 The ns-2 simulator The tool used for much research-level network simulations is ns, for network simulator and originally developed at the Information Sciences Institute. The ns simulator grew out of the REAL simulator developed by Srinivasan Keshav [SK88]; later development work was done by the Network Research Group at the Lawrence Berkeley National Laboratory. We will describe in this chapter the ns-2 simulator, hosted at www.isi.edu/nsnam/ns. There is now also an ns-3 simulator, available at www.nsnam.org. Because ns-3 is not backwards-compatible with ns-2 and the programming interface has changed considerably, we take the position that ns-3 is an entirely different package, though one likely someday to supercede ns-2 entirely. While there is a short introduction to ns-3 in this book ( 32 The ns-3 Network Simulator ), its use is arguably quite a bit more complicated for beginners, and the particular simulation examples presented below are well-suited to ns-2. While ns-3 supports more complex and realistic modeling, and is the tool of choice for serious research, this added complexity comes at a price in terms of conÔ¨Åguration and programming. The standard ns-2 traceÔ¨Åle format is also quite easy to work with using informal scripting. 827
An Introduction to Computer Networks, Release 2.0.11 Research-level use of ns-2 often involves building new modules in C++, and compiling them in to the system. For our purposes here, the stock ns-2 distribution is sufÔ¨Åcient. The simplest way to install ns-2 is probably with the ‚Äúallinone‚Äù distribution, which does still require compiling but comes with a very simple install script. (As of 2014, the ns-allinone-2.35 version appeared to compile only with gcc/g++ no more recent than version 4.6.) The native environment for ns-2 (and ns-3) is Linux. Perhaps the simplest approach for Windows users is to install a Linux virtual machine, and then install ns-2 under that. It is also possible to compile ns-2 under the Cygwin system; an older version of ns-2 may still be available as a Cygwin binary. To create an ns-2 simulation, we need to do the following (in addition to a modest amount of standard housekeeping). 
- deÔ¨Åne the network topology, including all nodes, links and router queuing rules 
- create some TCP (or UDP) connections, called Agents, and attach them to nodes 
- create some Applications ‚Äì usually FTP for bulk transfer or telnet for intermittent random packet generation ‚Äì and attach them to the agents 
- start the simulation Once started, the simulation runs for the designated amount of time, driven by the packets generated by the Application objects. As the simulated applications generate packets for transmission, the ns-2 system calculates when these packets arrive and depart from each node, and generates simulated acknowledgment packets as appropriate. Unless delays are explicitly introduced, node responses ‚Äì such as forwarding a packet or sending an ACK ‚Äì are instantaneous. That is, if a node begins sending a simulated packet from node N1 to N2 at time T=1.000 over a link with bandwidth 60 ms per packet and with propagation delay 200 ms, then at time T=1.260 N2 will have received the packet. N2 will then respond at that same instant, if a response is indicated, egby enqueuing the packet or by forwarding it if the queue is empty. Ns-2 does not necessarily require assigning IP addresses to every node, though this is possible in more elaborate simulations. Advanced use of ns-2 (and ns-3) often involves the introduction of randomization; for example, we will in 31.3 Two TCP Senders Competing introduce both random sliding-windows delays and trafÔ¨Åc generators that release packets at random times. While it is possible to seed the random-number generator so that different runs of the same experiment yield different outcomes, we will not do this here, so the randomnumber generator will always produce the same sequence. A consequence is that the same ns-2 script should yield exactly the same result each time it is run. 31.1.1 Using ns-2 The scripting interface for ns-2 uses the language Tcl, pronounced ‚Äútickle‚Äù; more precisely it is object-Tcl, or OTcl. For simple use, learning the general Tcl syntax is not necessary; one can proceed quite successfully by modifying standard examples. The network simulation is deÔ¨Åned in a Tcl Ô¨Åle, perhaps sim.tcl; to run the simulation one then runs the command ns sim.tcl 828 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 The result of running the ns-2 simulation will be to create some Ô¨Åles, and perhaps print some output. The most common Ô¨Åles created are the ns-2 trace Ô¨Åle ‚Äì perhaps sim.tr ‚Äì which contains a record for every packet arrival, departure and queue event, and a Ô¨Åle sim.nam for the network animator, nam, that allows visual display of the packets in motion (the ns-3 version of nam is known as NetAnim). The sliding-windows video in 8.2 Sliding Windows was created with nam. Within Tcl, variables can be assigned using the set command. Expressions in [ ... ] are evaluated. Numeric expressions must also use the expr command: set foo [expr $foo + 1] As in unix-style shell scripting, the value of a variable X is $X; the name X (without the $) is used when setting the value (in Perl and PHP, on the other hand, many variable names begin with $, which is included both when evaluating and setting the variable). Comments are on lines beginning with the ‚Äò#‚Äô character. Comments can notbe appended to a line that contains a statement (although it is possible Ô¨Årst to start a new logical line with ;). Objects in the simulation are generally created using built-in constructors; the constructor in the line below is the part in the square brackets (recall that the brackets must enclose an expression to be evaluated): set tcp0 [new Agent/TCP/Reno] Object attributes can then be assigned values; for example, the following sets the data portion of the packets in TCP connection tcp0 to 960 bytes: $tcp0 set packetSize_ 960 Object attributes are retrieved using set without a value; the following assigns variable ack0 the current value of the ack_ Ô¨Åeld oftcp0: set ack0 [$tcp0 set ack_] Thegoodput of a TCP connection is, properly, the number of application bytes received. This differs from thethroughput ‚Äì the total bytes sent ‚Äì in two ways: the latter includes both packet headers and retransmitted packets. The ack0 value above includes no retransmissions; we will occasionally refer to it as ‚Äúgoodput‚Äù in this sense. 31.2 A Single TCP Sender For our Ô¨Årst script we demonstrate a single sender sending through a router. Here is the topology we will build, with the delays and bandwidths: A R B10 Mbps 10 ms delay 50 ms delay800 kbps = 100 pkt/s The smaller bandwidth on the R‚ÄìB link makes it the bottleneck. The default TCP packet size in ns2 is 1000 bytes, so the bottleneck bandwidth is nominally 100 packets/sec or 0.1 packets/ms. The 31.2 A Single TCP Sender 829
An Introduction to Computer Networks, Release 2.0.11 bandwidthRTT noLoad product is 0.1 packets/ms 120 ms = 12 packets. Actually, the default size of 1000 bytes refers to the data segment, and there are an additional 40 bytes of TCP and IP header. We therefore setpacketSize_ to 960 so the actual transmitted size is 1000 bytes; this makes the bottleneck bandwidth exactly 100 packets/sec. We want the router R to have a queue capacity of 6 packets, plus the one currently being transmitted; we setqueue-limit = 7 for this. We create a TCP connection between A and B, create an ftp sender on top that, and run the simulation for 20 seconds. The nodes A, B and R are named; the links are not. The ns-2 default maximum window size is 20; we increase that to 100 with $tcp0 set window_ 100; otherwise we will see an artiÔ¨Åcial cap on the cwnd growth (in the next section we will increase this to 65000). The script itself is in a Ô¨Åle basic1.tcl, with the 1 here signifying a single sender. # basic1.tcl simulation: A---R---B #Create a simulator object set ns [new Simulator] #Open the nam file basic1.nam and the variable-trace file basic1.tr set namfile [open basic1.nam w] $ns namtrace-all $namfile set tracefile [open basic1.tr w] $ns trace-all $tracefile #Define a 'finish' procedure proc finish {} { global ns namfile tracefile $ns flush-trace close $namfile close $tracefile exit 0 } #Create the network nodes set A [$ns node] set R [$ns node] set B [$ns node] #Create a duplex link between the nodes $ns duplex-link $A $R 10Mb 10ms DropTail $ns duplex-link $R $B 800Kb 50ms DropTail # The queue size at $R is to be 7, including the packet being sent $ns queue-limit $R $B 7 # some hints for nam # color packets of flow 0 red $ns color 0 Red $ns duplex-link-op $A $R orient right $ns duplex-link-op $R $B orient right (continues on next page) 830 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) $ns duplex-link-op $R $B queuePos 0.5 # Create a TCP sending agent and attach it to A set tcp0 [new Agent/TCP/Reno] # We make our one-and-only flow be flow 0 $tcp0 set class_ 0 $tcp0 set window_ 100 $tcp0 set packetSize_ 960 $ns attach-agent $A $tcp0 # Let's trace some variables $tcp0 attach $tracefile $tcp0 tracevar cwnd_ $tcp0 tracevar ssthresh_ $tcp0 tracevar ack_ $tcp0 tracevar maxseq_ #Create a TCP receive agent (a traffic sink) and attach it to B set end0 [new Agent/TCPSink] $ns attach-agent $B $end0 #Connect the traffic source with the traffic sink $ns connect $tcp0 $end0 #Schedule the connection data flow; start sending data at T=0, stop at T=10.0 set myftp [new Application/FTP] $myftp attach-agent $tcp0 $ns at 0.0 "$myftp start" $ns at 10.0 "finish" #Run the simulation $ns run After running this script, there is no command-line output (because we did not ask for any); however, the Ô¨Åles basic1.tr and basic1.nam are created. Perhaps the simplest thing to do at this point is to view the animation with nam, using the command nam basic1.nam. In the animation we can see slow start at the beginning, as Ô¨Årst one, then two, then four and then eight packets are sent. A little past T=0.7, we can see a string of packet losses. This is visible in the animation as a tumbling series of red squares from the top of R‚Äôs queue. After that, the TCP sawtooth takes over; we alternate between the cwnd linear-increase phase (congestion avoidance), packet loss, and threshold slow start. During the linear-increase phase the bottleneck link is at Ô¨Årst incompletely utilized; once the bottleneck link is saturated the router queue begins to build. 31.2.1 Graph of cwnd v time Here is a graph of cwnd versus time, prepared (see below) from data in the trace Ô¨Åle basic1.tr: 31.2 A Single TCP Sender 831
An Introduction to Computer Networks, Release 2.0.11 Slow start is at the left edge. Unbounded slow start runs until about T=0.75, when a timeout occurs; bounded slow start runs from about T=1.2 to T=1.8. After that, all losses have been handled with fast recovery (we can tell this because cwnd does not drop below half its previous peak). The Ô¨Årst three teeth following slow start have heights ( cwnd peak values) of 20.931, 20.934 and 20.934 respectively; when the simulation is extended to 1000 seconds all subsequent peaks have exactly the same height, cwnd = 20.935. The spacing between the peaks is also constant, 1.946 seconds. Becausecwnd is incremented by ns-2 after each arriving ACK as described in 19.2.1 TCP Reno PerACK Responses, during the linear-increase phase there are a great many data points jammed together; the bunching effect is made stronger by the choice here of a large-enough dot size to make the slow-start points clearly visible. This gives the appearance of continuous line segments. Upon close examination, these line segments are slightly concave, as discussed in 22.5 Highspeed TCP, due to the increase in RTT as the queue Ô¨Ålls. Individual Ô¨Çights of packets can just be made out at the lower-left end of each tooth, especially the Ô¨Årst. 31.2.2 The Trace File To examine the simulation (or, for that matter, the animation) more quantitatively, we turn to a more detailed analysis of the trace Ô¨Åle, which contains records for all packet events plus (because it was requested) variable-trace information for cwnd_ ,ssthresh_ ,ack_ andmaxseq_; these were the variables for which we requested traces in the basic1.tcl Ô¨Åle above. The bulk of the trace-Ô¨Åle lines are event records; three sample records are below. (These are in the default event-record format for point-to-point links; ns-2 has other event-record formats for wireless. See use-newtrace in31.6 Wireless Simulation below.) r 0.58616 0 1 tcp 1000 ------- 0 0.0 2.0 28 43 + 0.58616 1 2 tcp 1000 ------- 0 0.0 2.0 28 43 d 0.58616 1 2 tcp 1000 ------- 0 0.0 2.0 28 43 832 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 The twelve event-record Ô¨Åelds are as follows: 1.rfor received, dfor dropped, +for enqueued, -for dequeued. Every arriving packet is enqueued, even if it is immediately dequeued. The third packet above was the Ô¨Årst dropped packet in the entire simulation. 2. the time, in seconds. 3. the number of the sending node, in the order of node deÔ¨Ånition and starting at 0. If the Ô¨Årst Ô¨Åeld was ‚Äú+‚Äù, ‚Äú-‚Äù or ‚Äúd‚Äù, this is the number of the node doing the enqueuing, dequeuing or dropping. Events beginning with ‚Äú-‚Äù represent this node sending the packet. 4. the number of the destination node. If the Ô¨Årst Ô¨Åeld was ‚Äúr‚Äù, this record represents the packet‚Äôs arrival at this node. 5. the protocol. 6. the packet size, 960 bytes of data (as we requested) plus 20 of TCP header and 20 of IP header. 7. some TCP Ô¨Çags, here represented as ‚Äú ------- ‚Äù because none of the Ô¨Çags are set. Flags include E and N for ECN and A for reduction in the advertised winsize. 8. the Ô¨Çow ID. Here we have only one: Ô¨Çow 0. This value can be set via the fid_ variable in the Tcl source Ô¨Åle; an example appears in the two-sender version below. The same Ô¨Çow ID is used for both directions of a TCP connection. 9. the source node (0.0), in form (node. connectionID). ConnectionID numbers here are simply an abstraction for connection endpoints; while they superÔ¨Åcially resemble port numbers, the node in question need not even simulate IP, and each connection has a unique connectionID at each end. ConnectionID numbers start at 0. 10. the destination node (2.0), again with connectionID. 11. the packet sequence number as a TCP packet, starting from 0. 12. a packet identiÔ¨Åer uniquely identifying this packet throughout the simulation; when a packet is forwarded on a new link it keeps its old sequence number but gets a new packet identiÔ¨Åer. The three trace lines above represent the arrival of packet 28 at R, the enqueuing of packet 28, and then the dropping of the packet. All these happen at the same instant. Mixed in with the event records are variable-trace records, indicating a particular variable has been changed. Here are two examples from t=0.3833: 0.38333 0 0 2 0 ack_ 3 0.38333 0 0 2 0 cwnd_ 5.000 The format of these lines is 1. time 2. source node of the Ô¨Çow 3. source port (as above, an abstract connection endpoint, not a simulated TCP port) 4. destination node of the Ô¨Çow 5. destination port 31.2 A Single TCP Sender 833
An Introduction to Computer Networks, Release 2.0.11 6. name of the traced variable 7. value of the traced variable The two variable-trace records above are from the instant when the variable cwnd_ was set to 5. It was initially 1 at the start of the simulation, and was incremented upon arrival of each of ack0, ack1, ack2 and ack3. The Ô¨Årst line shows the ack counter reaching 3 (that is, the arrival of ack3); the second line shows the resultant change in cwnd_. The graph above of cwnd v time was made by selecting out these cwnd_ lines and plotting the Ô¨Årst Ô¨Åeld (time) and the last. (Recall that during the linear-increase phase cwnd is incremented by 1.0/cwnd with each arriving new ACK.) The last ack in the traceÔ¨Åle is 9.98029 0 0 2 0 ack_ 808 Since ACKs started with number 0, this means we sent 809 packets successfully. The theoretical bandwidth was 100 packets/sec 10 sec = 1000 packets, so this is about an 81% goodput. Use of the ack_ value this way tells us how much data was actually delivered. An alternative statistic is the Ô¨Ånal value of maxseq_ which represents the number of distinct packets sent; the last maxseq_ line is 9.99029 0 0 2 0 maxseq_ 829 As can be seen from the cwnd -v-time graph above, slow start ends around T=2.0. If we measure goodput from then until the end, we do a little better than 81%. The Ô¨Årst data packet sent after T=2.0 is at 2.043184; it is data packet 72. 737 packets are sent from packet 72 until packet 808 at the end; 737 packets in 8.0 seconds is a goodput of 92%. It is not necessary to use the traceÔ¨Åle to get the Ô¨Ånal values of TCP variables such as ack_ andmaxseq_; they can be printed from within the Tcl script‚Äôs finish() procedure. The following example illustrates this, where ack_ andmaxseq_ come from the connection tcp0. Theglobal line lists global variables that are to be made available within the body of the procedure; tcp0 must be among these. proc finish {} { global ns nf f tcp0 $ns flush-trace close $namfile close $tracefile set lastACK [$tcp0 set ack_] set lastSEQ [$tcp0 set maxseq_] puts stdout "final ack: $lastACK, final seq num: $lastSEQ" exit 0 } For TCP sending agents, useful member variables to set include: 
- class_: the identifying number of a Ô¨Çow 
- window_: the maximum window size; the default is much too small. 
- packetSize_: we set this to 960 above so the total packet size would be 1000. Useful member variables either to trace or to print at the simulation‚Äôs end include: 
- maxseq_: the number of the last packet sent, starting at 1 for data packets 834 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 
- ack_: the number of the last ACK received 
- cwnd_: the current value of the congestion window 
- nrexmitpack_: the number of retransmitted packets To get a count of the data actually received, we need to look at the TCPsink object, $end0 above. There is no packet counter here, but we can retrieve the value bytes_ representing the total number of bytes received. This will include 40 bytes from the threeway handshake which can either be ignored or subtracted: set ACKed [expr round ( [$end0 set bytes_] / 1000.0)] This is a slightly better estimate of goodput. In very long simulations, however, this (or any other) byte count will wrap around long before any of the packet counters wrap around. In the example above every packet event was traced, a consequence of the line $ns trace-all $trace We could instead have asked only to trace particular links. For example, the following line would request tracing for the bottleneck (R √ù√ëB) link: $ns trace-queue $R $B $trace This is often useful when the overall volume of tracing is large, and we are interested in the bottleneck link only. In long simulations, full tracing can increase the runtime 10-fold; limiting tracing only to what is actually needed can be quite important. 31.2.3 Single Losses By examining the basic1.tr Ô¨Åle above for packet-drop records, we can conÔ¨Årm that only a single drop occurs at the end of each tooth, as was argued in 19.8 Single Packet Losses. After slow start Ô¨Ånishes at around T=2, the next few drops are at T=3.963408, T=5.909568 and T=7.855728. The Ô¨Årst of these drops is of Data[254], as is shown by the following record: d 3.963408 1 2 tcp 1000 ------- 0 0.0 2.0 254 531 Like most ‚Äúreal‚Äù implementations, the ns-2 implementation of TCP increments cwnd (cwnd_ in the traceÔ¨Åle) by 1/cwnd on each new ACK ( 19.2.1 TCP Reno Per-ACK Responses ). An additional packet is sent by A whenever cwnd is increased this way past another whole number; that is, whenever Ô¨Çoor( cwnd ) increases. At T=3.95181, cwnd_ was incremented to 20.001, triggering the double transmission of Data[253] and the doomed Data[254]. At this point the RTT is around 190 ms. The loss of Data[254] is discovered by Fast Retransmit when the third dupACK[253] arrives. The Ô¨Årst ACK[253] arrives at A at T=4.141808, and the dupACKs arrive every 10 ms, clocked by the 10 ms/packet transmission rate of R. Thus, A detects the loss at T=4.171808; at this time we see cwnd_ reduced by half to 10.465; the traceÔ¨Åle times for variables are only to 5 decimal places, so this is recorded as 4.17181 0 0 2 0 cwnd_ 10.465 31.2 A Single TCP Sender 835
An Introduction to Computer Networks, Release 2.0.11 That represents an elapsed time from when Data[254] was dropped of 207.7 ms, more than one RTT. As described in 19.8 Single Packet Losses, however, A stopped incrementing cwnd_ when the Ô¨Årst ACK[253] arrived at T=4.141808. The value of cwnd_ at that point is only 20.931, not quite large enough to trigger transmission of another back-to-back pair and thus eventually a second packet drop. 31.2.4 Reading the TraceÔ¨Åle in Python Deeper analysis of ns-2 data typically involves running some sort of script on the traceÔ¨Åles; we will mostly use the Python (python3) language for this, although the awk language is also traditional. The following is the programmer interface to a simple module (library) nstrace.py: 
- nsopen (Ô¨Ålename): opens the traceÔ¨Åle 
- isEvent (): returnstrue if the current line is a normal ns-2 event record 
- isVar (): returnstrue if the current line is an ns-2 variable-trace record 
- isEOF (): returnstrue if there are no more traceÔ¨Åle lines 
- getEvent (): returns a twelve-element tuple of the ns-2 event -trace values, each cast to the correct type. The ninth and tenth values, which are node.port pairs in the traceÔ¨Åle, are returned as (node port) sub-tuples. 
- getVar (): returns a seven-element tuple of ns-2 variable-trace values 
- skipline (): skips the current line (useful if we are interested only in event records, or only in variabletrace records, and want to ignore the other type of record) We will Ô¨Årst make use of this in 31.2.6.1 Link utilization measurement; see also 31.4 TCP Loss Events and Synchronized Losses. The nstrace.py Ô¨Åle above includes regular-expression checks to verify that each traceÔ¨Åle line has the correct format, but, as these are slow, they are disabled by default. Enabling these checks is potentially useful, however, if some wireless trace records are also included. 31.2.5 The nam Animation Let us now re-examine the nam animation, in light of what can be found in the trace Ô¨Åle. At T=0.120864, the Ô¨Årst 1000-byte data packet is sent (at T=0 a 40-byte SYN packet is sent); the actual packet identiÔ¨Åcation number is 1 so we will refer to it as Data[1]. At this point cwnd_ = 2, so Data[2] is enqueued at this same time, and sent at T=0.121664 (the delay exactly matches the A‚ÄìR link‚Äôs bandwidth of 8000 bits in 0.0008 sec). The Ô¨Årst loss occurs at T=0.58616, of Data[28]; at T=0.59616 Data[30] is lost. (Data[29] was not lost because R was able to send a packet and thus make room). From T=.707392 to T=.777392 we begin a string of losses: packets 42, 44, 46, 48, 50, 52, 54 and 56. At T=0.76579 the Ô¨Årst ACK[27] makes it back to A. The Ô¨Årst dupACK[27] arrives at T=0.77576; another arrives at T=0.78576 (10 ms later, exactly the bottleneck per-packet time!) and the third dupACK arrives at T=0.79576. At this point, Data[28] is retransmitted and cwnd is halved from 29 to 14.5. At T=0.985792, the sender receives ACK[29]. DupACK[29]‚Äôs are received at T=1.077024, T=1.087024, T=1.097024 and T=1.107024. Alas, this is TCP Reno, in Fast Recovery mode, and it is not implementing 836 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 Fast Retransmit while Fast Recovery is going on (TCP NewReno in effect Ô¨Åxes this). Therefore, the connection experiences a hard timeout at T=1.22579; the last previous event was at T=1.107024. At this point ssthresh is set to 7 and cwnd drops to 1. Slow start is used up to ssthresh = 7, at which point the sender switches to the linear-increase phase. 31.2.6 Single-sender Throughput Experiments According to the theoretical analysis in 19.7 TCP and Bottleneck Link Utilization, a queue size of close to zero should yield about a 75% bottleneck utilization, a queue size such that the mean cwnd equals the transit capacity should yield about 87.5%, and a queue size equal to the transit capacity should yield close to 100%. We now test this. We Ô¨Årst increase the per-link propagation times in the basic1.tcl simulation above to 50 and 100 ms: $ns duplex-link $A $R 10Mb 50ms DropTail $ns duplex-link $R $B 800Kb 100ms DropTail The bottleneck link here is 800 kbps, or 100 kBps, or 10 ms/packet, so these propagation-delay changes mean a round-trip transit capacity of 30 packets (31 if we include the bandwidth delay at R). In the table below, we run the simulation while varying the queue-limit parameter from 3 to 30. The simulations run for 1000 seconds, to minimize the effect of slow start. Tracing is disabled to reduce runtimes. The ‚Äúreceived‚Äù column gives the number of distinct packets received by B; if the link utilization were 100% then in 1,000 seconds B would receive 100,000 packets. queue_limit received utilization %, R √ëB 3 79767 79.8 4 80903 80.9 5 83313 83.3 8 87169 87.2 10 89320 89.3 12 91382 91.4 16 94570 94.6 20 97261 97.3 22 98028 98.0 26 99041 99.0 30 99567 99.6 In ns-2, every arriving packet is Ô¨Årst enqueued, even if it is immediately dequeued, and so queue-limit cannot actually be zero. A queue-limit of 1 or 2 gives very poor results, probably because of problems with slow start. The run here with queue-limit = 3 is not too far out of line with the 75% predicted by theory for a queue-limit close to zero. When queue-limit is 10, thencwnd will range from 20 to 40, and the link-unsaturated and queue-Ô¨Ålling phases should be of equal length. This leads to a theoretical link utilization of about (75%+100%)/2 = 87.5%; our measurement here of 89.3% is in good agreement. As queue-limit continues to increase, the link utilization rapidly approaches 100%, again as expected. 31.2 A Single TCP Sender 837
An Introduction to Computer Networks, Release 2.0.11 31.2.6.1 Link utilization measurement In the experiment above we estimated the utilization of the R √ëB link by the number of distinct packets arriving at B. But packet duplicate transmissions sometimes occur as well (see 31.2.6.4 Packets that are delivered twice ); these are part of the R √ëB link utilization but are hard to estimate (nominally, most packets retransmitted by A are dropped by R, but not all). If desired, we can get an exact value of the R √ëB link utilization through analysis of the ns-2 trace Ô¨Åle. In this Ô¨Åle R is node 1 and B is node 2 and our Ô¨Çow is Ô¨Çow 0; we look for all lines of the form -time 1 2tcp 1000------- 00.0 2.0 x y that is, with Ô¨Åeld1 = ‚Äò-‚Äò, Ô¨Åeld3 = 1, Ô¨Åeld4 = 2, Ô¨Åeld6 = 1000 and Ô¨Åeld8 = 0 (if we do not check Ô¨Åeld6=1000 then we count one extra packet, a simulated SYN). We then simply count these lines; here is a simple script to do this in Python using the nstrace module above: #!/usr/bin/python3 import nstrace import sys deflink_count(filename): SEND_NODE = 1 DEST_NODE = 2 FLOW = 0 count = 0 nstrace.nsopen(filename) while not nstrace.isEOF(): ifnstrace.isEvent(): (event, time, sendnode, dest, dummy, size, dummy, flow, dummy, √£√ëdummy, dummy, dummy) = nstrace.getEvent() if(event == "-" andsendnode == SEND_NODE anddest == DEST_NODE √£√ëandsize >= 1000 andflow == FLOW): count += 1 else: nstrace.skipline() print ("packet count:", count); link_count(sys.argv[1]) For completeness, here is the same program implemented in the Awk scripting language. BEGIN {count=0; SEND_NODE=1; DEST_NODE=2; FLOW=0} $1 == "-" { if ($3 == SEND_NODE && $4 == DEST_NODE && $6 >= 1000 && $8 == √£√ëFLOW) { count++; } } END {print count;} 838 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 31.2.6.2 ns-2 command-line parameters Experiments where we vary one parameter, egqueue-limit, are facilitated by passing in the parameter value on the command line. For example, in the basic1.tcl Ô¨Åle we can include the following: set queuesize $argv ... $ns queue-limit $R $B $queuesize Then, from the command line, we can run this as follows: ns basic1.tcl 5 If we want to run this simulation with parameters ranging from 0 to 10, a simple shell script is queue=0 while [ $queue -le 10 ] do ns basic1.tcl $queue queue=$(expr $queue + 1) done If we want to pass multiple parameters on the command line, we use lindex to separate out arguments from the $argv string; the Ô¨Årst argument is at position 0 (in bash and awk scripts, by comparison, the Ô¨Årst argument is $1). For two optional parameters, the Ô¨Årst representing queuesize and the second representing endtime, we would use if { $argc >= 1 } { set queuesize [expr [lindex $argv 0]] } if { $argc >= 2 } { set endtime [expr [lindex $argv 1]] } 31.2.6.3 Queue utilization In our previous experiment we examined link utilization when queue-limit was smaller than the bandwidthdelay product. Now suppose queue-limit is greater than the bandwidth delay product, so the bottleneck link is essentially never idle. We calculated in 19.7 TCP and Bottleneck Link Utilization what we might expect as an average queue utilization. If the transit capacity is 30 packets and the queue capacity is 40 then cwnd maxwould be 70 and cwnd minwould be 35; the queue utilization would vary from 70-30 = 40 down to 35-30 = 5, averaging around (40+5)/2 = 22.5. Let us run this as an ns-2 experiment. As before, we set the A‚ÄìR and R‚ÄìB propagation delays to 50 ms and 100 ms respectively, making the RTT noLoad 300 ms, for about 30 packets in transit. We also set the queue-limit value to 40. The simulation runs for 1000 seconds, enough, as it turns out, for about 50 TCP sawteeth. At the end of the run, the following Python script maintains a running time-weighted average of the queue size. Because the queue capacity exceeds the total transit capacity, the queue is seldom empty. 31.2 A Single TCP Sender 839
An Introduction to Computer Networks, Release 2.0.11 #!/usr/bin/python3 import nstrace import sys defqueuesize(filename): QUEUE_NODE = 1 nstrace.nsopen(filename) sum = 0.0 size= 0 prevtime=0 while not nstrace.isEOF(): ifnstrace.isEvent(): # counting regular trace lines (event, time, sendnode, dnode, proto, dummy, dummy, flow, dummy, √£√ëdummy, seqno, pktid) = nstrace.getEvent() if(sendnode != QUEUE_NODE): continue if(event == "r"):continue sum += size *(time -prevtime) prevtime = time if(event== 'd'): size -= 1 elif(event== "-"): size -= 1 elif(event== "+"): size += 1 else: nstrace.skipline() print("avg queue=", sum/time) queuesize(sys.argv[1]) The answer we get for the average queue size is about 23.76, which is in good agreement with our theoretical value of 22.5. 31.2.6.4 Packets that are delivered twice Every dropped TCP packet is ultimately transmitted twice, but classical TCP theory suggests that relatively few packets are actually delivered twice. This is pretty much true once the TCP sawtooth phase is reached, but can fail rather badly during slow start. The following Python script will count packets delivered two or more times. It uses a dictionary, COUNTS, which is indexed by sequence numbers. #!/usr/bin/python3 import nstrace import sys defdup_counter(filename): SEND_NODE = 1 DEST_NODE = 2 FLOW = 0 count = 0 COUNTS = {} nstrace.nsopen(filename) (continues on next page) 840 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) while not nstrace.isEOF(): ifnstrace.isEvent(): (event, time, sendnode, dest, dummy, size, dummy, flow, dummy, √£√ëdummy, seqno, dummy) = nstrace.getEvent() if(event == "r" anddest == DEST_NODE andsize >= 1000 andflow √£√ë== FLOW): if(seqno inCOUNTS): COUNTS[seqno] += 1 else: COUNTS[seqno] = 1 else: nstrace.skipline() forseqno insorted(COUNTS.keys()): if(COUNTS[seqno] > 1): print(seqno, COUNTS[seqno]) dup_counter(sys.argv[1]) When run on the basic1.tr Ô¨Åle above, it Ô¨Ånds 13 packets delivered twice, with TCP sequence numbers 43, 45, 47, 49, 51, 53, 55, 57, 58, 59, 60, 61 and 62. These are sent the second time between T=1.437824 and T=1.952752; the Ô¨Årst transmissions are at times between T=0.83536 and T=1.046592. If we look at our cwnd -v-time graph above, we see that these Ô¨Årst transmissions occurred during the gap between the end of the unbounded slow-start phase and the beginning of threshold-slow-start leading up to the TCP sawtooth. Slow start, in other words, is messy. 31.2.6.5 Loss rate versus cwnd: part 1 If we run the basic1.tcl simulation above until time 1000, there are 94915 packets acknowledged and 512 loss events. This yields a loss rate of p = 512/94915 = 0.00539, and so by the formula of 21.2 TCP Reno loss rate versus cwnd we should expect the average cwnd to be about 1.225/?p16.7. The true average cwnd is the number of packets sent divided by the elapsed time in RTTs, but as RTTs are not constant here (they get signiÔ¨Åcantly longer as the queue Ô¨Ålls), we turn to an approximation. From 31.2.1 Graph of cwnd v time we saw that the peak cwnd was 20.935; the mean cwnd should thus be about 3/4 of this, or 15.7. While not perfect, agreement here is quite reasonable. See also 31.4.3 Loss rate versus cwnd: part 2. 31.3 Two TCP Senders Competing Now let us create a simulation in which two TCP Reno senders compete for the bottleneck link, and see how fair an allocation each gets. According to the analysis in 20.3 TCP Reno Fairness with Synchronized Losses, this is really a test of the synchronized-loss hypothesis, and so we will also examine the ns-2 trace Ô¨Åles for losses and loss responses. We will start with ‚Äúclassic‚Äù TCP Reno, but eventually also consider SACK TCP. Note that, in terms of packet losses in the immediate vicinity of any one queue-Ô¨Ålling event, we can expect TCP Reno and SACK TCP to behave identically; they differ only in how they respond to losses. The initial topology will be as follows (though we will very soon raise the bandwidths tenfold, though not the propagation delays): 31.3 Two TCP Senders Competing 841
An Introduction to Computer Networks, Release 2.0.11 A BR D800 Kbps = 100 pkt/s8 Mbps 10 ms delay 8 Mbps varying delay100 ms delay Broadly speaking, the simulations here will demonstrate that the longer-delay B‚ÄìD connection receives less bandwidth than the A‚ÄìD connection, but not quite so much less as was predicted in 20.3 TCP Reno Fairness with Synchronized Losses. The synchronized-loss hypothesis increasingly fails as the B‚ÄìR delay increases, in that the B‚ÄìD connection begins to escape some of the packet-loss events experienced by the A‚ÄìD connection. We admit at the outset that we will not, however, obtain a quantitative answer to the question of bandwidth allocation. In fact, as we shall see, we run into some difÔ¨Åculties even formulating the proper question. In the course of developing the simulation, we encounter several potential problems: 1. The two senders can become synchronized in an unfortunate manner 2. When we resolve the previous issue by introducing randomness, the bandwidth division is sensitive to the method selected 3. As R‚Äôs queue Ô¨Ålls, the RTT may increase signiÔ¨Åcantly, thus undermining RTT-based measurements (31.3.9 The RTT Problem ) 4. Transient queue spikes may introduce unexpected losses 5. Coarse timeouts may introduce additional unexpected losses The experiments and analyses below divide into two broad categories. In the Ô¨Årst category, we make use only of the Ô¨Ånal goodput measurements for the two connections. We consider the Ô¨Årst two points of the list above in 31.3.4 Phase Effects, and the third in 31.3.9 The RTT Problem and31.3.10 Raising the Bandwidth. The Ô¨Årst category concludes with some simple loss modeling in 31.3.10.1 Possible models. In the second category, beginning at 31.4 TCP Loss Events and Synchronized Losses, we make use of the ns-2 traceÔ¨Åles to extract information about packet losses and the extent to which they are synchronized. Examples related to points four and Ô¨Åve of the list above are presented in 31.4.1 Some TCP Reno cwnd graphs. The second category concludes with 31.4.2 SACK TCP and Avoiding Loss Anomalies, in which we demonstrate that SACK TCP is, in terms of loss and recovery, much better behaved than TCP Reno. 31.3.1 The Tcl Script Below is a simpliÔ¨Åed version of the ns-2 script for our simulation; the full version is at basic2.tcl. The most important variable is the additional one-way delay on the B‚ÄìR link, here called delayB. Other deÔ¨Åned variables are queuesize (for R‚Äôs queue_limit), bottleneckBW (for the R‚ÄìD bandwidth), endtime (the length of the simulation), and overhead (for introducing some small degree of randomization, below). As with basic1.tcl, we set the packet size to 1000 bytes total (960 bytes TCP portion), and increase the advertised window size to 65000 (so it is never the limiting factor). 842 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 We have made the delayB value be a command-line parameter to the Tcl Ô¨Åle, so we can easily experiment with changing it (in the full version linked to above, overhead ,bottleneckBW ,endtime and queuesize are also parameters). The one-way propagation delay for the A‚ÄìD path is 10 ms + 100 ms = 110 ms, making the RTT 220 ms plus the bandwidth delays. At the bandwidths above, the bandwidth delay for data packets adds an additional 11 ms; ACKs contribute an almost-negligible 0.44 ms. We return to the script variable RTTNL, intended to approximate RTT noLoad, below. Withendtime =300, the theoretical maximum number of data packets that can be delivered is 30,000. If bottleneckBW = 0.8 Mbps (100 packets/sec) then the R‚ÄìD link can hold ten R √ù√ëD data packets in transit, plus another ten D √ù√ëR ACKs. In thefinish() procedure we have added code to print out the number of packets received by D for each connection; we could also extract this from the trace Ô¨Åle. To gain better control over printing, we have used the format command, which works something like C‚Äôs sprintf. It returns a string containing spliced-in numeric values replacing the corresponding %dor%f tokens in the control string; this returned string can then be printed with puts. The full version linked to above also contains some nam directives, support for command-line arguments, and arranges to name any traceÔ¨Åles with the same base Ô¨Ålename as the Tcl Ô¨Åle. # NS basic2.tcl example of two TCPs competing on the same link. # Create a simulator object set ns [new Simulator] #Open the trace file set trace [open basic2.tr w] $ns trace-all $trace ############## some globals (modify as desired) ############## # queuesize on bottleneck link set queuesize 20 # default run time, in seconds set endtime 300 # "overhead" of D>0 introduces a uniformly randomized delay d, 0 ¬§d¬§D; 0 √£√ëturns it off. set overhead 0 # delay on the A--R link, in ms set basedelay 10 # ADDITIONAL delay on the B--R link, in ms set delayB 0 # bandwidth on the bottleneck link, either 0.8 or 8.0 Mbit set bottleneckBW 0.8 # estimated no-load RTT for the first flow, in ms set RTTNL 220 ############## arrange for output ############## set outstr [format "parameters: delayB=%f overhead=%f bottleneckBW=%f" √£√ë$delayB $overhead $bottleneckBW] puts stdout $outstr (continues on next page) 31.3 Two TCP Senders Competing 843
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) # Define a 'finish' procedure that prints out progress for each connection proc finish {} { global ns tcp0 tcp1 end0 end1 queuesize trace delayB overhead RTTNL set ack0 [$tcp0 set ack_] set ack1 [$tcp1 set ack_] # counts of packets *received * set recv0 [expr round ( [$end0 set bytes_] / 1000.0)] set recv1 [expr round ( [$end1 set bytes_] / 1000.0)] # see numbers below in topology-creation section set rttratio [expr (2.0 *$delayB+$RTTNL)/$RTTNL] # actual ratio of throughputs fast/slow; the 1.0 forces floating point set actualratio [expr 1.0 *$recv0/$recv1] # theoretical ratio fast/slow with squaring; see text for discussion √£√ëof ratio1 and ratio2 set rttratio2 [expr $rttratio *$rttratio] set ratio1 [expr $actualratio/$rttratio] set ratio2 [expr $actualratio/$rttratio2] set outstr [format "%f %f %d %d %f %f %f %f %f" $delayB $overhead √£√ë$recv0 $recv1 $rttratio $rttratio2 $actualratio $ratio1 $ratio2 ] puts stdout $outstr $ns flush-trace close $trace exit 0 } ############### create network topology ############## # A # \ # \ # R---D (Destination) # / # / # B #Create four nodes set A [$ns node] set B [$ns node] set R [$ns node] set D [$ns node] set fastbw [expr $bottleneckBW *10] #Create links between the nodes; propdelay on B--R link is 10+$delayB ms $ns duplex-link $A $R ${fastbw}Mb ${basedelay}ms DropTail $ns duplex-link $B $R ${fastbw}Mb [expr $basedelay + $delayB]ms DropTail # this last link is the bottleneck; 1000 bytes at 0.80Mbps => 10 ms/packet # A--D one-way delay is thus 110 ms prop + 11 ms bandwidth # the values 0.8Mb, 100ms are from Floyd & Jacobson $ns duplex-link $R $D ${bottleneckBW}Mb 100ms DropTail (continues on next page) 844 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) $ns queue-limit $R $D $queuesize ############## create and connect TCP agents, and start ############## Agent/TCP set window_ 65000 Agent/TCP set packetSize_ 960 Agent/TCP set overhead_ $overhead #Create a TCP agent and attach it to node A, the delayed path set tcp0 [new Agent/TCP/Reno] $tcp0 set class_ 0 # set the flowid here, used as field 8 in the trace $tcp0 set fid_ 0 $tcp0 attach $trace $tcp0 tracevar cwnd_ $tcp0 tracevar ack_ $ns attach-agent $A $tcp0 set tcp1 [new Agent/TCP/Reno] $tcp1 set class_ 1 $tcp1 set fid_ 1 $tcp1 attach $trace $tcp1 tracevar cwnd_ $tcp1 tracevar ack_ $ns attach-agent $B $tcp1 set end0 [new Agent/TCPSink] $ns attach-agent $D $end0 set end1 [new Agent/TCPSink] $ns attach-agent $D $end1 #Connect the traffic source with the traffic sink $ns connect $tcp0 $end0 $ns connect $tcp1 $end1 #Schedule the connection data flow set ftp0 [new Application/FTP] $ftp0 attach-agent $tcp0 set ftp1 [new Application/FTP] $ftp1 attach-agent $tcp1 $ns at 0.0 "$ftp0 start" $ns at 0.0 "$ftp1 start" $ns at $endtime "finish" #Run the simulation $ns run 31.3 Two TCP Senders Competing 845
An Introduction to Computer Networks, Release 2.0.11 31.3.2 Equal Delays We Ô¨Årst try this out by running the simulation with equal delays on both A‚ÄìR and R‚ÄìB. The following values are printed out (arranged here vertically to allow annotation) value variable meaning 0.000000delayB Additional B‚ÄìR propagation delay, compared to A‚ÄìR delay 0.000000overhead overhead; a value of 0 means this is effectively disabled 14863recv0 Count of cumulative A‚ÄìD packets received at D (that is, goodput) 14771recv1 Count of cumulative B‚ÄìD packets received at D (again, goodput) 1.000000rttratio RTT_ratio: B‚ÄìD/A‚ÄìD (long/short) 1.000000rttratio2 The square of the previous value 1.006228actualratio Actual ratio of A‚ÄìD/B‚ÄìD goodput, that is, 14863/14771 (note change in order versus RTT_ratio) 1.006228ratio1 actual_ratio/RTT_ratio 1.006228ratio2 actual_ratio/RTT_ratio2 The one-way A‚ÄìD propagation delay is 110 ms; the bandwidth delays as noted above amount to 11.44 ms, 10 ms of which is on the R‚ÄìD link. This makes the A‚ÄìD RTT noLoad about 230 ms. The B‚ÄìD delay is, for the time being, the same, as delayB = 0. We set RTTNL = 220, and calculate the RTT ratio (within Tcl, in the Ô¨Ånish() procedure) as (2 delayB +RTTNL )/RTTNL. We really should use RTTNL =230 instead of 220 here, but 220 will be closer when we later change bottleneckBW to 8.0 Mbit/sec rather than 0.8, below. Either way, the difference is modest. Note that the above RTT calculations are for when the queue at R is empty; when the queue contains 20 packets this adds another 200 ms to the A √ù√ëD and B√ù√ëD times (the reverse direction is unchanged). This may make a rather large difference to the RTT ratio, and we will address it below, but does not matter yet because the propagation delays so far are identical. In the model of 20.3.3 TCP Reno RTT bias we explored a model in which we expect that ratio2, above, would be about 1.0. The Ô¨Ånal paragraph of 21.2.2 Unsynchronized TCP Losses hints at a possible model (theùõæ=ùúÜcase) in which ratio1 would be about 1.0. We will soon be in a position to test these theories experimentally. Note that the order of B and A in the goodput and RTT ratios is reversed, to reÔ¨Çect the expected inverse relationship. In the 300-second run here, 14863+14771 = 29634 packets are sent. This means that the bottleneck link is 98.8% utilized. In31.4.1 Some TCP Reno cwnd graphs we will introduce a script (teeth.py) to count and analyze the teeth of the TCP sawtooth. Applying this now, we Ô¨Ånd there are 67 loss events total, and thus 67 teeth, and in every loss event each Ô¨Çow loses exactly one packet. This is remarkably exact conformance to the synchronized-loss hypothesis of 20.3.3 TCP Reno RTT bias. So far. 31.3.3 Unequal Delays We now begin increasing the additional B‚ÄìR delay ( delayB ). Some preliminary data are in the table below, and point to a signiÔ¨Åcant problem: goodput ratios in the last column here are not varying smoothly. The value of 0 ms in the Ô¨Årst row means that the B‚ÄìR delay is equal to the A‚ÄìR delay; the value of 110 ms in the 846 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 last row means that the B‚ÄìD RTT noLoad is double the A‚ÄìD RTT noLoad. The column labeled (RTT ratio)2is the expected goodput ratio, according to the model of 20.3 TCP Reno Fairness with Synchronized Losses; the actual A‚ÄìD/B‚ÄìD goodput ratio is in the Ô¨Ånal column. delayB RTT ratio A‚ÄìD goodput B‚ÄìD goodput (RTT ratio)2goodput ratio 0 1.000 14863 14771 1.000 1.006 5 1.045 4229 24545 1.093 0.172 23 1.209 22142 6879 1.462 3.219 24 1.218 17683 9842 1.484 1.797 25 1.227 14958 13754 1.506 1.088 26 1.236 24034 5137 1.529 4.679 35 1.318 16932 11395 1.738 1.486 36 1.327 25790 3603 1.762 7.158 40 1.364 20005 8580 1.860 2.332 41 1.373 24977 4215 1.884 5.926 45 1.409 18437 10211 1.986 1.806 60 1.545 18891 9891 2.388 1.910 61 1.555 25834 3135 2.417 8.241 85 1.773 20463 8206 3.143 2.494 110 2.000 22624 5941 4.000 3.808 For a few rows, such as the Ô¨Årst and the last, agreement between the last two columns is quite good. However, there are some decidedly anomalous cases in between (particularly the numbers in bold ). AsdelayB changes from 35 to 36, the goodput ratio jumps from 1.486 to 7.158. Similar dramatic changes in goodput appear asdelayB ranges through the sets {23, 24, 25, 26}, {40, 41, 45}, and {60, 61}. These values were, admittedly, specially chosen by trial and error to illustrate relatively discontinuous behavior of the goodput ratio, but, still, what is going on? 31.3.4 Phase Effects This erratic behavior in the goodput ratio in the table above turns out to be due to what are called phase effects in [FJ92]; transmissions of the two TCP connections become precisely synchronized in some way that involves a persistent negative bias against one of them. What is happening is that a ‚Äúrace condition‚Äù occurs for the last remaining queue vacancy, and one connection consistently loses this race the majority of the time. 31.3.4.1 Single-sender phase effects We begin by taking a more detailed look at the bottleneck queue when no competition is involved. Consider a single sender A using a Ô¨Åxed window size to send to destination B through bottleneck router R (so the topology is A‚ÄìR‚ÄìB), and suppose the window size is large enough that R‚Äôs queue is not empty. For the sake of deÔ¨Åniteness, assume R‚Äôs bandwidth delay is 10 ms/packet; R will send packets every 10 ms, and an ACK will arrive back at A every 10 ms, and A will transmit every 10 ms. Now imagine that we have an output meter that reports the percentage that has been transmitted of the packet R is currently sending; it will go from 0% to 100% in 10 ms, and then back to 0% for the next packet. 31.3 Two TCP Senders Competing 847
An Introduction to Computer Networks, Release 2.0.11 Our Ô¨Årst observation is that at each instant when a packet from A fully arrives at R, R is always at exactly the same point in forwarding some earlier packet on towards B; the output meter always reads the same percentage. This percentage is called the phase of the connection, sometimes denoted ùúë. We can determine ùúëas follows. Consider the total elapsed time for the following: 
- R Ô¨Ånishes transmitting a packet and it arrives at B 
- its ACK makes it back to A 
- the data packet triggered by that ACK fully arrives at R In the absence of other congestion, this R-to-R time includes no variable queuing delays, and so is constant. The output-meter percentage above is determined by this elapsed time. If for example the R-to-R time is 83 ms, and Data[N] leaves R at T=0, then Data[N+winsize] (sent by A upon arrival of ACK[N]) will arrive at R when R has completed sending packets up through Data[N+8] (80 ms) and is 3 ms into transmitting Data[N+9]. We can get this last as a percentage by dividing 83 by R‚Äôs 10-ms bandwidth delay and taking the fractional part (in this case, 30%). Because the elapsed R-to-R time here is simply RTT noLoad minus the bandwidth delay at R, we can also compute the phase as ùúë= fractional_part(RTT noLoad(R‚Äôs bandwidth delay)). If we ratchet up the winsize until the queue becomes full when each new packet arrives, then the phase percentage represents the fraction of the time the queue has a vacancy. In the scenario above, if we start the clock at T=0 when R has Ô¨Ånished transmitting a packet, then the queue has a vacancy until T=3 when a new packet from A arrives. The queue is then full until T=10, when R starts transmitting the next packet in its queue. Finally, even in the presence of competition through R, the phase of a single connection remains constant provided there are no queuing delays along the bidirectional A‚ÄìB path except at R itself, and there only in the forward direction towards B. Other trafÔ¨Åc sent through R can only add delay in integral multiples of R‚Äôs bandwidth delay, and so cannot affect the A‚ÄìB phase. 31.3.4.2 Two-sender phase effects In the present simulation, we can by the remark in the previous paragraph calculate the phase for each sender; let these be ùúëAandùúëB. The signiÔ¨Åcance of phase to competition is that whenever A and B send packets that happen to arrive at R in the same 10-ms interval while R is forwarding some other packet, if the queue has only one vacancy then the connection with the smaller phase will always win it. As a concrete example, suppose that the respective RTT noLoad ‚Äôs of A and B are 221 and 263 ms. Then A‚Äôs phase is 0.1 (fractional_part(221 10)) and B‚Äôs is 0.3. The important thing is not that A‚Äôs packets take less time, but that in the event of a near-tie A‚Äôs packet must arrive Ô¨Årst at R. Imagine that R forwards a B packet and then, four packets (40 ms) later, forwards an A packet. The ACKs elicited by these packets will cause new packets to be sent by A and B; A‚Äôs packet will arrive Ô¨Årst at R followed 2 ms later by B‚Äôs packet. Of course, R is not likely to send an A packet four packets after every B packet, but when it does so, the arrival order is predetermined (in A‚Äôs favor) rather than random. Now consider what happens when a packet is dropped. If there is a single vacancy at R, and packets from A and B arrive in a near tie as above, then it will always be B‚Äôs packet that is dropped. The occasional packet-pair sent by A or B as part of the expansion of cwnd will be the ultimate cause of loss events, but the phase effect has introduced a persistent degree of bias in A‚Äôs favor. 848 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 We can visualize phase effects with ns-2 by letting delayB range over, say, 0 to 50 in small increments, and plotting the corresponding values of ratio2 (above). Classically we expect ratio2 to be close to 1.00. In the graph below, the blue curve represents the goodput ratio; it shows a marked (though not perfect) periodicity with period 5 ms. The orange curve represents (RTT_ratio)2; according to 20.3 TCP Reno Fairness with Synchronized Losses we would expect the blue and orange curves to be about the same. When the blue curve is high, the slower B‚ÄìD connection is proportionately at an unexpected disadvantage. Seldom do phase effects work in favor of the B‚ÄìD connection, because A‚Äôs phase here is quite small (0.144, based on A‚Äôs exact RTT noLoad of 231.44 ms). (If we change the A‚ÄìR propagation delay ( basedelay ) to 12 ms, making A‚Äôs phase 0.544, the blue curve oscillates somewhat more evenly both above and below the orange curve, but still with approximately the same amplitude.) Recall that a 5 ms change in delayB corresponds to a 10 ms change in the A‚ÄìD connection‚Äôs RTT, equal to router R‚Äôs transmission time. What is happening here is that as the B‚ÄìD connection‚Äôs RTT increases through a range of 10 ms, it cycles through from phase-effect neutrality to phase-effect deÔ¨Åcit and back. 31.3.5 Minimizing Phase Effects In the real world, the kind of precise transmission synchronization that leads to phase effects is seldom evident, though perhaps this has less to do with rarity and more with the fact that head-to-head TCP competitions are difÔ¨Åcult to observe intimately. Usually, however, there seems to be sufÔ¨Åcient other trafÔ¨Åc present to disrupt the synchronization. How can we break this synchronization in simulations? One way or another, we must inject some degree of randomization into the bulk TCP Ô¨Çows. Techniques introduced in [FJ92] to break synchronization in ns-2 simulations were random ‚Äútelnet‚Äù trafÔ¨Åc ‚Äì involving smaller packets sent according to a given random distribution ‚Äì and the use of random-drop queues (not included in the standard ns-2 distribution). The second, of course, means we are no longer simulating FIFO queues. A third way of addressing phase effects is to make use of the ns-2 overhead variable, which introduces some modest randomization in packet-departure times at the TCP sender. Because this technique is simpler, 31.3 Two TCP Senders Competing 849
An Introduction to Computer Networks, Release 2.0.11 we will start with it. One difference between the use of overhead and telnet trafÔ¨Åc is that the latter has the effect of introducing delays at all nodes of the network that carry the trafÔ¨Åc, not just at the TCP sources. 31.3.6 Phase Effects and overhead For our Ô¨Årst attempt at introducing phase-effect-avoiding randomization in the competing TCP Ô¨Çows, we will start with ns-2‚Äôs TCP overhead attribute. This is equal to 0 by default and is measured in units of seconds. Ifoverhead > 0, then the TCP source introduces a uniformly distributed random delay of between 0 and overhead seconds whenever an ACK arrives and the source is allowed to send a new packet. Because the distribution is uniform, the average delay so introduced is thus overhead /2. To introduce an average delay of 10 ms, therefore, one sets overhead = 0.02. Packets are always sent in order; if packet 2 is assigned a small overhead delay and packet 1 a large overhead delay, then packet 2 waits until packet 1 has been sent. For this reason, it is a good idea to keep the average overhead delay no more than the average packet interval (here 10 ms). The following graph shows four curves representing overhead values of 0, 0.005, 0.01 and 0.02 (that is, 5 ms, 10 ms and 20 ms). For each curve, ratio1 (not the actual goodput ratio and not ratio2) is plotted as a function of delayB as the latter ranges from 25 to 55 ms. The simulations run for 300 seconds, and bottleneckBW = 0.8. (We will return to the choice of ratio1 here in 31.3.9 The RTT Problem; the corresponding ratio2 graph is however quite similar, at least in terms of oscillatory behavior.) The dark-blue curve for overhead = 0 is wildly erratic due to phase effects; the light-blue curve for overhead = 0.005 has about half the oscillation. Even the light-green overhead = 0.01 curve exhibits some wiggling; it is not until overhead = 0.02 for the darker green curve that the graph really settles down. We conclude that the latter two values for overhead are quite effective at mitigating phase effects. One crude way to quantify the degree of graph oscillation is by calculating the mean deviation; the respective deviation values for the curves above are are 1.286, 0.638, 0.136 and 0.090. Recall that the time to send one packet on the bottleneck link is 0.01 seconds, and that the average delay introduced by overhead d is d/2; thus, when overhead is 0.02 each connection would, if acting alone, have an average sender delay equal to the bottleneck-link delay (though overhead delay is like propagation delay, and so a high overhead will not prevent queue buildup). 850 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 Compared to the 10-ms-per-packet R‚ÄìD transmission time, average delays of 5 and 10 ms per Ô¨Çow (overhead of 0.01 and 0.02 respectively) may not seem disproportionate. They are, however, quite large when compared to the 1.0 ms bandwidth delay on the A‚ÄìR and B‚ÄìR legs. Generally, if the goal is to reduce phase effects then overhead should be comparable to the bottleneck-router transmission rate. Using overhead > 0 does increase the RTT, but in this case not considerably. We conclude that using overhead to break the synchronization that leads to phase effects appears to have worked, at least in the sense that with the value of overhead = 0.02 the goodput ratio increases more-orless monotonically with increasing delayB. The problem with using overhead this way is that it does not correspond to any physical network delay or other phenomenon. Its use here represents a decidedly ad hoc strategy to introduce enough randomization that phase effects disappear. That said, it does seem to produce results similar to those obtained by injecting random ‚Äútelnet‚Äù trafÔ¨Åc, introduced in the following section. 31.3.7 Phase Effects and telnet trafÔ¨Åc We can also introduce anti-phase-effect randomization by making use of the ns-2 telnet application to generate low-to-moderate levels of random trafÔ¨Åc. This requires an additional two Agent/TCP objects, representing A‚ÄìD and B‚ÄìD telnet connections, to carry the trafÔ¨Åc; this telnet trafÔ¨Åc will then introduce slight delays in the corresponding bulk (ftp) trafÔ¨Åc. The size of the telnet packets sent is determined by the TCP agents‚Äô usualpacketSize_ attribute. For each telnet connection we create an Application/Telnet object and set its attribute interval_; in the script fragment below this is set to tninterval. This represents the average packet spacing in seconds; transmissions are then scheduled according to an exponential random distribution with interval_ as its mean. We remark that ‚Äúreal‚Äù telnet terminal-login trafÔ¨Åc seldom if ever follows an exponential random distribution, though this is not necessarily a concern as there are other common trafÔ¨Åc patterns ( egweb trafÔ¨Åc or database trafÔ¨Åc) that Ô¨Åt this model better. Actual (simulated) transmissions, however, are also constrained by the telnet connection‚Äôs sliding window. It is quite possible that the telnet application releases a new packet for transmission, but it cannot yet be sent because the telnet TCP connection‚Äôs sliding window is momentarily frozen, waiting for the next ACK. If the telnet packets encounter congestion and the interval_ is small enough then the sender may have a backlog of telnet packets in its outbound queue that are waiting for the sliding window to advance enough to permit their departure. set tcp10 [new Agent/TCP] $ns attach-agent $A $tcp10 set tcp11 [new Agent/TCP] $ns attach-agent $B $tcp11 set end10 [new Agent/TCPSink] set end11 [new Agent/TCPSink] $ns attach-agent $D $end10 $ns attach-agent $D $end11 set telnet0 [new Application/Telnet] set telnet1 [new Application/Telnet] (continues on next page) 31.3 Two TCP Senders Competing 851
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) set tninterval 0.001 ;# see text for discussion $telnet0 set interval_ $tninterval $tcp10 set packetSize_ 210 $telnet1 set interval_ $tninterval $tcp11 set packetSize_ 210 $telnet0 attach-agent $tcp10 $telnet1 attach-agent $tcp11 ‚ÄúReal‚Äù telnet packets, besides not necessarily being exponentially distributed, are most often quite small. In the simulations here we use an uncharacteristically large size of 210 bytes, leading to a total packet size of 250 bytes after the 40-byte simulated TCP/IP header is attached. We denote the latter number by actualSize. See exercise 9.0. The bandwidth theoretically consumed by the telnet connection is simply actualSize /$tninterval; the actual bandwidth may be lower if the telnet packets are encountering congestion as noted above. It is convenient to deÔ¨Åne an attribute tndensity that denotes the fraction of the R‚ÄìD link‚Äôs bandwith that the Telnet application will be allowed to use, eg2%. In this case we have $tninterval = actualSize / ($tndensity *$bottleneckBW) For example, if actualSize = 250 bytes, and $bottleneckBW corresponds to 1000 bytes every 10 ms, then the telnet connection could saturate the link if its packets were spaced 2.5 ms apart. However, if we want the telnet connection to use up to 5% of the bottleneck link, then $tninterval should be 2.5/0.05 = 50 ms (converted for ns-2 to 0.05 sec). The Ô¨Årst question we need to address is whether telnet trafÔ¨Åc can sufÔ¨Åciently dampen phase-effect oscillations, and, if so, at what densities. The following graph is the telnet version of that above in 31.3.6 Phase Effects and overhead ;bottleneckBW is still 0.8 but the simulations now run for 3000 seconds. The telnet total packet size is 250 bytes. The given telnet density percentages apply to each of the A‚ÄìD and B‚ÄìD telnet connections; the total telnet density is thus double the value shown. 852 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 As we hoped, the oscillation does indeed substantially Ô¨Çatten out as the telnet density increases from 0 (dark blue) to 1% (bright green); the mean deviation falls from 1.36 to 0.084. So far the use of telnet is very promising. Unfortunately, if we continue to increase the telnet density past 1%, the oscillation increases again, as can be seen in the next graph: The Ô¨Årst two curves, plotted in green, correspond to the ‚Äúgood‚Äù densities of the previous graph, with the vertical axis stretched by a factor of two. As densities increase, however, phase-effect oscillation returns, and the curves converge towards the heavier red curve at the top. What appears to be happening is that, beyond a density of 1% or so, the limiting factor in telnet transmission becomes the telnet sliding window rather than the random trafÔ¨Åc generation, as mentioned in the third paragraph of this section. Once the sliding window becomes the limiting factor on telnet packet transmission, the telnet connections behave much like ‚Äì and become synchronized with ‚Äì their corresponding bulk-trafÔ¨Åc ftp connections. At that point their ability to moderate phase effects is greatly diminished, as actual packet 31.3 Two TCP Senders Competing 853
An Introduction to Computer Networks, Release 2.0.11 departures no longer have anything to do with the exponential random distribution that generates the packets. Despite this last issue, the fact that small levels of random trafÔ¨Åc can lead to large reductions in phase effects can be taken as evidence that, in the real world, where other trafÔ¨Åc sources are ubiquitous, phase effects will seldom be a problem. 31.3.8 overhead versus telnet The next step is to compare the effect on the original bulk-trafÔ¨Åc Ô¨Çows of overhead randomization and telnet randomization. The graphs below plot ratio1 as a function of delayB as the latter ranges from 0 to 400 in increments of 5. The bottleneckBW is 0.8 Mbps, the queue capacity at R is 20, and the run-time is 3000 seconds. The four reddish-hued curves represent the result of using telnet with a packet size of 250, at densities ranging from 0.5% to 5%. These may be compared with the three green curves representing the use of overhead, with values 0.01, 0.015 and 0.02. While telnet with a density of 1% is in excellent agreement with the use of overhead, it is also apparent that smaller telnet densities give a larger ratio1 while larger densities give a smaller. This raises the awkward possibility that the exact mechanism by which we introduce randomization may have a material effect on the fairness ratios that we ultimately observe. There is no ‚Äúright‚Äù answer here; different randomization sources or levels may simply lead to differing fairness results. The advantage of using telnet for randomization is that it represents an actual network phenomenon, unlike overhead. The drawback to using telnet is that the effect on the bulk-trafÔ¨Åc goodput ratio is, as the graph above shows, somewhat sensitive to the exact value chosen for the telnet density. In the remainder of this chapter, we will continue to use the overhead model, for simplicity, though we do not claim this is a universally appropriate approach. 854 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 31.3.9 The RTT Problem In all three of the preceding graphs ( 31.3.6 Phase Effects and overhead ,31.3.7 Phase Effects and telnet trafÔ¨Åc and31.3.8 overhead versus telnet ), the green curves on the graphs appear to show that, once sufÔ¨Åcient randomization has been introduced to disrupt phase effects, ratio1 converges to 1.0. This, however, is in fact an artifact, due to the second Ô¨Çaw in our simulated network: RTT is not very constant. While RTT noLoad for the A‚ÄìD link is about 220 ms, queuing delays at R (with queuesize = 20) can almost double that by adding up to 20 10 ms = 200 ms. This means that the computed values for RTTratio are too large, and the computed values for ratio1 are thus too small. While one approach to address this problem is to keep careful track of RTT actual, a simpler strategy is to create a simulated network in which the queuing delay is small compared to the propagation delay and so the RTT is relatively constant. We turn to this next. 31.3.10 Raising the Bandwidth In modern high-bandwidth networks, queuing delays tend to be small compared to the propagation delay; see19.7 TCP and Bottleneck Link Utilization. To simulate such a network here, we simply increase the bandwidth of all the links tenfold, while leaving the existing propagation delays the same. We achieve this by setting bottleneckBW = 8.0 instead of 0.8. This makes the A‚ÄìD RTT noLoad equal to about 221 ms; queuing delays can now amount to at most an additional 20 ms. The value of overhead also needs to be scaled down by a factor of 10, to 0.002 sec, to reÔ¨Çect an average delay in the same range as the bottleneck-link packet transmission time. Here is a graph of results for bottleneckBW = 8.0, time = 3000, overhead = 0.002 and queuesize = 20. We still use 220 ms as the estimated RTT, though the actual RTT will range from 221 ms to 241 ms. The delayB parameter runs from 0 to 400 in steps of 1.0. The Ô¨Årst observation to make is that ratio1 is generally too large and ratio2 is generally too small, when compared to 1.0. In other words, neither is an especially good Ô¨Åt. This appears to be a fairly general phenomenon, in both simulation and the real world: TCP Reno throughput ratios tend to be somewhere between the corresponding RTT ratio and the square of the RTT ratio. The synchronized-loss hypothesis led to the prediction ( 20.3 TCP Reno Fairness with Synchronized Losses ) 31.3 Two TCP Senders Competing 855
An Introduction to Computer Networks, Release 2.0.11 that the goodput ratio would be close to RTT_ratio2. As this conclusion appears to fail, the hypothesis too must fail, at least to a degree: it must be the case that not all losses are shared. Throughout the graph we can observe a fair amount of ‚Äúnoise‚Äù variation. Most of this variation appears unrelated to the 5 ms period we would expect for phase effects (as in the graph at 31.3.4.2 Two-sender phase effects ). However, it is important to increment delayB in amounts much smaller than 5 ms in order to rule this out, hence the increment of 1.0 here. There are strong peaks at delayB = 110, 220 and 330. These delayB values correspond to increasing the RTT by integral multiples 2, 3 and 4 respectively, and the peaks are presumably related to some kind of higher-level phase effect. 31.3.10.1 Possible models If the synchronized-loss fairness model fails, with what do we replace it? Here are two ad hoc options. First, we can try to Ô¨Åt a curve of the form goodput_ratio = K (RTT_ratio)ùõº to the above data. If we do this, the value for the exponent ùõºcomes out to about 1.58, sort of a ‚Äúcompromise‚Äù between ratio2 ( ùõº=2) and ratio1 ( ùõº=1), although the value of the exponent here is somewhat sensitive to the details of the simulation. An entirely different curve to Ô¨Åt to the data, based on the appearance in the graph that ratio2 0.5 past 120, is goodput_ratio = 1/2 (RTT_ratio)2 We do not, however, possess for either of these formulas a model for the relative losses in the two primary TCP connections that is precise enough to offer an explanation of the formula (though see the Ô¨Ånal paragraph of31.4.2.2 Relative loss rates ). 31.3.10.2 Higher bandwidth and link utilization One consequence of raising the bottleneck bandwidth is that total link utilization drops, for delayB = 0, to 80% of the bandwidth of the bottleneck link, from 98%; this is in keeping with the analysis of 19.7 TCP and Bottleneck Link Utilization. The transit capacity is 220 packets and another 20 can be in the queue at R; thus an ideal sawtooth would oscillate between 120 and 240 packets. We do have two senders here, but whendelayB = 0 most losses are synchronized, meaning the two together behave like one sender with an additive-increase value of 2. As cwnd varies linearly from 120 to 240, it spends 5/6 of the time below the transit capacity of 220 packets ‚Äì during which period the average cwnd is (120+220)/2 = 170 ‚Äì and 1/6 of the time with the path 100% utilized; the weighted average estimating total goodput is thus (5/6) 170/220 + (1/6)1 = 81%. WhendelayB = 400, combined TCP Reno goodput falls to about 51% of the bandwidth of the bottleneck link. This low utilization, however, is indeed related to loss and timeouts; the corresponding combined goodput percentage for SACK TCP (which as we shall see in 31.4.2 SACK TCP and Avoiding Loss Anomalies is much better behaved) is 68%. 856 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 31.4 TCP Loss Events and Synchronized Losses If the synchronized-loss model is not entirely accurate, as we concluded from the graph above, what does happen with packet losses when the queue Ô¨Ålls? At this point we shift focus from analyzing goodput ratios to analyzing the underlying loss events, using the ns-2 traceÔ¨Åle. We will look at the synchronization of loss events between different connections and at how many individual packet losses may be involved in a single TCP loss response. One of the conclusions we will reach is that TCP Reno‚Äôs response to queue overÔ¨Çows in the face of competition is often quite messy, versus the single-loss behavior in the absence of competition as described above in 31.2.3 Single Losses. If we are trying to come up with a packet-loss model to replace the synchronized-loss hypothesis, it turns out that we would do better to switch to SACK TCP, though use of SACK TCP will not change the goodput ratios much at all. (The fact that Selective ACKs are nearly universal in the real world is another good reason to enable them here.) Packets are dropped when the queue Ô¨Ålls. It may be the case that only a single packet is dropped; it may be the case that multiple packets are dropped from each of multiple connections. We will refer to the set of packets dropped as a drop cluster. After a packet is dropped, TCP Reno discovers this just over one RTT after it was sent, through Fast Retransmit, and then responds by halving cwnd, through Fast Recovery. TCP Reno retransmits only one lost packet per RTT; TCP NewReno does the same but SACK TCP may retransmit multiple lost packets together. If enough packets were lost from a TCP Reno/NewReno connection, not all of them may be retransmitted by the point the retransmission-timeout timer expires (typically 1-2 seconds), resulting in a coarse timeout. At that point, TCP abandons its Fast-Recovery process, even if it had been progressing steadily. Eventually the TCP senders that have experienced packet loss reduce their cwnd, and thus the queue utilization drops. At that point we would classically not expect more losses until the senders involved have had time to grow their cwnd s through the additive-increase process to the point of again Ô¨Ålling the queue. As we shall see in 31.4.1.3 Transient queue peaks, however, this classical expectation is not entirely correct. It is possible that the packet losses associated with one full-queue period are spread out over sufÔ¨Åcient time (more than one RTT, at a minimum) that TCP Reno responds to them separately and halves cwnd more than once in rapid succession. We will refer to this as a loss-response cluster, or sometimes as a tooth cluster. In the ns-2 simulator, counting individual lost packets and TCP loss responses is straightforward enough. For TCP Reno, there are only two kinds of loss responses: Fast Recovery, in which cwnd is halved, and coarse timeout, in which cwnd is set to 1. Counting clusters, however, is more subjective; we need to decide when two drops or responses are close enough to one another that they should be counted as part of the same cluster. We use the notion of granularity here: two or more losses separated by less time than the granularity time interval are counted as a single event. We can also use granularity to decide when two loss responses in different connections are to be considered parts of the same event, or to tie loss responses to packet losses. The appropriate length of the granularity interval is not as clear-cut as might be hoped. In some cases a couple RTTs may be sufÔ¨Åcient, but note that the RTT noLoad of the B‚ÄìD connection above ranges from 0.22 sec to over 1.0 sec as delayB increases from 0 to 400 ms. In order to cover coarse timeouts, a granularity of from two to three seconds often seems to work well for packet drops. If we are trying to count losses to estimate the loss rate as in the formula cwnd = 1.225/?p as in 21.2 TCP 31.4 TCP Loss Events and Synchronized Losses 857
An Introduction to Computer Networks, Release 2.0.11 Reno loss rate versus cwnd, then we should count every loss response separately; the argument in 21.2 TCP Reno loss rate versus cwnd depended on counting allloss responses. The difference between one fastrecovery response and two in rapid succession is that in the latter case cwnd is halved twice, to about a quarter of its original value. However, if we are interested in whether or not losses between two connections are synchronized, we need again to make use of granularity to make sure two ‚Äúclose‚Äù losses are counted as one. In this setting, a granularity of one to two seconds is often sufÔ¨Åcient. 31.4.1 Some TCP Reno cwnd graphs We next turn to some examples of actual TCP behavior, and present a few hopefully representative cwnd graphs. In each Ô¨Ågure, the red line represents the longer-path (B‚ÄìD) Ô¨Çow and the green line represents the shorter (A‚ÄìD) Ô¨Çow. The graphs are of our usual simulation with bottleneckBW = 8.0 andoverhead = 0.002, and run for 300 seconds. ns-2 sensitivity Some of the graphs here were prepared with an earlier version of the basic2.tcl simulation script above in which the nodes A and B were reversed, which means that packet-arrival ties at R may be resolved in a different order. That is enough sometimes to lead to noticeably different sets of packet losses. While the immediate goal is to illuminate some of the above loss-clustering issues above, the graphs serve as well to illustrate the general behavior of TCP Reno competition and its variability. We will also use one of the graphs to explore the idea of transient queue spikes. In each case we can count teeth visually or via a Python script; see 31.4.2.1 Counting teeth in Python. In the latter case we must use an appropriate granularity interval ( eg2.0 seconds) if we want the count to agree even approximately with the visual count. Many times the tooth count is quite dependent on the exact value of the granularity. 31.4.1.1 delayB = 0 858 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 We start with the equal-RTTs graph, that is, delayB = 0. In this Ô¨Ågure the teeth (loss events) are almost completely synchronized; the only unsynchronized loss events are the green Ô¨Çow‚Äôs losses at T=100 and at about T=205. The two cwnd graphs, though, do not exactly move in lockstep. The red Ô¨Çow has three coarse timeouts (where cwnd drops to 0), at about T=30, T=125 and T=145; the green Ô¨Çow has seven coarse timeouts. The red graph gets a little ahead of the green in the interval 50-100, despite synchronized losses there. Just before T=50 the green graph has a fast-recovery response followed by a coarse timeout; the next three green losses also involve coarse timeouts. Despite perfect loss synchronization in the range from T=40 to T=90, the green graph ends up set back for a while because its three loss events all involve coarse timeouts while none of the red graph‚Äôs do. 31.4.1.2 delayB = 25 In thisdelayB = 25 graph, respective packet losses for the red and green Ô¨Çows are marked along the top, and thecwnd graphs are superimposed over a graph of the averaged queue utilization in gold. The time scale for queue-utilization averaging is about one RTT here. The queue graph is scaled vertically (by a factor of 8) so the queue values (maximum 20) are numerically comparable to the cwnd values (the transit capacity is about 230). There is one large red tooth from about T=40 to T=90 that corresponds to three green teeth; from about T=220 to T=275 there is one red tooth corresponding to two green teeth. Aside from these two points, representing three isolated green-only losses, the red and green teeth appear quite well synchronized. We also see evidence of loss-response clusters. At around T=143 the large green tooth peaks; halfway down there is a little notch ending at T=145 that represents a fast-recovery loss event interpreted by TCP as distinct. There is actually a third event, as well, representing a coarse timeout shortly after T=145, and then a fourth fast-recovery event at about T=152 which we will examine shortly. At T 80, the red tooth has a fast-recovery loss event followed very shortly by a coarse timeout; this happens for both Ô¨Çows at about T=220. Overall, the red path has 11 teeth if we use a tooth-counting granularity of 3 seconds, and 8 teeth if the granularity is 5 seconds. We do not count anything that happens in the slow-start phase, generally before T=3.0, nor do we count the ‚Äútooth‚Äù at T=300 when the graph ends. 31.4 TCP Loss Events and Synchronized Losses 859
An Introduction to Computer Networks, Release 2.0.11 The slope of the green teeth is slightly greater than the slope of the red teeth, representing the longer RTT for the red connection. As for the queue graph, there is perhaps more ‚Äúnoise‚Äù than expected, but generally the right edges of the teeth ‚Äì the TCP loss responses ‚Äì are very well aligned with peaks representing the queue Ô¨Ålling up. Recall that the transit capacity for Ô¨Çow1 here is about 230 packets and the queue capacity is about 20; we therefore in general expect the sum of the two cwnd s to range between 125 and 250, and that the queue should mostly remain empty until the sum reached 230. We can indeed conÔ¨Årm visually that in most cases the tooth peaks do indeed add up to about 250. 31.4.1.3 Transient queue peaks The loss in the graph above at around T=152 is a little peculiar; this is fully 10 seconds after the primary tooth peak that preceded it. Let us zoom in on it a little more closely, this time without any averaging of the queue-utilization graph. There are two very sharp, narrow peaks in the queue graph, just before T=146 and just after T=152, each causing packet drops for both Ô¨Çows. Neither peak, especially the latter one, appears to have much to do with the gradual queue-Ô¨Ålling and loss event at T=143. Neither one of these transient queue peaks is associated with the sum of the cwnd s approaching the maximum of about 250; at the T 146 peak the sum of the cwnd s is about 22+97 = 119 and at T 152 the sum is about 21+42 = 63. ACKs for either sender cannot return from the destination D faster than the bottleneck-link rate of 1 packet/ms; this might suggest new packets from senders A and B cannot arrive at R faster than a combined rate of 1 packet/ms. What is going on? What is happening is that each Ô¨Çow sends a Ô¨Çight of about 20 packets, spaced 1 ms apart, but by coincidence these two Ô¨Çights begin arriving at R at the same moment. The runup in the queue near T=152 occurs from T=152.100 to the Ô¨Årst drop at T=152.121. During this 21 ms interval, a Ô¨Çight of 20 packets arrive from node A (Ô¨Çow 0), and a Ô¨Çight of 19 packets arrive from node B (Ô¨Çow 1). These 39 packets in 21 ms means the queue utilization at R must increase by 39-21 = 18, which is sufÔ¨Åcient to overÔ¨Çow the queue as it was not quite empty beforehand. The ACK Ô¨Çights that triggered these data-packet Ô¨Çights were indeed spaced 1 ms apart, consistent with the bottleneck link, but the ACKs (and the data-packet Ô¨Çights that triggered those ACKs) passed through R at quite different times, because of the 25-ms difference in propagation delay on the A‚ÄìR and B‚ÄìR links. 860 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 Transient queue peaks like this complicate any theory of relative throughput based on gradually Ô¨Ålling the queue. Fortunately, while transient peaks themselves are quite common (as can be seen from the zoomed graph above), only occasionally do they amount to enough to overÔ¨Çow the queue. And, in the examples created here, the majority of transient overÔ¨Çow peaks (including the one analyzed above) are within a few seconds of a TCP coarse timeout response and may have some relationship to that timeout. The existence of transient queue peaks can be used to argue that queues should always have a reasonable ‚Äúsurge‚Äù capacity; or, more speciÔ¨Åcally, that bufferbloat should not be addressed simply by reducing the queue capacity to a very small level ( 21.5.1 Bufferbloat ). However, transient peaks are relatively infrequent. 31.4.1.4 delayB = 61 For this graph, note that the red and green loss events at T=130 are not quite aligned; the same happens at T=45 and T=100. We also have several multiple-loss-response clusters, at T=40, T=130, T=190 and T=230. The queue graph gives the appearance of much more solid yellow; this is due to a large number of transient queue spikes. Under greater magniÔ¨Åcation it becomes clear these spikes are still relatively sparse, however. There are transient queue overÔ¨Çows at T=46 and T=48, following a ‚Äúnormal‚Äù overÔ¨Çow at T=44, at T=101 following a normal overÔ¨Çow at T=99, at T=129 and T=131 following a normal overÔ¨Çow at T=126, and at T=191 following a normal overÔ¨Çow at T=188. 31.4 TCP Loss Events and Synchronized Losses 861
An Introduction to Computer Networks, Release 2.0.11 31.4.1.5 delayB = 120 Here we have (without the queue data) a good example of highly unsynchronized teeth: the green graph has 12 teeth, after the start, while the red graph has Ô¨Åve. But note that the red-graph loss events are a subset of the green loss events. 31.4.2 SACK TCP and Avoiding Loss Anomalies Neither the coarse timeouts nor the ‚Äúclustered‚Äù loss responses in the graphs above were anticipated in our original fairness model in 20.3 TCP Reno Fairness with Synchronized Losses. It is time to see if we can avoid these anomalies and thus obtain behavior closer to the classic sawtooth. It turns out that SACK TCP Ô¨Ålls the bill quite nicely. In the following subsection ( 31.4.2.1 Counting teeth in Python ) is a script for counting loss responses (‚Äúteeth‚Äù). If we run it on the traceÔ¨Åles from our TCP Reno simulations with bottleneckBW = 8.0 and time = 300, we Ô¨Ånd that, for each Ô¨Çow, about 30-35% of all loss responses are coarse timeouts. There is little if any dependence on the value for delayB. If we change the two tcp connections in the simulation to Agent/TCP/Newreno, the coarse-timeout fraction falls by over half, to under 15%. This is presumably because TCP NewReno is better able to handle multiple packet drops. However, when we change the TCP connections to use SACK TCP, the situation improves dramatically. We get essentially nocoarse timeouts. Runs for 3000 seconds, typically involving 100-200 cwnd -halving adjustments per Ô¨Çow, almost never have more than one coarse timeout and the majority of the time have none. Clusters of multiple loss responses also vanish almost entirely. In these 3000-second runs, there are usually 1-2 cases where two loss responses occur within 1.0 seconds (over 4 RTTs for the A‚ÄìD connection) of one another. SACK TCP‚Äôs smaller number of packet losses results in a marked improvement in goodput. Total link utilization ranges from 80.4% when delayB = 0 down to 68.3% when delayB = 400. For TCP Reno, the corresponding utilization range is from 77.5% down to 51.5%. 862 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 Note that switching to SACK TCP is unlikely to have a signiÔ¨Åcant impact on the distribution of packet losses; SACK TCP differs from TCP Reno only in the way it responds to losses. The SACK TCP sender is speciÔ¨Åed in ns-2 via Agent/TCP/Sack1. The receiver must also be SACKaware; this is done by creating the receiver with Agent/TCPSink/Sack1. When we switch to SACK TCP, the underlying fairness situation does not change much. Here is a graph similar to that above in 31.3.10 Raising the Bandwidth. The run time is 3000 seconds, bottleneckBW is 8 Mbps, and delayB runs from 0 to 400 in increments of 5. (We did not use an increment of 1, as in the similar graph in 31.3.10 Raising the Bandwidth, because we are by now conÔ¨Ådent that phase effects have been taken care of.) Ratio1 is shown in blue and ratio2 in green. We again try to Ô¨Åt curves as in 31.3.10.1 Possible models above. For the exponential model, goodput_ratio = K (RTT_ratio)ùõº, the value for the exponent ùõºcomes out this time to about 1.31, noticeably below TCP Reno‚Äôs value of 1.57. There remains, however, considerable ‚Äúnoise‚Äù despite the less-frequent sampling interval, and it is not clear the difference is signiÔ¨Åcant. The second model, goodput_ratio 0.5(RTT_ratio)2, still also appears to remain a good Ô¨Åt. 31.4.2.1 Counting teeth in Python Using our earlier Python nstrace.py module, we can easily count the times cwnd_ is reduced; these events correspond to loss responses. Anticipating more complex scenarios, we deÔ¨Åne a Python class flowstats to hold the per-Ô¨Çow information, though in this particular case we know there are exactly two Ô¨Çows. We count every timecwnd_ gets smaller; we also keep a count of coarse timeouts. As a practical matter, two closely spaced reductions in cwnd_ are always due to a fast-recovery event followed about a second later by a coarse timeout. If we want a count of each separate TCP response, we count them both. We do not count anything until after STARTPOINT. This is to avoid counting losses related to slow start. #!/usr/bin/python3 import nstrace import sys (continues on next page) 31.4 TCP Loss Events and Synchronized Losses 863
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) # counts all points where cwnd_ drops. STARTPOINT = 3.0 # wait for slow start to be done class flowstats: # python object to hold per-flow data def__init__(self): self.toothcount = 0 self.prevcwnd = 0 self.CTOcount = 0 # Coarse TimeOut count defcountpeaks(filename): globalSTARTPOINT nstrace.nsopen(filename) flow0 = flowstats() flow1 = flowstats() while not nstrace.isEOF(): ifnstrace.isVar(): # counting cwnd_ trace lines (time, snode, dummy, dummy, dummy, varname, cwnd) = nstrace. √£√ëgetVar() if(time < STARTPOINT): continue ifvarname != "cwnd_": continue ifsnode == 0: flow=flow0 else: flow=flow1 ifcwnd < flow.prevcwnd: flow.toothcount += 1 # count this as a tooth ifcwnd == 1.0: flow.CTOcount += 1 # coarse timeout flow.prevcwnd=cwnd else: nstrace.skipline() print ("flow 0 teeth:", flow0.toothcount, "flow 1 teeth:", flow1. √£√ëtoothcount) print ("flow 0 coarse timeouts:", flow0.CTOcount, "flow 1 coarse timeouts: √£√ë", flow1.CTOcount) countpeaks(sys.argv[1]) A more elaborate version of this script, set up to count clusters of teeth and clusters of drops, is available in teeth.py. If a new cluster starts at time T, all teeth/drops by time T + granularity are part of the same cluster; the next tooth/drop after T + granularity starts the next new cluster. 31.4.2.2 Relative loss rates At this point we accept that the A‚ÄìD/B‚ÄìD throughput ratio is generally smaller than the value predicted by the synchronized-loss hypothesis, and so the A‚ÄìD Ô¨Çow must have additional losses to account for this. Our next experiment is to count the loss events in each Ô¨Çow, and to identify the loss events common to both Ô¨Çows. 864 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 The following graph demonstrates the rise in A‚ÄìD losses as a percentage of the total, using SACK TCP. We use the tooth-counting script from the previous section, with a granularity of 1.0 sec. With SACK TCP it is rare for two drops in the same Ô¨Çow to occur close to one another; the granularity here is primarily to decide when two teeth in the two different Ô¨Çows are to be counted as shared. The blue region at the bottom represents the percentage of all loss-response events (teeth) that are shared between both Ô¨Çows. The green region in the middle represents the percentage of all loss-response events that apply only to the faster A‚ÄìD connection (Ô¨Çow 0); these rise to 30% very quickly and remain at about 50% asdelayB ranges from just over 100 up to 400. The uppermost yellow region represents the loss events that affected only the slower B‚ÄìD connection (Ô¨Çow 1); these are usually under 10%. As a rough approximation, if we assume a 50/50 division between shared (blue) and Ô¨Çow-0-only (green) loss events, we have losscount 0/losscount 1=ùõæ= 2. Applying the formula of 21.2.2 Unsynchronized TCP Losses, we get bandwidth 0/bandwidth 1= (RTT_ratio)2/2, or ratio2 = 1/2. While it is not at all clear this will continue to hold as delayB continues to increase beyond 400, it does suggest a possible underlying explanation for the second formula of 31.3.10.1 Possible models. 31.4.3 Loss rate versus cwnd: part 2 In21.2 TCP Reno loss rate versus cwnd we argued that the average value of cwnd was about K/?p, where the constant K was somewhere between 1.225 and 1.309. In 31.2.6.5 Loss rate versus cwnd: part 1 above we tested this for a single connection with very regular teeth; in this section we will test this hypothesis in the two-connections simulation. In order to avoid coarse timeouts, which were not included in the original model, we will use SACK TCP. Over the lifetime of a connection, the average cwnd is the number of packets sent divided by the number of RTTs. This simple relationship is slightly complicated by the fact that the RTT varies with the fullness of the bottleneck queue, but, as before, this effect can be minimized if we choose models where RTT noLoad is large compared with the queuing delay. We will as before set bottleneckBW = 8.0; at this bandwidth queuing delays add less than 10% to RTT noLoad. For the longer-path Ô¨Çow1, RTT noLoad220 + 2delayB ms. We will for both Ô¨Çows use the appropriate RTT noLoad as an approximation for RTT, and will use the number of packets acknowledged as an approximation for the number transmitted. These calculations give 31.4 TCP Loss Events and Synchronized Losses 865
An Introduction to Computer Networks, Release 2.0.11 the true average cwnd values in the table below. The estimated average cwnd values are from the formula K/?p, where the loss ratio p is the total number of teeth, as counted earlier, divided by the number of that Ô¨Çow‚Äôs packets. With the relatively high value for bottleneckBW it takes a long simulation to get a reasonable number of losses. The simulations used here ran 3000 seconds, long enough that each connection ended up with 100-200 losses. Here is the data, for each Ô¨Çow, using K=1.225. The bold columns represent the extent by which the ratio of the estimated average cwnd to the true average cwnd differs from unity. These error values are reasonably close to zero, suggesting that the formula cwnd = K/?p is reasonably accurate. delayB true avg cwnd0 est avg cwnd0 error0true avg cwnd1 est avg cwnd1 error1 0 89.2 93.5 4.8% 87.7 92.2 5.1% 10 92.5 95.9 3.6% 95.6 99.2 3.8% 20 95.6 99.2 3.7% 98.9 101.9 3.0% 40 104.9 108.9 3.8% 103.3 105.6 2.2% 70 117.0 121.6 3.9% 101.5 102.8 1.3% 100 121.9 126.9 4.1% 104.1 104.1 0% 150 125.2 129.8 3.7% 112.7 109.7 -2.6% 200 133.0 137.5 3.4% 95.9 93.3 -2.7% 250 133.4 138.2 3.6% 81.0 78.6 -3.0% 300 134.6 138.8 3.1% 74.2 70.9 -4.4% 350 135.2 139.1 2.9% 69.8 68.4 -2.0% 400 137.2 140.6 2.5% 60.4 58.5 -3.2% The table also clearly shows the rise in Ô¨Çow0‚Äôs cwnd, cwnd0, and also the fall in cwnd1. A related observation is that the absolute number of losses ‚Äì for either Ô¨Çow ‚Äì slowly declines as delayB increases, at least in the delayB¬§400 range we have been considering. For Ô¨Çow 1, with the longer RTT, this is because the number of packets transmitted drops precipitously (almost sevenfold) while cwnd ‚Äì and therefore the loss-event probability p ‚Äì stays relatively constant. For Ô¨Çow 0, the total number of packets sent rises as Ô¨Çow 1 drops to insigniÔ¨Åcance, but only by about 50%, and the average cwnd0 (above) rises sufÔ¨Åciently fast (due to Ô¨Çow 1‚Äôs decline) that total_losses = total_sent p = total_sentK/cwnd2 generally does not increase. 31.5 TCP Reno versus TCP Vegas In22.6 TCP Vegas we described an alternative to TCP Reno known as TCP Vegas; in 22.6.1 TCP Vegas versus TCP Reno we argued that when the capacity of a FIFO bottleneck queue was signiÔ¨Åcant relative to the transit capacity, TCP Vegas would be at a disadvantage, but competition might be fairer when the queue capacity was small. Here we will test that theory. In our standard model in 31.3 Two TCP Senders Competing withbottleneckBW = 8.0 Mbps, A‚ÄìR and B‚ÄìR delays of 10 ms and an R‚ÄìD delay of 100 ms, the transit capacity for the A‚ÄìD and B‚ÄìD paths is 220 866 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 packets. We will simulate a competition between TCP Reno and TCP Vegas for queue sizes from 10 to 220; as earlier, we will use overhead = 0.002. In our Ô¨Årst attempt, with the default TCP Vegas parameters of ùõº=1 and ùõΩ=3, things start out rather evenly: when the queuesize is 10 the Reno/Vegas goodput ratio is 1.02. However, by the time the queue size is 220, the ratio has climbed almost to 22; TCP Vegas is swamped. Raising ùõºandùõΩhelps a little for larger queues (perhaps at the expense of performance at small queue sizes); here is the graph for ùõº=3 and ùõΩ=6: The vertical axis plots the Reno-to-Vegas goodput ratio; the horizontal axis represents the queue capacity. The performance of TCP Vegas falls off quite quickly as the queue size increases. One reason the larger ùõº may help is that this slightly increases the range in which TCP Vegas behaves like TCP Reno. (For a rather different outcome using fair queuing rather than FIFO queuing, see 23.6.1 Fair Queuing and Bufferbloat .) To create an ns-2 TCP Vegas connection and set ùõºandùõΩone uses set tcp1 [new Agent/TCP/Vegas] $tcp1 set v_alpha_ 3 $tcp1 set v_beta_ 6 In prior simulations we have also made the following setting, in order to make the total TCP packetsize including headers be 1000: Agent/TCP set packetSize_ 960 It turns out that for TCP Vegas objects in ns-2, the packetSize_ includes the headers, as can be veriÔ¨Åed by looking at the traceÔ¨Åle, and so we need 31.5 TCP Reno versus TCP Vegas 867
An Introduction to Computer Networks, Release 2.0.11 Agent/TCP/Vegas set packetSize_ 1000 Here is acwnd -v-time graph comparing TCP Reno and TCP Vegas; the queuesize is 20, bottleneckBW is 8 Mbps,overhead is 0.002, and ùõº=3 and ùõΩ=6. The Ô¨Årst 300 seconds are shown. During this period the bandwidth ratio is about 1.1; it rises to close to 1.3 (all in TCP Reno‚Äôs favor) when T=1000. The red plot represents TCP Reno and the green represents TCP Vegas. The green plot shows some spikes that probably represent implementation artifacts. Five to ten seconds before each sharp TCP Reno peak, TCP Vegas has its own softer peak. The RTT has begun to rise, and TCP Vegas recognizes this and begins decrementing cwnd by 1 each RTT. At the point of packet loss, TCP Vegas begins incrementing cwnd again. During the cwnd -decrement phase, TCP Vegas falls behind relative to TCP Reno, but it may catch up during the subsequent increment phase because TCP Vegas often avoids the cwnd =cwnd /2 multiplicative decrease and so often continues after a loss event with a larger cwnd than TCP Reno. We conclude that, for smaller bottleneck-queue sizes, TCP Vegas does indeed hold its own. Unfortunately, in the scenario here the bottleneck-queue size has to be quite small for this to work; TCP Vegas suffers in competition with TCP Reno even for moderate queue sizes. That said, queue capacities out there in the real Internet tend to increase much more slowly than bandwidth, and there may be real-world situations were TCP Vegas performs quite well when compared to TCP Reno. 31.6 Wireless Simulation When simulating wireless networks, there are no links; all the conÔ¨Åguration work goes into setting up the nodes, the trafÔ¨Åc and the wireless behavior itself. Wireless nodes have multiple wireless-speciÔ¨Åc attributes, such as the antenna type and radio-propagation model. Nodes are also now in charge of packet queuing; before this was the responsibility of the links. Finally, nodes have coordinates for position and, if mobility is introduced, velocities. For wired links the user must set the bandwidth and delay. For wireless, these are both generally provided by the wireless model. Propagation delay is simply the distance divided by the speed of light. Bandwidth is usually built in to the particular wireless model chosen; for the Mac/802_11 model, it is available in 868 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 attributedataRate_ (which can be set). To Ô¨Ånd the current value, one can print [Mac/802_11 set dataRate_]; in ns-2 version 2.35 it is 1mb. Ad hoc wireless networks must also be conÔ¨Ågured with a routing protocol, so that paths may be found from one node to another. We looked brieÔ¨Çy at DSDV in 13.4.1 DSDV; there are many others. The maximum range of a node is determined by its power level; this can be set with node-config below (using the txPower attribute), but the default is often used. In the ns-2 source code, in Ô¨Åle wireless-phy.cc, the variable Pt_ ‚Äì for transmitter power ‚Äì is declared; the default value of 0.28183815 translates to a physical range of 250 meters using the appropriate radio-attenuation model. We create a simulation here in which one node ( mover ) moves horizontally above a sequence of Ô¨Åxedposition nodes (stored in the Tcl array rownodes ). The leftmost Ô¨Åxed-position node transmits continuously to themover node; as the mover node progresses, packets must be routed through other Ô¨Åxed-position nodes. The Ô¨Åxed-position nodes here are 200 m apart, and the mover node is 150 m above their line; this means that the mover reaches the edge of the range of the ith rownode when it is directly above the i+1th rownode. We use Ad hoc On-demand Distance Vector (AODV) as the routing protocol. When the mover moves out of range of one Ô¨Åxed-position node, AODV Ô¨Ånds a new route (which will be via the next Ô¨Åxed-position node) quite quickly; we return to this below. DSDV ( 13.4.1 DSDV ) is much slower, which leads to many packet losses until the new route is discovered. Of course, whether a given routing mechanism is fast enough depends very much on the speed of the mover; the simulation here does not perform nearly as well if the time is set to 10 seconds rather than 100 as the mover moves too fast even for AODV to keep up. Because there are so many conÔ¨Åguration parameters, to keep them together we adopt the common convention of making them all attributes of a single Tcl object, named opt. We list the simulation Ô¨Åle itself in pieces, with annotation; the complete Ô¨Åle is at wireless.tcl. We begin with the options. # ====================================================================== # Define options # ====================================================================== set opt(chan) Channel/WirelessChannel ;# channel type set opt(prop) Propagation/TwoRayGround ;# radio-propagation model set opt(netif) Phy/WirelessPhy ;# network interface type set opt(mac) Mac/802_11 ;# MAC type set opt(ifq) Queue/DropTail/PriQueue ;# interface queue type set opt(ll) LL ;# link layer type set opt(ant) Antenna/OmniAntenna ;# antenna model set opt(ifqlen) 50 ;# max packet in ifq set opt(bottomrow) 5 ;# number of bottom-row nodes set opt(spacing) 200 ;# spacing between bottom√£√ërow nodes set opt(mheight) 150 ;# height of moving node √£√ëabove bottom-row nodes set opt(brheight) 50 ;# height of bottom-row √£√ënodes from bottom edge set opt(x) [expr ($opt(bottomrow)-1) *$opt(spacing)+1] ;# x √£√ëcoordinate of topology set opt(y) 300 ;# y coordinate of topology (continues on next page) 31.6 Wireless Simulation 869
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) set opt(adhocRouting) AODV ;# routing protocol set opt(finish) 100 ;# time to stop simulation # the next value is the speed in meters/sec to move across the field set opt(speed) [expr 1.0 *$opt(x)/$opt(finish)] TheChannel/WirelessChannel class represents the physical terrestrial wireless medium; there is also aChannel/Sat class for satellite radio. The Propagation/TwoRayGround is a particular radiopropagation model. The TwoRayGround model takes into account ground reÔ¨Çection; for larger inter-node distances d, the received power level is proportional to 1/d4. Other models are the free-space model (in which received power at distance d is proportional to 1/d2) and the shadowing model, which takes into account other types of interference. Further details can be found in the Radio Propagation Models chapter of the ns-2 manual. ThePhy/WirelessPhy class speciÔ¨Åes the standard wireless-node interface to the network; alternatives includePhy/WirelessPhyExt with additional options and a satellite-speciÔ¨Åc Phy/Sat. TheMac/ 802_11 class speciÔ¨Åes IEEE 802.11 (that is, Wi-Fi) behavior; other options cover things like generic CSMA/CA, Aloha, and satellite. The Queue/DropTail/PriQueue class speciÔ¨Åes the queuing behavior of each node; the opt(ifqlen) value determines the maximum queue length and so corresponds to thequeue-limit value for wired links. The LLclass, for Link Layer, deÔ¨Ånes things like the behavior of ARP on the network. TheAntenna/OmniAntenna class deÔ¨Ånes a standard omnidirectional antenna. There are many kinds of directional antennas in the real world ‚Äì egparabolic dishes and waveguide ‚Äúcantennas‚Äù ‚Äì and a few have been implemented as ns-2 add-ons. The next values are speciÔ¨Åc to our particular layout. The opt(bottomrow) value determines the number of Ô¨Åxed-position nodes in the simulation. The spacing between adjacent bottom-row nodes is opt(spacing) meters. The moving node mover moves at height 150 meters above this Ô¨Åxed row. Whenmover is directly above a Ô¨Åxed node, it is thus at distance?(2002+ 1502) = 250 from the previous Ô¨Åxed node, at which point the previous node is out of range. The Ô¨Åxed row itself is 50 meters above the bottom of the topology. The opt(x) andopt(y) values are the dimensions of the simulation, in meters; the number of bottom-row nodes and their spacing determine opt(x). As mentioned earlier, we use the AODV routing mechanism. When the mover node moves out of range of the bottom-row node that it is currently in contact with, AODV receives notice of the failed transmission from the Wi-Fi link layer (ultimately this news originates from the absence of the Wi-Fi link-layer ACK). This triggers an immediate search for a new route, which typically takes less than 50 ms to complete. The earlier DSDV ( 13.4.1 DSDV ) mechanism does not use Wi-Fi link-layer feedback and so does not look for a new route until the next regularly scheduled round of distance-vector announcements, which might be several seconds away. Other routing mechanisms include TORA, PUMA, and OLSR. The Ô¨Ånishing time opt(finish) also represents the time the moving node takes to move across all the bottom-row nodes; the necessary speed is calculated in opt(speed). If the Ô¨Ånishing time is reduced, the mover speed increases, and so the routing mechanism has less time to Ô¨Ånd updated routes. The next section of Tcl code sets up general bookkeeping: 870 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 # create the simulator object set ns [new Simulator] # set up tracing $ns use-newtrace set tracefd [open wireless.tr w] set namtrace [open wireless.nam w] $ns trace-all $tracefd $ns namtrace-all-wireless $namtrace $opt(x) $opt(y) # create and define the topography object and layout set topo [new Topography] $topo load_flatgrid $opt(x) $opt(y) # create an instance of General Operations Director, which keeps track of √£√ënodes and # node-to-node reachability. The parameter is the total number of nodes in √£√ëthe simulation. create-god [expr $opt(bottomrow) + 1] Theuse-newtrace option enables a different tracing mechanism, in which each attribute except the Ô¨Årst is preÔ¨Åxed by an identifying tag, so that parsing is no longer position-dependent. We look at an example below. Note the special option namtrace-all-wireless for tracing for nam, and the dimension parameters opt(x) andopt(y). The next step is to create a Topography object to hold the layout (still to be determined). Finally, we create a General Operations Director, which holds information about the layout not necessarily available to any node. The next step is to call node-config, which passes many of the opt() parameters to ns and which inÔ¨Çuences future node creation: # general node configuration set chan1 [new $opt(chan)] $ns node-config -adhocRouting $opt(adhocRouting) \ -llType $opt(ll) \ -macType $opt(mac) \ -ifqType $opt(ifq) \ -ifqLen $opt(ifqlen) \ -antType $opt(ant) \ -propType $opt(prop) \ -phyType $opt(netif) \ -channel $chan1 \ -topoInstance $topo \ -wiredRouting OFF \ -agentTrace ON \ -routerTrace ON \ -macTrace OFF Finally we create our nodes. The bottom-row nodes are created within a Tcl for-loop, and are stored in a 31.6 Wireless Simulation 871
An Introduction to Computer Networks, Release 2.0.11 Tcl arrayrownode(). For each node we set its coordinates ( X_,Y_andZ_); it is at this point that the rownode() nodes are given positions along the horizontal line y=50 and spaced opt(spacing) apart. # create the bottom-row nodes as a node array $rownode(), and the moving node √£√ëas $mover for {set i 0} {$i < $opt(bottomrow)} {incr i} { set rownode($i) [$ns node] $rownode($i) set X_ [expr $i *$opt(spacing)] $rownode($i) set Y_ $opt(brheight) $rownode($i) set Z_ 0 } set mover [$ns node] $mover set X_ 0 $mover set Y_ [expr $opt(mheight) + $opt(brheight)] $mover set Z_ 0 We now make the mover node move, using setdest. If the node reaches the destination supplied in setdest, it stops, but it is also possible to change its direction at later times using additional setdest calls, if a zig-zag path is desired. Various external utilities are available to create a Ô¨Åle of Tcl commands to create a large number of nodes each with a designated motion; such a Ô¨Åle can then be imported into the main Tcl Ô¨Åle. set moverdestX [expr $opt(x) - 1] $ns at 0 "$mover setdest $moverdestX [$mover set Y_] $opt(speed)" Next we create a UDP agent and a CBR (Constant Bit Rate) application, and set up a connection from rownode(0) tomover. CBR trafÔ¨Åc does notuse sliding windows. # setup UDP connection, using CBR traffic set udp [new Agent/UDP] set null [new Agent/Null] $ns attach-agent $rownode(0) $udp $ns attach-agent $mover $null $ns connect $udp $null set cbr1 [new Application/Traffic/CBR] $cbr1 set packetSize_ 512 $cbr1 set rate_ 200kb $cbr1 attach-agent $udp $ns at 0 "$cbr1 start" $ns at $opt(finish) "$cbr1 stop" The remainder of the Tcl Ô¨Åle includes additional bookkeeping for nam, a finish{} procedure, and the startup of the simulation. # tell nam the initial node position (taken from node attributes) # and size (supplied as a parameter) (continues on next page) 872 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) for {set i 0} {$i < $opt(bottomrow)} {incr i} { $ns initial_node_pos $rownode($i) 10 } $ns initial_node_pos $mover 20 # set the color of the mover node in nam $mover color blue $ns at 0.0 "$mover color blue" $ns at $opt(finish) "finish" proc finish {} { global ns tracefd namtrace $ns flush-trace close $tracefd close $namtrace exit 0 } # begin simulation $ns run The simulation can be viewed from the nam Ô¨Åle, available at wireless.nam. In the simulation, the mover node moves across the topography, over the bottom-row nodes. The CBR trafÔ¨Åc reaches mover from rownode(0) Ô¨Årst directly, then via rownode(1), then viarownode(1) andrownode(2) ,etc. The motion of the mover node is best seen by speeding up the animation frame rate using the nam control for this, though doing this means that aliasing effects often make the CBR trafÔ¨Åc appear to be moving in the opposite direction. Above is one frame from the animation, with the mover node is almost (but not quite) directly over rownode(3), and so is close to losing contact with rownode(2). Two CBR packets can be seen en route; one has almost reached rownode(2) and one is about a third of the way from rownode(2) up to the bluemover node. The packets are not shown to scale; see exercise 17. 31.6 Wireless Simulation 873
An Introduction to Computer Networks, Release 2.0.11 The traceÔ¨Åle is speciÔ¨Åc to wireless networking, and even without the use of use-newtrace has a rather different format from the link-based simulations earlier. The newtrace format begins with a letter for send/receive/ drop/forward; after that, each logged attribute is identiÔ¨Åed with a preÔ¨Åxed tag rather than by position. Full details can be found in the ns-2 manual. Here is an edited record of the Ô¨Årst packet drop (the initialdindicates a drop-event record): d -t 22.586212333 -Hs 0 -Hd 5 ... -Nl RTR -Nw CBK ... -Ii 1100 ... -Pn cbr - √£√ëPi 1100 ... The-ttag indicates the time. The -Hs and-Hd tags indicate the source and destination, respectively. The-Nl tag indicates the ‚Äúlevel‚Äù (RouTeR) at which the loss was logged, and the -Nw tag indicates the cause:CBK, for ‚ÄúCallBacK‚Äù, means that the packet loss was detected at the link layer but the information was passed up to the routing layer. The -Ii tag is the packet‚Äôs unique serial number, and the Ptags supply information about the constant-bit-rate agent. We can use the traceÔ¨Åle to Ô¨Ånd clusters of drops beginning at times 22.586, 47.575, 72.707 and 97.540, corresponding roughly to the times when the route used to reach the $mover node shifts to passing through one more bottom-row node. Between t=72.707 and t=97.540 there are several other somewhat more mysterious clusters of drops; some of these clusters may be related to ordinary queue overÔ¨Çow but others may reÔ¨Çect decreasing reliability of the forwarding mechanism as the path grows longer. 31.7 Epilog Simulations using ns (either ns-2 or ns-3) are a central part of networks research. Most scientiÔ¨Åc papers addressing comparisons between TCP Ô¨Çavors refer to ns simulations to at least some degree; ns is also widely used for non-TCP research (especially wireless). But simulations are seldom a matter of a small number of runs. New protocols must be tested in a wide range of conditions, with varying bandwidths, delays and levels of background trafÔ¨Åc. Head-to-head comparisons in isolation, such as our Ô¨Årst runs in 31.3.3 Unequal Delays, can be very misleading. Good simulation design, in other words, is not easy. Our simulations here involved extremely simple networks. A great deal of effort has been expended by the ns community in the past decade to create simulations involving much larger sets of nodes; the ultimate goal is to create realistic simulations of the Internet itself. We refer the interested reader to [FP01]. 31.8 Exercises 1.0. In the graph in 31.2.1 Graph of cwnd v time, examine the trace Ô¨Åle to see what accounts for the dot at the start of each tooth (at times approximately 4.1, 6.1 and 8.0). Note that the solid parts of the graph are as solid as they are because fractional cwnd values are used; cwnd is incremented by 1/ cwnd on receipt of each ACK. 2.0. A problem with the single-sender link-utilization experiment at 31.2.6 Single-sender Throughput Experiments was that the smallest practical value for queue-limit was 3, which is 10% of the path transit capacity. Repeat the experiment but arrange for the path transit capacity to be at least 100 packets, 874 31 Network Simulations: ns-2
An Introduction to Computer Networks, Release 2.0.11 making aqueue-limit of 3 much smaller proportionally. Be sure the simulation runs long enough that it includes multiple teeth. What link utilization do you get? Also try queue-limit values of 4 and 5. 3.0. Create a single-sender simulation in which the path transit capacity is 90 packets, and the bottleneck queue-limit is 30. This should mean cwnd varies between 60 and 120. Be sure the simulation runs long enough that it includes many teeth. a. What link utilization do you observe? b. What queue utilization do you observe? 4.0. Use the basic2 model with equal propagation delays ( delayB = 0), but delay the starting time for the Ô¨Årst connection. Let this delay time be startdelay0; at the end of the basic2.tcl Ô¨Åle, you will have $ns at $startdelay0 "$ftp0 start" Try this for startdelay0 ranging from 0 to 40 ms, in increments of 1.0 ms. Graph the ratio1 or ratio2 values as a function of startdelay0. Do you get a graph like the one in 31.3.4.2 Two-sender phase effects ? 5.0. If the bottleneck link forwards at 10 ms/packet ( bottleneckBW = 0.8), then in 300 seconds we can send 30,000 packets. What percentage of this, total, are sent by two competing senders as in 31.3 Two TCP Senders Competing, fordelayB = 0, 50, 100, 200 and 400. 6.0. Repeat the previous exercise for bottleneckBW = 8.0, that is, a bottleneck rate of 1 ms/packet. 7.0. Pick a case above where the total is less than 100%. Write a script that keeps track of cwnd 0+cwnd 1, and measure how much time this quantity is less than the transit capacity. What is the average of cwnd 0+ cwnd 1over those periods when it is less than the transit capacity? 8.0. In the model of 31.3 Two TCP Senders Competing, it is often the case that for small delayB ,eg delayB < 5, the longer-path B‚ÄìD connection has greater throughput. Demonstrate this. Use an appropriate value ofoverhead (eg0.02 or 0.002). 9.0. In generating the Ô¨Årst graph in 31.3.7 Phase Effects and telnet trafÔ¨Åc, we used a packetSize_ of 210 bytes, for an actualSize of 250 bytes. Generate a similar graph using in the simulations a much smaller value of packetSize_ ,eg10 or 20 bytes. Note that for a given value of tndensity, as the actualSize shrinks so should the tninterval. 10.0. In 31.3.7 Phase Effects and telnet trafÔ¨Åc the telnet connections ran from A and B to D, so the telnet trafÔ¨Åc competed with the bulk ftp trafÔ¨Åc on the bottleneck link. Change the simulation so the telnet trafÔ¨Åc runs only as far as R. The variable tndensity should now represent the fraction of the A‚ÄìR and B‚ÄìR bandwidth that is used for telnet. Try values for tndensity of from 5% to 50% (note these densities are quite high). Generate a graph like the Ô¨Årst graph in 31.3.7 Phase Effects and telnet trafÔ¨Åc, illustrating whether this form of telnet trafÔ¨Åc is effective at reducing phase effects. 11.0 Again using the telnet simulation of the previous exercise, in which the telnet trafÔ¨Åc runs only as far as R, generate a graph comparing ratio1 for the bulk ftp trafÔ¨Åc when randomization comes from: 
- overhead values in the range discussed in the text ( eg0.01 or 0.02 for bottleneckBW = 0.8 Mbps) 
- A‚ÄìR and B‚ÄìR telnet trafÔ¨Åc with tndensity in the range 5% to 50%. 31.8 Exercises 875
An Introduction to Computer Networks, Release 2.0.11 The goal should be a graph comparable to that of 31.3.8 overhead versus telnet. Do the two randomization mechanisms ‚Äì overhead and telnet ‚Äì still yield comparable values for ratio1? 12.0. Repeat the same experiment as in exercise 8.0, but using telnet instead of overhead. Try it with A‚ÄìD/B‚ÄìD telnet trafÔ¨Åc and tndensity around 1%, and also A‚ÄìR/B‚ÄìR trafÔ¨Åc as in exercise 10.0 with a tndensity around 10%. The effect may be even more marked. 13.0. Analyze the packet drops for each Ô¨Çow for the Reno-versus-Vegas competition shown in the second graph (red v green) of 31.5 TCP Reno versus TCP Vegas. In that simulation, bottleneckBW = 8.0 Mbps, delayB = 0,queuesize = 20, ùõº=3 and ùõΩ=6; the full simulation ran for 1000 seconds. (a). How many drop clusters are for the Reno Ô¨Çow only? (b). How many drop clusters are for the Vegas Ô¨Çow only? (c). How many shared drop clusters are there? Use a drop-cluster granularity of 2.0 seconds. 14.0. Generate a cwnd -versus-time graph for TCP Reno versus TCP Vegas, like the second graph in 31.5 TCP Reno versus TCP Vegas, except using the default ùõº=1. Does the TCP Vegas connection perform better or worse? 15.0. Compare two TCP Vegas connections as in 31.3 Two TCP Senders Competing, fordelayB varying from 0 to 400 (perhaps in steps of about 20). Use bottleneckBW = 8 Mbps, and run the simulations for at least 1000 seconds. Use the default ùõºandùõΩ(usually ùõº=1 and ùõΩ=3). Is ratio11 or ratio21 a better Ô¨Åt? How does the overall fairness compare with what ratio1 1 or ratio21 would predict? Does either ratio appear roughly constant? If so, what is its value? 16.0. Repeat the previous exercise for a much larger value of ùõº, say ùõº=10. Set ùõΩ=ùõº+2, and be sure queuesize is larger than 2 ùõΩ(recall that, in ns, ùõºisv_alpha_ andùõΩisv_beta_ ). If both connections manage to keep the same number of packets in the bottleneck queue at R, then both should get about the same goodput, by the queue-competition rule of 20.2.2 Example 2: router competition .Do they? Is the fairness situation better or worse than with the default ùõºandùõΩ? 17.0. In nam animations involving point-to-point links, packet lengths are displayed proportionally: if a link has a propagation delay of 10 ms and a bandwidth of 1 packet/ms, then each packet will be displayed with length 1/10 the link length. Is this true of wireless as well? Consider the animation (and single displayed frame) of 31.6 Wireless Simulation. Assume the signal propagation speed is c 300 m/¬µsec, that the nodes are 300 m apart, and that the bandwidth is 1 Mbps. (a). How long is a single bit? (That is, how far does the signal travel in the time needed to send a single bit?) (b). If a sender transmits continuously, how many bits will it send before its Ô¨Årst bit reaches its destination? (c). In the nam animation of 31.6 Wireless Simulation, is it plausible that what is rendered for the CBR packets represents just the Ô¨Årst bit of the packet? Is the scale about right for a single bit? (d). What might be a more accurate animated representation of wireless packets? 876 31 Network Simulations: ns-2
32 THE NS-3 NETWORK SIMULATOR In this chapter we take a somewhat cursory look at the ns-3 simulator, intended as a replacement for ns-2. The project is managed by the NS-3 Consortium, and all materials are available at www.nsnam.org. Ns-3 represents a rather sharp break from ns-2. Gone is the Tcl programming interface; instead, ns-3 simulation programs are written in the C++ language, with extensive calls to the ns-3 library, although they are often still referred to as simulation ‚Äúscripts‚Äù. As the simulator core itself is also written in C++, this in some cases allows improved interaction between conÔ¨Åguration and execution. However, conÔ¨Åguration and execution are still in most cases quite separate: at the end of the simulation script comes a call Simulator::Run() ‚Äì akin to ns-2‚Äôs $ns run ‚Äì at which point the user-written C++ has done its job and the library takes over. To conÔ¨Ågure a simple simulation, an ns-2 Tcl script had to create nodes and links, create network-connection ‚Äúagents‚Äù attached to nodes, and create trafÔ¨Åc-generating applications attached to agents. Much the same applies to ns-3, but in addition each node must be conÔ¨Ågured with its network interfaces, and each network interface must be assigned an IP address. 32.1 Installing and Running ns-3 We here outline the steps for installing ns-3 under Linux from the ‚Äúallinone‚Äù tar Ô¨Åle, assuming that all prerequisite packages (such as gcc) are already in place. Much more general installation instructions can be found at www.nsnam.org. In particular, serious users are likely to want to download the current Mercurial repository directly. Information is also available for Windows and Macintosh installation, although perhaps the simplest option for Windows users is to run ns-3 in a Linux virtual machine. The Ô¨Årst step is to unzip the tar Ô¨Åle; this should leave a directory named ns-allinone-3.nn, where nn reÔ¨Çects the version number (20 in the author‚Äôs installation as of this 2014 writing). This directory is the root of the ns-3 system; it contains a build.py (python) script and the primary ns-3 directory ns-3.nn. All that is necessary is to run the build.py script: ./build.py Considerable conÔ¨Åguration and then compiler output should ensue, hopefully terminating with a list of ‚ÄúModules built‚Äù and ‚ÄúModules not built‚Äù. From this point on, most ns-3 work will take place in the subdirectory ns-3.nn, that is, in ns-allinone-3.nn/ns3.nn. This development directory contains the source directory src, the script directory scratch, and the execution script waf. The development directory also contains a directory examples containing a rich set of example scripts. The scripts in examples/tutorial are described in depth in the ns-3 tutorial in doc/tutorial. 32.1.1 Running a Script Let us now run a script, for example, the Ô¨Åle Ô¨Årst.cc included in the examples/tutorial directory. We Ô¨Årst copy this Ô¨Åle into the directory ‚Äúscratch‚Äù, and then, in the parent development directory, enter the command 877
An Introduction to Computer Networks, Release 2.0.11 ./waf --run first The program is compiled and, if compilation is successful, is run. In fact, every uncompiled program in the scratch directory is compiled, meaning that projects in progress that are not yet compilable must be kept elsewhere. One convenient strategy is to maintain multiple project directories, and link them symbolically to scratch as needed. The ns-3 system includes support for command-line options; the following example illustrates the passing by command line of the value 3 for the variable nCsma: ./waf --run "second --nCsma=3" 32.1.2 Compilation Errors By default, ns-3 enables the -Werror option to the compiler, meaning that all warnings are treated as errors. This is good practice for contributed or published scripts, but can be rather exasperating for beginners. To disable this, edit the Ô¨Åle waf-tools/cÔ¨Çags.py (relative to the development directory). Change the line self.warnings_flags = [[ '-Wall'], ['-Werror' ], ['-Wextra' ]] to self.warnings_flags = [[ '-Wall'], ['-Wextra' ]] Then, in the development directory, run ./waf configure ./waf build 32.2 A Single TCP Sender We begin by translating the single-TCP-sender script of 31.2 A Single TCP Sender. The full program is in basic1.cc; we now review most of it line-by-line; some standard things such as #include directives are omitted. /* Network topology: A----R----B A--R: 10 Mbps / 10 ms delay R--B: 800 kbps / 50 ms delay queue at R: size 7 */ using namespace ns3; std::string fileNameRoot = "basic1"; // base name fortrace files, etc (continues on next page) 878 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) void CwndChange (Ptr<OutputStreamWrapper> stream, uint32_t oldCwnd, uint32_t √£√ënewCwnd) { *stream->GetStream () << Simulator::Now ().GetSeconds () << " "<< newCwnd √£√ë<< std::endl; } static void TraceCwnd () // Trace changes to the congestion window { AsciiTraceHelper ascii; Ptr<OutputStreamWrapper> stream = ascii.CreateFileStream (fileNameRoot + ". √£√ëcwnd"); Config::ConnectWithoutContext ( "/NodeList/0/$ns3::TcpL4Protocol/SocketList/ √£√ë0/CongestionWindow", MakeBoundCallback (&CwndChange,stream)); } The function TraceCwnd() arranges for tracing of cwnd; the function CwndChange is acallback, invoked by the ns-3 system whenever cwnd changes. Such callbacks are common in ns-3. The parameter string beginning /NodeList/0/... is an example of the conÔ¨Åguration namespace. Each ns-3 attribute can be accessed this way. See 32.2.2 The Ascii TraceÔ¨Åle below. int main (int argc, char *argv[]) { int tcpSegmentSize = 1000; Config::SetDefault ( "ns3::TcpSocket::SegmentSize", UintegerValue √£√ë(tcpSegmentSize)); Config::SetDefault ( "ns3::TcpSocket::DelAckCount", UintegerValue (0)); Config::SetDefault ( "ns3::TcpL4Protocol::SocketType", StringValue ( √£√ë"ns3::TcpReno" )); Config::SetDefault ( "ns3::RttEstimator::MinRTO", TimeValue (MilliSeconds √£√ë(500))); The use of Config::SetDefault() allows us to conÔ¨Ågure objects that will not exist until some later point, perhaps not until the ns-3 simulator is running. The Ô¨Årst parameter is an attribute string, of the formns3:: class::attribute. A partial list of attributes is at https://www.nsnam.org/docs/release/3.19/ doxygen/group___attribute_list.html. Attributes of a class can also be determined by a command such as the following: ./waf --run "basic1 --PrintAttributes=ns3::TcpSocket The advantage of the Config::SetDefault mechanism is that often objects are created indirectly, perhaps by ‚Äúhelper‚Äù classes, and so direct setting of class properties can be problematic. It is perfectly acceptable to issue some Config::SetDefault calls, then create some objects (perhaps implicitly), and then change the defaults (again with Config::SetDefault ) for creation of additional objects. We pick the TCP congesion-control algorithm by setting ns3::TcpL4Protocol::SocketType. Options are TcpRfc793 (no congestion control), TcpTahoe ,TcpReno ,TcpNewReno and 32.2 A Single TCP Sender 879
An Introduction to Computer Networks, Release 2.0.11 TcpWestwood. TCP Cubic and SACK TCP are not supported natively (though they are available if the Network Simulation Cradle is installed). Setting the DelAckCount attribute to 0 disables delayed ACKs. Setting the MinRTO value to 500 ms avoids some unexpected hard timeouts. We will return to both of these below in 32.2.3 Unexpected Timeouts and Other Phenomena. Next comes our local variables and command-line-option processing. In ns-3 the latter is handled via theCommandLine object, which also recognized the --PrintAttributes option above. Using the --PrintHelp option gives a list of variables that can be set via command-line arguments. unsigned int runtime = 20; // seconds int delayAR = 10; // ms int delayRB = 50; // ms double bottleneckBW= 0.8; // Mbps double fastBW = 10; // Mbps uint32_t queuesize = 7; uint32_t maxBytes = 0; // 0 means "unlimited" CommandLine cmd; // Here, we define command line options overriding some of the above. cmd.AddValue ( "runtime" ,"How long the applications should send data", √£√ëruntime); cmd.AddValue ( "delayRB" ,"Delay on the R--B link, in ms", delayRB); cmd.AddValue ( "queuesize" ,"queue size at R", queuesize); cmd.AddValue ( "tcpSegmentSize" ,"TCP segment size", tcpSegmentSize); cmd.Parse (argc, argv); std::cout << "queuesize=" << queuesize << ", delayRB=" << delayRB << √£√ëstd::endl; Next we create three nodes, illustrating the use of smart pointers and CreateObject(). Ptr<Node> A = CreateObject<Node> (); Ptr<Node> R = CreateObject<Node> (); Ptr<Node> B = CreateObject<Node> (); ClassPtr is a ‚Äúsmart pointer‚Äù that manages memory through reference counting. The template function CreateObject acts as the ns-3 preferred alternative to operator new. Parameters for objects created this way can be supplied via Config::SetDefault, or by some later method call applied to the Ptr object. For Node objects, for example, we might call A -> AddDevice(...). A convenient alternative to creating nodes individually is to create a container of nodes all at once: NodeContainer allNodes; allNodes.Create(3); Ptr<Node> A = allNodes.Get(0); ... After the nodes are in place we create our point-to-point links, using the PointToPointHelper class. We also create NetDeviceContainer objects; we don‚Äôt use these here (we could simply call AR. Install(A,R) ), but will need them below when assigning IPv4 addresses. 880 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 // use PointToPointChannel andPointToPointNetDevice NetDeviceContainer devAR, devRB; PointToPointHelper AR, RB; // create point-to-point link from Ato R AR.SetDeviceAttribute ( "DataRate", DataRateValue (DataRate (fastBW *1000* √£√ë1000))); AR.SetChannelAttribute ( "Delay", TimeValue (MilliSeconds (delayAR))); devAR = AR.Install(A, R); // create point-to-point link from Rto B RB.SetDeviceAttribute ( "DataRate", DataRateValue (DataRate (bottleneckBW * √£√ë1000*1000))); RB.SetChannelAttribute ( "Delay", TimeValue (MilliSeconds (delayRB))); RB.SetQueue( "ns3::DropTailQueue" ,"MaxPackets", UintegerValue(queuesize)); devRB = RB.Install(R,B); Next we hand out IPv4 addresses. The Ipv4AddressHelper class can help us with individual LANs ( eg A‚ÄìR and R‚ÄìB), but it is up to us to make sure our two LANs are on different subnets. If we attempt to put A and B on the same subnet, routing will simply fail, just as it would if we were to do this with real network nodes. InternetStackHelper internet; internet.Install (A); internet.Install (R); internet.Install (B); // Assign IP addresses Ipv4AddressHelper ipv4; ipv4.SetBase ( "10.0.0.0" ,"255.255.255.0" ); Ipv4InterfaceContainer ipv4Interfaces; ipv4Interfaces.Add (ipv4.Assign (devAR)); ipv4.SetBase ( "10.0.1.0" ,"255.255.255.0" ); ipv4Interfaces.Add (ipv4.Assign(devRB)); Ipv4GlobalRoutingHelper::PopulateRoutingTables (); Next we print out the addresses assigned. This gives us a peek at the GetObject template and the ns-3 object-aggregation model. The original Node objects we created earlier were quite generic; they gained their Ipv4 component in the code above. Now we retrieve that component with the GetObject<Ipv4>() calls below. Ptr<Ipv4> A4 = A->GetObject<Ipv4>(); // gets node A 's IPv4 subsystem Ptr<Ipv4> B4 = B->GetObject<Ipv4>(); Ptr<Ipv4> R4 = R->GetObject<Ipv4>(); Ipv4Address Aaddr = A4->GetAddress(1,0).GetLocal(); Ipv4Address Baddr = B4->GetAddress(1,0).GetLocal(); Ipv4Address Raddr = R4->GetAddress(1,0).GetLocal(); std::cout << "A's address: " << Aaddr << std::endl; (continues on next page) 32.2 A Single TCP Sender 881
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) std::cout << "B's address: " << Baddr << std::endl; std::cout << "R's #1 address: " << Raddr << std::endl; std::cout << "R's #2 address: " << R4->GetAddress(2,0).GetLocal() << √£√ëstd::endl; In general, A->GetObject<T> returns the component of type Tthat has been ‚Äúaggregated‚Äù to Ptr<Object> A; often this aggregation is invisible to the script programmer but an understanding of how it works is sometimes useful. The aggregation is handled by the ns-3 Object class, which contains an internal list m_aggregates of aggregated companion objects. At most one object of a given type can be aggregated to another, making GetObject<T> unambiguous. Given a Ptr<Object> A, we can obtain an iterator over the aggregated companions via A->GetAggregateIterator(), of type Object::AggregateIterator. From each Ptr<const Object> B returned by this iterator, we can callB->GetInstanceTypeId().GetName() to get the class name of B. TheGetAddress() calls take two parameters; the Ô¨Årst specÔ¨Åes the interface (a value of 0 gives the loopback interface) and the second distinguishes between multiple addresses assigned to the same interface (which is not happening here). The call A4->GetAddress(1,0) returns an Ipv4InterfaceAddress object containing, among other things, an IP address, a broadcast address and a netmask; GetLocal() returns the Ô¨Årst of these. Next we create the receiver on B, using a PacketSinkHelper. A receiver is, in essense, a read-only form of an application server. // create a sink on B uint16_t Bport = 80; Address sinkAaddr(InetSocketAddress (Ipv4Address::GetAny (), Bport)); PacketSinkHelper sinkA ( "ns3::TcpSocketFactory", sinkAaddr); ApplicationContainer sinkAppA = sinkA.Install (B); sinkAppA.Start (Seconds (0.01)); // the following means the receiver will run 1 min longer than the sender app. sinkAppA.Stop (Seconds (runtime + 60.0)); Address sinkAddr(InetSocketAddress(Baddr, Bport)); Now comes the sending application, on A. We must conÔ¨Ågure and create a BulkSendApplication, attach it to A, and arrange for a connection to be created to B. The BulkSendHelper class simpliÔ¨Åes this. BulkSendHelper sourceAhelper ( "ns3::TcpSocketFactory", sinkAddr); sourceAhelper.SetAttribute ( "MaxBytes", UintegerValue (maxBytes)); sourceAhelper.SetAttribute ( "SendSize", UintegerValue (tcpSegmentSize)); ApplicationContainer sourceAppsA = sourceAhelper.Install (A); sourceAppsA.Start (Seconds (0.0)); sourceAppsA.Stop (Seconds (runtime)); If we did not want to use the helper class here, the easiest way to create the BulkSendApplication is with an ObjectFactory. We conÔ¨Ågure the factory with the type we want to create and the relevant conÔ¨Åguration parameters, and then call factory.Create(). (We could have used the Config::SetDefault() mechanism and CreateObject() as well.) 882 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 ObjectFactory factory; factory.SetTypeId ( "ns3::BulkSendApplication" ); factory.Set ( "Protocol", StringValue ( "ns3::TcpSocketFactory" )); factory.Set ( "MaxBytes", UintegerValue (maxBytes)); factory.Set ( "SendSize", UintegerValue (tcpSegmentSize)); factory.Set ( "Remote", AddressValue (sinkAddr)); Ptr<Object> bulkSendAppObj = factory.Create(); Ptr<Application> bulkSendApp = bulkSendAppObj -> GetObject<Application>(); bulkSendApp->SetStartTime(Seconds(0.0)); bulkSendApp->SetStopTime(Seconds(runtime)); A->AddApplication(bulkSendApp); The above gives us no direct access to the actual TCP connection. Yet another alternative is to start by creating the TCP socket and connecting it: Ptr<Socket> tcpsock = Socket::CreateSocket (A, TcpSocketFactory::GetTypeId √£√ë()); tcpsock->Bind(); tcpsock->Connect(sinkAddr); However, there is then no mechanism for creating a BulkSendApplication that uses a pre-existing socket. (For a workaround, see the tutorial example fifth.cc .) Before beginning execution, we set up tracing; we will look at the traceÔ¨Åle format later. We use the AR PointToPointHelper class here, but both ascii and pcap tracing apply to the entire A‚ÄìR‚ÄìB network. // Set up tracing AsciiTraceHelper ascii; std::string tfname = fileNameRoot + ".tr"; AR.EnableAsciiAll (ascii.CreateFileStream (tfname)); // Setup tracing forcwnd Simulator::Schedule(Seconds(0.01),&TraceCwnd); // this Time cannot be 0. √£√ë0 // This tells ns-3 to generate pcap traces, including "-node#-dev#-" in √£√ëfilename AR.EnablePcapAll (fileNameRoot); // ".pcap" suffix isadded automatically This last creates four .pcap Ô¨Åles, eg basic1-0-0.pcap basic1-1-0.pcap basic1-1-1.pcap basic1-2-0.pcap The Ô¨Årst number refers to the node (A=0, R=1, B=2) and the second to the interface. A packet arriving at R but dropped there will appear in the second .pcap Ô¨Åle but not the third. These Ô¨Åles can be viewed with WireShark. Finally we are ready to start the simulator! The BulkSendApplication will stop at time runtime, but trafÔ¨Åc may be in progress. We allow it an additional 60 seconds to clear. We also, after the simulation has run, print out the number of bytes received by B. 32.2 A Single TCP Sender 883
An Introduction to Computer Networks, Release 2.0.11 Simulator::Stop (Seconds (runtime+60)); Simulator::Run (); Ptr<PacketSink> sink1 = DynamicCast<PacketSink> (sinkAppA.Get (0)); std::cout << "Total Bytes Received from A: " << sink1->GetTotalRx () << √£√ëstd::endl; return0; } 32.2.1 Running the Script When we run the script and plot the cwnd trace data (here for about 12 seconds), we get the following: Compare this graph to that in 31.2.1 Graph of cwnd v time produced by ns-2. The slow-start phase earlier ended at around 2.0 and now ends closer to 3.0. There are several modest differences, including the halving ofcwnd just before T=1 and the peak around T=2.6; these were not apparent in the ns-2 graph. After slow-start is over, the graphs are quite similar; cwnd ranges from 10 to 21. The period before was 1.946 seconds; here it is 2.0548; the difference is likely due to a more accurate implementation of the recovery algorithm. One striking difference is the presence of the near-vertical line of dots just after each peak. What is happening here is that ns-3 implements the cwnd inÔ¨Çation/deÔ¨Çation algorithm outlined at the tail end of 19.4 TCP Reno and Fast Recovery. When three dupACKs are received, cwnd is set tocwnd /2 + 3, and is then allowed to increase to 1.5 cwnd. See the end of 19.4 TCP Reno and Fast Recovery. 32.2.2 The Ascii TraceÔ¨Åle Below are four lines from the traceÔ¨Åle, starting with the record showing packet 271 (Seq=271001) being dropped by R. 884 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 d 4.9823 /NodeList/1/DeviceList/1/$ns3::PointToPointNetDevice/TxQueue/Drop √£√ëns3::PppHeader (Point-to-Point Protocol: IP (0x0021)) ns3::Ipv4Header (tos √£√ë0x0 DSCP Default ECN Not-ECT ttl 63 id 296 protocol 6 offset (bytes) 0 √£√ëflags [none] length: 1040 10.0.0.1 > 10.0.1.2) ns3::TcpHeader (49153 > 80 [ √£√ëACK ] Seq=271001 Ack=1 Win=65535) Payload (size=1000) r 4.98312 /NodeList/2/DeviceList/0/$ns3::PointToPointNetDevice/MacRx √£√ëns3::Ipv4Header (tos 0x0 DSCP Default ECN Not-ECT ttl 63 id 283 protocol 6 √£√ëoffset (bytes) 0 flags [none] length: 1040 10.0.0.1 > 10.0.1.2) √£√ëns3::TcpHeader (49153 > 80 [ ACK ] Seq=258001 Ack=1 Win=65535) Payload √£√ë(size=1000) + 4.98312 /NodeList/2/DeviceList/0/$ns3::PointToPointNetDevice/TxQueue/ √£√ëEnqueue ns3::PppHeader (Point-to-Point Protocol: IP (0x0021)) √£√ëns3::Ipv4Header (tos 0x0 DSCP Default ECN Not-ECT ttl 64 id 271 protocol 6 √£√ëoffset (bytes) 0 flags [none] length: 40 10.0.1.2 > 10.0.0.1) √£√ëns3::TcpHeader (80 > 49153 [ ACK ] Seq=1 Ack=259001 Win=65535) - 4.98312 /NodeList/2/DeviceList/0/$ns3::PointToPointNetDevice/TxQueue/ √£√ëDequeue ns3::PppHeader (Point-to-Point Protocol: IP (0x0021)) √£√ëns3::Ipv4Header (tos 0x0 DSCP Default ECN Not-ECT ttl 64 id 271 protocol 6 √£√ëoffset (bytes) 0 flags [none] length: 40 10.0.1.2 > 10.0.0.1) √£√ëns3::TcpHeader (80 > 49153 [ ACK ] Seq=1 Ack=259001 Win=65535) As with ns-2, the Ô¨Årst letter indicates the action: rfor received, dfor dropped, +for enqueued, -for dequeued. For Wi-Fi traceÔ¨Åles, tis for transmitted. The second Ô¨Åeld represents the time. The third Ô¨Åeld represents the name of the event in the conÔ¨Åguration namespace, sometimes called the conÔ¨Åguration path name. The NodeList value represents the node (A=0, etc), theDeviceList represents the interface, and the Ô¨Ånal part of the name repeats the action: Drop ,MacRx ,Enqueue ,Dequeue. After that come a series of class names ( egns3::Ipv4Header ,ns3::TcpHeader ), from the ns-3 attribute system, followed in each case by a parenthesized list of class-speciÔ¨Åc trace information. In the output above, the Ô¨Ånal three records all refer to node B ( /NodeList/2/ ). Packet 258 has just arrived (Seq=258001), and ACK 259001 is then enqueued and sent. 32.2.3 Unexpected Timeouts and Other Phenomena In the discussion of the script above at 32.2 A Single TCP Sender we mentioned that we set ns3::TcpSocket::DelAckCount to 0, to disable delayed ACKs, and ns3::RttEstimator::MinRTO to 500 ms, to avoid unexpected timeouts. If we comment out the line disabling delayed ACKs, little changes in our graph, except that the spacing between consecutive TCP teeth now almost doubles to 3.776. This is because with delayed ACKs the receiver sends only half as many ACKs, and the sender does not take this into account when incrementing cwnd (that is, the sender does not implement the suggestion of RFC 3465 mentioned in 19.2.1 TCP Reno Per-ACK Responses ). If we leave out the MinRTO adjustment, andsettcpSegmentSize to 960, we get a more serious problem: the graph now looks something like this: 32.2 A Single TCP Sender 885
An Introduction to Computer Networks, Release 2.0.11 We can enable ns-3‚Äôs internal logging in the TcpReno class by entering the commands below, before running the script. (In some cases, as with WifiHelper::EnableLogComponents(), logging output can be enabled from within the script.) Once enabled, logging output is written to stderr. NS_LOG=TcpReno=level_info export NS_LOG The log output shows the initial dupACK at 8.54: 8.54069 [node 0] Triple dupack. Reset cwnd to 12960, ssthresh to 10080 But then, despite Fast Recovery proceding normally, we get a hard timeout: 8.71463 [node 0] RTO. Reset cwnd to 960, ssthresh to 14400, restart from √£√ëseqnum510721 What is happening here is that the RTO interval was just a little too short, probably due to the use of the ‚Äúawkward‚Äù segment size of 960. After the timeout, there is another triple-dupACK! 8.90344 [node 0] Triple dupack. Reset cwnd to 6240, ssthresh to 3360 Shortly thereafter, at T=8.98, cwnd is reset to 3360, in accordance with the Fast Recovery rules. The overall effect is that cwnd is reset, not to 10, but to about 3.4 (in packets). This signiÔ¨Åcantly slows down throughput. In recovering from the hard timeout, the sequence number is reset to Seq=510721 (packet 532), as this was the last packet acknowledged. Unfortunately, several later packets had in fact made it through to B. By looking at the traceÔ¨Åle, we can see that at T=8.7818, B received Seq=538561, or packet 561. Thus, when A begins retransmitting packets 533, 534, etcafter the timeout, B‚Äôs response is to send the ACK the highest packet it has received, packet 561 (Ack=539521). 886 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 This scenario is not what the designers of Fast Recovery had in mind; it is likely triggered by a tooconservative timeout estimate. Still, exactly how to Ô¨Åx it is an interesting question; one approach might be to ignore, in Fast Recovery, triple dupACKs of packets now beyond what the sender is currently sending. 32.3 Wireless We next present the wireless simulation of 31.6 Wireless Simulation. The full script is at wireless.cc; the animation output for the netanim player is at wireless.xml. As before, we have one mover node moving horizontally 150 meters above a row of Ô¨Åve Ô¨Åxed nodes spaced 200 meters apart. The limit of transmission is set to be 250 meters, meaning that a Ô¨Åxed node goes out of range of the mover node just as the latter passes over directly above the next Ô¨Åxed node. As before, we use Ad hoc On-demand Distance Vector (AODV) as the routing protocol. When the mover passes over Ô¨Åxed node N, it goes out of range of Ô¨Åxed node N-1, at which point AODV Ô¨Ånds a new route to mover through Ô¨Åxed node N. As in ns-2, wireless simulations tend to require considerably more conÔ¨Åguration than point-to-point simulations. We now review the source code line-by-line. We start with two callback functions and the global variables they will need to access. using namespace ns3; Ptr<ConstantVelocityMobilityModel> cvmm; double position_interval = 1.0; std::string tracebase = "scratch/wireless"; // two callbacks void printPosition() { Vector thePos = cvmm->GetPosition(); Simulator::Schedule(Seconds(position_interval), &printPosition); std::cout << "position: " << thePos << std::endl; } void stopMover() { cvmm -> SetVelocity(Vector(0,0,0)); } 32.3 Wireless 887
An Introduction to Computer Networks, Release 2.0.11 Next comes the data rate: int main (int argc, char *argv[]) { std::string phyMode = "DsssRate1Mbps"; ThephyMode string represents the Wi-Fi data rate (and modulation technique). DSSS rates are DsssRate1Mbps ,DsssRate2Mbps ,DsssRate5_5Mbps andDsssRate11Mbps. Also available areErpOfdmRate constants to 54 Mbps and OfdmRate constants to 150 Mbps with a 40 MHz bandwidth (GetOfdmRate150MbpsBW40MHz ). All these are deÔ¨Åned in src/wiÔ¨Å/model/wiÔ¨Å-phy.cc. Next are the variables that determine the layout and network behavior. The factor variable allows slowing down the speed of the mover node but correspondingly extending the runtime (though the new-routediscovery time is notscaled): int bottomrow = 5; // number of bottom-row nodes int spacing = 200; // between bottom-row nodes int mheight = 150; // height of mover above bottom row int brheight = 50; // height of bottom row int X = (bottomrow-1) *spacing+1; // X isthe horizontal √£√ëdimension of the field int packetsize = 500; double factor = 1.0; // allows slowing down rate andextending runtime; same √£√ëtotal# of packets int endtime = (int)100 *factor; double speed = (X-1.0)/endtime; double bitrate = 80 *1000.0/factor; // *average*transmission rate, inbits/ √£√ësec uint32_t interval = 1000 *packetsize *8/bitrate *1000; // inmicrosec uint32_t packetcount = 1000000 *endtime/ interval; std::cout << "interval = " << interval << ", rate=" << bitrate << ", √£√ëpacketcount=" << packetcount << std::endl; There are some niceties in calculating the packet transmission interval above; if we do it instead as 1000000*packetsize*8/bitrate then we sometimes run into 32-bit overÔ¨Çow problems or integer-divisionroundoff problems. Now we conÔ¨Ågure some Wi-Fi settings. // disable fragmentation forframes below 2200 bytes Config::SetDefault ( "ns3::WifiRemoteStationManager::FragmentationThreshold", √£√ëStringValue ( "2200")); // turn off RTS/CTS forframes below 2200 bytes Config::SetDefault ( "ns3::WifiRemoteStationManager::RtsCtsThreshold", √£√ëStringValue ( "2200")); // Set non-unicast data rate to be the same asthat of unicast Config::SetDefault ( "ns3::WifiRemoteStationManager::NonUnicastMode", √£√ëStringValue (phyMode)); Here we create the mover node withCreateObject<Node>(), but the Ô¨Åxed nodes are created via a NodeContainer, as is more typical with larger simulations 888 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 // Create nodes NodeContainer fixedpos; fixedpos.Create(bottomrow); Ptr<Node> lowerleft = fixedpos.Get(0); Ptr<Node> mover = CreateObject<Node>(); Now we put together a set of ‚Äúhelper‚Äù objects for more Wi-Fi conÔ¨Åguration. We must conÔ¨Ågure both the PHY (physical) and MAC layers. // The below set of helpers will help us to put together the desired Wi-Fi √£√ëbehavior WifiHelper wifi; wifi.SetStandard (WIFI_PHY_STANDARD_80211b); wifi.SetRemoteStationManager ( "ns3::AarfWifiManager" ); // Use AARF rate √£√ëcontrol The AARF rate changes can be viewed by enabling the appropriate logging with, at the shell level before ./waf ,NS_LOG=AarfWifiManager=level_debug. We are not otherwise interested in rate scaling (4.2.2 Dynamic Rate Scaling ) here, though. The PHY layer helper is YansWifiPhyHelper. The YANS project (Yet Another Network Simulator) was an inÔ¨Çuential precursor to ns-3; see [LH06]. Note the AddPropagationLoss conÔ¨Åguration, where we set the Wi-Fi range to 250 meters. The MAC layer helper is NqosWifiMacHelper; the ‚Äúnqos‚Äù means ‚Äúno quality-of-service‚Äù, ieno use of Wi-Fi PCF ( 4.2.7 Wi-Fi Polling Mode ). // The PHY layer here is "yans" YansWifiPhyHelper wifiPhyHelper = YansWifiPhyHelper::Default (); // for .pcap tracing // wifiPhyHelper.SetPcapDataLinkType (YansWifiPhyHelper::DLT_IEEE802_11_ √£√ëRADIO); YansWifiChannelHelper wifiChannelHelper; // *not*::Default() ! wifiChannelHelper.SetPropagationDelay ( √£√ë"ns3::ConstantSpeedPropagationDelayModel"); // pld: default? // the following has an absolute cutoff at distance > 250 wifiChannelHelper.AddPropagationLoss ("ns3::RangePropagationLossModel", √£√ë"MaxRange", DoubleValue(250)); Ptr<YansWifiChannel> pchan = wifiChannelHelper.Create (); wifiPhyHelper.SetChannel (pchan); // Add a non-QoS upper-MAC layer "AdhocWifiMac", and set rate control NqosWifiMacHelper wifiMacHelper = NqosWifiMacHelper::Default (); wifiMacHelper.SetType ("ns3::AdhocWifiMac"); NetDeviceContainer devices = wifi.Install (wifiPhyHelper, wifiMacHelper, √£√ëfixedpos); devices.Add (wifi.Install (wifiPhyHelper, wifiMacHelper, mover)); At this point the basic Wi-Fi conÔ¨Åguration is done! The next step is to work on the positions and motion. First we establish the positions of the Ô¨Åxed nodes. MobilityHelper sessile; // forfixed nodes (continues on next page) 32.3 Wireless 889
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) Ptr<ListPositionAllocator> positionAlloc = CreateObject<ListPositionAllocator> √£√ë(); int Xpos = 0; for(int i=0; i<bottomrow; i++) { positionAlloc->Add(Vector(Xpos, brheight, 0.0)); Xpos += spacing; } sessile.SetPositionAllocator (positionAlloc); sessile.SetMobilityModel ( "ns3::ConstantPositionMobilityModel" ); sessile.Install (fixedpos); Next we set up the mover node.ConstantVelocityMobilityModel is a subclass of MobilityModel. At the end we print out a couple things just for conÔ¨Årmation. Vector pos (0, mheight+brheight, 0); Vector vel (speed, 0, 0); MobilityHelper mobile; mobile.SetMobilityModel( "ns3::ConstantVelocityMobilityModel" ); // no √£√ëAttributes mobile.Install(mover); cvmm = mover->GetObject<ConstantVelocityMobilityModel> (); cvmm->SetPosition(pos); cvmm->SetVelocity(vel); std::cout << "position: " << cvmm->GetPosition() << " velocity: " << cvmm-> √£√ëGetVelocity() << std::endl; std::cout << "mover mobility model: " << mobile.GetMobilityModelType() << √£√ëstd::endl; Now we conÔ¨Ågure Ad hoc On-demand Distance Vector routing. AodvHelper aodv; OlsrHelper olsr; Ipv4ListRoutingHelper listrouting; //listrouting.Add(olsr, 10); // generates less √£√ëtraffic listrouting.Add(aodv, 10); // fastest to find new √£√ëroutes Uncommenting the olsr line (and commenting out the last line) is all that is necessary to change to OLSR routing. OLSR is slower to Ô¨Ånd new routes, but sends less trafÔ¨Åc. Now we set up the IP addresses. This is straightforward as all the nodes are on a single subnet. InternetStackHelper internet; internet.SetRoutingHelper(listrouting); internet.Install (fixedpos); internet.Install (mover); Ipv4AddressHelper ipv4; NS_LOG_INFO ( "Assign IP Addresses." ); ipv4.SetBase ( "10.1.1.0" ,"255.255.255.0" ); // there isonly one √£√ësubnet (continues on next page) 890 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) Ipv4InterfaceContainer i = ipv4.Assign (devices); Now we create a receiving application UdpServer on nodemover, and a sending application UdpClient on the lower-left node. These applications generate their own sequence numbers, which show up in the ns-3 traceÔ¨Åles marked with ns3::SeqTsHeader. As in 32.2 A Single TCP Sender, we use Config::SetDefault() andCreateObject<>() to construct the applications. uint16_t port = 80; // create a receiving application (UdpServer) on node mover Address sinkaddr(InetSocketAddress (Ipv4Address::GetAny (), port)); Config::SetDefault( "ns3::UdpServer::Port", UintegerValue(port)); Ptr<UdpServer> UdpRecvApp = CreateObject<UdpServer>(); UdpRecvApp->SetStartTime(Seconds(0.0)); UdpRecvApp->SetStopTime(Seconds(endtime+60)); mover->AddApplication(UdpRecvApp); Ptr<Ipv4> m4 = mover->GetObject<Ipv4>(); Ipv4Address Maddr = m4->GetAddress(1,0).GetLocal(); std::cout << "IPv4 address of mover: " << Maddr << std::endl; Address moverAddress (InetSocketAddress (Maddr, port)); Here is the UdpClient sending application: Config::SetDefault( "ns3::UdpClient::MaxPackets", UintegerValue(packetcount)); Config::SetDefault( "ns3::UdpClient::PacketSize", UintegerValue(packetsize)); Config::SetDefault( "ns3::UdpClient::Interval", TimeValue (MicroSeconds √£√ë(interval))); Ptr<UdpClient> UdpSendApp = CreateObject<UdpClient>(); UdpSendApp -> SetRemote(Maddr, port); UdpSendApp -> SetStartTime(Seconds(0.0)); UdpSendApp -> SetStopTime(Seconds(endtime)); lowerleft->AddApplication(UdpSendApp); We now set up tracing. The Ô¨Årst, commented-out line enables pcap-format tracing, which we do not need here. TheYansWifiPhyHelper object supports tracing only of ‚Äúreceive‚Äù ( r) and ‚Äútransmit‚Äù ( t) records; thePointtoPointHelper of32.2 A Single TCP Sender also traced enqueue and drop records. //wifiPhyHelper.EnablePcap (tracebase, devices); AsciiTraceHelper ascii; wifiPhyHelper.EnableAsciiAll (ascii.CreateFileStream (tracebase + ".tr")); // create animation file, to be run with'netanim' AnimationInterface anim (tracebase + ".xml"); anim.SetMobilityPollInterval(Seconds(0.1)); If we view the animation with netanim, the moving node‚Äôs motion is clear. The mover node, however, sometimes appears to transmit back to both the Ô¨Åxed-row node below left and the Ô¨Åxed-row node below right. These transmissions represent the Wi-Fi link-layer ACKs; they appear to be sent to two Ô¨Åxed-row 32.3 Wireless 891
An Introduction to Computer Networks, Release 2.0.11 nodes because what netanim is actually displaying with its blue links is transmission every other node in range. We can also ‚Äúview‚Äù the motion in text format by uncommenting the Ô¨Årst line below. //Simulator::Schedule(Seconds(position_interval), &printPosition); Simulator::Schedule(Seconds(endtime), &stopMover); Finally it is time to run the simulator, and print some Ô¨Ånal output. Simulator::Stop(Seconds (endtime+60)); Simulator::Run (); Simulator::Destroy (); int pktsRecd = UdpRecvApp->GetReceived(); std::cout << "packets received: " << pktsRecd << std::endl; std::cout << "packets recorded as lost: " << (UdpRecvApp->GetLost()) << √£√ëstd::endl; std::cout << "packets actually lost: " << (packetcount - pktsRecd) << √£√ëstd::endl; return0; } 32.3.1 TraceÔ¨Åle Analysis The traceÔ¨Åle provides no enqueue records, and Wi-Fi doesn‚Äôt have Ô¨Åxed links; how can we verify that packets are being forwarded correctly? One thing we cando with the traceÔ¨Åle is to look at each value of the UdpServer application sequence number, and record 
- when it was received by node mover 
- when it was transmitted by any Ô¨Åxed-row node If we do this, we get output like the following: packet 0 received at 0.0248642, forwarded by 0 at 0.0201597 packet 1 received at 0.0547045, forwarded by 0 at 0.05 ... packet 499 received at 24.9506, forwarded by 0 at 24.95 packet 500 NOT recd, forwarded by 0 at 25, forwarded by 0 at 25.0019, √£√ëforwarded by 0 at 25.0035, forwarded by 0 at 25.0071, forwarded by 0 at 25. √£√ë0097, forwarded by 0 at 25.0159, forwarded by 0 at 25.0281 packet 501 received at 25.0864, forwarded by 0 at 25.0767, forwarded by 1 at √£√ë25.0817 packet 502 received at 25.1098, forwarded by 0 at 25.1, forwarded by 1 at 25. √£√ë1051 ... packet 1000 NOT recd, forwarded by 0 at 50, forwarded by 1 at 50.001, √£√ëforwarded by 1 at 50.003, forwarded by 1 at 50.0059, forwarded by 1 at 50. √£√ë0087, forwarded by 1 at 50.0151, forwarded by 1 at 50.0239, forwarded by 1 √£√ëat 50.0341(continues on next page) 892 32 The ns-3 Network Simulator
An Introduction to Computer Networks, Release 2.0.11 (continued from previous page) packet 1001 received at 50.082, forwarded by 0 at 50.0683, forwarded by 1 at √£√ë50.0722, forwarded by 2 at 50.0773 packet 1002 received at 50.1107, forwarded by 0 at 50.1, forwarded by 1 at 50. √£√ë101, forwarded by 2 at 50.106 ... packet 1499 received at 74.9525, forwarded by 0 at 74.95, forwarded by 1 at √£√ë74.951, forwarded by 2 at 74.9519 packet 1500 NOT recd, forwarded by 0 at 75, forwarded by 1 at 75.001, √£√ëforwarded by 2 at 75.0019, forwarded by 2 at 75.0039, forwarded by 2 at 75. √£√ë005, forwarded by 2 at 75.0084, forwarded by 2 at 75.0124, forwarded by 2 √£√ëat 75.0277, forwarded by 2 at 75.0361 packet 1501 NOT recd, forwarded by 0 at 75.05 packet 1502 received at 75.1484, forwarded by 0 at 75.1287, forwarded by 1 at √£√ë75.1299, forwarded by 1 at 75.1314, forwarded by 1 at 75.1326, forwarded by √£√ë2 at 75.1386, forwarded by 3 at 75.1437 packet 1503 received at 75.1621, forwarded by 0 at 75.15, forwarded by 1 at √£√ë75.151, forwarded by 2 at 75.1523, forwarded by 3 at 75.1574 ... That is, packets 0-499 were transmitted only by node 0. Packet 500 was never received by mover, but there were seven transmission attempts; these seven attempts follow the rules described in 4.2.1 Wi-Fi and Collisions. Packets starting at 501 were transmitted by node 0 and then later by node 1. Similarly, packet 1000 was lost, and after that each packet arriving at mover was Ô¨Årst transmitted by nodes 0, 1 and 2, in that order. In other words, packets are indeed being forwarded rightward along the line of Ô¨Åxed-row nodes until a node is reached that is in range of mover. 32.3.2 AODV Performance If we change the line listrouting.Add(aodv, 10); to listrouting.Add(dsdv, 10); we Ô¨Ånd that the loss count goes from 4 packets out of 2000 to 398 out of 2000; for OLSR routing the loss count is 426. As we discussed in 31.6 Wireless Simulation, the loss of one data packet triggers the AODV implementation to look for a new route. The DSDV and OLSR implementations, on the other hand, only look for new routes at regularly spaced intervals. 32.4 Exercises In preparation. 32.4 Exercises 893
An Introduction to Computer Networks, Release 2.0.11 894 32 The ns-3 Network Simulator
33 BIBLIOGRAPHY Note that RFCs are not included here. 895
An Introduction to Computer Networks, Release 2.0.11 896 33 Bibliography
EXERCISE-NUMBERING CONVERSION TABLES Here is a table for converting Edition 2 exercise numbers (left column) to Edition 1.x chapter and exercise number. To do the lookup in the reverse (and probably more useful) direction, you will have to search for the pair (1e chapter, 1e exercise). : An Overview of Networks ( 1 An Overview of Networks ) 2e exercise 1e chapter 1e exercise 1.0 1 1.0 2.0 1 2.0 3.0 1 2.5 4.0 1 2.7 5.0 1 3.0 6.0 1 4.0 7.0 1 5.0 8.0 1 6.0 9.0 1 7.0 10.0 1 7.5 11.0 1 8.0 12.0 1 9.0 : Ethernet Basics ( 2 Ethernet Basics ) 2e exercise 1e chapter 1e exercise 1.0 2 1.0 2.0 2 2.0 3.0 2 2.7 4.0 2 3.0 5.0 2 4.0 6.0 2 5.0 7.0 2 6.0 8.0 2 7.0 9.0 2 9.0 10.0 2 11.0 : Advanced Ethernet ( 3 Advanced Ethernet ) 897
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 2 8.0 2.0 2 8.5 3.0 2 9.0 4.0 2 10.0 5.0 2 12.0 6.0 2 12.2 7.0 2 13.0 8.0 2 14.0 : Wireless LANs ( 4 Wireless LANs ) 2e exercise 1e chapter 1e exercise 1.0 3 4.0 2.0 3 4.5 3.0 3 5.0 4.0 3 6.0 5.0 3 7.0 6.0 3 13.0 7.0 3 14.0 : Other LANs ( 5 Other LANs ) 2e exercise 1e chapter 1e exercise 1.0 3 1.0 2.0 3 2.0 3.0 3 3.0 4.0 3 8.0 5.0 3 9.0 6.0 3 10.0 7.0 3 10.5 8.0 3 11.0 9.0 3 12.0 : Links ( 6 Links ) 2e exercise 1e chapter 1e exercise 1.0 4 1.0 2.0 4 2.0 3.0 4 3.0 4.0 4 3.5 5.0 4 4.0 6.0 4 5.0 : Packets ( 7 Packets ) 898 Exercise-Numbering Conversion Tables
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 5 1.0 2.0 5 2.0 3.0 5 3.0 4.0 5 3.5 5.0 5 3.6 6.0 5 4.0 7.0 5 5.0 8.0 5 6.0 9.0 5 7.0 10.0 5 8.0 11.0 5 9.0 12.0 5 10.0 13.0 5 10.5 14.0 5 11.0 15.0 5 12.0 16.0 5 13.0 17.0 5 14.0 18.0 5 15.0 19.0 5 16.0 : Abstract Sliding Windows ( 8 Abstract Sliding Windows ) 2e exercise 1e chapter 1e exercise 1.0 6 1.0 2.0 6 1.5 3.0 6 2.0 4.0 6 2.5 5.0 6 3.0 6.0 6 4.0 7.0 6 5.0 8.0 6 6.0 9.0 6 6.5 10.0 6 7.0 11.0 6 7.5 12.0 6 8.0 13.0 6 8.5 14.0 6 9.0 15.0 6 10.0 16.0 6 11.0 17.0 6 12.0 : IP version 4 ( 9 IP version 4 ) 899
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 7 1.0 2.0 7 2.0 3.0 7 3.0 4.0 7 4.0 5.0 7 5.0 6.0 7 6.0 7.0 7 6.5 8.0 7 7.0 : IPv4 Companion Protocols ( 10 IPv4 Companion Protocols ) 2e exercise 1e chapter 1e exercise 1.0 7 8.0 2.0 7 9.0 3.0 7 10.0 4.0 7 11.0 : IPv6 ( 11 IPv6 ) 2e exercise 1e chapter 1e exercise 1.0 8 1.0 2.0 8 2.0 3.0 new 4.0 8 6.0 : IPv6 Additional Features ( 12 IPv6 Additional Features ) 2e exercise 1e chapter 1e exercise 1.0 8 3.0 2.0 8 4.0 3.0 8 5.0 4.0 8 7.0 : Routing-Update Algorithms ( 13 Routing-Update Algorithms ) 900 Exercise-Numbering Conversion Tables
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 9 1.0 2.0 9 2.0 3.0 9 2.5 4.0 9 3.0 5.0 9 3.5 6.0 9 4.0 7.0 9 5.0 8.0 9 5.5 9.0 9 6.0 10.0 9 7.0 11.0 9 8.0 12.0 9 9.0 13.0 9 10.0 14.0 9 11.0 15.0 9 12.0 16.0 9 13.0 : Large-Scale IP Routing ( 14 Large-Scale IP Routing ) 2e exercise 1e chapter 1e exercise 1.0 10 0.5 2.0 10 1.0 3.0 10 2.0 4.0 10 3.0 5.0 10 4.0 6.0 10 5.0 7.0 10 5.5 8.0 10 6.0 9.0 10 9.0 : Border Gateway Protocol ( 15 Border Gateway Protocol (BGP) ) 2e exercise 1e chapter 1e exercise 1.0 10 7.0 2.0 10 8.0 3.0 10 10.0 4.0 10 11.0 5.0 10 12.0 : UDP ( 16 UDP Transport ) 901
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 11 1.0 2.0 11 2.0 3.0 11 3.0 4.0 11 4.0 5.0 11 5.0 6.0 11 6.0 7.0 11 7.0 8.0 11 7.1 9.0 11 7.5 10.0 11 8.0 11.0 11 9.0 12.0 11 10.0 13.0 11 11.0 14.0 11 12.0 15.0 11 13.0 16.0 11 14.0 : TCP Transport Basics ( 17 TCP Transport Basics ) 2e exercise 1e chapter 1e exercise 1.0 12 1.0 2.0 12 2.0 3.0 12 2.5 4.0 12 3.0 5.0 12 4.0 6.0 12 4.5 7.0 12 5.0 8.0 12 6.0 9.0 12 6.5 10.0 12 7.0 11.0 12 8.0 12.0 12 9.0 13.0 12 14.0 14.0 12 15.0 : TCP Issues and Alternatives ( 18 TCP Issues and Alternatives ) 2e exercise 1e chapter 1e exercise 1.0 12 10.0 2.0 12 10.5 3.0 12 11.0 4.0 12 12.0 5.0 12 13.0 902 Exercise-Numbering Conversion Tables
An Introduction to Computer Networks, Release 2.0.11 : TCP Reno and Congestion Management ( 19 TCP Reno and Congestion Management ) 2e exercise 1e chapter 1e exercise 1.0 13 1.0 2.0 13 2.0 3.0 13 3.0 4.0 13 4.0 5.0 13 5.0 6.0 13 6.0 7.0 13 7.0 8.0 13 8.0 9.0 13 9.0 10.0 13 10.0 11.0 13 11.0 12.0 13 12.0 13.0 13 12.5 14.0 13 13.0 15.0 13 14.0 : Dynamics of TCP ( 20 Dynamics of TCP ) 2e exercise 1e chapter 1e exercise 1.0 14 0.5 2.0 14 1.0 3.0 14 2.0 4.0 14 2.5 5.0 14 3.0 6.0 14 3.5 7.0 14 4.0 8.0 14 5.0 9.0 14 6.0 10.0 14 9.0 11.0 14 10.0 12.0 new : Further Dynamics of TCP ( 21 Further Dynamics of TCP ) 903
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 14 7.0 2.0 14 8.0 3.0 14 8.5 4.0 14 9.0 5.0 14 11.0 6.0 14 12.0 7.0 14 13.0 8.0 14 13.5 9.0 14 14.0 10.0 14 15.0 11.0 14 16.0 12.0 14 16.5 13.0 14 16.6 14.0 14 16.7 15.0 14 17.0 16.0 14 18.0 : Newer TCP Implementations ( 22 Newer TCP Implementations ) 2e exercise 1e chapter 1e exercise 1.0 15 1.0 2.0 15 2.0 3.0 15 3.0 4.0 15 4.0 5.0 15 5.0 6.0 15 6.0 7.0 15 7.0 8.0 15 8.0 9.0 15 9.0 10.0 15 10.0 11.0 15 11.0 12.0 15 12.0 13.0 15 13.0 14.0 15 14.0 : Queuing and Scheduling ( 23 Queuing and Scheduling ) 904 Exercise-Numbering Conversion Tables
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 19 1.0 2.0 19 2.0 3.0 19 3.0 4.0 19 4.0 5.0 19 5.0 6.0 19 5.5 7.0 19 6.0 8.0 19 6.5 9.0 19 7.0 10.0 19 8.0 11.0 19 9.0 12.0 19 10.0 13.0 19 11.0 14.0 19 17.0 : Token Bucket Rate Limiting ( 24 Token Bucket Rate Limiting ) 2e exercise 1e chapter 1e exercise 1.0 19 12.0 2.0 19 13.0 3.0 19 14.0 4.0 19 15.0 5.0 19 16.0 6.0 19 18.0 7.0 19 19.0 8.0 19 20.0 9.0 19 21.0 10.0 19 22.0 : Quality of Service ( 25 Quality of Service ) 2e exercise 1e chapter 1e exercise 1.0 20 1.0 2.0 20 2.0 3.0 20 3.0 4.0 20 4.0 5.0 20 5.0 : Network Management and SNMP ( 26 Network Management and SNMP ) 905
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 21 1.0 2.0 new 3.0 21 4.0 4.0 21 5.0, SNMPv1 version : SNMP versions 2 and 3 ( 27 SNMP versions 2 and 3 ) 2e exercise 1e chapter 1e exercise 1.0 21 2.0 2.0 21 3.0 3.0 21 5.0, SNMPv2 version 4.0 21 6.0 5.0 21 7.0 6.0 21 8.0 7.0 21 9.0 8.0 21 10.0 9.0 21 11.0 10.0 21 12.0 11.0 21 13.0 : Security ( 28 Security ) 2e exercise 1e chapter 1e exercise 1.0 22 1.0 2.0 22 2.0 3.0 22 3.0 4.0 22 4.0 5.0 22 5.0 6.0 22 6.0 7.0 22 7.0 8.0 22 8.0 9.0 22 9.0 : Public Key Encryption ( 29 Public-Key Encryption ) 2e exercise 1e chapter 1e exercise 1.0 22 10.0 2.0 22 11.0 3.0 22 12.0 : Mininet ( 30 Mininet ) 906 Exercise-Numbering Conversion Tables
An Introduction to Computer Networks, Release 2.0.11 2e exercise 1e chapter 1e exercise 1.0 18 1.0 2.0 18 2.0 3.0 18 3.0 4.0 18 4.0 5.0 18 5.0 6.0 18 6.0 7.0 18 7.0 8.0 18 8.0 9.0 18 9.0 10.0 18 10.0 : Network Simulations: ns-2 ( 31 Network Simulations: ns-2 ) 2e exercise 1e chapter 1e exercise 1.0 16 1.0 2.0 16 2.0 3.0 16 3.0 4.0 16 4.0 5.0 16 5.0 6.0 16 6.0 7.0 16 7.0 8.0 16 8.0 9.0 16 9.0 10.0 16 10.0 11.0 16 11.0 12.0 16 12.0 13.0 16 13.0 14.0 16 14.0 15.0 16 15.0 16.0 16 16.0 17.0 16 17.0 907
An Introduction to Computer Networks, Release 2.0.11 908 Exercise-Numbering Conversion Tables
34 SELECTED SOLUTIONS In this chapter we present solutions (in some cases partial solutions) to exercises marked with a ‚ô¢. 34.1 Solutions for An Overview of Networks 1.18 Exercises Exercise 4.0 A B,C,D direct E D (or C) B A,C direct D A E C C A,B,E direct D E (or A) D A,E direct B A C E (or A) E C,D direct A C (or D) B C Exercise 9.0(d) (destination D): According to S1‚Äôs forwarding table, the next_hop to D is S2. According to S2‚Äôs table, the next_hop to D is S5. According to S5‚Äôs table, the next_hop to D is S6. 909
An Introduction to Computer Networks, Release 2.0.11 According to S6‚Äôs table, the next_hop to D is S12. The path from S1 to D is thus S1‚ÄìS2‚ÄìS5‚ÄìS6‚ÄìS12. Exercise 10.0(a) The shortest path from A to F is A‚ÄìS1‚Äì1‚ÄìS2‚Äì2‚ÄìS5‚Äì1‚ÄìS6‚ÄìF, for a total cost of 1+2+1 = 4. 34.2 Solutions for Ethernet 2.6 Exercises Exercise 3.0 When A sends to D, all switches use fallback-to-Ô¨Çooding as no switch knows where D is. All switches S1-S4, though, learn where A is. When D sends to A, S2 knows where A is and so routes the packet directly to S1, which also knows where A is. S3 and S4 do not learn where D is. When A sends to B, all switches again use fallback-to-Ô¨Çooding, but no switch learns anything new. When B sends to D, S4 uses fallback-to-Ô¨Çooding as it does not know where D is. However, S2 does know where D is, and so S2 forwards the packet only to D. S2 and S4 learn where B is. switch known destinations S1 AD S2 ABD S3 A S4 AB 34.3 Solutions for Advanced Ethernet 3.6 Exercises Exercise 2.0 (a). S1, as root, keeps all three of its links to S3, S4 and S5. Of S2‚Äôs three links in the direction of the root (that is, to S3, S4 and S5), it keeps only its link to S3 as S3 has the lowest ID. 910 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 S1 S2 S5 S4 S3 The path from S2 to S5 is S2‚ÄìS3‚ÄìS1‚ÄìS5. Exercise 5.0 1. h1√ëh2: this packet is reported to C, as destination h2 is not known by S. C installs on S the rule that h1 can be reached via port 1. The packet is then Ô¨Çooded. 2. h2√ëh1: this packet is notreported to C, as destination h1 isknown by S. The packet is not Ô¨Çooded. 3. h3√ëh1: this packet is again not reported to C. S delivers the packet normally, without Ô¨Çooding. 4. h2√ëh3: this packet is reported to C, as destination h3 is not known by S. C installs on S the rule that h2 can be reached via port 2. The packet is then Ô¨Çooded. Exercise 7.0 Table for S1 only: h1√ëh2: S1 reports to C and learns where h1 is h2√ëh1: S1 does not report, and forwards normally; destination h1 is known h1√ëh3: S1 reports to C as h3 is not known, but S1 already knows where h1 is h3√ëh1: S1 does not report, and forwards normally; destination h1 is known h2√ëh3: S1 reports to C and learns where h2 is h3√ëh2: S1 does not report, and forwards normally; destination h2 is known S1‚Äôs table ends up with forwarding entries for h1 and h2, but not h3. 34.4 Solutions for Wireless LANs 4.6 Exercises 34.4 Solutions for Wireless LANs 911
An Introduction to Computer Networks, Release 2.0.11 Exercise 2.0 The three sending nodes can be laid out at the vertices of an equilateral triangle, with the receiving node in the center. Alternatively, with impermeable walls the following arrangement works: sender1 receiver sender2 sender1 The latter arrangement generalizes to almost any number of senders. For the former, senders can be laid out at the vertices of a square, or tetrahedron, or possibly a pentagon, but geometry enforces an eventual limit. Exercise 3.0(b) If T is the length of the contention interval, in ¬µsec, then transmission and contention alternate with lengths 151‚ÄìT‚Äì151‚ÄìT‚Äì.. .. The fraction of bandwidth lost to contention is T/(T+151). 34.5 Solutions for Other LANs 5.7 Exercises Exercise 3.0(b) One simple strategy is for stations to compare themselves to one another according to the numeric value of their MAC addresses. The station with the smallest MAC address becomes S 0, the station with the nextsmallest address becomes S 1,etc. Exercise 7.0 The connections are as follows, where the VCI number is shown between the endpoints of each link: A‚Äì1‚ÄìS1‚Äì2‚ÄìS2‚Äì3‚ÄìS4‚Äì2‚ÄìD B‚Äì1‚ÄìS2‚Äì2‚ÄìS4‚Äì1‚ÄìS3‚Äì3‚ÄìS1‚Äì2‚ÄìA 34.6 Solutions for Links Exercise 4.0 The binary ASCII for the three letters is N 0100 1110 e 0110 0101 t 0111 0100 If we look up the 4-bit ‚Äúnybbles‚Äù in the right column above in the 4B/5B table at 6.1.4 4B/5B, we get 912 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 data symbol 0100 01010 1110 11100 0110 01110 0101 01011 0111 01111 0100 01010 Putting all these symbols together, the encoding is (with spaces added for readability) 01010 11100 01110 01011 01111 01010 34.7 Solutions for Packets 7.6 Exercises Exercise 3.0 (a). The bandwidth delay for a 600-byte packet is 300 ¬µsec. The packet has a 300 ¬µsec bandwidth delay and a 600 ¬µsec propagation delay for each link, for a total of 2 900 = 1800 ¬µsec. We have the following timeline: T=0 A begins sending the packet T=300 A Ô¨Ånishes sending the packet T=600 S begins receiving the packet T=900 S Ô¨Ånishes receiving the packet and begins sending to B T=1200 S Ô¨Ånishes sending the packet T=1500 B begins receiving the packet T=1800 B Ô¨Ånishes receiving the packet (b). The bandwidth delay for each 300-byte packet is now 150 ¬µsec, so the Ô¨Årst packet arrives at S at T=750. The second packet arrives one bandwidth delay ‚Äì 150 ¬µsec ‚Äì later, at T=900 ¬µsec. The Ô¨Årst packet takes another 750 ¬µsec to travel from S to B, arriving at T=1500; the second packet arrives 150 ¬µsec later at T=1650. Here is the timeline: 34.7 Solutions for Packets 913
An Introduction to Computer Networks, Release 2.0.11 T=0 A begins sending packet 1 T=150 A Ô¨Ånishes sending packet 1, starts on packet 2 T=300 A Ô¨Ånishes sending packet 2 T=600 S begins receiving packet 1 T=750 S Ô¨Ånishes receiving packet 1 and begins sending it to B S begins receiving packet 2 T=900 S Ô¨Ånishes receiving packet 2 and begins sending it to B T=1350 B begins receiving packet 1 T=1500 B has received all of packet 1 and starts receiving packet 2 T=1650 B Ô¨Ånishes receiving packet 2 Here‚Äôs the data for (b) in ladder-diagram format (not quite to scale): T=0 T=150 T=300 T=600 T=750 T=900 T=1150 T=1350 T=1500 T=1650 The long propagation delay means that A Ô¨Ånishes all transmissions before S begins receiving anything, and S Ô¨Ånishes all transmissions before B begins receiving anything. With a smaller propagation delay, these A/S/B events are likely to overlap. Exercise 18.0(a) The received data is 10100010 and the received code bits are 0111. We Ô¨Årst calculate the four code bits for the received data: 1: parity over all bits 1: parity over second half: 1010 0010 0: parity over these bits: 10 1000100: parity over these bits: 1 0100010 So the incorrect received code bits are 0111. The Ô¨Årst bit of the received code tells us that the error is in the data (rather than the code bits). The second ‚Äì correct ‚Äì bit of the received code tells us that the error is in the Ô¨Årst half, that is, in the non-bold bits of 1010 0010. The third bit of the received code tells us that the error is in the bold bits of 10 100010, so we‚Äôre down to 914 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 the third or fourth bit. Finally, the fourth bit of the received code tells us the error is in the bold bits of 10100010, which means the fourth bit is it, and the corrected data is 101 10010 (making the data the same as that in the example in 7.4.2.1 Hamming Codes ). 34.8 Solutions for Sliding Windows 8.5 Exercises Exercise 4.0 The network is C S1infinitely fast S21 pkt/sec D1 pkt/sec (a). The second part of formula 4 of 8.3.2 RTT Calculations is queue_usage = winsize ‚Äì bandwidth  RTT noLoad. We know winsize = 6, bandwidth = 1 packet/sec and RTT noLoad = 4, so this works out to 2 packets in the queue. (b). At T=0, C sends packets 1-6 to S1. S1 begins sending packet 1 to S2; packets 2-6 are queued. At T=1, S1 begins sending packet 2 to S2; S2 begins sending packet 1 to D At T=2, S1 begins sending packet 3 to S2 and S2 begins sending packet 2 to D. Packet 1 has reached D, which begins sending ACK1 to S2. At T=3, S1 begins sending packet 4 to S2, S2 begins sending packet 3 to D, D begins sending ACK2 to S2, and S2 begins sending ACK1 to S1. At T=4, S1 begins sending packet 5 to S1, S2 begins sending packet 4 to D, D begins sending ACK3 to S2, and S2 begins sending ACK2 to S1. ACK1 has arrived at S1, and is instantly forwarded to C. The window slides forward by one, from [1-6] to [2-7], and C sends new packet 7, which instantly arrives at S1 and is placed in the queue there. Here is the table. The column ‚ÄúS1 √ëS2‚Äù is the data packet S1 is currently sending to S2; the column ‚ÄúS2√ëS1‚Äù is the ACK that S2 is currently sending to S1; similarly for ‚ÄúS2 √ëD‚Äù and ‚ÄúD√ëS2‚Äù. 34.8 Solutions for Sliding Windows 915
An Introduction to Computer Networks, Release 2.0.11 TC sends S1 queues S1√ëS2 S2√ëDACK D√ëS2 S2√ëS1 01,2,3,4,5,6 2,3,4,5,6 1 1 3,4,5,6 2 1 2 4,5,6 3 2 1 3 5,6 4 3 2 1 47 6,7 5 4 3 2 58 7,8 6 5 4 3 69 8,9 7 6 5 4 710 9,10 8 7 6 5 811 10,11 9 8 7 6 Exercise 11.0 (a). The formula, from 8.2.1 Bandwidth Delay, is winsize = bandwidth RTT noLoad = 1000 pkts/sec  0.1 sec = 100 packets. (b). This is the Ô¨Årst line of formula 3 of 8.3.2 RTT Calculations. For convenience, we switch to units of ms. Queue_usage = throughput (RTT actual ‚Äì RTT noLoad ) = 1 pkt/ms(130ms ‚Äì 100ms) = 30 packets. (c). We use formula 4 of 8.3.2 RTT Calculations. We have RTT actual = winsize / bandwidth = (100+50)/(1 pkt/ms) = 150 ms. Exercise 14.0 If Data[8] is in the receiver‚Äôs window, then Data[4] must have been received. For the sender to have sent Data[4] it must have previously received ACK[0]. Therefore, all transmissions of Data[0] must precede the Ô¨Årst transmission of Data[4], and so must precede the Ô¨Årst successful transmission of Data[4]. Because of non-reordering, no Data[0] can arrive after Data[4]. 34.9 Solutions for IPv4 9.11 Exercises Exercise 4.0 Forwarding table for A: destination next_hop 200.0.5.0/24 direct 200.0.6.0/24 direct default B Exercise 6.0(d) The subnet preÔ¨Åx is 10.0.168.0/21. The /21 means that the preÔ¨Åx consists of the Ô¨Årst two bytes (16 bits) and the Ô¨Årst 5 (=21-16) bits of the third byte. 916 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 Expressing the third byte, 168, in binary, we get 10101|000. We now convert the third byte of each of the four addresses to binary, and see which match this to the Ô¨Årst Ô¨Åve bits. Bits after the ‚Äú|‚Äù mark are ignored. 10.0.166.1: 166 = 10100|110; not a match 10.0.170.3: 170 = 10101|010; this is a match 10.0.174.5: 174 = 10101|110; this is a match 10.0.177.7: 177 = 10110|001; not a match Exercise 7.0 (a). 240 is 11110000 in binary, and so adds 4 1-bits. Together with the 16 1-bits from the Ô¨Årst two bytes of the mask, this is /20 (d). /20 is 16 1-bits (two bytes) plus 4 additional 1-bits. Four 1-bits starting a byte is 11110000, or 240; the full mask is 255.255.240.0 34.10 Solutions for Routing-Update Algorithms 13.9 Exercises Exercise 3.0 For destination A, R1‚Äôs report is likely the same as it sent earlier, when R‚Äôs route to A was Ô¨Årst established. There is no change. For destination B, R‚Äôs total distance using R1 would be 2+1 = 3. This is tied with R‚Äôs route to B using next_hop R2, and so there is no change. For destination C, R1 is reporting an increase in cost. R‚Äôs next_hop to C is R1, and so R must increase its cost to C to 4+1 = 5. For destination D, R‚Äôs total cost using R1 is 3+1 = 4, cheaper than R‚Äôs previous cost of 5 using next_hop R3. R changes to the R1 route. The updated table is destination cost next hop A 2 R1 no change; same next_hop B 3 R2 no change; tie C 5 R1 source increase D 4 R1 lower-cost route found 34.11 Solutions for Large-Scale IP Routing 14.7 Exercises 34.10 Solutions for Routing-Update Algorithms 917
An Introduction to Computer Networks, Release 2.0.11 Exercise 1.0 (i) 200.63.1.1 matches only A, as 63 = 0011 1111 in binary and 64 = 0100 0000. The Ô¨Årst two bits do not match, ruling out the /10 preÔ¨Åx. (ii) 200.80.1.1 matches A and B, but not C, as 80 = 0101 0000. Destination B is the longer match. (iii) 200.72.1.1 matches A, B and C, but not D, as 72 = 0100 1000. Destination C is the longest match. (iv) 200.64.1.1 matches A, B, C and D; D is the longest match. Exercise 6.0(a) P‚Äôs table destination next hop 52.0.0.0/8 Q 53.0.0.0/8 R 51.10.0.0/16 A 51.23.0.0/16 B Q‚Äôs table destination next hop 51.0.0.0/8 P 53.0.0.0/8 R 52.14.0.0/16 C 52.15.0.0/16 D R‚Äôs table destination next hop 51.0.0.0/8 P 52.0.0.0/8 Q 34.12 Solutions for Border Gateway Protocol 15.14 Exercises Exercise 1.0(a) P will receive a route from Q with AS-path xQ,Sy, and a route from R with AS-path xR,Sy. 34.13 Solutions for UDP Exercise 2.0(a) 918 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 Two seconds after Data[3] was sent and lost, both sender and receiver will time out. The sender will retransmit Data[3]; the receiver will retransmit ACK[2]. The latter will be ignored, as the sender is not retransmitting on duplicates. When the retransmitted Data[3] arrives at the receiver, the receiver will send ACK[3] and the transfer will continue. 34.14 Solutions for TCP Reno and Congestion Management 17.10 Exercises Exercise 13.0(b) Lettbe the transit capacity; then the total cwnd maxist(1+f). At this point it is convenient to normalize the units of capacity measurement so t(1+f) = 2. Then the link-unsaturated and queue-Ô¨Ålling phases have lengths in the proportion t‚Äì1 to ft. The average height of the tooth during the link-unsaturated phase is (t+1)/2, and the average utilization percentage is thus ( t+1)/2 t. This gives a utilization of (t+1)/2 t(t‚Äì1) + ft We next eliminate t. After normalization, we have t(1+f) = 2, and so t= 2/(1+ f). Plugging this in above and simplifying gives utilization = (3 + 6 f‚Äìf2)/4(1+ f) 34.15 Solutions for Dynamics of TCP 20.5 Exercises Exercise 3.0(a) Because the window size of 50 is larger than the bandwidth delay product is 40 packets, packets are sent at the bottleneck rate of 5 packets/ms. As packets spend 1 ms on the C‚ÄìR1 link, on average this link must be carrying 5 packets. The same applies to the R2‚ÄìD link; for the R1‚ÄìR2 link with a 2.0 ms propagation delay, the number of packets carried will be 2.0 5 = 10. Alternatively, from the bandwidth delay product of 40 packets we conclude 40 packets are in transit. Of these, half are acknowledgments. The other half are distributed in proportion to the link propagation delays, yielding 5, 10, and 5 packets. Exercise 4.0 At the point when A‚Äôs bandwidth is 2 packets/ms, B‚Äôs bandwidth is 4 packets/ms, and so B has 4 20 = 80 packets in transit. As B‚Äôs winsize is 120, this leaves 120‚Äì80 = 40 packets in R‚Äôs queue. For A to have half as much bandwidth, it must have half the queue capacity, or 20 packets. With a bandwidth of 2 packets/ms and an RTT noLoad of 40 ms, A will have 2 40 = 80 packets in transit. A‚Äôs winsize is then 20+80 = 100. We can conÔ¨Årm this using the following formulas from 20.2.3 Example 3: competition and queue utilization: 34.14 Solutions for TCP Reno and Congestion Management 919
An Introduction to Computer Networks, Release 2.0.11 ùõºQ = w A‚Äì 2ùõº(dA+d) ùõΩQ = w B‚Äì 2ùõΩ(dB+d) However, these formulas were derived under the assumption that the bottleneck bandwidth was normalized to 1. To apply them here, we need to measure time in units of 1/6 ms; that is, all times need to be multiplied by 6. We have: dA= 90 dB= 30 d = 30 wB= 120 ùõº=1/3, ùõΩ=2/3 Substituting these into the second formula, we get the total queue utilization Q = 180 ‚Äì 120 = 60, in agreement with our earlier answer. From the Ô¨Årst formula we now solve for w A=ùõº(Q + 2(d A+d)) = (1/3)(60 + 240) = 100, also in agreement with our earlier answer. Exercise 5.0(a) The bottleneck bandwidth is 5 packets/ms, and as the bottleneck link is saturated, this will be the transmission rate on all links. For 5 packets to be in transmission, we need a propagation delay of 5 packets / (5 packets/ms) = 1.0 ms. Exercise 6.0 (a). If the A‚ÄìB connection has a bandwidth of 3 packets/sec and a RTT noLoad of 16 ms, then a window size of 316 = 48 is necessary. Similarly, the window size for the C‚ÄìD connection is 3 10 = 30 packets. It remains to show that these window sizes actually result in an equal division of bandwidth. The A‚ÄìB bandwidth cannot exceed 50% in the steady state, because of the limitation imposed by the R2‚ÄìR3 link. If the C‚ÄìD bandwidth exceeds 50%, then the bandwidth-delay product exceeds 30 = w C, and so the C‚ÄìD connection can have no packets in the queue at R1, leaving the A‚ÄìB connection with no queuing delays. This means A sends w Apackets in 16 ms, for a bandwidth of 3 pkts/ms. 34.16 Solutions for Dynamics of TCP 21.10 Exercises Exercise 2.0(a) Thecwnd will vary between 500 and 1000, with an average of 750 packets. Losses occur at intervals of 500 RTTs, during which 500*750 packets have been sent. 920 34 Selected Solutions
An Introduction to Computer Networks, Release 2.0.11 34.17 Solutions for Queuing and Scheduling Exercise 6.0(a) Because all three subqueues are always active, the bit-by-bit virtual clock always moves in lockstep with the real clock, and we can calculate Ô¨Ånishing times according to the real clock. Here are transmissions, in order, with Ô¨Ånishing times taken to be cumulative bytes. Queue size Ô¨Ånishing time 1 190 190 1 190 380 1 190 570 1 190 760 1 190 950 2 1000 1000 1 190 1140 3 1200 1200 1 190 1330 1 190 1520 1 190 1710 1 190 1900 2 1000 2000 1 190 2090 1 190 2280 3 1200 2400 1 190 2470 Exercise 6.0(b) Initially each subqueue has a deÔ¨Åcit of 0. Queue packets sent deÔ¨Åcit 1 5, =950 bytes 50 2 1 0 3 pass 1000 1 5, =950 bytes 100 2 1 0 3 1 800 1 5, =950 bytes 150 2 1 0 3 1 600 1 6, =1140 bytes 10 2 1 0 3 1 400 34.17 Solutions for Queuing and Scheduling 921
An Introduction to Computer Networks, Release 2.0.11 34.18 Solutions for Mininet Exercise 6.0(a) When h1 sends its broadcast ARP, the controller learns where h1 is relative to s1, s2 and s3. When h2 replies to h2, there is not yet a rule for h1 at s2, and so s2 sends the packet to the controller, which installs rules for destinations h1 and h2 in s2. The packet is then Ô¨Çooded. When it arrives at s1, the controller installs rules for h1 and h2 and s1. The packet also arrives at s3, by Ô¨Çooding, so the controller installs rules for h1 and h2 in s3. When h3 sends its broadcast packet, all of s1-s3 see the packet. Because the packet is broadcast, all three switches report the packet to the controller. When h1 replies to h3, no switches have a rule for destination h3, and the controller knows how to reach h1 and h3 from each switch, so a new rule for reaching h3 is installed at each switch. 922 34 Selected Solutions
INDICES AND TABLES 
- genindex 
- search 923
An Introduction to Computer Networks, Release 2.0.11 924 Indices and tables
BIBLIOGRAPHY [AKSLYZ21] Anubhavnidhi Abhashkumar, Kausik Subramanian, Alexey Andreyev, Hyojeong Kim, Nanda Kishore Salem, Jingyi Yang, Petr Lapukhov, Aditya Akella and Hongyi Zeng, ‚ÄúRunning BGP in Data Centers at Scale‚Äù, 18th USENIX Symposium on Networked Systems Design and IMplementation (NSDI 21), April 2021. [ABDGHSTVWZ15] David Adrian, Karthikeyan Bhargavan, Zakir Durumeric, Pierrick Gaudry, Matthew Green, J. Alex Halderman, Nadia Heninger, Drew Springall, Emmanuel Thom√©, Luke Valenta, Benjamin VanderSloot, Eric Wustrow, Santiago Zanella-B√©guelin, Paul Zimmermann, ‚ÄúImperfect Forward Secrecy: How DifÔ¨Åe-Hellman Fails in Practice‚Äù, preprint at weakdh.org, May 2015. [AEH75] Eralp Akkoyunlu, Kattamuri Ekanadham and R V Huber, ‚ÄúSome constraints and tradeoffs in the design of network communications‚Äù, SOSP ‚Äò75: Proceedings of the Ô¨Åfth ACM symposium on Operating systems and principles, November 1975. [AO96] Aleph One (Elias Levy), ‚ÄúSmashing The Stack For Fun And ProÔ¨Åt‚Äù, Phrack volume 7 number 49, 1996, available at http://insecure.org/stf/smashstack.html. [AGMPS10] Mohammad Alizadeh, Albert Greenberg, David Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta and Murari Sridharan, ‚ÄúData Center TCP (DCTCP)‚Äù, Proceedings of ACM SIGCOMM 2010, September 2010. [AYSKMP13] Mohammed Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown, Balaji Prabhakar, and Scott Shenker‚Äù, ‚ÄúpFabric: Minimal near-optimal datacenter transport‚Äù, SIGCOMM, August 2013. [AP99] Mark Allman and Vern Paxson, ‚ÄúOn Estimating End-to-End Network Path Properties‚Äù, Proceedings of ACM SIGCOMM 1999, August 1999. [ACPRT16] Jo√´l Alwen, Binyi Chen, Krzysztof Pietrzak, Leonid Reyzin and Stefano Tessaro, ‚ÄúScrypt is Maximally Memory-Hard‚Äù, Cryptology ePrint Archive, Report 2016/989, 2016, available at https://eprint.iacr.org/2016/989. [AHLR07] Chris Anley, John Heasman, Felix ‚ÄúFX‚Äù Linder and Gerardo Richarte, ‚ÄúThe Shellcoder‚Äôs Handbook‚Äù, second edition, Wiley, 2007. [AKM04] Guido Appenzeller, Isaac Keslassy and Nick McKeown, ‚ÄúSizing Router Buffers‚Äù, ACM SIGCOMM Computer Communication Review, October 2004 925
An Introduction to Computer Networks, Release 2.0.11 [JA05] John Assalone, ‚ÄúExploiting the GDI+ JPEG COM Marker Integer UnderÔ¨Çow Vulnerability‚Äù, Global Information Assurance CertiÔ¨Åcation technical note, January 2005, available at www.giac.org/paper/gcih/679/exploiting-gdi-plus-jpeg-marker-integer-underÔ¨Çowvulnerability/106878. [PB62] Paul Baran, ‚ÄúOn Distributed Computing Networks‚Äù, Rand Corporation Technical Report P-2626, 1962. [BCL09] Steven Bauer, David Clark and William Lehr, ‚ÄúThe Evolution of Internet Congestion‚Äù, Telecommunications Policy Research Conference (TPRC) 2009, August 2009, available at ssrn.com/abstract=1999830. [MB06] Mihir Bellare, ‚ÄúNew Proofs for NMAC and HMAC: Security without CollisionResistance‚Äù, Advances in Cryptology - CRYPTO ‚Äò06 Proceedings, Lecture Notes in Computer Science volume 4117, Springer-Verlag, 2006. [BCK96] Mihir Bellare, Ran Canetti and Hugo Krawczyk, ‚ÄúKeying Hash Functions for Message Authentication‚Äù, Advances in Cryptology - CRYPTO ‚Äò96 Proceedings, Lecture Notes in Computer Science volume 1109, Springer-Verlag, 1996. [BN00] Mihir Bellare and Chanathip Namprempre, ‚ÄúAuthenticated Encryption: Relations among notions and analysis of the generic composition paradigm‚Äù, Advances in Cryptology ‚Äî ASIACRYPT 2000 / Lecture Notes in Computer Science volume 1976, Springer-Verlag, 2000; updated version July 2007. [BZ97] Jon Bennett and Hui Zhang, ‚ÄúHierarchical Packet Fair Queueing Algorithms‚Äù, IEEE/ACM Transactions on Networking, volume 5, October 1997. [DB08] Daniel Bernstein, ‚ÄúThe Salsa20 family of stream ciphers‚Äù, Chapter, New Stream Cipher Designs, Matthew Robshaw and Olivier Billet, editors, Springer-Verlag, 2008. [JB05] John Bickett, ‚ÄúBit-rate Selection in Wireless Networks‚Äù, MS Thesis, Massachusetts Institute of Technology, 2005. [BCTCU16] Timm B√∂ttger, Felix Cuadrado, Gareth Tyson, Ignacio Castro and Steve Uhlig, ‚ÄúOpen Connect Everywhere: A Glimpse at the Internet Ecosystem through the Lens of the NetÔ¨Çix CDN‚Äù, ARXIV, arXiv e-print (arXiv:1606.05519), June 2016. [BP95] Lawrence Brakmo and Larry Peterson, ‚ÄúTCP Vegas: End to End Congestion Avoidance on a Global Internet‚Äù, IEEE Journal on Selected Areas in Communications, volume 13 number 8, 1995. [BBGO08] Vladimir Brik, Suman Banerjee, Marco Gruteser and Sangho Oh, ‚ÄúWireless Device IdentiÔ¨Åcation with Radiometric Signatures‚Äù, Proceedings of the 14th ACM International Conference on Mobile Computing and Networking (MobiCom ‚Äò08), September 2008. [AB03] Andreis Brouwer, ‚ÄúHackers Hut‚Äù, http://www.win.tue.nl/~aeb/linux/hh/hh.html, April 1, 2003 [CF04] Carlo Caini and Rosario Firrincieli, ‚ÄúTCP Hybla: a TCP enhancement for heterogeneous networks‚Äù, International Journal of Satellite Communications and Networking, volume 22, pp 547-566, 2004. 926 Bibliography
An Introduction to Computer Networks, Release 2.0.11 [CGYJ16] Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh and Van Jacobson, ‚ÄúBBR Congestion-Based Congestion Control‚Äù, ACM Queue, volume 14 number 5, September-October 2016. [CM03] Zehra Cataltepe and Prat Moghe, ‚ÄúCharacterizing Nature and Location of Congestion on the Public Internet‚Äù, Proceedings of the Eighth IEEE International Symposium on Computers and Communication, 2003. [CDLMR00] Stefania Cavallar, Bruce Dodson, Arjen K Lenstra, Walter Lioen, Peter Montgomery, Brian Murphy, Herman te Riele, Karen Aardal, Jeff Gilchrist, Gerard Guillerm, Paul Leyland, Joel Marchand, Fran√ßois Morain, Alec Muffett, Craig Putnam, Chris Putnam and Paul Zimmermann, ‚ÄúFactorization of a 512-bit RSA Modulus‚Äù, Advances in Cryptology ‚Äî EUROCRYPT 2000, Lecture Notes in Computer Science volume 1807, Springer-Verlag, 2000. [CK74] Vinton G Cerf and Robert E Kahn, ‚ÄúA Protocol for Packet Network Intercommunication‚Äù, IEEE Transactions on Communications, volume 22 number 5, May 1974. [CJ89] Dah-Ming Chiu and Raj Jain, ‚ÄúAnalysis of the Increase/Decrease Algorithms for Congestion Avoidance in Computer Networks‚Äù, Journal of Computer Networks volume 17, pp. 1-14, 1989. [CJ91] David Clark and Van Jacobson, ‚ÄúFlexible and efÔ¨Åcient resource management for datagram networks‚Äù, Presentation, September 1991 [CSZ92] David Clark, Scott Shenker and Lixia Zhang, ‚ÄúSupporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism‚Äù, Proceedings of ACM SIGCOMM 1992, August 1992. [CBcDHL14] David Clark, Steven Bauer, kc claffy, Amogh Dhamdhere, BBradley Huffaker, William Lehr, and Matthew Luckie, ‚ÄúMeasurement and Analysis of Internet Interconnection and Congestion‚Äù, Telecommunications Policy Research Conference (TPRC) 2014, September 2014. [DR02] Joan Daemen and Vincent Rijmen, ‚ÄúThe Design of Rijndael: AES ‚Äì The Advanced Encryption Standard.‚Äù, Springer-Verlag, 2002. [DS78] Yogen Dalal and Carl Sunshine, ‚ÄúConnection Management in Transport Protocols‚Äù, Computer Networks 2, 1978. [ID89] Ivan D√•mgard, ‚ÄúA Design Principle for Hash Functions‚Äù, Advances in Cryptology - CRYPTO ‚Äò89 Proceedings, Lecture Notes in Computer Science volume 435, SpringerVerlag, 1989. [DBCP97] Mikael Degermark, Andrej Brodnik, Svante Carlsson, and Stephen Pink, ‚ÄúSmall forwarding tables for fast routing lookups‚Äù, Proceedings of the ACM SIGCOMM ‚Äò97 conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, September 1997. [DKS89] Alan Demers, Srinivasan Keshav and Scott Shenker, ‚ÄúAnalysis and Simulation of a Fair Queueing Algorithm‚Äù, ACM SIGCOMM Proceedings on Communications Architectures and Protocols, 1989. [DH76] WhitÔ¨Åeld DifÔ¨Åe and Martin Hellman, ‚ÄúNew Directions in Cryptography‚Äù, IEEE Transactions on Information Theory, volume IT-22, November 1976. Bibliography 927
An Introduction to Computer Networks, Release 2.0.11 [EGMR05] Mihaela Enachescu, Ashish Goel, Yashar Ganjali, Nick McKeown and Tim Roughgarden. ‚ÄúPart III: Routers with Very Small Buffers‚Äù, ACM SIGCOMM Computer Communication Review, volume 35 number 2, July 2005. [FF96] Kevin Fall and Sally Floyd, ‚ÄúSimulation-based Comparisons of Tahoe, Reno and SACK TCP‚Äù, ACM SIGCOMM Computer Communication Review, July 1996. [FRZ13] Nick Feamster, Jennifer Rexford and Ellen Zegura, ‚ÄúThe Road to SDN: An Intellectual History of Programmable Networks‚Äù, ACM Queue, December 2013. [FGMPC02] Roberto Ferorelli, Luigi Grieco, Saverio Mascolo, G Piscitelli, P Camarda, ‚ÄúLive Internet Measurements Using Westwood+ TCP Congestion Control‚Äù, IEEE Global Telecommunications Conference, 2002. [F91] Sally Floyd, ‚ÄúConnections with Multiple Congested Gateways in Packet-Switched Networks, Part 1‚Äù, ACM SIGCOMM Computer Communication Review, October 1991. [FJ92] Sally Floyd and Van Jacobson, ‚ÄúOn TrafÔ¨Åc Phase Effects in Packet-Switched Gateways‚Äù, Internetworking: Research and Experience, volume 3, pp 115-156, 1992. [FJ93] Sally Floyd and Van Jacobson, ‚ÄúRandom Early Detection Gateways for Congestion Avoidance‚Äù, IEEE/ACM Transactions on Networking, volume 1, August 1993. [FJ95] Sally Floyd and Van Jacobson, ‚ÄúLink-sharing and Resource Management Models for Packet Networks‚Äù, IEEE/ACM Transactions on Networking, volume 3, June 1995. [FP01] Sally Floyd and Vern Paxson, ‚ÄúDifÔ¨Åculties in Simulating the Internet‚Äù, IEEE/ACM Transactions on Networking, volume 9, August 2001. [FMS01] Scott Fluhrer, Itsik Mantin and Adi Shamir, ‚ÄúWeaknesses in the Key Scheduling Algorithm of RC4‚Äù, SAC ‚Äò01 Revised Papers from the 8th Annual International Workshop on Selected Areas in Cryptography, Springer-Verlag, 2001. [FL03] Cheng Fu and Soung Liew, ‚ÄúTCP Veno: TCP Enhancement for Transmission over Wireless Access Networks‚Äù, IEEE Journal on Selected Areas in Communications, volume 21 number 2, February 2003. [LG01] Lixin Gao, ‚ÄúOn Inferring Autonomous System Relationships in the Internet‚Äù, IEEE/ACM Transactions on Networking, volume 9, December 2001. [GR01] Lixin Gao and Jennifer Rexford, ‚ÄúStable Internet Routing without Global Coordination‚Äù, IEEE/ACM Transactions on Networking, volume 9, December 2001. [GNKARS15] Peter Gao, Akshay Narayan, Gauram Kumar, Rachit Agarwal, Sylvia Ratnasamy and Scott Schenker, ‚ÄúpHost: Distributed Near-Optimal Datacenter Transport Over Commodity Network Fabric‚Äù, CoNEXT ‚Äò15, December 2015. [JG93] Jose J Garcia-Lunes-Aceves, ‚ÄúLoop-Free Routing Using Diffusing Computations‚Äù, IEEE/ACM Transactions on Networking, volume 1, February 1993. [GP11] L Gharai and C Perkins, ‚ÄúRTP with TCP Friendly Rate Control‚Äù, Internet Draft, http://tools. ietf.org/html/draft-gharai-avtcore-rtp-tfrc-00. [GV02] Sergey Gorinsky and Harrick Vin, ‚ÄúExtended Analysis of Binary Adjustment Algorithms‚Äù, Technical Report TR2002-39, Department of Computer Sciences, University of Texas at Austin, 2002. 928 Bibliography
An Introduction to Computer Networks, Release 2.0.11 [GM03] Luigi Grieco and Saverio Mascolo, ‚ÄúEnd-to-End Bandwidth Estimation for Congestion Control in Packet Networks‚Äù, Proceedings of the Second International Workshop on Quality of Service in Multiservice IP Networks, 2003. [GM04] Luigi Grieco and Saverio Mascolo, Performance Evaluation and Comparison of Westwood+, New Reno, and Vegas TCP Congestion Control, ACM SIGCOMM Computer Communication Review, volume 34 number 2, April 2004. [GG03] Marco Gruteser and Dirk Grunwald, ‚ÄúEnhancing Location Privacy in Wireless LAN Through Disposable Interface IdentiÔ¨Åers:A Quantitative Analysis‚Äù, Proceedings of the 1st ACM International Workshop on Wireless Mobile Applications and Services on WLAN Hotspots (WMASH ‚Äò03), September, 2003. [HRX08] Sangtae Ha, Injong Rhee and Lisong Xu, ‚ÄúCUBIC: A New TCP-Friendly High-Speed TCP Variant‚Äù, ACM SIGOPS Operating Systems Review - Research and developments in the Linux kernel, volume 42 number 5, July 2008. [SH04] Steve Hanna, ‚ÄúShellcoding for Linux and Windows Tutorial‚Äù, http://www.vividmachines. com/shellcode/shellcode.html, July 2004 [DH08] Dan Harkins, ‚ÄúSimultaneous Authentication of Equals: A Secure, Password-Based Key Exchange for Mesh Networks‚Äù, 2008 Second International Conference on Sensor Technologies and Applications (Sensorcomm 2008), August 2008. [MH04] Martin Hellman, ‚ÄúOral history interview with Martin Hellman‚Äù, Charles Babbage Institute, 2004. Retrieved from the University of Minnesota Digital Conservancy, http://purl.umn. edu/107353. [JH96] Janey Hoe, ‚ÄúImproving the Start-up Behavior of a Congestion Control Scheme for TCP‚Äù, ACM SIGCOMM Symposium on Communications Architectures and Protocols, August 1996. [HVB01] Gavin Holland, Nitin Vaidya and Paramvir Bahl, ‚ÄúA rate-adaptive MAC protocol for multiHop wireless networks‚Äù, MobiCon ‚Äò01: Proceedings of the 7th annual International Conference on Mobile Computing and Networking, 2001. [HSBS15] Xueheng Hu, Lixing Song, Dirk Van Bruggen and Aaron Striegel, ‚ÄúIs There WiFi Yet? How Aggressive WiFi Probe Requests Deteriorate Energy and Throughput‚Äù, Proceedings of the 2015 Internet Measurement Conference, October 2015. [CH99] Christian Huitema, Routing in the Internet, second edition, Prentice Hall, 1999. [HBT99] Paul Hurley, Jean-Yves Le Boudec and Patrick Thiran, ‚ÄúA Note on the Fairness of Additive Increase and Multiplicative Decrease‚Äù, Proceedings of ITC-16, 1999. [JK88] Van Jacobson and Michael Karels, ‚ÄúCongestion Avoidance and Control‚Äù, Proceedings of the Sigcomm ‚Äò88 Symposium, volume 18(4), 1988. [JKMOPSVWZ13] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla, Urs H√∂lzle, Stephen Stuart and Amin Vahdat, ‚ÄúB4: Experience with a Globally-Deployed Software DeÔ¨Åned WAN‚Äù, Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM, August 12-16, 2013. Bibliography 929
An Introduction to Computer Networks, Release 2.0.11 [JWL04] Cheng Jin, David Wei and Steven Low, ‚ÄúFAST TCP: Motivation, Architecture, Algorithms, Performance‚Äù, IEEE INFOCOM 2004 Proceedings, March 2004. [KM97] Ad Kamerman and Leo Monteban, ‚ÄúWaveLAN-II: A high-performance wireless LAN for the unlicensed band‚Äù, AT&T Bell Laboratories Technical Journal, volume 2 number 3, 1997. [SK88] Srinivasan Keshav, ‚ÄúREAL: A Network Simulator‚Äù (Technical Report), University of California at Berkeley, 1988. [KKCQ06] Jongseok Kim, Seongkwan Kim, Sunghyun Choi and Daji Qiao, ‚ÄúCARA: Collision-Aware Rate Adaptation for IEEE 802.11 WLANs‚Äù, IEEE INFOCOM 2006 Proceedings, April 2006. [LK78] Leonard Kleinrock, ‚ÄúOn Flow Control in Computer Networks‚Äù, Proceedings of the International Conference on Communications, June 1978. [KK22] Moshe Kol and Amit Klein, ‚ÄúDevice Tracking via Linux‚Äôs New TCP Source Port Selection Algorithm‚Äù, arXiv:2209.12993v2, September 2022 [to appear in 32nd USENIX Security Conference, 2023]. [LH06] Mathieu Lacage and Thomas Henderson, ‚ÄúYet Another Network Simulator‚Äù, Proceedings of WNS2 ‚Äò06: Workshop on ns-2: the IP network simulator, 2006. [LM91] Xuejia Lai and James L. Massey, ‚ÄúA Proposal for a New Block Encryption Standard‚Äù, EUROCRYPT ‚Äò90 Proceedings of the workshop on the theory and application of cryptographic techniques on Advances in cryptology, Springer-Verlag, 1991. [LKCT96] Eliot Lear, Jennifer Katinsky, Jeff CofÔ¨Ån and Diane Tharp, ‚ÄúRenumbering: Threat or Menace?‚Äù, Tenth USENIX System Administration Conference, Chicago, 1996. [LS18] Carl Lebsack and Rob Shakir, ‚ÄúSNMP is dead‚Äù, presentation at NANOG (North American Network Operators Group), 2018, available at https://research.google/pubs/pub47773. [LSL05] DJ Leith, RN Shorten and Y Lee, ‚ÄúH-TCP: A framework for congestion control in highspeed and long-distance networks‚Äù, Hamilton Institute Technical Report, August 2005. [LSM07] DJ Leith, RN Shorten and G McCullagh, ‚ÄúExperimental evaluation of Cubic-TCP‚Äù, Extended version of paper presented at Proc. Protocols for Fast Long Distance Networks, 2007. [LBS06] Shao Liu, Tamer Basar and R Srikant, ‚ÄúTCP-Illinois: A Loss and Delay-Based Congestion Control Algorithm for High-Speed Networks‚Äù, Proceedings of the 1st international conference on performance evaluation methodologies and tools, 2006. [AM90] Allison Mankin, ‚ÄúRandom Drop Congestion Control‚Äù, ACM SIGCOMM Symposium on Communications Architectures and Protocols, 1990. [MCGSW01] Saverio Mascolo, Claudio Casetti, Mario Gerla, MY Sanadidi, Ren Wang, ‚ÄúTCP westwood: Bandwidth estimation for enhanced transport over wireless links‚Äù, MobiCon ‚Äò01: Proceedings of the 7th annual International Conference on Mobile Computing and Networking, 2001. [McK90] Paul McKenney, ‚ÄúStochastic Fairness Queuing‚Äù, IEEE INFOCOM ‚Äò90 Proceedings, June 1990. 930 Bibliography
An Introduction to Computer Networks, Release 2.0.11 [MABPPRST08] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker and Jonathan Turner, ‚ÄúOpenFlow: Enabling innovation in campus networks‚Äù, ACM SIGCOMM Computer Communications Review, April 2008. [RM78] Ralph Merkle, ‚ÄúSecure Communications over Insecure Channels‚Äù, Communications of the ACM, volume 21, April 1978. [RM88] Ralph Merkle, ‚ÄúA Digital Signature Based on a Conventional Encryption Function‚Äù, Advances in Cryptology ‚Äî CRYPTO ‚Äò87, Lecture Notes in Computer Science volume 293, Springer-Verlag, 1988. [MH81] Ralph Merkle and Martin Hellman, ‚ÄúOn the Security of Multiple Encryption‚Äù, Communications of the ACM, volume 24, July 1981. [MB76] Robert Metcalfe and David Boggs, ‚ÄúEthernet: Distributed Packet Switching for Local Computer Networks‚Äù, Communications of the ACM, volume 19 number 7, 1976. [MW00] Jeonghoon Mo and Jean Walrand, ‚ÄúFair End-to-End Window-Based Congestion Control‚Äù, IEEE/ACM Transactions on Networking, volume 8 number 5, October 2000. [JM92] Jeffrey Mogul, ‚ÄúObserving TCP Dynamics in Real Networks‚Äù, ACM SIGCOMM Symposium on Communications Architectures and Protocols, 1992. [MM01] Jeffrey Mogul and Greg Minshall, ‚ÄúRethinking the TCP Nagle Algorithm‚Äù, ACM SIGCOMM Computer Communication Review, January 2001. [MM94] Mart Molle, ‚ÄúA New Binary Logarithmic Arbitration Method for Ethernet‚Äù, Technical Report CSRI-298, Computer Systems Research Institute, University of Toronto, 1994. [MLAO18] Behnam Montazeri, Yilong Li, Mohammad Alizadeh and John Ousterhout ‚ÄúHoma: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities‚Äù, ACM SIGCOMM conference on Applications, technologies, architectures, and protocols for computer communication, August 2018. [RTM85] Robert T Morris, ‚ÄúA Weakness in the 4.2BSD Unix TCP/IP Software‚Äù, AT&T Bell Laboratories Technical Report, February 1985. [MEGK10] Andreas M√ºller, Nathan Evans, Christian Grothof and Samy Kamkar, ‚ÄúAutonomous NAT Traversal‚Äù, IEEE Tenth International Conference on Peer-to-Peer Computing, August 2010. [NJ12] Kathleen Nichols and Van Jacobson, ‚ÄúControlling Queue Delay‚Äù, ACM Queue, May 2012. [OKM96] Teunis Ott, JHB Kemperman and Matt Mathis, ‚ÄúThe stationary behavior of ideal TCP congestion avoidance‚Äù, Technical Report, 1996. [PFTK98] Jitendra Padhye, Victor Firoiu, Don Towsley and Jim Kurose, ‚ÄúModeling TCP Throughput: A Simple Model and its Empirical Validation‚Äù, ACM SIGCOMM conference on Applications, technologies, architectures, and protocols for computer communication, 1998. [PG93] Abhay Parekh and Robert Gallager, ‚ÄúA Generalized Processor Sharing Approach to Flow Control in Integrated Services Networks - The Single-Node Case‚Äù, IEEE/ACM Transactions on Networking, volume 1 number 3, June 1993. [PG94] Abhay Parekh and Robert Gallager, ‚ÄúA Generalized Processor Sharing Approach to Flow Control in Integrated Services Networks - The Multiple Node Case‚Äù, IEEE/ACM Transactions on Networking, volume 2 number 2, April 1994. Bibliography 931
An Introduction to Computer Networks, Release 2.0.11 [VP97] Vern Paxson, ‚ÄúEnd-to-End Internet Packet Dynamics‚Äù, ACM SIGCOMM conference on Applications, technologies, architectures, and protocols for computer communication, 1997. [PWZMTQ17] Changhua Pei, Zhi Wang, Youjian Zhao, Zihan Wang, Yuan Meng, Dan Pei, Yuanquan Peng, Wenliang Tang and Xiaodong Qu, ‚ÄúWhy It Takes So Long to Connect to a WiFi Access Point‚Äù, IEEE International Conference on Computer Communications, May 2017. [CP09] Colin Percival, ‚ÄúStronger Key Derivation Via Sequential Memory-Hard Functions‚Äù, BSDCan - The Technical BSD Conference, May 2009. [PB94] Charles Perkins and Pravin Bhagwat, ‚ÄúHighly Dynamic Destination-Sequenced DistanceVector Routing (DSDV) for Mobile Computers‚Äù, ACM SIGCOMM Computer Communications Review, volume 24 number 4, October 1994. [PR99] Charles Perkins and Elizabeth Royer, ‚ÄúAd-hoc On-Demand Distance Vector Routing‚Äù, Proceedings of the Second IEEE Workshop on Mobile Computing Systems and Applications, February 1999. [RP85] Radia Perlman, ‚ÄúAn Algorithm for Distributed Computation of a Spanning Tree in an Extended LAN‚Äù, ACM SIGCOMM Computer Communication Review 15(4), 1985. [RP04] Radia Perlman, ‚ÄúRbridges: Transparent Routing‚Äù, IEEE INFOCOM 2004 Proceedings, March 2004. [POBSF14] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah and Hans Fugal, ‚ÄúFastpass: A Centralized ‚ÄúZero-Queue‚Äù Datacenter Network‚Äù, SIGCOMM ‚Äò14, August 2014. [JP88] John M Pollard, ‚ÄúFactoring with Cubic Integers‚Äù, unpublished manuscript circulated 1988; included in ‚ÄúThe Development of the Number Field Sieve‚Äù, Lecture Notes in Mathematics volume 1554, Springer-Verlag, 1993. [PDG12] Balaji Prabhakar, Katherine N Dektar and Deborah M Gordon, ‚ÄúThe Regulation of Ant Colony Foraging Activity without Spatial Information‚Äù, PLoS Computational Biology 8(8), http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002670 [PN98] Thomas Ptacek and Timothy Newsham, ‚ÄúInsertion, Evasion, and Denial of Service: Eluding Network Intrusion Detection‚Äù, Technical report, Secure Networks Inc, January 1998. [RJ90] Kadangode Ramakrishnan and Raj Jain, ‚ÄúA Binary Feedback Scheme for Congestion Avoidance in Computer Networks‚Äù, ACM Transactions on Computer Systems, volume 8 number 2, May 1990. [RX05] Injong Rhee and Lisong Xu, ‚ÄúCubic: A new TCP-friendly high-speed TCP variant,‚Äù 3rd International Workshop on Protocols for Fast Long-Distance Networks, February 2005. [RR91] Ronald Rivest, ‚ÄúThe MD4 message digest algorithm‚Äù, Advances in Cryptology - CRYPTO ‚Äò90 Proceedings, Springer-Verlag, 1991. [RSA78] Ronald Rivest, Adi Shamir and Leonard Adleman, ‚ÄúA Method for Obtaining Digital Signatures and Public-Key Cryptosystems‚Äù, Communications of the ACM, volume 21, February 1978. [RB22] Eric Rye and Robert Beverly, ‚ÄúIPvSeeYou: Exploiting Leaked IdentiÔ¨Åers in IPv6 for StreetLevel Geolocation‚Äù, arXiv:2208.06767, August 2022. 932 Bibliography
An Introduction to Computer Networks, Release 2.0.11 [SRC84] Jerome Saltzer, David Reed and David Clark, ‚ÄúEnd-to-End Arguments in System Design‚Äù, ACM Transactions on Computer Systems, volume 2 number 4, November 1984. [BS93] Bruce Schneier, ‚ÄúDescription of a New Variable-Length Key, 64-Bit Block Cipher (BlowÔ¨Åsh)‚Äù, Fast Software Encryption, Cambridge Security Workshop Proceedings (December 1993), Springer-Verlag, 1994. [SM90] Nachum Shacham and Paul McKenney, ‚ÄúPacket recovery in high-speed networks using coding and buffer management‚Äù, IEEE INFOCOM ‚Äò90 Proceedings, June 1990. [SP03] Umesh Shankar and Vern Paxson, ‚ÄúActive Mapping: Resisting NIDS Evasion without Altering TrafÔ¨Åc‚Äù, Proceedings of the 2003 IEEE Symposium on Security and Privacy, 2003. [SV96] M Shreedhar and George Varghese, ‚ÄúEfÔ¨Åcient Fair Queuing Using DeÔ¨Åcit Round Robin‚Äù, IEEE/ACM Transactions on Networking, volume 4 number 3, June 1996. [JS05] John Ivan Simon, ‚ÄúJohn Simon on Music: Criticism, 1979-2005‚Äù, Applause Books, 2005. [SKS06] Rade Stanojevi ¬¥c, Christopher Kellett and Robert Shorten, ‚ÄúAdaptive Tuning of Drop-Tail Buffers for Reducing Queuing Delays‚Äù, IEEE Communications Letters, volume 10 number 7, August 2006. [TSZS06] Kun Tan, Jingmin Song, Qian Zhang and Murari Sidharan, ‚ÄúCompound TCP: A Scalable and TCP-friendly Congestion Control for High-speed Networks‚Äù, 4th International Workshop on Protocols for Fast Long-Distance Networks (PFLDNet), 2006. [TR88] Andrew S. Tanenbaum and Robbert van Renesse, ‚ÄúA Critique of the Remote Procedure Call Paradigm‚Äù, Proceedings of the European Teleinformatics Conference (EUTECO 88), North-Hollnd, Amsterdam, 1988, pp. 775-783. [TWHL05] Ao Tang, Jintao Wang, Sanjay Hegde and Steven Low, ‚ÄúEquilibrium and Fairness of Networks Shared by TCP Reno and Vegas/FAST‚Äù, Telecommunications Systems special issue on High Speed Transport Protocols, 2005. [TWP07] Erik Tews, Ralf-Philipp Weinmann and Andrei Pyshkin, ‚ÄúBreaking 104-bit WEP in less than 60 seconds‚Äù, WISA‚Äô07 Proceedings of the 8th International Conference on Information Security Applications, Springer-Verlag, 2007. [VMCCP16] Mathy Vanhoef, C√©lestin Matte, Mathieu Cunche, Leonardo Cardoso and Frank Piessens, ‚ÄúWhy MAC Address Randomization is not Enough: An Analysis of Wi-Fi Network Discovery Mechanisms‚Äù, ACM Asia Conference on Computer and Communications Security, May 2016. [VP17] Mathy Vanhoef and Frank Piessens, ‚ÄúKey Reinstallation Attacks: Forcing Nonce Reuse in WPA2‚Äù, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, October 2017. [VGE00] Kannan Varadhan, Ramesh Govindan and Deborah Estrin, ‚ÄúPersistent Route Oscillations in Inter-domain Routing‚Äù, Computer Networks, volume 32, January, 2000. [SV02] Serge Vaudenay, ‚ÄúSecurity Flaws Induced by CBC Padding ‚Äì Applications to SSL, IPSEC, WTLS.. . ‚Äù, EUROCRYPT ‚Äò02 Proceedings, 2002. [WJLH06] David Wei, Cheng Jin, Steven Low and Sanjay Hegde, ‚ÄúFAST TCP: Motivation, Architecture, Algorithms, Performance‚Äù, ACM Transactions on Networking, December 2006. Bibliography 933
An Introduction to Computer Networks, Release 2.0.11 [WM05] Damon Wischik and Nick McKeown, ‚ÄúPart I: Buffer Sizes for Core Routers‚Äù, ACM SIGCOMM Computer Communication Review, volume 35 number 2, July 2005. [YW19] Christopher S Yoo and David Wishnick, ‚ÄúLowering Legal Barriers to RPKI Adoption‚Äù, U of Penn Law School, Public Law Research Paper No. 19-02, January 8, 2019. Available at SSRN: https://ssrn.com/abstract=3308619. [LZ89] Lixia Zhang, ‚ÄúA New Architecture for Packet Switching Network Protocols‚Äù, PhD Thesis, Massachusetts Institute of Technology, 1989. [LZ91] Lixia Zhang, ‚ÄúVirtualClock: A New TrafÔ¨Åc Control Algorithm for Packet-Switched Networks‚Äù, ACM Transactions on Computer Systems, volume 9 number 2, May 1991. [ZSC91] Lixia Zhang, Scott Shenker and David Clark, ‚ÄúObservations on the Dynamics of a Congestion Control Algorithm: The Effects of Two-Way TrafÔ¨Åc‚Äù, ACM SIGCOMM Symposium on Communications Architectures and Protocols, 1991. 934 Bibliography
INDEX Symbols .onion addresses, 234 /64, IPv6, 275 /etc/resolv.conf, 233 ‚Äò‚Äòcwnd‚Äò‚Äò, initial value, 452 ‚Äò‚Äòcwnd‚Äò‚Äò, monitoring, 804 0-RTT protection, QUIC, 440 0-RTT protection, TLS v1.3, 756 1-RTT protection, QUIC, 440 1-RTT protection, TLS v1.3, 756 127.0.1.1, 233 2-D parity, 171 2.4 GHz, 99 3DES, 726 4B/5B, 151 4G, 124 5 GHz, 99 6LoWPAN, 98 6in4, IPv6, 282 802.11, 98 802.11i, IEEE, 114 802.11r handoff, 111 802.16, 124 802.1D, IEEE, 63 802.1Q, 74 802.1X, IEEE, 117 802.3, IEEE Ethernet, 48 A A and AAAA records, DNS, 276 A record, DNS, 230 accelerated open, TCP, 424 access point, CDN, 34 access point, Wi-Fi, 107 accurate costs, 298 ACD, IPv4, 243 ACK compression, 528ACK, TCP, 395 acknowledgment, 31 acknowledgment number, TCP, 395 acknowledgments, QUIC, 438 ACKs of unsent data, TCP, 409 ACK[N], 179 active close, TCP, 411 active queue management, 501 active subqueue, 558 ad hoc configuration, Wi-Fi, 107 ad hoc wireless netw, 122 adaptive droptail algorithm, 505 ADDITIONAL, DNS, 234 additive increase, multiplicative decrease, 446 address, 14 address configuration, manual IPv6, 282 address ownership, QUIC, 440 address randomization, 711 Address Resolution Protocol, 242 AddressFamily, 369 Administratively Prohibited, 249 admission control, RSVP, 614 ADT, 505 advertised window size, 426 aero, 230 AES, 727 AF drop precedence, 620 AfriNIC, 29 agent configuration, SNMP, 660 agent, SNMP, 635 AH, IPsec, 763 AIMD, 446, 499 algorithm, AODV, 301 algorithm, distance-vector, 292 algorithm, DSDV, 300 935
An Introduction to Computer Networks, Release 2.0.11 algorithm, EIGRP, 305 algorithm, Ethernet learning, 62 algorithm, exponential backoff, 54 algorithm, fair queuing bit-by-bit round-robin, 559 algorithm, fair queuing GPS, 562 algorithm, fair queuing, quantum, 568 algorithm, Floyd-Warshall, 823 algorithm, hierarchical weighted fair queuing, 577 algorithm, HWMP, 304 algorithm, Karn/Partridge, 429 algorithm, link-state, 306 algorithm, loop-free distance vector, 300 algorithm, Nagle, 427 algorithm, Shortest-Path First, 307 algorithm, spanning-tree, 69 algorithms, forwarding table lookup, 211 Alice, 719 alignment, memory, 204 all-nodes multicast address, 259 all-routers multicast address, 259 ALOHA, 57 AMI, 153 ANSWER, DNS, 234 anternet, 443 anycast address, IPv6, 257 anycast, via BGP, 347 Aodh, 719 AODV, 301, 893 APNIC, 29 AQM, 501 ARC4, 728 ARCFOUR, 728 architecture, network, 635 argon2, 722 ARIN, 29 arithmetic, fast, 734 ARP, 242 ARP cache, 242 ARP failover, 244 ARP spoofing, 244 ARPANET, 38 ARQ protocols, 179 AS-path, 334AS-set, 336 ASLR, 711 ASN.1, 638, 640 ASN.1 enumerated type, 656 association, Wi-Fi, 107 Assured Forwarding, 620 Assured Forwarding PHB, 618 asymmetric routes, 327 Asynchronous Transfer Mode, 142 at-least-once semantics, 385 ATM, 14, 142 augmentation, SNMP, 672 authentication header, IPsec, 763 authentication with secure-hash functions, 721 authenticator, WPA, 117 authoritative nameserver, 230 AUTHORITY, DNS, 234 autoconfiguration, IPv4, 246 autonomous system, 291, 319, 334 B B8ZS, 153 backbone, 28 backoff, Ethernet, 54 backup link, BGP, 344 backwards compatibility, TCP, 518 bad news, distance-vector, 293 badssl.com, 749 band width, wireless, 94 bandwidth, 13 bandwidth delay, 164, 185 bandwidth delay, 161 bandwidth guarantees, 591 bandwidth, emulating, 800 base station, WiMAX, 125 basic encoding rules, 662 BBR TCP, 542 BBRR, 559 bcrypt, 722 BDP, 185 beacon packets, Wi-Fi, 108 beacon, Wi-Fi, 111 beefcafe, 282 BER, 662 Berkeley Packet Filter, 811 Berkeley Unix, 40 best-effort, 26, 31 936 Index
An Introduction to Computer Networks, Release 2.0.11 best-path selection, BGP, 337 BGP, 333, 351 BGP relationships, 348 BGP routing registries, 354 BGP security, 354 BGP speaker, 334 BGP table size, 339 big-endian, 15, 386 binary data, 371 bind(), 366 bind() and TCP, 407 bit stuffing, 152 bit-by-bit round robin, 559 biz, 230 BLAM, 56 Blowfish, 727 Bluetooth, 98 Bob, 719 boot counter, 386 border gateway protocol, 333 border routers, 299 bottleneck link, 187, 472 BPDU message, spanning tree, 70 bpf, 811 bps, 13, 22 broadcast IPv4 address, 207 broadcast, Ethernet, 23 broadcast, MANETs, 123 BSD, 40 buffer overflow, 35 buffer overflow, heap, 712 bufferbloat, 463, 501, 570 bugs, 764 byte stuffing, 152 C CA, 748 cache poisoning, DNS, 240 cache, DNS, 231, 232 CAM table, 77 canonical name, DNS, 237 Canopy, 130 capture effect, Ethernet, 55 care-of address, 225 carrier Ethernet, 136 CBC mode, 727 CDN and IntServ, 615 CDN closest edge server, 241CDNs, 34 cell-loss priority bit, ATM, 142 censorship, 754 certificate authorities, 743 certificate authority, 748 certificate pinning, 750 certificate revocation, 751 certificate revocation list, 751 CFB mode, 729 CGA, 265 chain of trust, 766 challenge-handshake authentication, 722 channel width, 94 channel, Wi-Fi, 99 ChaosNet, 236 chap authentication, 722 checksum offloading, 61, 401 checksum, TCP, 394 checksum, UDP, 361 Christmas day attack, 422 chrome://flags, 437 chrome://net-internals, 437 CIDR, 319 cipher feedback mode, 729 cipher modes, 727 Cisco, 74, 305, 343 cisco command-line interface, 798 class A/B/C addressing, 24 Class Selector PHB, 618 class, queuing discipline, 555 classful queuing discipline, 555 Classless Internet Domain Routing, 319 clear-to-send, Wi-Fi, 102 client, 31 client-server, 33 ClientHello, TLS, 753 ClientKey, 723 ClientSignature, 723 cliff, 20, 445 CLNP, 39 clock recovery, 149 clock synchronization, 149 close, TCP, 397, 412 closest CDN edge server, 241 CMIP, 634 CMNS, 39 Index 937
An Introduction to Computer Networks, Release 2.0.11 CMOT, 634 CNAME, DNS, 237 CoDel, 505 Cogent Communications, 288 collision, 22, 47 collision attack, MD5, 720 collision avoidance, 102 collision detection, 48, 57, 58 collision detection, wireless, 93 collision domain, 61 collision problem, hash, 719 collision, Wi-Fi, 100 commands, Mininet, 784 community attribute, BGP, 343, 344 community, SNMP, 660 concave cwnd graph, 521 configuring FreeRADIUS, 118 configuring WPA2-Enterprise, 118 congestion, 20, 26, 32 congestion avoidance phase, 445 congestion bit, 502 congestion window, 444 connect(), 369, 371 connection, 14 connection reuse in TIMEWAIT, 421 connection table, virtual circuit, 139 connection-oriented, 393 connection-oriented networking, 26 connectionless networking, 26 connectx(), 407 conservative, 38 Content-Addressable Memory, 77 content-addressable memory, 213 Content-Distribution Networks, 34 contention, 20 contention interval, 57 contributing source, RTP, 625 control packet, Wi-Fi, 98 control packet, Wi-Fi ACK, 101 control packet, Wi-Fi RTS/CTS, 102 convergence to TCP fairness, 500, 518 convex cwnd graph, 521 counter mode, 729 CRC code, 169 cross-site scripting, 716 crossbar switch, 76cryptographically generated address, 234, 265 cryptograpy, 738 CSMA, 56 CSMA persistence, 56 CSMA/CA, 102 CSMA/CD, 48 CSRC, 625 CTR mode, 729 CTS, Wi-Fi, 102 Cubic, TCP, 537 cumulative ACK, 184 customer, BGP, 348 cut-through, 21 cwnd, 444 CWR, 395, 502 cyclical redundancy check, 169 D DAD, IPv6, 267 DALLY, TFTP, 379 DANE, 772 dark website, 774 Data Center TCP, 533 data rate, 13 data types, SNMPv1, 640 datacenter, 84 Datagram Congestion Control Protocol, 363 datagram forwarding, 14 Data[N], 179 DCCP, 363 DCCP congestion control, 499 DCCP connection establishment, 433 DCTCP, 533 deadbeef, 330 DECbit, 502 DECnet, 502 default forwarding entry, 211 default route, 18, 29 defense in depth, security, 774 deflation, cwnd, 454 delay constraints, 594 delay, bandwidth, 161 delay, emulating, 800 delay, largest-packet, 167 delay, propagation, 161 delay, queuing, 161 938 Index
An Introduction to Computer Networks, Release 2.0.11 delay, store-and-forward, 161 delay-based congestion control, 523 delayed ACKs, 426, 450 denial of service attack, 204 dense wavelength-division multiplexing, 158 DES, 726 Destination Unreachable, 249 DHCP, 245 DHCP Relay, 247 DHCPv6, 266, 270 Differentiated Services, 203, 617 Diffie-Hellman-Merkle key exchange, 733 DiffServ, 617 DIFS, Wi-Fi, 101, 122 dig, 36, 235 digital signatures, RSA, 740 discrete logarithm problem, 734 distance vector, loop-free versions, 300 distance-vector, 29 distance-vector routing update, 292 distribution network, Wi-Fi, 111 DIX Ethernet, 48 DNS, 30, 229, 246, 367, 369, 371, 383, 405 DNS A and AAAA records, 276 DNS and CDNs, 241 DNS and IPv6, 267, 268, 276 DNS attack, 2018, 774 DNS authentication, 772 DNS over HTTPS, 775 DNS resolver, 231 DNS server, public, 231 DNS time-to-live, 232 DNS, round-robin, 234 DNSKEY, DNS, 766 DNSSEC, 766 DNSSEC OK flag, 766 dnssec-failed.org, 771 DoH, 775 domain fronting, 754 Domain Name System, 229 domain name system, 30 domain registrar, 230 domain registrar, DNS, 230 Dont Fragment bit, 209 doze mode, Wi-Fi, 109draft standard, 40 Dragonfly, 735 drop precedence, DiffServ AF, 620 DS, 203 DS domain, 617 DS field, IPv4 header, 618 DS, DNS, 766 DS1 line, 154 DS3 line, 155 DSDV, 300, 893 DSO channel, 154 dual stack, 276 DuckDuckGo, 234 dumbbell network topology, 480 duplicate address detection, IPv4, 243 duplicate connection request, 379 duplicate-address detection, IPv6, 267 durian, 680 DWDM, 158 dynamic rate scaling, 104 E EAP, 117 EAPOL, 117 EasyConnect, Wi-Fi, 120 EasyMesh Wi-Fi, 112 ECB mode, 727 ECE, 395, 502 Echo Request/Reply, 248 ECMP, 312, 820 ECN, 203, 502 ECN and VPNs, 136 ECT, 502 edge server, CDN, 34 edge server, closest, 241 EFS, 455 EGP, 335 EIGRP, 305 elephant flow, 310 elevator algorithm, 386 elliptic-curve cryptography, 739 emulating bandwidth, 800 emulating delay, 800 emulating queue size, 800 eNB, LTE, 125 Index 939
An Introduction to Computer Networks, Release 2.0.11 encapsulated security payload, IPsec, 763 encoding, 371 encrypt-and-MAC, 730 encrypt-then-MAC, 730 encryption, 724 end-to-end encryption, 743 End-to-End principle, 179, 394 engines, SNMPv3, 690 enumerated type, SNMP, 656 equal-cost multipath routing, 312, 820 error message, ICMP, 248 error-correcting code, 171 error-detection code, 167 ESP, IPsec, 763 ESS, Wi-Fi, 111 estimated flightsize, 455 Ethernet, 22 Ethernet address, 23 Ethernet hub, 49 Ethernet repeater, 49 Ethernet switch, 61 Ethernet switch hardware, 75 ethtool, Linux, 401 Euclidean algorithm, 739 EUI-64 identifier, 255 evasion, intrusion detection, 717 exactly-once semantics, 384 Expedited Forwarding, 618 Expedited Forwarding PHB, 618 expiration date, certificate, 751 Explicit Congestion Notification, 203, 502 exponential backoff, Ethernet, 54 exponential backoff, Wi-Fi, 102 exponential growth, 448 export filtering, BGP, 337 extended interface table, SNMP, 675 extended-validation certificates, 750 extender, Wi-Fi, 112 Extensible Authentication Protocol, 117 extension headers, 259 exterior routing, 333 extreme TCP unfairness, 486F face:b00c, 277 facebookcorewwwi, 234 factoring RSA keys, 741 fading, 95 fair queuing, 556 fair queuing and AF, 621 fair queuing and EF, 619 fairness, 32 fairness, TCP, 472, 480, 493, 518 fallback to flooding, 62 Fantasia, 182 fast arithmetic, 734 Fast Ethernet, 59 fast handoff, Wi-Fi, 111 Fast Open, TCP, 424 fast primality testing, 735 Fast Recovery, 454 fast retransmit, 453 fastest sequence, token-bucket, 589 FCAPS, 633 Feistel network, 725 FIB, 339 Fibonacci sequence, 582 FIFO queuing, 471 fill time, voice, 154 filtering, BGP, 337 filterspec, 613 FIN, 395 FIN packet, 397 finishing order, WFQ, 564 firewall, 35, 80, 83 fixed wireless, 128 flights of packets, 445 flightsize, 543 flightsize, estimated, 455 flow control, 183 flow control, QUIC, 438 flow control, TCP, 428 Flow Label, 254 flow specification, 583 flow tables, 81 flow, IPv6, 254 flow, TCP, 818 flowspec, RSVP, 613 Floyd-Warshall algorithm, 823 fluid model, fair queuing, 562 940 Index
An Introduction to Computer Networks, Release 2.0.11 foraging, 443 foreign agent, 225 fortune, 234 forward secrecy, 742 forwarding delay, 21 Forwarding Information Base, 339 forwarding table, 16 forwarding, IP, 27 forwarding, MANET, 124 forwarding-table algorithms, 211 four-way handshake, Wi-Fi, 114 foxes, 443 fragment header, IPv6, 261 fragment offset, 208 fragmentation, 26 Fragmentation Required, 249 fragmentation, IP, 207 fragmentation, Wi-Fi, 104 frame, 14 frames, QUIC, 437 framing, 152 FreeRADIUS, 118 frequency band, 94 Friendliness, TCP, 497 Frost, Robert, 138 full-duplex Ethernet, 60 fwmark, 311, 811 G gamers and CDNs, 615 gaming private networks, 615 generalized processor sharing, 562 generic hierarchical queuing, 572 geoDNS, 241 geographical routing, 328 geolocation, IP, 327 GET request, HTTP, 408 getaddrinfo(), 276 getAllByName(), Java, 367 getAllByName(), java, 369 GetBulk, SNMPv2, 670 getByName(), 277 getByName(), Java, 276 getByName(), java, 369 gethostbyname(), 276 gigabit Ethernet, 60 glibc-2.2.4, 712 global scope, IPv6 addresses, 273gNMI, 665 GNS, 231 Gnu DNS, 231 Gnu Name System, 231 goodput, 13, 829 governments, DANE, 773 GPS, fair queuing, 562 granularity, loss counting, 857 gratuitous ARP, 243 greediness in TCP, 484 group, OpenFlow, 820 gRPC, 387, 665 GTC, EAP, 118 H H-TCP, 536 Hadoop file system, 388 half-closed, TCP, 411 half-open, TCP, 411 Hamilton TCP, 536 Hamming code, 171 handoff, Wi-Fi, 111 Handshake packet, QUIC, 439 handshake protocol, TLS, 752 Happy Eyeballs, IPv6, 279 hard fail, OCSP, 752 hardware, Ethernet switch, 75 Hash Message Authentication Code, 721 hash, password, 722 HDLC, 152 head-of-line blocking, 32, 362, 607 header, 14 header, Ethernet, 49 header, IPv4, 202 header, IPv6, 253 header, TCP, 394 header, UDP, 361 heap buffer overflow, 712 heap vulnerability, 712 Hellman (DifÔ¨Åe-Hellman-Merkel ), 733 Hello, spanning-tree, 70 henhouse, 443 hidden node problem, 94 hidden-node collisions, 103 hidden-node problem, 103 hierarchical routing, 213, 319, 321 hierarchical token bucket, 595 Index 941
An Introduction to Computer Networks, Release 2.0.11 hierarchical token bucket, Linux, 597 high-bandwidth TCP problem, 506 Highspeed TCP, 519 hijacking, BGP, 354 history, RMON, 683 HMAC, 692, 721 hold down, 297 Homa, RPC, 387 home address, 225 home agent, 225 host command, 36 host key, ssh, 746 Host Top N, RMON, 685 Host Unreachable, 249 host-specific forwarding, 136 hot-potato routing, 327 htb, Linux, 597, 808 htonl, 373 htons, 373 HTTP, 408, 420 https, 744 hub, Ethernet, 49 hulu, 605 Hurricane Electric, 288 HWMP, 304 Hybla, TCP, 533 I IAB, 38 IANA, 25, 29, 657 ICANN, 230 ICMP, 247 ICMPv6, 273 IDEA, 727 idempotency and 0-RTT, 756 idempotent, 385 IDENT field, 208 IEEE, 48, 51 IEEE 802.11, 98 IEEE 802.1D, 63 IEEE 802.1Q, 74 IEEE 802.1X, 117 IEEE 802.3 Ethernet, 48 IETF, 38 ifconfig, 36 ifDescr, 656 ifIndex, 655IFS, Wi-Fi, 101 ifType, SNMP, 656 ifXTable, SNMP, 675 IKEv2, 765 Illinois, TCP, 529 implementations, at least two, 40 import filtering, BGP, 337 incarnation, connection, 376 incast, TCP, 536 India, 236 inetCidrRouteTable, 679 inflation, cwnd, 454 info, 230 infrastructure configuration, Wi-Fi, 107 Initial packet, QUIC, 439 initial sequence number, TCP, 395, 399, 421 initial value of ‚Äò‚Äòcwnd‚Äò‚Äò, 452 initialization vector, 727 input queue, switch, 77 instability, BGP, 352 integrated services, 607 integrated services, generic, 603 interconnection fabric, 84 interface, 204 interface identifier, ipv6, 255 interface table, extended, SNMP, 675 interior routing, 291 interior routing with BGP, 347 Internet Architecture Board, 38 Internet checksum, 168 Internet Engineering Task Force, 38 Internet exchange point, 325 Internet Key Exchange, 765 Internet Routing Registry, 354 Internet Society, 38 intersymbol interference, 95 intrusion detection, 717 IntServ, 607 IP, 13, 24 ip command (Linux ), 36 IP forwarding, 27 IP fragmentation, 207 IP multicast, 608 IP network, 25 IP-in-IP encapsulation, 225 IP_BIND_ADDRESS_NO_PORT, Linux, 407 942 Index
An Introduction to Computer Networks, Release 2.0.11 ipconfig, 36 IPsec, 763 iptables, 311, 811 IPv4 header, 202 IPv6, 253, 259 IPv6 /64, 275 IPv6 address configuration, manual, 282 IPv6 addresses, 254 IPv6 connections, link-local, 281 IPv6 extension headers, 259 IPv6 header, 253 ipv6 interface identifier, 255 IPv6 link-local connections, 281 IPv6 multicast, 259 IPv6 Neighbor Discovery, 262 IPv6 programming, 367 IPv6 tunnel, 282 IPvSeeYou, 256 irregular prime, 28 IS-IS, 306 ISM band, 99 ISN, 399, 421 ISOC, 38 ISP, 17 IV, 727 IXP, 325 J jail, staying out of, 325 Java getAllByName(), 367 java getAllByName(), 369 Java getByName(), 276 java getByName(), 369 javascript, 716 jitter, 33, 463, 501, 605, 626 join, multicast, 610 joining a Wi-Fi network, 108 JPEG heap vulnerability, 714 JSON, 373 jumbogram, IPv6, 260 K Karn/Partridge algorithm, 429 kB, 13 KeepAlive, TCP, 430 key exchange, TLS, 755 key-scheduling algorithm, RC4, 729key-signing parties, 743 key-value store, SDN, 87 keystream, 727 KiB, 13 kings, 38 knee, 20, 445, 523 known_hosts, ssh, 746 KRACK attack, WPA, 116 L LACNIC, 29 ladder diagram, 163 lag and CDNs, 615 LAN, 13, 22 LAN layer, 51 LARTC, 310 latching on, TFTP, 375 layer 3 switch, 218 layers, 13 leaky bucket, alternative formulation, 588 learning, Ethernet switch, 24, 62, 82 legacy routing, 322 length-extension vulnerability, 720 liberal, 38 licensing this book, 4 lightning, 158 Limited Transmit, TCP Reno, 457 limiting delay, 592 link-layer ACK, 101 link-local address, 256 link-state packets, 306 link-state routing update, 306 Linux, 107, 224, 242, 245, 281, 282, 515, 529, 537, 555 Linux advanced routing, 310 Linux htb, 597 Linux IPv6 routing, 263 Linux sfq, 569 Lisp, 725 listen, 31 little-endian, 15 load balancer, Pox, 820 load-balancing, 345 load-balancing, SDN, 85 load-balancing, traditional, 86 local traffic, 337 local-area network, 22 Index 943
An Introduction to Computer Networks, Release 2.0.11 logical link layer, 13 logjam attack, 734 lollipop numbering, 307 longest-match rule, 321, 325 looking-glass, 288 loopback address, 206 loopback interface, 204 loss recovery, sliding windows, 187 loss synchronization, 486 loss synchronization, TCP, 484 loss-based congestion control, 523 loss-tolerant, 362, 604 lossy-link TCP problem, 508 lost final ACK, 377 lost final ACK, TFTP, 379 LRO, TCP, 401 LSA, 306 LSO, TCP, 401 LSP, 306 LTE, 124 M MAC address, 23 MAC address randomization, 110 MAC flooding attack, 64 MAC layer, 51 MAC-then-encrypt, 730 MAE, 325 MAE-East, 325 man-in-the-middle attack, 734, 743 managed device, SNMP, 635 management frame protection, Wi-Fi, 120 management packet, Wi-Fi, 98, 108 manager, SNMP, 635 Manchester encoding, 150 MANET, 122 mangling, 82, 311 MANRS, 354, 356 manual IPv6 address configuration, 282 MapReduce, 536 market, IPv4 addresses, 30 master secret, TLS, 755 Matrix, RMON, 686 max-min fairness, 493 Maximum Transfer Unit, 208 MB, 13Mbone, 612 Mbps, 13, 22 MD5, 720 MED, 327, 338, 341 media-access control, 51 Merkle (DifÔ¨Åe-Hellman-Merkel ), 733 Merkle-D√•mgard construction, 720 mesh network, 122 mesh network, HWMP, 304 MiB, 13 MIB browsers, 651 MIB-2, 652 MIC, 115 Michael, 115 Mickey Mouse, 182 Microsoft SNMP agent, 661 middlebox, 222 middleboxes, 399 middleboxes and ECN, 503 MIMO, 105 minimal network configuration, 246 minimizing route cost, 298 minimum RTO, TCP, 535 Mininet, 780 Mininet commands, 784 MISO, 106 Mitnick, Kevin, 23, 422 mixer, RTP, 624 mobile IP, 224 mobile wireless network, 122 modified EUI-64 identifier, 255 monitoring ‚Äò‚Äòcwnd‚Äò‚Äò, 804 MPLS, 628 MPTCP, 431 msieve, 778 MTU, 208 multi-exit discriminator, 338, 341 multi-protocol label switching, 628 multicast, 259 multicast address allocation, 612 multicast IP address, 207 multicast programming, 792 multicast subscription, 610 multicast tree, 608 multicast, Ethernet, 50 multicast, IP, 25, 608 multihomed, 205, 324 multihoming and ARP, 245 944 Index
An Introduction to Computer Networks, Release 2.0.11 multihoming and TCP, 402 multihoming and UDP, 366 multipath interference, 95 Multipath TCP, 431 multiple addresses, IPv6, 269 multiple flow tables, 83 multiple losses, 518 multiple token buckets, 589 MUST, 38 MX records, DNS, 239 N Nagle algorithm, 427 naked domain, DNS, 238 nameserver, 230 NAT, 218, 226, 284, 361, 394 NAT and ICMP, 249 NAT and IPsec, 766 NAT problems, 220 NAT Traversal, 222 NAT-PT, IPv6-to-IPv4, 286 NAT64, 286 nc, 371 Neighbor Advertisement, IPv6, 264 neighbor discovery security, 264 Neighbor Discovery, IPv6, 262 Neighbor Solicitation, IPv6, 264 net neutrality, 604 Net-SNMP, 635, 639, 651, 653, 661 Net-SNMP and SNMPv3, 696 netcat, 37, 371, 408 netcat HTTP GET request, 408 netem qdisc, 801 netsh (Windows ), 36 netstat, 37, 410 network address, 25 network address translation, 218 network architecture, 635 network entry, WiMAX, 128 Network File System, 384 network interface, Ethernet, 23 network management, 554 Network Management System, 634, 665 network model five layer, 13 four layer, 13 seven layer, 39 network number, 25network prefix, 25 network prefix, IPv6, 258 Network Unreachable, 249 NewReno, TCP, 458 next_hop, 16 NEXT_HOP attribute, BGP, 341 NFS, 384 NMS, 634, 665 no-transit, BGP, 344 no-valley theorem, BGP, 351 node information message, IPv6, 274 non-compliant, token-bucket, 585 non-congestive loss, 508 non-executable stack, 711 non-recursive DNS lookup, 234 non-repudiation, 719, 722 nonce, 722, 723 nonce, cryptographic, 753 nonpersistence, 56 NOPslide, 709 noSuchObject, SNMP, 669 NoTCP Manifesto, 362 NRZ, 149 NRZI, 150 NS record, DNS, 230 ns-2 trace file, 832 ns-2 tracefiles, reading with python, 836 NSEC, 770 NSEC, DNS, 766 NSEC3, DNS, 766 NSFNet, 38 NSIS, 622 nslookup, 36, 235, 277 ntohl, 373 ntohs, 373 number-field sieve, 741 NX page bit, 711 NXDOMAIN, DNS, 234 O Object ID, SNMP, 637 OBJECT-IDENTITY, SNMP, 669 OBJECT-TYPE, SNMP, 641 OC-3, 156 OCSP, 751 OCSP stapling, 752 OFDM, 95 Index 945
An Introduction to Computer Networks, Release 2.0.11 OFDMA, 95 offloading, TCP, 401 OID, SNMP, 637 old duplicate packets, 376 old duplicates, TCP, 419, 420 OLSR, 893 on-demand mode, HWMP, 304 one-time pad, 728 ones-complement, 168 onion addresses, 234 OpenBSD, 712 OpenFlow, 81 openssl, 119 openSSL programming, 757 Opportunistic Wireless Encryption, 120 Optical Transport Network, 157 optimistic DAD, 267 opus, 605 ork, 122 orthogonal frequency-division multiplexing, 95 OSI, 38 OSPF, 306 OTN, 157 overhead, ns-2, 850 P packet loss rate, 495 packet pairs, 479 packet size, 165 PAP, EAP, 118 Parekh-Gallager claim, 567 Parekh-Gallager theorem, 598 parking-lot topology, 494 partial ACKs, 458 passive close, TCP, 411 password hash, 722 password sniffing, 23, 245 path attributes, BGP, 340 path bandwidth, 187 Path MTU Discovery, 210, 249 path MTU discovery, TCP, 425 PATH packet, RSVP, 613 Paul Dukas, 182 PAUSE frame, Ethernet, 62 PAWS, TCP, 423 PBKDF2, 722pcap, 37 pcapng, 37 PCF Wi-Fi, 121 PEAP, 118 peer, BGP, 348 peer-to-peer, 33 per-hop behaviors, DiffServ, 618 perfect forward secrecy, 742 persist timer, TCP, 428 persistence, 56 phase effects, TCP, 847 PHBs, DiffServ, 618 physical address, Ethernet, 23 physical layer, 13 PIFS, Wi-Fi, 122 PIM-SM, 610 ping, 35, 248 ping6, 281 pinning, TLS certificates, 750 pipe drain, 187 pipelining, SMTP, 172 PKI, 743 playback buffer, 605 PMK, 802.1X, 117 Pogonomyrmex, 443 Point of Presence, 34 point-to-point protocol, 152 poison reverse, 297 policing, 585 policy-based routing, 81, 310 Pollard‚Äôs rho algorithm, 778 polling mode, Wi-Fi, 121 polling, TCP, 428 POODLE vulnerability, 744 Port Control Protocol, 221 port exhaustion, TCP, 420 port numbers, UDP, 361 port security, Ethernet, 64 Port Unreachable, 249 potatoes, hot, 327 power and wireless, 98 power management, Wi-Fi, 109 Pox controller, 812 PPP, 152 PPPoE, 152 pre-master secret, TLS, 755 prefix information, IPv6, 263 prekey, and forward secrecy, 742 946 Index
An Introduction to Computer Networks, Release 2.0.11 presentation layer, 39 presidents, 38 primality testing, fast, 735 Prime Number Theorem, 735 primitive root modulo p, 734 priority for small packets, WFQ, 564 priority queuing, 471, 555 priority queuing and AF, 621 privacy, 93 privacy extensions, SLAAC, 269 private IPv4 address, 206 private IPv6 address, 258 PRNG, 727 proactive mode, HWMP, 304 probe request, Wi-Fi, 110 probing, IPv6 networks, 256 promiscuous mode, 50 propagation delay, 161 proportional fairness, 494 protection against wrapped segments, TCP, 423 protocol graph, 40 protocol-independent multicast, 610 provider, BGP, 348 provider-based routing, 323 proxy ARP, 243 pseudorandom number generator, 727 PSH, 395 PSK, WPA2, 114 PTK, 114 public DNS server, 231 public-key encryption, 738 public-key infrastructure, 743 pulse stuffing, TDM, 155 push, TCP, 395 pwnat, 222 python tracefile script, 838‚Äì840, 863 python, reading ns-2 tracefiles, 836 Q QNAME minimization, DNS, 238 QoS, 603 QoS and routing, 291 quagga command-line interface, 798 quagga router, 794 quality of service, 17, 26, 291, 603 quantum algorithm, fair queuing, 568 Query Identifier, ICMP, 248query name minimization, DNS, 238 query, ICMP, 248 queue capacity, typical, 463, 501 queue overflow, 26 queue size, emulating, 800 queue utilization, token bucket, 592 queue-competition rule, 473 queuing delay, 161 queuing discipline, 555 queuing theory, 471 queuing, priority, 555 QUIC, 362, 435 quiet time on startup, TCP, 424 R race condition, Pox, 819 RADIUS, 117 radvd, 263, 284 random drop queuing, 471 Random Early Detection, 504 randomization of MAC addresses, 110 ranging intervals, WiMAX, 127 ranging, WiMAX and LTE, 126 rate control, Wi-Fi, 104 rate scaling, Wi-Fi, 104 rate-adaptive traffic, 605 RC4, 728 real-time fair queuing, 556 Real-Time Protocol, 498 real-time traffic, 603, 605 Real-time Transport Protocol, 33 reassembly, 26 reassociation, Wi-Fi, 111 reboot and RPC, 386 reboots, 380 Record Route, IP option, 204 recursive DNS lookup, 234 RED, 504 Reed-Solomon codes, 158 regional registry, 29 registry, regional, 29 reliable, 393 reliable flooding, 306 Remote Procedure Call, 383 rendezvous point, multicast, 610 Reno, 40, 445 Reno vs Vegas, 866 repeater, Ethernet, 49 Index 947
An Introduction to Computer Networks, Release 2.0.11 repeaters, Wi-Fi, 112 request for comment, 38 request-to-send, Wi-Fi, 102 request/reply, 33, 383, 393 reservations, 26 reset, TCP, 395 resolver, DNS, 231 resource record, DNS, 230 RESV packet, RSVP, 613 retransmit-on-duplicate, 181 retransmit-on-timeout, 179 return-to-libc attack, 704 reverse DNS, 239 reverse-path filtering, 311 RFC, 38 RFC 1034, 229, 238, 239 RFC 1035, 233 RFC 1065, 636 RFC 1066, 652 RFC 1122, 17, 202, 206, 244, 361, 366, 394, 400, 402, 413, 421, 426, 429‚Äì431, 442 RFC 1123, 381 RFC 1142, 306 RFC 1155, 636, 638‚Äì641, 644 RFC 1213, 636, 639, 652‚Äì654, 656, 658, 659, 678, 680, 681 RFC 1271, 681, 683 RFC 1286, 676 RFC 1312, 677 RFC 1321, 720 RFC 1323, 423 RFC 1350, 374, 376, 377, 381 RFC 1354, 677, 678 RFC 1441, 636 RFC 1442, 636, 669 RFC 1450, 674 RFC 1518, 320 RFC 1519, 320 RFC 1550, 253 RFC 1644, 424 RFC 1650, 675 RFC 1661, 135 RFC 1700, 15, 372 RFC 1710, 253 RFC 1812, 224 RFC 1831, 385 RFC 1832, 386 RFC 1854, 172RFC 1883, 253 RFC 1884, 253 RFC 1901, 637, 669 RFC 1909, 669 RFC 1948, 422 RFC 1981, 426 RFC 1994, 723 RFC 2001, 455 RFC 2003, 225 RFC 2011, 677 RFC 2026, 40 RFC 2096, 678, 679 RFC 2104, 692, 721 RFC 2119, 38 RFC 2131, 246 RFC 2136, 269 RFC 2233, 658 RFC 2264, 637 RFC 2308, 233 RFC 2309, 505 RFC 2324, 209 RFC 2328, 306 RFC 2362, 610 RFC 2386, 291 RFC 2409, 734 RFC 2414, 453 RFC 2433, 723 RFC 2453, 292, 298, 791 RFC 2460, 168, 253, 254, 260, 262, 394 RFC 2461, 262 RFC 2464, 259 RFC 2473, 260 RFC 2474, 203 RFC 2475, 617 RFC 2481, 203, 503 RFC 2529, 283 RFC 2570, 689 RFC 2574, 689 RFC 2578, 636, 644, 662, 669 RFC 2579, 687 RFC 2581, 426, 451 RFC 2582, 458 RFC 2597, 618, 620, 621 RFC 2616, 408 RFC 2622, 355 RFC 2650, 355 RFC 2675, 260 RFC 2759, 723 948 Index
An Introduction to Computer Networks, Release 2.0.11 RFC 2766, 286 RFC 2780, 260 RFC 2786, 693 RFC 2819, 681, 682 RFC 2827, 204, 423 RFC 2851, 680 RFC 2856, 669 RFC 2863, 654‚Äì657, 675, 679 RFC 2865, 117 RFC 2898, 722 RFC 2925, 688 RFC 3022, 219, 249 RFC 3042, 457 RFC 3056, 283 RFC 3092, 397 RFC 3168, 503 RFC 3207, 754 RFC 3246, 619 RFC 3261, 220, 627 RFC 3360, 399, 503 RFC 3410, 689 RFC 3411, 691 RFC 3414, 637, 689, 690, 692, 695 RFC 3415, 662, 689, 694 RFC 3418, 674 RFC 3448, 498 RFC 3465, 450, 885 RFC 3513, 255 RFC 3514, 209 RFC 3519, 226 RFC 3526, 734 RFC 3540, 503 RFC 3550, 625, 627 RFC 3551, 625, 626 RFC 3561, 302, 304 RFC 3635, 675 RFC 3649, 506, 519‚Äì521 RFC 3715, 766 RFC 3748, 117 RFC 3756, 265 RFC 3757, 770 RFC 3775, 264 RFC 3826, 690 RFC 3833, 241 RFC 3879, 258 RFC 3927, 257 RFC 3947, 766 RFC 3948, 766RFC 3971, 265 RFC 3972, 266 RFC 4001, 680 RFC 4007, 255, 281 RFC 4022, 680 RFC 4033, 766 RFC 4034, 766 RFC 4035, 766 RFC 4080, 622 RFC 4188, 676 RFC 4193, 258 RFC 4213, 283 RFC 4251, 745 RFC 4252, 745 RFC 4253, 745, 747 RFC 4271, 334 RFC 4273, 653 RFC 4291, 253, 255, 256, 258 RFC 4292, 678, 679 RFC 4293, 677 RFC 4294, 263 RFC 4301, 764 RFC 4303, 262, 764 RFC 4340, 363 RFC 4341, 499 RFC 4342, 499 RFC 4344, 745 RFC 4380, 205 RFC 4429, 267 RFC 4443, 273 RFC 4451, 343 RFC 4472, 277 RFC 4502, 681 RFC 4524, 637 RFC 4560, 688 RFC 4566, 625 RFC 4620, 275 RFC 4648, 770 RFC 4681, 471 RFC 4861, 257, 262, 265 RFC 4862, 267, 268 RFC 4919, 98 RFC 4941, 269 RFC 4953, 399 RFC 4961, 624 RFC 4966, 286 RFC 5065, 348 RFC 5095, 261 Index 949
An Introduction to Computer Networks, Release 2.0.11 RFC 5116, 440 RFC 5227, 243 RFC 5246, 747 RFC 5247, 117 RFC 5280, 637 RFC 5321, 636, 754 RFC 5348, 498 RFC 5508, 249 RFC 5681, 455, 457 RFC 5694, 34 RFC 5702, 770 RFC 5802, 723 RFC 5925, 423 RFC 5944, 225 RFC 5952, 255 RFC 5961, 399 RFC 5974, 622 RFC 6040, 136 RFC 6052, 287 RFC 6056, 407 RFC 6057, 622 RFC 6066, 753 RFC 6093, 400 RFC 6105, 266 RFC 6106, 268 RFC 6125, 748 RFC 6146, 286 RFC 6147, 286 RFC 6151, 721 RFC 6164, 275 RFC 6182, 431 RFC 6275, 260, 261 RFC 6282, 98 RFC 6298, 430, 536, 607 RFC 6325, 78 RFC 6356, 431 RFC 6394, 773 RFC 6472, 336 RFC 6553, 260 RFC 6554, 261 RFC 6564, 260, 262 RFC 6582, 458 RFC 6598, 207 RFC 6633, 249, 503 RFC 6698, 772 RFC 6724, 268, 277, 278 RFC 6742, 277 RFC 6824, 431RFC 6840, 357 RFC 6887, 221 RFC 6891, 766 RFC 6918, 249 RFC 6928, 453 RFC 6960, 751 RFC 7045, 262 RFC 7208, 240 RFC 7217, 256, 257, 267‚Äì269 RFC 7296, 136, 765 RFC 7413, 424 RFC 7421, 253 RFC 7469, 751 RFC 7540, 408 RFC 7567, 502, 505 RFC 7568, 747 RFC 761, 38 RFC 7664, 735 RFC 7686, 234 RFC 7707, 256 RFC 7721, 270 RFC 7816, 238 RFC 783, 374, 381 RFC 7844, 110 RFC 7858, 232, 775 RFC 7860, 690 RFC 7871, 241, 242 RFC 791, 25, 202, 208 RFC 792, 247 RFC 793, 38, 394, 396, 399, 400, 408, 410, 413, 415 RFC 7934, 270 RFC 7938, 347 RFC 8110, 120 RFC 821, 636 RFC 8257, 534 RFC 8305, 279 RFC 8312, 538 RFC 8446, 756 RFC 8484, 232, 775 RFC 854, 400 RFC 896, 427 RFC 8999, 436 RFC 9000, 436 RFC 9001, 436 RFC 9002, 436 RFC 9114, 436 RFC 917, 213 950 Index
An Introduction to Computer Networks, Release 2.0.11 RFC 9293, 394 RFC 950, 213, 214 RFC 970, 556 RFC 988, 25 RIB, 339 Rinjdael, 727 RIPE, 29 RMON, 681 ROA, RPKI, 357 roaming, Wi-Fi, 111 root nameserver, 231 round-robin DNS, 234 round-trip time, 163 route, 37 route aggregation, IP, 336 route leak, 354 router, 18 Router Advertisement, IPv6, 262 Router Discovery, IPv6, 262 Router Solicitation, IPv6, 262 routerless IPv6 examples, 280 routing and addressing, 201 routing domain, 291, 319 routing header, IPv6, 260 Routing Information Base, 339 routing loop, 19, 296 routing loop, ephemeral, 306 routing policies, BGP, 337 routing policy database, 310 Routing Policy Specification Language, 355 routing registries, BGP, 354 routing update algorithms, 289 routing, IP, 27 routing, MANET, 124 RPC, 383 rpcgen, 386 RPKI and BGP, 357 RPSL, 355 RR, DNS, 230 RRset, DNS, 240 RRSIG, DNS, 766 RSA, 739 RSA factoring challenge, 741 RST, 395 RSVP, 607, 612 RTCP, 626 RTCP measurement of, 626RTO, TCP, 429 RTO, TCP minimum, 535 RTP, 33, 498 RTP and VoIP, 626 RTP mixer, 624 RTS, Wi-Fi, 102 RTT, 163 RTT bias in TCP Reno, 484 RTT inflation, 190 RTT-noLoad, 164 S SACK TCP, 460 SACK TCP in ns-2, 862 SAE, 735 Salsa20, 726 salt, password, 722 satellite Internet, 130 satellite-link TCP problem, 508 sawtooth, TCP, 446, 450, 507, 831, 858, 868 scalable routing, 201 scanning, IPv6 networks, 256 scope, IPv6 address, 255 scope, IPv6 link-local, 256 SCRAM authentication, 723 scrypt, 722 SCTP, 432 SDN, 80 search warrant, 744 secure DNS, 766 secure hash functions, 719 secure neighbor discovery, 265 secure shell, 745 security, 700 security association, IPsec, 764 segment, 14 segmentation, 26 segments, TCP, 395 select group, 820 select() call, 794 Selective ACKs, TCP, 460 selective export property, 351 selector, IPsec, 764 self-ARP, 243 self-clocking, 184 SEND, 265 sequence number, TCP, 395 serial execution, RPC, 386 Index 951
An Introduction to Computer Networks, Release 2.0.11 server, 31 server pod, Facebook, 347 ServerHello, TLS, 753 ServerName extension, TLS, 753 session key, 724 session layer, 39 sfq, 569 SHA-1, 720 SHA-2, 720 Shannon-Hartley theorem, 94 shaping, 585 shared IPv4 address block, 206 shared-key ciphers, 724 shared-memory switch, 76 shellcode, 707 shortest-path bridging, 78 Shortest-Path First algorithm, 307 SHOULD, 38 shutdown, TCP, 411, 412 sibling family, BGP, 350 sibling, BGP, 348 SIFS, Wi-Fi, 101 signaling losses, 504 signatures, intrusion, 717 signatures, RSA, 740 silly window syndrome, TCP, 428 SIMO, 106 Simple Network Management Protocol, 634 SimpleHTTPServer, 789 simplex-talk, TCP, 402 simplex-talk, UDP, 364 simultaneous authentication of equals, 735 simultaneous open, TCP, 410 single link-state, 29 single-responsibility principle, 364 singlebell network topology, 475 site resolver, DNS, 231 site-local IPv6 address, 258 size, packet, 165 SLAAC, 266, 268 SLAAC privacy extensions, 269 sliding windows, 31, 184 sliding windows, TCP, 426 slot time, Wi-Fi, 101 slow convergence, 296 small-packet priority, WFQ, 564SMI, 641 SMTP, 172, 754 SNI, TLS, 753 SNMP, 634, 679 SNMP agent configuration, 660 SNMP agents and managers, 635 SNMP alternatives, 665 SNMP enumerated type, 656 SNMP versions, 636 SNMPv1 data types, 640 SNMPv3 engines, 690 Snorri Sturluson, 119 SO_LINGER, TCP, 413 socket, 31 socket address, 31 socketpair, 393 soft fail, OCSP, 752 soft state, 607 software-defined networking, 80 SONET, 155 Sorcerer‚Äôs Apprentice bug, 182 Source Quench, 249, 503 source-specific multicast tree, 611 spanning-tree algorithm, 69 sparse-mode multicast, 610 spatial streams, MIMO, 107 SPB, 78 speex, 605 SPF algorithm, 307 spine plane, Facebook, 84, 347 split horizon, 297 spoofing, IP, 203 spoofing, TCP, 399 SQL injection, 716 squatting, IP prefix, 354 SSH, 244 ssh, 744, 745 ssh host key, 746 ssh, mininet, 788 SSID, Wi-Fi, 107, 111 ssl, 744 SSL programming, 757 SSRC, 625 stack canary, 711 stalk, TCP, 402 stalk, UDP, 364 star topology, 49 STARTTLS, 754 952 Index
An Introduction to Computer Networks, Release 2.0.11 state diagram, TCP, 408 stateless autoconfiguration, 266 stateless forwarding, 17 STM-1, 156 stochastic fair queuing, 569 stop-and-wait transport, 179 stop-and-wait, TFTP, 380 store-and-forward, 21 store-and-forward delay, 161 StoredKey, 723 stream ciphers, 727 Stream Control Transmission Protocol, 432 stream-oriented, 393 streaming telemetry, 665 streaming video, 33, 606 streams, QUIC, 437 STS-1, 155 STS-3, 156 stub resolver, 233 subnet mask, 214 subnets, 213 subnets, IPv6, 258, 275 subnets, vs switching, 218 subpoena, 744 subqueue, 555 subscription, multicast, 610 Sun RPC, 385 superfish, 750 supplicant, WPA, 117 switch, 18 switch fabrics, 76 switch, crossbar, 76 switch, shared-memory, 76 switching, vs subnets, 218 switchline.py, 786 Switzerland, 236 symbol, data, 61, 151 symmetric ciphers, 724 SYN, 395 SYN flooding, 396 SYN packet, 396 synchronization source, RTP, 625 synchronized loss hypothesis, TCP, 485 synchronized loss, TCP, 448, 480 synchronized loss, TCP Reno, 484 synchronized states, TCP, 409T T/TCP, 424 T1 line, 154 T3 line, 155 tables, SNMP, 641 Tahoe, 40, 445 tail drop, 471 tangle, cords, 98 tbf, Linux, 808 tc, Linux, 310, 597, 808 TCAM, 213 TCO, TCP, 401 TCP, 13, 31, 392, 417 TCP accelerated open, 424 TCP application close, 412 TCP BBR, 542 TCP checksum offloading, 401 TCP close, 397 TCP Cubic, 537 TCP fairness, 480, 493 TCP Fast Open, 424 TCP Friendliness, 497 TCP Hamilton, 536 TCP header, 394 TCP Hybla, 533 TCP Illinois, 529 TCP minimum RTO, 535 TCP NewReno, 458 TCP NewReno in ns-2, 862 TCP old duplicates, 419 TCP Reno, 445, 454 TCP Reno Limited Transmit, 457 TCP sawtooth, 446, 450, 507, 831, 858, 868 TCP segmentation offloading, 401 TCP state diagram, 408 TCP Tahoe, 445 TCP timeout interval, 607 TCP Vegas, 570, 866 TCP Westwood, 527 TCP Westwood+, 528 TCP, Highspeed, 519 TCP, SACK, 460 TCP_NODELAY, 428 TCP_QUICKACK, 427 TDM, 153 Teredo tunneling, 205 termshark, 37 Index 953
An Introduction to Computer Networks, Release 2.0.11 terrestrial broadband, 128 terrestrial wireless, 128 TestAndIncr, 672 TEXTUAL-CONVENTION, SNMP, 669 TFO (TCP Fast Open ), 424 TFTP, 373 thepiratebay, 231 thermonuclear, 38 three-way handshake, 396 three-way handshake, TCP, 421 threshold slow start, 450 throughput, 13 tier-1 provider, 288, 350 Time to Live, 203 time-division multiplexing, 153 time-to-live, DNS, 232 timeout and retransmission, 31 timeout interval, TCP, 429, 607 Timestamp, IP option, 204 TIMEWAIT connection reuse, 421 TIMEWAIT, TCP, 420 TJX attack, 93, 702 tls, 744 TLS client example, 761 TLS connection setup, 752 TLS handshake protocol, 752 TLS programming, 757 TLS server example, 759 TLS version 1.3, 755 token bucket, 583 token bucket queue utilization, 592 token bus Ethernet, 138 token ring, 137 token-bucket applications, 590 token-bucket, RSVP, 613 topology, 18 topology table, EIGRP, 305 Tor project, 234 ToS and routing, 291 TP4, 39 trace file, ns-2, 832 tracefiles, ns-2, reading with python, 836 traceroute, 36, 250 tracking, Wi-Fi, 110 trading, 161 traffic amplification, QUIC, 440 traffic amplification, UDP, 362traffic anomalies, 717 traffic engineering, 19, 310, 344, 603 traffic management, 554 tragedy of the commons, 443 Trango, 130 transient queue peak, 860 transit capacity, 185 transit traffic, 337, 344 Transmission Control Protocol, 31 transmission, Ethernet, 54 Transport layer, 31 transport mode, IPsec, 764 traps, SNMP, 636 tree, 18 trie, forwarding table, 212 triggered updates, 297 TRILL, 78 triple DES, 726 Trivial File Transport Protocol, 373 TRR, DNS, 775 trust anchor, 766 trust anchors, 265 trust and public keys, 743 trust on first use, SSH, 746 trust on first use, TLS, 750 trust, DANE, 773 trusted recursive resolver, 775 tshark, 37 TSO, TCP, 401 Tspec, 613 TTL, 203 tunnel mode, IPsec, 764 tunnel, IPv6, 282 tunneling, 135, 221 two implementations, 40 two-generals problem, 378 twos-complement, 168 Type of Service, 203 U u32, 811 UDP, 32, 361, 369, 371 UDP advisory, 362 UDP, for real-time traffic, 607 unbounded slow start, 450 unicast, 24 unique-local IPv6 address, 258 unknown destinations, Ethernet, 62 954 Index
An Introduction to Computer Networks, Release 2.0.11 unlicensed spectrum, 99 unnumbered IP interface, 223 upgrades, network, 21 uplink scheduling, WiMAX and LTE, 126 URG, 395 User Datagram Protocol, 32 usmUserTable, 694 utilities, network, 35 V VACM, 662 VarBind list, 646 VCI, 139 video, streaming, 606 videoconferencing and CDNs, 615 virtual circuit, 14, 26, 138 virtual hosting, 237 Virtual LANs, 74 virtual link, 135 virtual private network, 135 virtual tributary, 157 VLANs, 74 voice over IP, 26 void, cast into, 774 VoIP, 26 VoIP and RTP, 626 VoIP bandwidth guarantees, 591 voting, 38 VPN, 135 VPNs and ECN, 136 W W^X, 712 war driving, 256 wavelength-division multiplexing, 158 web of trust, 743 web server, 789 weighted fair queuing, 557 WEP encryption failure, 702 WEP, Wi-Fi, 114 Westwood, TCP, 527 Wi-Fi, 98 Wi-Fi EasyMesh, 112 Wi-Fi extender, 112 Wi-Fi fragmentation, 104 Wi-Fi polling mode, 121Wi-Fi repeaters, 112 Wi-Fi security, 93 Wi-Fi, HWMP mesh, 304 WiMAX, 124 window, 184 window scale option, TCP, 426 window size, 31, 184 Windows, 204, 205 Windows XP SP1 vulnerability, 714 winsize, 184 wireless, 98 wireless LANs, 93 wireless, fixed, 128 wireless, satellite, 130 wireless, terrestrial, 128 WireShark, 279, 437, 883 wireshark, 37 WireShark, TCP example, 400 work-conserving queuing, 556 WPA authenticator, 117 WPA supplicant, 117 WPA, Wi-Fi, 114 WPA-Enterprise, 117 WPA-Personal, 114 WPA2, 729 WPA2-Enterprise, configuring, 118 WPA3, 120, 735 write-or-execute, 712 wtf, 230 X X3DH encryption, 742 XD page bit, 711 XDR, 386 xkcd, 328, 717 XML, 373 XSS, 716 Z ZigBee, 98 zone breaks, DNS, 238 zone identifier, IPv6, 281 zones, DNS, 230 ZSK, DNS, 767 Index 955