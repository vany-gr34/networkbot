mot_cl√©,post_et_reponses
ICMP latency monitoring,"hello all need some guidance here. i currently manage a smallmedium enterprise network with nexus 3k, nexus 2348 and nexus 9k switches in the datacenter. theres some intermittent slowness observed with some legacy applications and i need to identify whats causing it. we use solarwinds to monitor the infrastructure and nothing jumps out to me as the culprit. no oversubscription, no bottlenecks, no interface errors on the hosts where the application or database server is hosted. tried to show packet captures to prove that theres no network latency but nobody listens. is there any tool out there that can help really dissect this issue and point us in the right direction? at this point, i just need the problem to get resolved. thanks. || nexusshow interface counters errors the column all the way to the right is outdiscards. pay very close attention to that column. hit the space bar a bunch of times until you see indiscards. pay very close attention to that column, just to be thorough. solarwinds isnt precise enough to tell you if congestion is occurring. if eth11 is a 10gbe interface and eth12 is a 10gbe interface, and they both are receiving a 6gbps stream of traffic destined to a device on eth13, which is also a 10gbe port then you have 12gbps of traffic trying to fit into a 10gbps interface. this is congestion in a lan switch. since not all the traffic can fit, some of it must be buffered and sent when time allows. no switch has unlimited buffer memory. when buffer exhaustion occurs, and a packet must be dropped it will show up as an outdiscard. nexusshow interface flowcontrol flowcontrol is dumb. in my opinion, flow control should be disabled on every switch interface unless the device connected to that interface specifically says flow control is a bestpractice in its implementation guide. flowcontrol is a primitive form of early congestion control. when enabled on both ends, if either device estimates that it is about to run out of buffer memory capacity it can fire a pause frame at the connected device and demand that that device stop sending any traffic for some number of microseconds. from your switchs perspective, an rxpause is a pause frame received from the device connected on that switchport. a server is ashing this switch to hold up for a second. from your switchs perspective an txpause is a pause frame sent from this switch to the connected device asking that device to hold up for a second. flowcontrol doesnt care about qos prioritization. flowcontrol doesnt understand that some packets are more important than others. this is because flowcontrol is dumb. if your switch and the connected server have both negotiated flowcontrol to be on and you are not seeing any pause requests then neither device is crying for help to manage congestion. this suggests no congestion in the network is occurring. if your switch has flowcontrol disabled but you are receiving assloads of pause requests from the connected device, that device is the problem. he cant handle all the traffic you are sending him. send less traffic, or tune optimize that device so he can handle traffic better. here is the story you are trying to establish and support using data. the nexus 93180 switch only has 40mbytes of packet buffer memory in the whole box. that is the sum total of all possible storage in the switch for application traffic. solarwinds can help you depict how much total traffic is flowing through the switch at any given time. 40mb of storage is a very slim fraction of one second before it runs out of buffer capacity and starts dropping packets. if you arent dropping packets then the packets must be entering and exiting the switch really damned fast, if they werent youd fill the buffer and start dropping. a solarwinds graph might not be granular enough to show that interface utilization hit 135 utilization for eight seconds, but it is granular enough to show that you dropped 800 packets in the past 5 minutes on the switch port the server is connected to. if you arent dropping packets then you delivered them in a timely manner. if the network delivered the sql query request to the sql server in a tiny fraction of one second, and then you had to wait 37 seconds to receive the database response the problem isnt the network, the problem is inside the sql server. the usual suspects inside a database server are inefficient query bad programming cpu too busy inefficient query bad programming not enough ram inefficient query bad programming disk response time too slow inefficient query bad programming record locking multiple db operations are fighting over the exact same data at the same time inefficient query bad programming in case i forgot to mention it, more often than any other rootcause for a database performance problem is the developer is hitting the sql server with an inefficient database query. now, to answer your other question is there a product that can solve this? yes, but its expensive as fuck. what youre asking about is an application performance monitoring tool. the products listed in the topright quadrant are considered by gartner to be the bestofbreed products. if you engage cisco to watch a demo of appdynamics, or engage the dynatrace people for a demo of their product your whole department should start foaming at the mouth over how fantastically useful the data is. they can tell you exactly why your application is so slow. right down to the query string that is causing the problem, and can suggest a way to write a new string that might work better. this is gonna cost you an arm, a leg and somebodys kidney. but thats not your problem. let them make their sales pitch and let the big boss say no. you will have done your job bringing in a toptier solution to the problem. || sounds like the appdatabase teams need to do more digging instead of blaming the network || i recommend taking 2 concurrent packet captures, then analyze in wireshark 1 capture at the sourceclient and 2 capture at the destination serverapplication || smarter replies before mine, but this happened to us when the asic was saturated. might have 10g interfaces but commutative traffic on all interfaces causes traffic to buffer as the asic is saturated. check switch model capacity."
ICMP latency monitoring,"hi everyone. i have a reasoning problem with our server guys. since a few weeks our vdi guys had some ica latency issues and some slow vdi sessions. and as always, the network is to blame. weve been troubleshooting for weeks and no one knows what exactly to look for. no one can tell us either. the only thing our colleagues are arguing about is that we sometimes have 56 pings 3ms out of 100 pings. this discussion we are having is not really useful in my opinion. ive been doing this for quite a while and have seen this behavior on several networks, but have never considered it a problem or an indication of any problem. but now im starting to doubt myself and need an assessment. avg. ping latency is actually always 1ms. would you say if i ping a baremetal windows lets say a domain controller host with a network client that occasional ping latencies 3ms are a problem? all this in the internal network. is this a normal picture in an internal routed network as well as nonrouted network? sorry... i feel stupid to ask that... || completely normal. anyone worried about some 3ms pings is looking in the wrong area. vdi sessions for remote users are commonly much higher than that with fine performance so how does he think 3ms ping is an issue? || i seriously doubt occasional 3ms pings are your problem. i spent a decade troubleshooting application performance issues for an enterprise isp and if your application cant handle a couple of extra ms then something is wrong with your application. you need to get some packet captures from both ends and see what is actually happening at the network level whenever the performance issues occur. if nothing is happening there, the server guys need to look at the servers. || only solution is to use wireshark to capture the packets at the moment the issue is there. but yeah, the issue will most probably be random... do you have any management environment available where you can stream network telemetry data to? so you can go back in time and see the real throughput at the moment the issue occured no tap aggregation environment? i think this is the moment to talk about network observability to your management. looking for weeks to find an issue is costly and the users will not be happy. || vdi is 100 okay over even 60ms latency. the vdi solution is not optimized, or it is and youre blocking traffic with a firewall. vdi should leverage modern protocols that can use udp for performance firewalls can block this, causing a failback to tcp. things to check if the above is not the case what is slow? file system access? if the files are large and not local to the vdi infra, use sharepoint. internet browsingeverything? is dns misconfigured? like a configured dns server that doesnt exist causing timeouts for every request that isnt cached? blocky video in video conferencing? look into teams optimization, zoom optimization, etc... every conferencing app has specific requirements for good performance on vdi local hardware offloading. if its horizon, are they keeping up with modern vmware tools keeping the golden image optimized following best practices? hint installing everything directly on one companywide golden image is retarded || you could do packet capture on both client and server ends, and check how long does it take for server to respond after it gets request. || 3ms lol || first of all ping doesnt mean a lot, especially if you are pinging two or from a routerswitch the icmp echo request usually goes to the control plane, and many devices have underpowered cpus. its totally normal for an occasional ping to be a lot higher than the others, especially on busy servers. if ping is the only evidence they have for you, laugh them out of the room while taking their problem seriously. second there are many kinds of networking issues you can have. packet loss, packet delay, jitter and all of this can occur in micro bursts, for multiple seconds, or continually. diagnosing this stuff can be hard, and i reckon it happens more than people think. you need to gather more information on what issues are occurring. the network is slow is absolutely useless, they need to do the groundwork to find out this issue so you have a chance in hell to troubleshoot it. before your guys start blaming the network they need to look at their system resources. also if youre doing vdi over the internet, then almost all bets are off. start with a reproducible test, doing x causes issue, without this its almost impossible to debug. once you can reproduce the issue take a packet capture from the source and destination servers. then use wireshark on those packet captures to look for dropped packets or packet delay, wireshark has lots of tools for this. || it could be an indicator for very short congestions in your network or on the server links, which usually is not visible in bandwidth graphs. congestion often comes along with packet loss and jitter and this can make clients or both ends adjusting a jitter buffer to too high values, which then feels like a slow vdi session all depends on software and configurations. so, yes, these higher latencies itself are not the problem itself but the symptoms of a problem. find out what pathes in your network are most affected by the problem. if all pathes are equally affected, the links to the servers might be saturated or if all connected to the same switch the switchs uplink might be. to really check the bandwidths in detail, you can mirror the port of an uplink or server, capture with wireshark and generate io graphs to really see spikes in bandwidth and what might cause it. || should be fine. 3ms jitter is nothing for icmp. id be curious to know the jitter and loss measurements for the actual ica sessions though. if your icmp jitter is due to an underlying network issue like enabling ethernet congestion control then theres room for improving things. but also, i am curious to know what the actual symptoms are. slow vdi is very rarely related to networking. some oddball things where you out of order packets can cause tcp congestion collapse, for example. || what 3ms means ? 1000ms ? 10ms ? give at least max, min, iqr, variance or sd of the series. changed or introduced something recently? difficult to blame something for latency if is 10 ms on local net ... look elsewhere imho.. || if you can, have a test device as close to the complaint section of the network as possible and run a constant ipslatest traffic then you can show it sidebyside the complaint to compare. || 2 ms to a windows host is nothing, most nic drivers place icmp traffic on the bottom of priorities to process. heck, cpu load on the host could cause that too. its a red herring. now 60ms is a different story. || to bring some peace to your mind, look at setting up a smokeping server to monitor various aspects of your network. this will give you ammo to understand what is going on. that said, worrying about the occasional ping spike is like someone pixelpeeping a dslr and complaining about quality. icmp typically isnt treated with priority and some occasional latency is expected. with a smokeping server you can look for trends as you configure smokepings cadenence and it can provide historical graphs, jitter, etc. who knows, maybe there is an unrelated issues where you can correlate those ping spikes to large file transfers or something, but, on the whole, i wouldnt worry about it. i would really direct folks to the infrastructure hosting the citrix environment to look for bottlenecks there if they are experiencing issues. || check and see if the clientserver are using udp for transport and if so disabling udp so it can only use tcp. this solved our issue with vdi sessions freezing. || if it is on wireless that would cause the 13100 bad pings."
ICMP latency monitoring,"im running a network with several fs switches using rapid stp, aggregation, etc. and a few synology nas units on a dedicated storage network. my switches are in a single vlansubnet, and overall connectivity between switches is solid. however, my synology units are acting up two of them appear intermittently, and one never shows up. what ive done so far switch configuration stp all fs switches are in rapid stp mode with all ports including agg interfaces showing as designated and forwarding. aggregation is set up on 100g ports broken out into 25g lanes, but the nas uses only one 25g lane. the switch reports 50gbs on the aggregated port. mac arp monitoring the core switchs mac table is populated on vlan 1, yet the problematic nass mac is either intermittent or missing. ive verified that the physical links cables, sfp28 modules appear solid, and the link on both the switch and nas side is autonegotiating to 25g. cpu traffic statistics cpu traffic statistics both transmit and receive are low and dont indicate overload. ive attempted builtin packet capture via the cli using monitor cpu capture packet startstop and transferred the pcap file for analysis in wireshark. questions for the community 1. aggregation breakout has anyone encountered issues with a synology nas connected via a single 25g lane from a 100g breakout? do i need to force any specific settings on the nas or switch to ensure stability when only one lane is used? 2. intermittent mac learning what could cause a nass mac to intermittently drop from the switchs mac table even though the physical link is stable? could arp or a misconfiguration e.g., vlan tagging inconsistencies be at fault? 3. packet loop or latency im also seeing traceroute anomalies e.g., 3k ms latency, incomplete traceroute suggesting packets may be looping within the switch. could this be related to the aggregation or perhaps misbehaving multicastigmp traffic? any tips on using the builtin packet capture effectively on fs switches to isolate these issues? any insights or similar experiences would be greatly appreciated. thanks in advance for your help! im using fs switches running fsos v7.4.8 and synology nas units with 25g sfp28 uplinks. || when you say aggregation, are you referring to multi chassis lag? if you have multiple core switches running mclag, you likely dont need rstp and you could be seeing intermittent blocking of a port, which wouldnt be necessary in an mclag scenario. some of these questions would probably be easier to address with a quick draw up of your network layout if you have one to add. ive connected synology nases to single lanes of qsfp28 breakouts to a pair of mlag downstream switches without issue in the past, so you should be able to get this working reliably. bit of a dumb question, but you did configure the port breakout on the switch side, right? if its addressing the entire block of 4 as one port, youll probably get some bizarre behavior when connecting to multiple downstream devices."
ICMP latency monitoring,"i chose troubleshooting for the flair, because that is how this came up, but this is really more of a current state of the technology. let me give you the background on this, so, i am not a network engineer or administrator, i am a technical support engineer, who supports payment processing systems and mostly atms for retail banks and credit unions in the us. i work for one of the big fintech service providers that you have never heard of, unless you have worked for a bank. frequently i work cases where an atm is offline or not connected, sometimes it is a local issue with the atm, sometimes its because the bank or their msp makes a change to something and there are unintended consequences, like all of a banks atms being knocked offline. frequently this is due to something along the lines of either bad documentation, the documentation not being read, or the person who designed the change wasnt looking at how the change will affect things at a wide enough scope. i get it, these guys have a lot of work to do, sometimes stuff gets missed, it happens to me too. i am our groups network troubleshooting guy, i get asked to review packet captures, or help clients or their msps identify the source of the breakdown in communications. since i dont usually have to configure any network devices, i dont keep up on the current level of what is available, which is why i am asking this here. i have a bit of a background in software, and one concept in software development is regression testing, which is testing existing functions of a program to make sure new updates or changes didnt break them inadvertently. my question is, are there any current solutions, commercial or open source, that can do this for network infrastructure? i am thinking of something where i can list critical traffic flows through a device and generate packets or traffic for them to validate those flows are still working after a change is made? i know i could write tests in python and scapy to generate the traffic i want and validate if it was working, and i could containerize it to be deployed on a subnet, but before going into such effort, i want to see if anything like that already exists? google gemini didnt have much, and i know endpoint monitoring is also a possible solution but checking that an endpoint is online with an icmp packet doesnt validate application layer connectivity, and usually application monitoring has timers built in to reduce false positives. id want something that would show a comms issue immediately after a change was rolled in. i appreciate any thoughts or advice you all have regarding this. this wouldnt be a tool that i would use, but ideally it could be used by network engineering teams to validate changes they make. thanks! || pyats designed internally by cisco, and later open sourced || look into cisco pyats || a properly designed nms can do this. you are right that icmp packets dont validate application layer connectivity, but an nms can use more than icmp. the hard part is building the tests, and installing the probes where they are most useful. configure the nms to check your applications every few minutes, get alerts when a probe fails n times. || maybe even an emulator like eveng. they can basically run real network os devices pcs servers, and firewall appliances. you can then capture data and even provide external internet || yeah and arista has anta || you dont want regression testing, you want monitoring. google gemini didnt have much, and i know endpoint monitoring is also a possible solution but checking that an endpoint is online with an icmp packet doesnt validate application layer connectivity, and usually application monitoring has timers built in to reduce false positives. id want something that would show a comms issue immediately after a change was rolled in. any service which isnt written by a complete cartoon should support a healthcheck endpoint. this is the same endpoint which loadbalancers will use to determine whether the service is available to receive traffic. id want something that would show a comms issue immediately after a change was rolled in. monitoring systems have configurable timers, so if you really do want a zerotolerance check, you can configure one. check out icinga2. || while not specifically designed for what you are doing, i am wondering if it might work. juniper mist aps have a digital twin service built into them on code 0.14 and higher. what this does is determine if basic services are available dhcpdnsarp to the gw, you could also add specific applications via ip address. these tests run once an hour by design but you can trigger them to run the tests at any time. you could basically plug the ap power might be a consideration into the port the atm was connected to and let her go."
ICMP latency monitoring,"our nms for realtime alerting and monitoring is castlerock which is just a big ping box with snmp capabilities. essentially a spokes tunnel is pinged via the hub, so if hub to spoke1 stays up but spoke1 to spoke2 goes down, we wont get an alarm. aside from snmp trapsinforms and syslogs, are there any other solutions youve conjured up for this scenario to get real time alerts? edit 2 these are actually statically mapped and bgp peered. we have customers that need to communicate directly to each other over spoke to spoke connections as they are all over the world and the traffic is latency sensitive. this is high dollar data and an unplanned drop can cost them thousands of dollars. niche industry. edit 1 i just thought of a solution. spoke2 can advertise a loop back to spoke1 only which in turn advertises it to the hub for icmp polling. of course the icmp echo reply at spoke2 would take the hub causing asymmetric routing which could give false positives. to get symmetric routing would have to do a pbr local policy on spoke2. other caveat is if spoke1 to hub goes down that will obviously trigger loop back at spoke 2, but that false positives can be overcome with logic andor education. still open to other ideas or criticisms of this idea. || i guess my question would be why would you want an alert when a spoke to spoke tunnel goes down? having dynamicondemand tunnels between spokes is one of the selling points of dmvpn. they should be going updown as needed and i dont want all those alerts spoke to hub tunnels going down? yes, i want to know. spoke to spoke going down? thats working as intended. || i think you answered your own question. snmp traps or syslog and alert based on the syslog message. || i know this may not be cisco, but ipsla seems tailor made to monitor and alert on more sophisticated topologies. i take it funds and additional tooling are probably limiting factors, but ipsla from spoke to spoke to would measure performance really well. || if you want to retain icmp monitoring only, you could move to a two cloud model. this would result in two tunnel interfaces on the spoke, each with only 1 hub. || dmvpn spoketospoke traffic is dynamic in nature. the first couple of packets will go through the hub. after that a temporary connection between the two spikes is created and traffic is sent that way via nhrp assuming a phase iii dmvpn. if spoketospoke traffic isnt possible, its sent from spokehubspoke, thus ensuring traffic will usually get there. good luck monitoring in that scenario. you might be able to do some syslog monitoring to see when a direct connection is made, but that will be difficult to monitor. || honestly id accept your current monitoring solution isnt good enough and move to something that can actually alert these drop. librenms is free and could do this with syslog or snmp. || monitor the bgp sessions should work if they are set up like you say. || you can try to use prtg for covering the ipsla monitoring builtin sensors based on snmp will read the response time and notify you. same tool, custom snmp to poll bgp neighbors table. you can download a free trial and then use the free 100 sensor edition. ps castlerock. havent heard that product name for a long while. it is obsoleteunsupported in the last 4 years at least. || librenms will monitor bgp states out of the box but syslog is probably going to be faster if this is time sensitive. there is also gnmic which is what i just setup on my aristas and its almost instant. || im... a bit lost actually. why do you care if spoke to spoke tunnels go up or down? the entire point of dmvpn is that the tunnels are deleted if unused and instantly rebuilt as required. if i didnt know better, i would say that you dont really trust the dmvpn implementation in the sense that you arent sure if its going peer to peer. thats not a monitoring problem. i dont really get what you are trying to reach here."
ICMP latency monitoring,"is there a solarwinds netpath alternative out there. other than manageengines? this works well for us but i really hate solarwinds these days and we really only have it now for monitoring netpath and latency between locations. || netbeez might be suitable || bloody messed up the title, sorry! || checkmk is nice depending what you want out of it. || smokeping? || ciscos thousand eyes?"
ICMP latency monitoring,"here is my resume. i currently work for a big school district in the usa. i have been applying so many jobs but no calls. why do you think? i am on a visa for your reference but at least i should get an interview but no luck. anyone in the usa hiring a network engineer let me know. roasting and suggestions are welcome. professional summary experienced network engineer with a masters degree in information technology management and over four years of expertise in designing, deploying, and troubleshooting complex network systems. proven skills in configuring and maintaining secure and efficient global networks, with a strong foundation in network protocols, service management, and automation. adept at collaborating with crossfunctional teams to ensure seamless project execution and regulatory compliance. continuously innovating to enhance network resilience and efficiency. technical skills operating systems macos, windows ticketing tools servicenow, jira network management tools solar winds, zabbix, prtg,splunk load balancer f5 load balancer routers switches cisco, juniper firewalls palo alto wireless juniper mist, cisco meraki, net gear education university masters in information technology management gpa 3.6 2024 bachelors in agriculture science gpa 3.82020 certifications certified ccna200301 awsccp i intend to earn this certification by the end of feb 2025. graduate assistant university aug 2022jan 2023 facilitated engaging and interactive sessions to explain complex networking related topics to a class of 24 students, using effective communication skills to improve students performance. managed multiple priorities such as grading and administrative deadlines, provided adequate feedback for assignments and lastminute classroom instruction duties. school district network engineer may 2024present oversaw network services across 38 locations, providing support for 27,000 students and 2,500 staff members. set up and implemented stack wise virtual vss on cisco catalyst 9500 and 9300 switches. tracked network performance through solarwinds, zabbix, and netflow. configured and managed remote access and sitetosite vpns using panorama for palo alto devices. ensured compliance with cipa by overseeing lightspeed relay for 26,000 chromebooks. revamped subnetting and vlans, transitioning the districts ip range to thereby reducing costs by deploying windows server 2022 for dhcp instead of infoblox. configured and deployed 2300 juniper mist ap45 access points across the school district by creating wlan templates in such a way based on the preshared key entered for the specific ssid the endpoint is placed in the vlan associated and increasing the segmentation. upgraded nxos on nexus 7010 switches utilizing an inservice software upgrade issu. implemented cisco identity services engine ise for unified access control and policy management. streamlined network configurations through the automation of ansible playbooks for improved efficiency and consistency. transferred igp from eigrp to ospf and set up a tacacs server for aaa services. expertise in security nat, pat, threat prevention, url filtering, and risk analysis for palo alto firewalls. experience in tcpip protocol suite ip, arp, icmp, tcp, udp, snmp, ftp, tftp, dns, dhcp, ssh, smtp, ntp, hsrp. experienced with radius and tacacs, ldap, active directory server integrations for mfaduo. retail limited associate network engineer mar 2020july 2022 provided network support for 30 retail sites, troubleshooting and resolving connectivity issues. configured and managed vlans, intervlan routing, and lan protocols. administered cisco, juniper routers and switches, including ios upgrades via tftp. implemented acls to secure network access and configured remote access on cisco routers. monitored and configured bgp, ospf, eigrp, and rip protocols for seamless network operations. oversaw deployment and troubleshooting of l2l3 technologies such as stp, trunking, and etherchannel. key projects ip migration project redesigned and implemented a new subnetting and vlan structure for a school district, including migrating the ip range from 172.16.0.016 to 10.0.0.08, leading to improved scalability and cost savings. firewall security enhancement configured advanced security features on palo alto firewalls, including nat, ips, ids and threat prevention, url filtering. network automation utilized ansible to streamline the deployment of configurations across multiple network devices, significantly reducing manual effort and errors. wan optimization replaced legacy cisco wan routers with aruba silver peak sdwan, enhancing network performance and reliability. wireless project configured and deployed 2300 juniper mist ap45 access points across the school district and increasing the speed of internet connectivity. || put your most valuable skills first, not windows i put my most recent job first and then work bacl but thats personal preference || ritcareerquestions || i stopped reading at the operational systems. if you are a network engineer, id expect ios, nxos, eos, maybe even linux if you ran bird or something, but not macos and windows... i lied, i didnt stop reading. i read a little more. migrating the ip range from 172.16.0.016 to 10.0.0.08, leading to improved scalability and cost savings. i really want to hear how migrating from one rfc 1918 space to another results in cost savings. || you removed infoblox in all honesty though, drop the agricultural science degree from the top of the resume. i almost stopped reading there. drop the teaching associate stuff too. focus on what youve achieved and why, revamping an ip schema isnt that big of a deal, but reducing operational overheads like trainingmanagement by moving to windows from infoblox is an easier sell. if it saves time and money because junior techs already know win server and cant understand ipam without hand holding, then highlight that as a benefit. feel free to fire over a doc in pm and ill give you some pointers if you like. most importantly, remember that nontechnical hrmanagement are likely reading these cvs first and foremost so tailor it to them. || lots of experience! i would def rearrange and put your experience first and education last. as a technical person that has been on both sides of technical interviews, i dont give two shits about your education, and most other technical folks dont either. its a great accomplishment, but it doesnt really give us a lot of perspective on how you work through problems. i see your skills list and immediately think of questions to ask to determine if its surface level knowledge or you really understand it like can you tell me about your migration plan for moving from eigrp to ospf? was it multiarea, and how did you manage redistribution? how did you deal with orphan hosts when upgrading the 7ks? i would also suggest rewriting some of your bullet points in spar framework situation problem action result format so the nontechnical types can see your value. instead of implemented cisco identity services engine ise for unified access control and policy management. , try something like reduced the burden of policy management and access control overhead by implementing cisco identity services engine ise, decreasing configuration errors by 33.3333. youre favorite llm can help with this part. good luck! if i was on a team that was hiring, i would want to interview you. || agreed. id put your key projects where the graduate assistant is. no one cares about that lol || honestly, it reads weak, like you do not have a lot of experience, or very limited experience. what sort of job are you going for? || lose the trivial stuff, software upgrades on a cv?! || here, i trimmed some fat out this for you professional summary experienced network engineer with a masters degree and 4 years of expertise in designing, deploying, operating network systems. skilled at collaboration and project delivery. environmental familiarity windows, mac os, cisco, juniper, aruba. microsoft office suite, ticketing systems, network monitoring systems, idsipsnlbfirewallsroutersswitching. experience led interactive sessions to explain networking related topics using effective communication skills graded and provided feedback for students assignments oversaw network services across 38 locations, providing support for 27,000 students and 2,500 staff members. deployed cisco catalyst switch stacks operated network management platforms configured and managed vpns ensured compliance with cipa organizational level l3 design defined and deployed mist policy to deploy juniper aps at scale implemented cisco identity services engine ise without killing anyone or going insane deployed ansible playbooks for improved efficiency and consistency. migrated igp from eigrp to ospf provided network support for 30 retail sites, troubleshooting and resolving connectivity issues. key projects migrated a school district from 172.16.0.016 to 10.0.0.08 to support scaling of network structure and operations designed and implemented l47 security in a pa firewall environment configured and deployed ztp deployment using ansible to automate switch deployment replaced cisco routers with aruba edgeconnect to implement organization wide sdwan deployed 2300 juniper access points using mist configuration education masters in information technology management bachelors in agriculture science certifications ccna awsccp feb 2025 availability can start today references available upon request || no roasties brother, good work and xp || cool"
ICMP latency monitoring,"hi all! please help me solve this problem. im at a loss here. host a 10.40.2.10623 is an lxd container running on a baremetal server with ubuntu. it is directly connected to an arista dcs7050qx32sr eos 4.28.10.1m within the vrf private. the arista switch is directly connected to a cisco catalyst wsc385048t stack consisting of two switches running ios xe 16.6.6. mpls ldp connectivity between cisco and arista is established using a typical configuration ospf for backbone routing, followed by ldp and mpbgp. host b 10.40.4.2024 is a baremetal server running ubuntu, directly connected to the cisco catalyst in the same vrf private. heres the scheme the issue is that packets between host a and host b are being dropped somewhere within the mpls network. pings between the hosts fail. however, pings to gateways and interfaces on the same device are successful. mpls ldp is established between cisco and arista, and mpls pings works in both directions. route labels are correct. the following commands were used for diagnostics show mpls ldp neighbor show mpls ldp detail show mpls ldp bindings show mpls forwardingtable all commands return correct and expected values. outputs can be provided upon request. the correct routes for the aforementioned networks are present in the vrf private on both devices. icmp requests from host a are visible in a tcpdump on host b and in the cisco monitor session and replies are being sent back. 123443.875069 ip 10.40.4.20 10.40.2.106 icmp echo request, id 64, seq 12, length 64 123443.875118 ip 10.40.2.106 10.40.4.20 icmp echo reply, id 64, seq 12, length 64 123444.904640 ip 10.40.4.20 10.40.2.106 icmp echo request, id 64, seq 13, length 64 123444.904676 ip 10.40.2.106 10.40.4.20 icmp echo reply, id 64, seq 13, length 64 however, these replies do not appear on host a and in the tcpdump on the arista. when pinging in the reverse direction from b to a, tcpdump on both the arista and host a shows no traffic. the mtu is set to 1500 across all devices. increasing the mtu on the cisco requires a reboot, which could lead to potential disruptions. notably, a similar ciscotocisco setup works without any issues. cisco configuration interface tengigabitethernet213 description core to arista no switchport ip address 10.200.40.32 255.255.255.254 ipv6 address hidden ipv6 enable ipv6 ospf encryption null mpls ip mpls mtu 1580 ospfv3 authentication ipsec spi 256 sha1 7 hidden ospfv3 1 ipv6 area 0 ospfv3 1 ipv6 network pointtopoint ospfv3 1 ipv4 area 0 ospfv3 1 ipv4 network pointtopoint bfd template habrcore end arista configuration interface ethernet281 description core to cisco mtu 1500 no switchport ip address 10.200.40.3331 bfd interval 200 minrx 200 multiplier 3 ipv6 enable ipv6 address hidden mpls ldp interface no ospfv3 passiveinterface ospfv3 network pointtopoint ospfv3 authentication ipsec spi 256 sha1 7 hidden ospfv3 ipv4 area 0.0.0.0 ospfv3 ipv6 area 0.0.0.0 on the cisco side, the mpls mtu 1580 configuration is present. its impact on the setup is not entirely clear, nor is it clear whether a similar configuration can be applied on the arista side. questions why is traffic between host a and host b not passing through mpls, despite the configurations appearing correct? how does the mpls mtu 1580 setting on cisco influence mpls behavior, and is there an equivalent configuration for arista? are there additional diagnostic steps or configuration checks that could help identify the issue? any insights or suggestions would be greatly appreciated! || had a similar issue between iosxe and arista. it was mtu. it had to match exactly in both boxes."
ICMP latency monitoring,"hi everyone, this scenario has me stumped. our network traffic bound for cdn thru our isp is experiencing high packet loss and latency. our isp is blaming cdn and saying theres nothing wrong with their network. when i run a traceroute to any destination to cdn, i go thru an isp lag 30 and theres an extra hop marked as hop 5. if i traceroute to the other 30 ip in the lag, i do not experience latency or see the extra hop hop 5. could anyone explain to me what this extra hop is and what could be going wrong to cause this latency? the issue comes and goes and mostly during business hours is when we experience the latency and packet loss oversubscription on circuit?. this network path is only used for cdn traffic, all other internet traffic takes different pathroutesrouters and is not experiencing latency or packet loss. isp actually told us they dont own and 5.5.5.50. that this is owned by cdn however, whois lookup clearly has the isp listed as the owners. also, how are they able to provide configuration from the router if they dont own it? very strange... we are dealing with tier 1 support and unfortunately, i am not able to own this case and get it escalated. i just provide the logs, my observations and hope for the best. thank you. from isp configuration other 00h00m00s lag100 lag100 dynamic 03h39m13s lag100 lag100 default path taken for traffic bound to cdn what is this extra hop on 5 ? traceroute host traceroute to 5.5.5.50 5.5.5.50, 30 hops max, 60 byte packets 1 0.163 ms 0.152 ms 0.304 ms internal network 2 0.676 ms 0.719 ms 0.718 ms internal network 3 ms 0.869 ms 0.809 ms public ip onprem 4 ms 2.815 ms 2.864 ms isp edge router 5 ?????????????? 6 143.089 ms 147.272 ms 147.269 ms isp lag10 router observed extremely high pings packet loss of 1520. ping host ping 5.5.5.50 5.5.5.50 5684 bytes of data. 64 bytes from 5.5.5.50 icmpseq1 ttl58 time260.6 ms 64 bytes from 5.5.5.50 icmpseq2 ttl58 time262.8 ms 64 bytes from 5.5.5.50 icmpseq3 ttl58 time349.5 ms 64 bytes from 5.5.5.50 icmpseq4 ttl58 time285.7 ms secondary path not taken part of the isp 30 lag but not showing extra hop or latency when tracerouteping observed no extra hop latency traceroute host traceroute to 5.5.5.49 5.5.5.49, 30 hops max, 60 byte packets 1 0.145 ms 0.173 ms 0.291 ms internal network 2 0.731 ms 0.731 ms 0.671 ms internal network 3 0.869 ms 0.856 ms 0.801 ms public ip onprem 4 2.354 ms 2.397 ms 2.401 ms isp edge router 5 2.362 ms 2.307 ms 2.449 ms isp lag10 router observed no latency or packet loss. ping host ping 5.5.5.49 5.5.5.49 5684 bytes of data. 64 bytes from 5.5.5.49 icmpseq1 ttl60 time2.46 ms 64 bytes from 5.5.5.49 icmpseq2 ttl60 time2.82 ms 64 bytes from 5.5.5.49 icmpseq3 ttl60 time2.41 ms from isp perspective ping logs they provided 4.4.4.4isp edge router ping 5.5.5.50 source 4.4.4.4 rapid count 100000 ping 5.5.5..50 56 data bytes !!!!snip!!!!c ping statistics 26409 packets transmitted, 26403 packets received, 0 packet loss roundtrip minavgmaxstddev 2.5565.44732.5623.074 ms not sure why they pinged 4.4.4.5 from source 5.5.5.49 part of the lag but we arent seeing these in use. 5.5.5.49 isp lag10 router ping 4.4.4.5 source 5.5.5.49 rapid count 10000 ping 56 data bytes !!!snip!!!!! ping statistics 10000 packets transmitted, 10000 packets received, 0.00 packet loss roundtrip min 1.44ms, avg 1.47ms, max 3.36ms, stddev 0.071ms || your understandable obfuscation of the real ips makes this a bit hard to follow, but it seems to me like the 30 is a transit link between someone your isp? and the cdn. so your .49 is on your isps pe, and the .50 is on the cdns peer. in this case the is probably the .49 router which might not send icmp replies on the ingress interface. if the above is true it likely is, then your isp is probably right you can hit their router without any issues, but the cdn side is a mess. tough to tell why...maybe its riding a wave to the other side of the planet, maybe their interface is oversubscribed, who knows. || its hard to diagnose a routing issue with fake data. we had issues with cdns that use anycast with tcp which imo is inherently a bad idea, where client traffic on our core can take different paths with ecmp. when we then have redundant peerings with that cdn it might happen that they end up in different datacenters with that cdn. we had to prepend one of the paths to get rid of that issue. || google my traceroute and pingplotter. you can test from source to destination then test from destination to source. the outputs from both directions will help you find where a problem might be. be sure to read up on how the tools work because you can get false positives if you dont know what youre doing with them. || set up a vps in digitalocean or something and put a static route to it over the problematic link and then do some testing to that. || if your cdn has a direct peering relationship with your isp you may be able to get them to pursue this for you. otherwise, do all the usual things to get out from under a bad tier 1 agent keep asking for a manager, requeue the ticket, find other numbers to call, or complain to an ombudsman. || use mtr || you need a traceroute from your cdns viewpoint as well. youre assuming that all the routing is symmetric and that the responses are coming back in the same route theyre going out. its entirely possible the routing is synetic and the error lies in a different circuit than any youre seeing in your outbound path. || this is a great sub. || traceroute monitor or mtr is what you want to use. its a combination ping, and traceroute, showing you the hops but also the loss percentage at each hop. run it a few times so you understand the output. its invaluable for finding trouble links and hops. example hop 1 100 sent received, hop 2 100 sent 20 received, hop 3 100 sent 100 received, hop 2 is not problematic. example hop 1 100 sent 100 received, hop 2 100 sent 78 received, hop 3 100 sent 75 received, hop 2 router and path links in and out should be checked thoroughly. number 5 isnt concerning in the slightest, as its just a hop that either icmp is blocked, filtered, or policed on traffic destined for the router. so, if you see a ton of loss to a certain hop but none to the hops after, its not an actual issue, particularly when reading traceroute monitors. latency and delays are typically due to oversubscription, as its is too much data for the pipe. if you can hit the end destination, it tells you routing is in place, and oversubscription or hardware issues may be the cause. could be firewalls along the way, or your own. if the isp shows you clean traceroute monitors, but you see loss to the same destination, it can be a subnet specific issue that may need investigating, or its an issue on your side of the demarc. edit finally, if the issue is only with a single website, and everything else on the internet is fine, reachable and latency issue free, open a ticket with the website, and provide your information, as its not going to be an issue with your isp but with the web server hosts network. || are you ebgp peering with your isp? do you have another isp that you can test with? in situations like this, ive had luck with getting on a call with the vendor in question, not sure if you can here, but having them troubleshoot the connectivity back to your connection. i had an issue with five9 a few years ago that return traffic was going back through he that was experiencing high latency through their network. between five9, my team and he we were able to resolve the issue. || if youre willing, send me the real traceroutes along with source and destination ips and ill take a look. || pcap north as far as you can. || is this over a vpn? check pcaps for fragments || check your natpat pool. || this is probably the wrong crowd to get sympathy for blaming a network provider for something that seems likely not their fault. did you get anywhere with this amazing cdn to troubleshoot or validate their part of this? did you try the same origin with another cdn?"
ICMP latency monitoring,"hi, we have 3 existing fortiaps right now. all of them are 231f. we are planning to buy another fortiap which is 431g one of our fortiaps has 7090 devices connected to it most of the time. the area where this fortiap is installed is where most of our employees are stationed. would it be okay if the new fortiap that we are going to buy is installed near the existing fortiap ive just mentioned? the ceiling where we are going to install the new ap is the only place where there is a readily placed utp cable. it seems that the electrical contractor made a mistake during renovation by placing the poe mounting area near each other. all of the fortiaps are running in tunnel mode. broadcasting 7 ssids on different networks. here is the list of our ssids 1. it department 2. vips 3. employees 4. marketing phones 5. partners contractors 6. guests 7. ip phones so, i was thinking that the new fortiap 431g will only be broadcasting 3 ssids it department, employees, and vips to reduce the load and remove those 3 ssids on the fortiap231f near it. is this setup okay? i am checking the wifi controller on fortigate, and there were no interfering ssids for all the fortiaps. there are just times when the fortiap with 7090 devices has some clients with poor radio channel utilization. also when checking our wireless uptime network monitoring for pinging and our isps public ip there are instances of high latency and rtos for 1 second when multiple devices are connected on fortiap. fortiap 1 the ap which im talking about channels radio1 6 radio2 40 fortiap 2 channels radio1 6 radio2 48 fortiap 3 channels radio1 1 radio2 161 || you should consider consolidating your ssids.. having too many ssids will clog up airtime with beacons. next you should deploy the new ap but dont mess around with ssid distribution. just use client offloading and let the controller decide which ap will service clients. and please consider using rrm. last thing you want is having two aps on the same channel next each other"
