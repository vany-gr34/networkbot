mot_cl√©,post_et_reponses
proactive network maintenance,"i searched in this sub for the past couple of hours for past posts about network performance and resources to become better at creating performant networks or troubleshooting performance related issues. personally, i feel like i have a good handle on network availability and security in terms of design, implementation, and maintenance. however, i cannot say the same about performance. so does any one have good recommendations in the realm of network performance? i am looking to level up in that area but i dont know where to start. || make sure your mtu sizes are good. anything beyond that is probably not a network issue. || look for the section network performance related resources at the bottom of learn about bdp and nagle algorithm good luck."
proactive network maintenance,"hi everyone, i work for a software company and our company has been pushing us to go all in on ai this year. weve had several meetings and there have been some super neat projects that have been shown by various development teams or things of that nature but i feel like i cant find anything useful that we can point to other than stuff weve been using for years like our ncm or firewall related logs alerting us proactively or what not. today we were told that if we arent using ai that we are being left behind and i feel super discouraged because we get asked by our management that we need to show that we are using ai in our daily tasks but yet other than what i mentioned above i cant point to anything. ive been in it for 20 years and been a network engineer for 11 of those and its not that im resistant to change but i dont know where to really start the network is the heart of everything that everyone uses. how are you using ai in your daily work just looking for examples or maybe think outside of the box i feel like im not seeing the big picture or that one thing of here is something cool you can do and implement thanks for reading. || hey chat gpt, here is my private network configuration data, please dont share it with anyone else || i use ai to write emails back to management. technically, it has no real value beyond barfing out error prone snippets. || i dont."
proactive network maintenance,"i have been searching to try and find an answer but i keep coming up blank. so any thoughts will be appreciated. i have asked both dell software support and dell networking but neither of them has an answer. the networking group does not have any best practice for how to setup the switch for use with hyperv to best take advantage of vlt networking. i have dell pro support plus on all my equipment. the dell network team says it is a hyperv question on how they want it setup. the dell software support says this is a dell networking question and they both think they are independent. i am running hyperv and using powershell to create a virtual set using hypervport for load balancing. i have a 3 node cluster running 75 virtual servers on the cluster set does not support lacp my hyperv host are connected to two dell switches that are running dell os10 setup with vlt all servers are the same the following is an example of one server 1 connected to switch 1 with 2 ports connected to switch 2 with 2 ports all 4 ports on server 1 are in a single set virtual switch i have added host os, cluster network and backup network as virtual nics off the main set so the os sees the host os, cluster network and backup network iscsi is on dedicated nics that are not part of set and are using mpio with a nic connected to each switch. to best handle efficient routing of traffic between virtual servers and fast notification of down link events what is the preferred method of setup from the switch side of the equation. i run 10 rds session host servers using fslogix for profile storage so network latency matters to give my users a good experience. option 1 do nothing on the ports at the switch level. this requires that all traffic be routed and can put a lot of traffic on the backplane of the vlti interface between the switches because it does not optimize traffic. option 2 setup a port channel with lacp set to static. this will communicate to the vlt switches the group of ports are together for routing and notification and not creating loops. my understanding is this also helps with routing of traffic and notification during loss of 1 switch i.e. maintenance windows for switch. option 3 doing an lbfo nic team that does support lacp then apply the set switch to the team was an option but is not the recommended method from microsoft. also this only gives you one vmmq because the set only sees one nic so it cannot take advantaged of all 4 nics for offloading traffic. option 4 some other method best load balancing for vlt switches vnic is the guest nic and pnic is the physical nic currently all my virtual servers have 1 vnic best practice from microsoft is to use hypervport for all 10gb or faster nics. option 1 hypervport this basically sets a vm to a card the distribution is done by the os and just load them up in a round robin fashion. this vnic1 connects to pnic1 vnic2 connects to pnic2 vnic3 connects to pnic3 vnic4 connects to pnic4 vnic5 connects to pnic1 etc. option 2 dynamic the traffic from vnics gets send out on all 4 pnics in round robin but only one pnic can receive traffic. i do not know if it the process is smart enough to know that it is talking with a vm guest that also on the same switch then it would only send out on the pnics that are connected with that same switch. this could generate a lot of traffic on the vlti backplane if half of the packets are coming from the other switch. i must be over thinking this which is not unusual for me but the lack of documentation is pretty astounding considering this technology has been around for 10 years. || vlt seems to be the dell version of vpc. dont do anything on the switches, set is switch independent and works fine as is. || yep. no fancy config on switches. how load is balanced and bandwidth utilized depends on if the port is set to dynamic or hyper v. you should probably use hyperv port for your vms. refer to the guide from lenovo below which details set requirements and setup for hyperconverged. it is the same for converged, but includes rdma setup. microsoft also has documentation on this on learn.microsoft.com. || yes you dont really do any lag or anything. just present the ports and set itself will handle it."
proactive network maintenance,"i have problem in title. i want to migrate aps to meraki cloud from existing network and i found this presentation after upgrading system to 17.9.6 option to migrate ap has appeared but there are no entries there. i checked the inventory and have no misconfigured aps in either category, additionally i reapplied country setting just to make sure. what could be wrong here? after googling, i dont see any troubleshooting options, everyone assumes that if country is ok, it should work. guide is new last month, though i see that migrating the wlc itself is now requiring higher version 17.12. anyone can confirm if that it is the reason? i would prefer to avoid upgrade of big version in current time as i wont have comfort of any longer maintenance periods till summer. || "
proactive network maintenance,"i work at a global company with multiple sites connected by mpls circuits being replaced by ipvpn and site to site vpns over the isps for when the ipvpns between sites go down for maintenance, issues, etc. i started my career as a network engineer for a brief time, but quickly shifted my focus to information security, but i still help the network team out from time to time when they need it. a couple of years ago, with the help of a 3rd party, i helped the network team redo the internal routing at our company from bgp that a previous employee had done, moving to ospf. ospf worked well and routing failed over quickly. we never really had any issues. fast forward to today, the previous employee is back at the company and wants to switch everything back to bgp internally. we have about 30 sites worldwide, but the internal routing between sites isnt that complicated. i always thought that bgp was better as the name suggests for use on a border with isps or where you would otherwise have large routing tables that bgp could handle more efficiently. not as an internal routing protocol. bgp just seems very clunky and slow for failovers between mpls circuits and the isp vpn. however, i have been out of networking for too long and i could very well be wrong, so looking to see what other people thought. let me know and please be kind, as i have been out of networking for some time now. || bgp can fail over very quickly subsecond if you use it in combination with bfd. || i just switched our corp from ospf to ebgp. bgp allows for easy filtering on any router and route manipulation is unmatched. also, cloud providers only seem to support bgp, so if cloud expansion is in the cards, then it makes sense to deploy it internally. i ran ospf for 25 years. bgp is better in almost every way. || the rule of carrier networks or big networks have always been use igp ospf or isis for calculating topology and use bgp on top of it for propagating routes. for flexibility reasons. bgp is way more flexible to filter routes or play with them. plus bgp enables other capacities coupled with mpls. that being said if you are in a smallmiddle corporate network and you have no problem with your current implementation just keep it. there is no value in making such a migration or at least challenge the guy who sells it."
proactive network maintenance,"vlan 10 admin office includes staff wifi workstations, laptops, the printer, the time clock machine, and staff wifi for office staff. a policy will be implemented to ensure personal devices connect only to the guest wifi vlan 30 to maintain network security. vlan 20 pos payment systems amazon workspaces, pos system and credit card readers. vlan 30 guest wifi isolated from all internal systems, allowing only internet access. this includes three separate guest wifi networks covering the clubhouse, the course, and the driving range. vlan 40 iot media tvs, ensuring separation from businesscritical traffic. vlan 50 servers backups hosts the inhouse server and facilitates controlled access for vlan 10 and vlan 20. vlan 60 voip phone system dedicated vlan for the 14 voip phones to ensure call quality and reliability without interference from other network traffic. implementation strategy deploy a layer 3 switch to manage vlan routing while maintaining security. configure firewall rules to allow controlled communication between vlans where necessary. implement quality of service qos to prioritize critical pos, voip, and admin traffic. secure guest wifi by isolating it from internal vlans. futureproof the network for upcoming expansion and additional it infrastructure. implement ubiquiti networking equipment utilize ubiquiti access points, switches, and controllers for seamless wifi and network management. deploy atera it management software atera provides remote monitoring, network diagnostics, and automated maintenance, reducing downtime and increasing efficiency. || will likely get an a || personally, id be looking at terminating the l3 on the firewall not the l3 switch. also for fun, i prefer 666 for guest d || id keep switches at layer 2 and put layer 3 on the firewall. a little more centralized for all the rules."
proactive network maintenance,"straight up. i understand the business efficiency gains from having one person able to administer thousands of devices, but there has to be a point of detrimental or limited returns, having that much knowledge in one persons head. theres a reason i went into technical maintenance instead of software development though, i just do not like writing out code. its not fun. its not engaging. its boring, rigid and thoughtless. every job posting i see requires beyond the basic scripting requirements, wanting python, cc or some kind of webbased software development framework like node, javascript or worse. everything has to be automated, you have to know version control, git, cicd pipelines to a virtualized lab in the cloud and dont forget to be a cloud engineer too. where does it end? at what point are the fundamental networks of the world going to run so poorly because nobody understands the actual networking aspect of the systems, theyre just good software engineers? is it really in the best interest of the business to have indeterminable network crashes because the knowledge of being a network engineer is gone? or maybe this is just me falling into the late 30s i dont want to learn anything anymore slump. i dont think it is, im just not interested in being a code monkey. || knowing the basics of a scripting language like python or powershell doesnt mean youre a software engineer. i run the infrastructure group where i work and i wouldnt hire someone without basic scripting ability, this is 2025 not 2005. you dont have to build an automation system from scratch but you also shouldnt waste hours on a task that a handful of lines of python could do instead. i see this on the windows side of the house as well. so many people that were button clickers from the 2000s that refused to learn anything command line now wonder why theyre not getting promotions or turned down for interviews. your switches run linux bruv, learn the tools. || you think the i dont want to learn new things slump is bad in your 30s? add a couple decades onto that. glad i was able to retire last year at 60. now i can learn fun stuff without cluttering my brain with junk. i still have nortel meridian commands floating around up there lol. or atm commands for marconi switches. || voice engineers didnt want to learn networking either but voip happened. adapt or retire."
proactive network maintenance,"hi folks, our company is relocating our dc to a new location. the backbone network includes a cisco aci fabric and other nonaci networking stuff. we need a phased migration approach so as to keep the downtime at a minimum. we have planned to extend layer 2 across locations oldnew via an evpn vxlan fabric using two pairs of spare switches in each location, dark fiber underlay in order to migrate workloads on the nonaci environment. workload first, then a few networking devices then the l3 gateways. however, the cisco aci fabric seems to be a roadblock as we dont plan to run multipodsite or have no interests in reconfiguring the whole thing to avoid confusion and headache during the migration phase. how should i approach this so that we dont need break the fabric? the fabric is the gateway of core workloads, using pbr to redirect traffic to firewalls. its a very different architecture from our edge workloads on nonaci networking stuff, with gateway placed on the edge firewalls. maintenance windows are very stringent at 4 hours maximum each of planned downtime. || your company is delusional to expect you to do a slow move to a new dc without buying any new hardware to facilitate the move. im in the middle of a slow move using aci as well, but i built out a new fabric and did multi site. with a generous dose of vendor design support. this isnt the sort of thing you can just do on the cheap. at a minimum id be asking for a pair of spines and pair of leafs to get the ball rolling. or a 3 day outage window to just do a full lift and shift lol. how big is your existing aci fabric? how many dark fiber pairs between sites? do you already have nexus dashboard deployed? thoughts and prayers my friend. || extend it to nexus switching via epg layer 2 outs, dont install aci in the new site. convert your aci to nexus || you will also use aci on new site or not? and is the aci fabric network centric or app centric? assuming its aci with network centric, and we can do per vlan migration, you can create in the new aci fabric with l2only trunked to the existing fabric, while the gateway in the existing fabric. migrate the entire workload on that bd, after all endpoint is moved to the new fabric, then move the gateway from old fabric to the new one during maintenance window. this is assuming you already have the license for vmotion between vcenter in different dc, or any other workload migration strategy."
proactive network maintenance,"the industrial company i work at has an outdoor device that is wired into our network over cat6. for maintenance reasons, mechanics unplug and plug in the rj45 connector several times a week. of course, it ends up breaking a lot. i am trying to find a rugged connector made to be unplugged and replugged a lot. i see phoenix contact has something that looks like it may work part vsbupnip2093bli2,0 i am wondering if any of you have had good luck with this product, or something else? edit in case anyone else has this problem, i resolved this, found a company called amphenol pcd that makes custom rugged network equipment, for plant, military, marine, aerospace, etc || ethercon would be my normal recommendation, but thats not going to plug directly into your equipment without running the corresponding panel mount somewhere. ive had good success with these rugged connectors in cases where i cant use ethercon. || like ethercon connectors? used in live events for durability and to overcome rhe inherit brittle plastic tab disconnecting and preventing positive lock || "
proactive network maintenance,"the conventional wisdom is that if your subnet is too large, youre doing it wrong. the reasons ive learned boil down to alongside vlans, segmenting your network is safer, and changesmistakes target only the specific affected network segments excessive subnets can cause flooding from multicast and broadcast packets but dont these reasons have nothing to do with the subnet, and everything to do with the number of devices in your subnet? what if i want a large subnet just to make the ip numbers nice? thats exactly what im considering using a 15 subnet for the sake of ease of organization. this is a secondary, specialty, physically separate lan for our san, which hosts 100 or so devices. currently its a 21 and more numbers will simply organize better, which will improve maintenance. for isolation, id rather try to implement pvlan, since 90 of those devices shouldnt be talking to each other anyway, and the other 10 are promiscuous servers. || why would you care what the numbers are? we did that when i first started and i guess it made sense to whomever did it but once i started isp stuff i completely gave up one whatever a nice number is supposed to be. they arent vanity plates. || youre gonna love ipv6 || the other problem with using excessively large subnets is address exhaustion. but we have all of the 10.0.0.08 network plus 172.1612 and 192.16816. yeah, and poor planning can still lead to running out of addresses. i had a customer run out of 108 addresses because they assigned a 16 to every site regardless of size, so after 250 sites there were no 16s left. even if this is an isolated vlan it can cause problems. sounds like these are for hosts that have a backend san network, which means that 15 cant be used anywhere else on the network because then those hosts wont be able to reach those addresses."
